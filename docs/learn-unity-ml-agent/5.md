

# 五、玩游戏

我们已经介绍了一些非常复杂的例子，并构建了一些相当智能的代理。我们学到的用于 RL，更具体地说，PPO 的技术是最前沿的，但正如我们所知，它们仍然有其局限性。ML 研究人员继续在多个领域挑战极限，如网络架构和训练设置。在上一章中，我们研究了在多种环境中训练多名代理人的一种方式。在这一章中，我们将探索在一个环境中，我们可以利用多代理人和/或大脑的各种新颖的训练策略，从对抗和合作的自我游戏到模仿和课程学习。这将涵盖大多数剩余的 Unity 示例，以下是我们将涵盖的主要主题的总结:

*   多代理环境
*   对抗性自我游戏
*   决策和按需决策
*   模仿学习
*   课程学习
*   练习

我们将从上一章停止的地方继续，或多或少，这意味着你现在应该很容易设置和运行一个训练器。如果你还没有使用外部大脑进行训练，请务必查看第 4 章[、*使用深度学习*。](part0072.html#24L8G0-c0290fc391a247b5ae9c3b6066c9fb32)



# 多代理环境

这可能是一个有趣的实验，但事实证明，让代理人相互竞争真的可以加强训练，嗯，这很酷。在使用多个代理时，我们可以设置一些配置。我们要看的例子使用了多个竞争代理共享的一个大脑。打开 Unity，按照本练习设置场景:

1.  加载位于`Assets/ML-Agents/Examples/BananaCollectors/`文件夹中的`BananaCollectorBananaRL`场景文件。
2.  把大脑留给玩家；改了就改回来。
3.  在 Unity 中运行场景。使用 **WASD** 键在场景中移动代理立方体并收集香蕉。请注意，有多个多维数据集做出了相同的响应。这是因为每个代理人都使用相同的大脑。
4.  在**层次**窗口中展开 **RLArea** 对象，如下图所示:

![](img/00084.jpeg)

检查 RLArea 和代理

5.  注意 **RLArea** 下的五个**代理**对象。这些是将针对单一大脑进行训练的特工。运行完第一个示例后，您回来复制更多的代理来测试它对训练的影响。
6.  将**大脑**切换到**外部**。确保你的项目设置为使用外部大脑(如果你需要回顾这一点，请返回[第 3 章](part0051.html#1GKCM0-c0290fc391a247b5ae9c3b6066c9fb32)，*用 Python 进行深度强化学习*)。如果你已经运行了这个项目的外部大脑，你不需要任何额外的设置。

在这个例子中也有几个环境允许我们用 A3C 训练，但是现在，我们只使用单一环境。请随意返回并在启用多个环境的情况下尝试这个示例。

7.  从菜单中，选择文件|构建设置....取消选中任何其他活动场景，并确保`BananaRL`场景是唯一活动的场景。您可能需要使用“添加开放场景”按钮。
8.  将环境构建到`python`文件夹中。
9.  打开 Python 或 Anaconda 提示符。激活`ml-agents`并导航至`'ml-agents'`文件夹。
10.  使用以下代码运行训练器:

```
      python python/learn.py python/python.exe --run-id=banana1 --train
```

11.  观看示例运行。本例中的 Unity 环境窗口足够大，因此您可以看到正在进行的大多数活动。游戏的目标是让代理收集黄色香蕉，同时避免蓝色香蕉。有趣的是，特工可以发射激光来冻结对方特工。该示例显示在下面的屏幕截图中:

![](img/00085.jpeg)

运行香蕉收集器多代理示例

你会注意到，在这个例子中，回报的均值和标准差迅速累积。这是一个人的奖励值发生一些变化的结果，但是这个特殊的例子非常适合多代理人训练。根据您正在构建的游戏或模拟，使用单个大脑的 ML 智能体可能是一种很好的训练方式。

请随意返回并启用多个环境，以便使用多个 A3C 代理在多个环境中训练多个代理。在下一节中，我们将看另一个例子，这个例子的特点是使用多个代理和多个大脑的对抗和合作游戏的混合。



# 对抗性自我游戏

我们看到的最后一个例子最好定义为竞争性多代理训练场景，代理通过相互竞争收集香蕉或冻结其他代理来学习。在本节中，我们将了解另一种类似的训练形式，即利用一种称为对抗性自我游戏的反向奖励方案，让代理与代理进行较量。当竞争代理接受奖励时，反向奖励用于惩罚对手代理。通过下面的练习，让我们看看 Unity ML-Agents 足球(football)示例中的情况:

1.  打开 Unity 到位于`Assets/ML-Agents/Examples/Soccer/Scenes`文件夹中的`SoccerTwos`场景。
2.  运行场景并使用 **WASD** 键来扮演所有四个特工。当你玩够了的时候停止这个场景。
3.  在层次窗口中展开学院对象。
4.  选择 StrikerBrain 并将其切换到外部。
5.  选择目标大脑并将其切换到外部。
6.  从菜单中，选择文件|构建设置....单击添加开放场景按钮并禁用其他场景，这样只有`SoccerTwos`场景处于活动状态。
7.  将环境构建到`python`文件夹中。
8.  启动 Python 或 Anaconda 提示符并激活`ml-agents`。然后，导航到`ml-agents`文件夹。
9.  使用以下代码启动训练器:

```
      python python/learn.py python/python.exe --run-id=soccor1 --train
```

10.  观看训练课是非常有趣的，所以请留意 Unity 环境窗口和控制台，以便了解训练进度。注意大脑是如何使用反向奖励的，如下图所示:

![](img/00086.jpeg)

观看足球经纪人的训练进度

罢工者的大脑目前得到的是负奖励，而守门员的大脑得到的是正奖励。使用反向奖励可以让两个大脑训练到一个共同的目标，即使它们也在互相竞争。在下一个例子中，我们将把我们在 Unity 中训练过的大脑用作内部大脑。



# 使用内部大脑

在多种场景中训练代理可能很有趣，但归根结底，我们最终希望能够在游戏或适当的模拟中使用这些代理。现在，我们已经设置了一个训练场景来娱乐我们，让我们启用它，以便我们可以与一些代理进行足球比赛。按照这个练习来设置场景，以便您可以使用内部大脑:

1.  从菜单中，选择编辑|项目设置|播放器。

2.  在其他设置下的脚本定义符号中输入`ENABLE_TENSORFLOW`，如下图所示:

![](img/00087.jpeg)

设置脚本定义启用张量流的符号

3.  设置此项将通过 **TensorFlowSharp** 启用 **TensorFlow** 模型的内部运行。
4.  找到**学院**对象并展开它以暴露**打击者大脑**和**目标大脑**对象。选择**敲击键**并按下*Ctrl*+*D*(MAC OS 上为*命令* + *D* )复制大脑。
5.  将原来的 **StrikerBrain** 和 **GoalieBrain** 设置为使用**内部** brain 类型。切换大脑类型时，确保 **TensorFlow** 属性下的**图形模型**设置为**足球**，如以下截图所示:

![](img/00088.jpeg)

检查图形模型是否设置为足球

6.  把你刚刚复制的新的 **StrikerBrain(1)** 留给**玩家**大脑类型。这将允许你玩游戏对代理。

7.  展开`SoccerFieldsTwos->Players`对象以显示四个玩家对象。选择 **Striker(1)** 对象，将其**大脑**设置为 **StrikerBrain(1)** 玩家大脑，如下图所示:

![](img/00089.jpeg)

在玩家立方体上设置大脑

8.  这将设置代理(玩家)使用我们复制的玩家大脑类型。
9.  按下**播放**按钮运行游戏。使用 **WASD** 键控制射手，看看你能得分多少。玩了一段时间后，你很快就会开始意识到代理们学得有多好。

这是一个很好的例子，它快速展示了给定足够的训练时间和设置，你可以多么容易地为大多数游戏场景构建代理。更重要的是，决策代码嵌入在一个轻量级的张量流图中，它领先于其他人工智能解决方案。我们仍然没有使用我们训练的新大脑，所以我们将在下一节中这样做。



# 内部使用训练有素的大脑

在下一个练习中，我们想在足球比赛中使用我们之前训练的大脑作为代理人的大脑。这将给我们一个很好的比较，默认的 Unity 训练的大脑与我们在第一次练习中训练的大脑相比如何。

如果你没有完成第一个练习，现在就回去做。我们现在开始有趣的东西，你肯定不想错过下面的练习，我们将在一个游戏中使用内部训练过的大脑:

1.  打开文件浏览器，打开`'ml-agents'/models/soccor1`文件夹。该文件夹的名称将与您在训练命令行参数中使用的`run-id`相匹配。
2.  将`.bytes`文件(本例中命名为`python_soccer.bytes`)拖至`Assets/ML-Agents/Examples/Soccer/TFModels`文件夹，如下图所示:

![](img/00090.jpeg)

将 TensorFlow 模型文件拖动到 TFModels 文件夹

3.  定位 **StrikerBrain** ，点击目标图标，选择`python_soccor1` TextAsset，设置**图形模型**，如下图所示:

![](img/00091.jpeg)

在 StrikerBrain 中设置图形模型

4.  该文件被称为 **TextAsset** ，但它实际上是一个保存了**张量流**图的二进制字节文件。
5.  将**目标大脑**更改为相同的图形模型。两个大脑都包含在同一个图表中。我们可以用**图形范围**参数来表示哪个大脑是哪个。再次，让球员前锋大脑保持原样。
6.  按 Play 运行游戏。用 **WASD** 键玩游戏。

你会注意到的第一件事是代理人玩得不太好。这可能是因为我们没有使用所有的训练选项。现在将是一个很好的时机，让我们回到过去，用 A3C 和其他我们已经学过的选项重新训练足球大脑。

现在我们能够使用内部大脑，训练选项升级到几乎无限数量的配置。在下一节中，我们将了解一种有趣的训练方法，它可以让我们的代理按需做出决策。



# 决策和按需决策

您可能已经注意到，在之前的练习中玩足球游戏时，游戏有时会进入慢动作。这是因为代理大脑消耗了太多的处理能力，从而降低了游戏的帧速率。这可能是一个问题，正如我们从几个代理运行中看到的那样。这样做的原因是，我们目前让代理/大脑每五帧或 1/12 秒做出一个决定。虽然这对于训练来说很棒，但是在真实的游戏中，我们可能希望我们的代理能够以和人类一样的速度做出反应。这可以消除性能问题，因为代理现在决策的频率大大降低了。我们可以使用一个名为**按需决策**和**决策频率**的特性对此进行调整。打开 Unity，打开我们使用的上一个足球示例，然后进行以下练习:

1.  从`Assets/ML-Agents/Examples/Soccer/Scenes`文件夹中加载 SoccerTwos 场景。
2.  在**层级**窗口中找到**球员/前锋**对象并选择它。

3.  找到**决策频率**属性，并将其从`5`更改为另一个值，向上或向下，如下图所示:

![](img/00092.jpeg)

将决策频率设置为不同的值

4.  值为 5 表示代理每五帧执行一次操作或做出一次决定。每秒 60 帧，这代表了 1/12 秒或`.083`秒的反应时间，这对一个人来说有点快。人类的反应时间可能因年龄、性别和许多其他因素而异，但出于我们的目的，我们希望使用一个从`.25`到`.5`秒的值。
5.  将**决策频率**属性设置为`30`，因为这将代表`.5`秒的反应时间。
6.  打开**守门员**和**守门员(1)** 代理，并将它们设置为使用值`30`。

7.  打开 **Striker(1)** 代理，将其**决策频率**属性设置为`1`。记住:这是我们的玩家代理，我们不想阻碍它的反应时间，因为我们想比较我们自己和代理的反应。
8.  按下播放按钮运行场景，用 **WASD** 键玩游戏。你注意到什么不同了吗？

是的，你可能不会马上注意到任何大的不同。随着游戏的进行，您可能会注意到游戏保持了相当的流动性，没有变慢，但更重要的是，代理似乎以相同的速度做出反应。虽然我们将反应时间降低了 6 倍，但试剂的反应速度似乎仍然相同。这主要是由于我们对形势的感知，但重要的是以低决策频率训练，以高决策频率比赛。在下一节中，我们将研究另一种技术，这种技术可以让我们更好地控制何时做出决策。



# 跳动的香蕉

如果您希望在模拟或游戏中更加准确，您可能希望为不同的输入信号或事件设置不同的反应时间。例如，您可能只希望代理在碰到一个对象或触发器后做出反应，因此不需要代理大脑对空输入做出反应。Unity 在 ML-Agents 中实现了一个称为按需决策的特性，它允许代理在做出决策之前等待。当然，Unity 对此有一个示例，所以让我们打开编辑器，按照这个练习:

1.  在`Assets/ML-Agents/Examples/Bouncer`文件夹中定位并打开**保镖**场景。
2.  在**层级**窗口中找到并选择**弹跳器**。将**脑类型**设置为**内部**并确保**图形模型**设置为**文本资产**字节文件。
3.  按下播放按钮运行场景，并观看代理为香蕉跳。

最后一个例子虽然很快，但却是按需决策的一个很好的例子。此示例中的代理在 Bouncer 代理脚本上设置了以下内容，如以下屏幕截图所示:

![](img/00093.jpeg)

Setting an On-Demand Decision property on the Agent

当使用按需决策时，我们需要在 C#代理代码中调用一个名为`RequestDecision`的请求。让我们仔细看看`BouncerAgent`脚本和`FixedUpdate`方法，看看这是什么样子，如下面的代码所示:

```
 private void FixedUpdate()
 {
   if ((Physics.Raycast(transform.position, new Vector3(0f,-1f,0f), 0.51f))
     && jumpCooldown <= 0f)
   {
     RequestDecision();
     jumpLeft -= 1;
     jumpCooldown = 0.1f;
     rb.velocity = default(Vector3);
   }
   jumpCooldown -= Time.fixedDeltaTime;
   if (gameObject.transform.position.y < -1)
   {
     AddReward(-1);
     Done();
     return;
   }
   if ((gameObject.transform.localPosition.x < -19)
     ||(gameObject.transform.localPosition.x >19)
     || (gameObject.transform.localPosition.z < -19)
     || (gameObject.transform.localPosition.z > 19))
   {
   AddReward(-1);
   Done();
   return;
   }

   if (jumpLeft == 0)
   {
     Done();
   }
   bodyObject.transform.rotation = Quaternion.Lerp(bodyObject.transform.rotation,
   Quaternion.LookRotation(lookDir),
   Time.fixedDeltaTime * 10f);
 }
```

`FixedUpdate`中的第一行通过使用指向下方的矢量`(0,-1,0)`到`.51`单位的距离来检查游戏对象是否接近地板，确保`jumpCooldown`小于`0`。当代理接近地面并完成等待`jumpCooldown`时间后，它调用`RequestDecision()`向大脑发出信号，表示该做决定了。虽然大脑被设置为使用**按需**决策，但大脑在`RequestDecision`被调用之前不会采取行动。

`FixedUpdate`, if you are less familiar with Unity, is the method that gets called for each physics time update and is tied to the physics update cycle. This is different than the `Update` method, which gets called for each render frame. The difference is subtle but you generally want to put collision detection code in `FixedUpdate`.

`FixedUpdate`中的其余代码检查代理是否已经脱离平台。如果是的话，这一集就会重新开始。否则，代理将按照我们之前在脚本中设置的观察方向(`lookDir`)旋转。如果你在理解剩下的代码时有困难，那么你需要温习一下你的 C#和 Unity 编程技巧。

按需决策是管理多个代理的性能和给你的代理更现实的行为的一个很好的方法。在构建代理时，你需要记住的一件事是他们实际上是如何与其他玩家或者代理交互的。能够调整决策频率和时机是强大的功能，我们将在第 6 章、*重新审视 Terrarium ——一个 ML 智能体生态系统*中花更多时间讨论。

在下一节中，我们将继续介绍更有趣的训练技巧，我们将通过让代理模仿我们的动作来训练他们。



# 模仿学习

模仿学习是一种很酷的训练技术，我们可以用它来训练代理人。这在具有重复动作的复杂训练场景中有巨大的好处。像乒乓球或网球这样的游戏非常适合这种类型的训练，因为游戏动作是重复的。由于代理是通过示例学习的，因此消除了随机搜索动作或探索的需要，并且训练性能显著提高。Unity 有一个网球的例子，这是展示这种训练的一个很好的候选。让我们跳到下一个练习，在这里我们设置了模仿学习:

1.  打开位于`Assets/ML-Agents/Examples/Tennis`文件夹中的**网球**场景。
2.  在**层级**中定位**代理**大脑对象。重命名对象`Student`。将**脑型**设置为**外置**。
3.  选择**玩家**大脑对象，重命名为`Teacher`。将**大脑类型**设置为**玩家**，并确保大脑设置为**广播**，如下截图所示:

![](img/00094.jpeg)

设置教学大脑进行广播

4.  将**教师**大脑设置为**广播**允许大脑与训练者交流。大脑会向训练者广播它的观察和行动空间，这使得学生大脑能够从老师的经验中学习。

5.  在**天线区域**下定位`AgentA`和`AgentB`物体。选择任一代理，将其**大脑**属性设置为**教师**，如以下截图所示:

![](img/00095.jpeg)

设置 AgentA 使用教师(玩家)的大脑

6.  **老师**是大脑，你，玩家，将用它来教**学生**。
7.  在层次窗口中再次找到并选择教师大脑。然后，在**检查器**窗口中，点击窗口底部的添加组件按钮。搜索 BC 教师助手组件(脚本)并将其添加到对象中。该组件将允许我们使用**记录键**和**复位键**开启/关闭模仿训练，如以下截图所示:

![](img/00096.jpeg)

设置 BC 教师助手键

8.  从菜单中，选择文件|构建设置....这将打开构建设置对话框。确保将网球场景设置为唯一打开的场景，并单击**构建**按钮。

场景构建到我们的 Unity 环境中后，我们现在需要用适当的训练配置来配置`trainer_config.yaml`文件。



# 设置克隆行为训练器

我们在进行模仿学习时使用的训练器叫做**行为克隆**。这种训练器与我们以前多次使用的 **PPO** 训练器相匹配，但它被扩展为从训练或玩家大脑中获取观察和动作空间输入。幸运的是，配置非常相似，只需要一些特殊的定制。按照本练习完成教练的配置并开始游戏:

1.  在 Visual Studio 代码或您喜欢的编辑器中打开`trainer_config.yaml`文件，并在文件末尾添加以下新部分:

```
      Student:
        trainer: imitation
        max_steps: 10000
        summary_freq: 1000
        brain_to_imitate: Teacher
              batch_size: 16
        batches_per_epoch: 5
        num_layers: 4
              hidden_units: 64
        use_recurrent: false
        sequence_length: 16
        buffer_size: 128
```

2.  新的部分为我们的训练大脑创建了配置，称为**学生**。回想一下，这是我们之前重命名为**学生**的大脑对象。我们从 **PPO** 设置训练师**模仿**，这是我们通常设置的。我们正在使用的模仿算法的实现叫做行为克隆。行为克隆是模仿学习最简单的形式，但是正如你将会看到的，工作很容易就结束了。完成编辑后保存文件。
3.  打开 Python 或 Anaconda 提示符，激活`ml-agents`并导航到`'ml-agents'`文件夹。
4.  通过输入以下教练命令开始训练课程:

```
      python python/learn.py python/python.exe --run-id=tennis1 --train 
 --slow
```

5.  注意`--slow`的用法。这使得训练能够以一种允许我们与实际环境互动的方式进行。
6.  随着环境的启动，您会注意到环境扩展到一个更大的窗口，并且您可以控制其中一个代理。使用 **WASD** 键来控制代理人，看看你能多好地回球。当你玩这个游戏时，你应该注意到你做的许多相同的动作，学习代理也会做。这是一种非常有趣的训练特工的方式，它本身也是一个非常好的浪费时间的游戏。
7.  您可以使用**重置奖励** ( **R 键**)或**重置经验** ( **C 键**)分别启用/禁用训练和重置经验。您还会在环境窗口中看到以下快捷键:

![](img/00097.jpeg)

使用教师大脑(玩家)来教导代理

8.  您可以尝试通过按下 **C** 来重置体验，或者通过按下 **R** 来禁用记录体验。给他们一个尝试，看看这些对代理有什么影响。

正如您刚刚看到的，模仿学习无疑是训练代理人的最有趣、最吸引人的方式。这种训练方法还为训练游戏代理或需要执行一些记忆或冗余任务集的代理提供了潜在的无限可能，无论是在游戏中还是构建更复杂的东西。正如我们所看到的，即使制作一个玩家教代理人的游戏也是可能的。

在下一节中，我们将训练游戏升级到更复杂的任务。这一次，我们将了解一种阶段或课程训练形式，它非常适合训练代理完成特别困难的多阶段任务。



# 课程学习

随着你越来越熟悉构建代理，你将开始处理更大的问题，这些问题是如此之大，以至于把任务分解成不同的难度可能更好，就像游戏一样。问题分解后，可以用课程学习来处理。课程学习是指代理分阶段或分层次学习一项任务，难度不断增加。这和我们人类学习任务的方式没有什么不同，例如，走路；我们首先学习滚动，然后爬行，站立，蹒跚，然后行走。我们凭直觉学会了如何走这条路，但我们的代理朋友需要一点帮助，至少现在是这样。

让我们回到 Unity，看一个主要为课程学习配置的例子，它只需要我们的一点帮助来完成设置。按照本练习设置课程学习方案:

1.  打开`Assets/ML-Agents/Examples/WallJump/Scenes`文件夹中的`WallJump`场景。如果你已经完成了之前的所有练习，你现在将已经处理了所有的 Unity 示例场景，你一定会成为一名 **RL** 大师。

2.  在**层次**中选择**学院**对象，注意**跳墙学院**组件下的**重置参数**，如下图所示:

![](img/00098.jpeg)

检查学院对象的重置参数

3.  我们并没有过多地使用这些参数，但是它们被用来定义各种不同的环境变量，这些变量可能会随着复位而改变。在**课程学习**中，我们使用这些参数作为定义难度的方法。在当前示例中，我们有一个定义最小墙高(`big_wall_min_height`)和最大墙高(`big_well_max_height`)的参数。我们的教练会在训练中使用这些参数来调整墙壁的高度。
4.  将小脑壳和**大脑壳**的脑壳类型都改为外部。你现在应该可以在睡觉的时候做这个了。
5.  营造外部训练的环境。还是那句话，你应该在睡梦中知道这一点。

6.  打开 Visual Studio 代码或您喜欢的文本编辑器，创建一个名为`curricula.json`的新文件。将文件保存到`python`文件夹。
7.  使用以下 JSON 编辑`curricula.json`文件:

```
      {
       "measure" : "reward",
       "thresholds" : [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],
       "min_lesson_length" : 2,
       "signal_smoothing" : true, 
       "parameters" :  {
          "big_wall_min_height" : [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 
          4.0, 4.5],
          "big_wall_max_height" : [4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 
          8.0, 8.5]
        }
      }
```

JSON stands for JavaScript Object Notation. While its origins are JavaScript, the notation itself is now the standard for most configuration settings. If you are not familiar with the format, learn it.

8.  在这个配置(JSON)文件中，我们定义了几个参数，定义如下:

    *   `measure`:决定衡量成功的过程。允许值为:
        *   **奖励**:基于返回的累计奖励
        *   **进度**:代理完成任务的进度
    *   `thresholds` ( `array float`):确定课程增加的分值。这里，所有课程都设置为`.5`。
    *   `min_lesson_length` ( `int`):设置在增加课程之前报告进度测量的次数。
    *   `signal_smoothing` ( `true` / `false`):如果`true`，它使用信号平滑算法在`.75x`融合新的和在`.25x`测量的旧的。
    *   `parameters`:这是我们将**学院**中的**重置参数**与我们希望代理训练的范围相匹配的地方。您会注意到，墙的最小和最大高度值增加了 0.5。在我们的例子中，这意味着墙会越来越高。
9.  打开一个 **Python** 或 **Anaconda** 提示符，现在你应该知道剩下的部分了。在一行中使用以下命令运行训练器:

```
      python python/learn.py python/python.exe --run-id=walljump1 --train 
      --slow                            
      --curriculum=python/curricula.json
```

10.  我们在这里使用的新参数是`--curriculum=`，它指向我们之前配置的训练文件。随着训练课程的进行，您将看到代理在越来越高的墙上工作，如下面的屏幕截图所示:

![](img/00099.jpeg)

在环境中运行的课程训练会话

代理运行时，也要注意提示窗口。你会看到代理人在越来越高的墙上训练，正如前面的摘录所示。这种形式的训练可以应用于所有形式的情况，正如我们将在[第 6 章](part0099.html#2UD7M0-c0290fc391a247b5ae9c3b6066c9fb32)、*再访玻璃缸——ML 智能体生态系统*中看到的，例如，我们将使用课程训练在各种训练环境中循环。



# 练习

这一章试图涵盖大量的材料，但没有提供许多实用的新例子。一定要尝试下面的一些练习来积累你作为 RL 大师的经验:

1.  返回香蕉收集器，使用 *Ctrl + D* 或(macOS 上的*命令* + *D* )添加几个代理。如何在不使训练场景滞后或变慢太多的情况下继续添加代理？
2.  将 Soccor 示例转换为对其中一个玩家使用模仿学习。如果你选择守门员类型，然后设置一个守门员球员作为老师，一个作为学生。
3.  将 GridWorld 示例转换为使用模仿学习。创建新的教师代理，并将现有代理转换为学生。我们已经详细介绍了这个例子，所以这将是一个很好的对比，可以看出训练效果的不同。
4.  通过添加额外的训练重置参数来扩展 WallJump 示例。你可以通过把代理放在离墙越来越远的地方来增加难度，这样也使得代理首先必须找到墙。
5.  将 GridWorld 示例转换为使用课程学习。您需要为想要设置的最小/最大值创建新的重置参数。你可以增加格子的大小和障碍的数量，但是减少奖励来制造更困难的训练环境。看看你能否胜过现有的模型。

即使做一两次练习也比什么都不做好。



# 摘要

这是令人兴奋的一章，我们已经能够玩几种不同的训练场景。我们开始着眼于将我们的训练扩展到仍然使用单个大脑的 ML 智能体环境。接下来，我们研究了一种称为对抗自我游戏的多代理训练，它允许我们使用反向奖励系统训练成对的代理。然后，我们讨论了如何配置代理，使其以特定的频率甚至按需做出决策。之后，我们看了另一种叫做模仿学习的新颖的训练方法。这个训练场景让我们可以打网球，同时还可以教一个代理打网球。最后，我们用另一种称为课程学习的训练技术完成了这一章，这种技术允许我们随着时间的推移逐渐增加代理训练的复杂性。

在本章中，我们练习了 Unity 的最后一个示例，应该对代理训练的各种技术非常熟悉。在下一章中，我们将建立我们自己的 ML 智能体世界，这意味着不断进化智能体，并赋予我们自己应用这些技术的技能。