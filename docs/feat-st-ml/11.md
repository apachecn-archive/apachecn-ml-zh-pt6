<title>B18024_08_ePub</title>

# 第八章:用例——客户流失预测

在最后一章中，我们讨论了市场上可用的**盛宴**特色商店的替代品。我们看了一些来自云提供商的功能商店产品，它们是**机器学习** ( **ML** )平台产品的一部分，即 SageMaker、Vertex AI 和 Databricks。我们还考察了其他几家供应商，他们提供可与您的云提供商一起使用的托管功能存储，即 Tecton 和 Hopsworks，其中的 Hopsworks 也是开源的。为了对托管特性存储有所了解，我们在 SageMaker 特性存储上进行了一次测试，并简要讨论了 ML 最佳实践。

在本章中，我们将讨论一个使用电信数据集的端到端客户流失用例。我们将介绍数据清理、特性工程、特性摄取、模型训练、部署和监控。在本练习中，我们将使用一个托管功能商店——Amazon sage maker。选择 SageMaker 而不是我们在上一章讨论的其他选择的原因仅仅是因为该软件的试用版易于访问。

本章的目的是使用托管功能商店，一步一步地浏览客户流失预测 ML 用例。这将使您了解它与自我管理的特征库有何不同，以及特征库在基本特征监控和模型监控方面的帮助。

在本章中，我们将依次讨论以下主题:

*   基础设施设置
*   问题和数据集简介
*   数据处理和特征工程
*   功能组定义和摄取
*   模特培训
*   模型预测法
*   功能监控
*   模型监控

# 技术要求

为了浏览示例并更好地理解本章，理解前面章节中涉及的主题将会很有用，但不是必需的。要理解本章中的代码示例，您需要熟悉笔记本环境，它可以是本地设置，如 Jupyter 笔记本，也可以是在线笔记本环境，如 Google Colab、Kaggle 或 SageMaker。您还需要一个对 SageMaker 和 Glue 控制台具有完全访问权限的 AWS 帐户。您可以创建一个新帐户，并在试用期内免费使用所有服务。您可以在下面的 GitHub 链接中找到该书的代码示例:

https://github . com/packt publishing/Feature-Store-for-Machine-Learning/tree/main/chapter 08

# 基础设施设置

对于本章的练习，我们将需要一个 S3 存储桶来存储数据，一个 IAM 角色，以及一个能够访问 SageMaker 功能存储和 S3 存储桶的 IAM 用户。由于我们已经创建了所有这些资源，我将跳过这一步。请参考 [*第 4 章*](B18024_04_ePub.xhtml#_idTextAnchor065) 、*添加特性存储到 ML 模型*中的 S3 桶创建，以及 [*第 7 章*](B18024_07_ePub.xhtml#_idTextAnchor113) 、*盛宴备选方案和 ML 最佳实践*中的 IAM 角色和 IAM 用户创建。就本章的初始设置而言，这就是我们所需要的。

重要说明

我试图使用尽可能少的 AWS SageMaker 资源，因为如果你的免费试用已经结束，它会产生费用。您可以使用 SageMaker Studio 来获得更好的笔记本体验以及功能商店的 UI。

# 问题和数据集介绍

在本练习中，我们将使用电信客户流失数据集，该数据集可从 Kaggle 上的网址 https://www . ka ggle . com/datasets/blast char/telco-customer-churn 获得。练习的目的是使用这个数据集，为模型训练准备数据，并训练一个 XGBoost 模型来预测客户流失。数据集有 21 列，列名不言自明。以下是数据集的预览:

![Figure 8.1 – Telecom dataset
](img/B18024_08_001.jpg)

图 8.1–电信数据集

*图 8.1* 显示了标记的电信客户流失数据集。`customerID`列是客户的 ID。除了`Churn`之外的所有其他列表示属性集，而`Churn`列是目标列。

接下来让我们动手来执行特征工程。

# 数据处理和特征工程

在此部分，让我们使用电信客户流失数据集，并生成可用于训练模型的特征。让我们创建一个笔记本，将其命名为`feature-engineering.ipynb`，并安装所需的依赖项:

```
!pip install pandas sklearn python-slugify s3fs sagemaker
```

库安装完成后，读取数据。在这个练习中，我从 Kaggle 下载了数据，并将其保存在一个可以从笔记本中访问的位置。

跟随命令的从 S3 读取数据:

```
import os
```

```
import numpy as np
```

```
import pandas as pd
```

```
from slugify import slugify
```

```
from sklearn.preprocessing import LabelEncoder
```

```
from sklearn.preprocessing import StandardScaler
```

```
""" If you are executing the notebook outside AWS(Local jupyter lab, google collab or kaggle etc.), please uncomment the following 3 lines of code and set the AWS credentials """
```

```
#os.environ["AWS_ACCESS_KEY_ID"] = "<aws_key>"
```

```
#os.environ["AWS_SECRET_ACCESS_KEY"] = "<aws_secret>"
```

```
#os.environ["AWS_DEFAULT_REGION"] = "us-east-1"
```

```
telcom = pd.read_csv("s3://<bucket_name_path>/telco-customer-churn.csv")
```

重要说明

如果您在 AWS 之外执行笔记本，那么使用环境变量设置用户凭证。

如果预览该数据集，有几列需要重新格式化、转换为分类列或删除空值。让我们一个接一个地执行这些转换。

`TotalCharges`列包含几个空字符串。让我们删除包含空值或 null 值的行:

```
# Replace empty strings with nan
```

```
churn_data['TotalCharges'] = churn_data["TotalCharges"].replace(" ",np.nan)
```

```
# remove null values
```

```
churn_data = churn_data[churn_data["TotalCharges"].notnull()]
```

```
churn_data = churn_data.reset_index()[churn_data.columns]
```

```
churn_data["TotalCharges"] = churn_data["TotalCharges"].astype(float)
```

前面的代码块用`np.nan`替换所有的空字符串，并删除`TotalCharges`列中包含 null 的所有行。

接下来，让我们看看`tenure`一栏。这一个有整数值，代表客户的任期，以月为单位。除了价值，我们还可以将客户分为三组:短期(0-24 个月)、中期(24-48 个月)和长期(超过 48 个月)。

下面的代码使用定义的组添加 customer `tenure_group`列:

```
# Create tenure_group columns using the tenure
```

```
def tenure_label(churn_data) :
```

```
    if churn_data["tenure"] <= 24 :
```

```
        return "0-24"
```

```
    elif (churn_data["tenure"] > 24) & (churn_data["tenure"] <= 48) :
```

```
        return "24-48"
```

```
    elif churn_data["tenure"] > 48:
```

```
        return "48-end"
```

```
churn_data["tenure_group"] = churn_data.apply(
```

```
    lambda churn_data: tenure_label(churn_data), axis = 1)
```

前面的代码块创建分类列`tenure_group`，它将有三个值`0-24`、`24-48`和`48-end`，这取决于客户占有期的长度。

数据集中的一些列依赖于其他列。比如`OnlineSecurity`要看客户有没有`InternetService`。因此，这些列中的一些，即`OnlineSecurity`、`OnlineBackup`、`DeviceProtection`、`TechSupport`、`StreamingTV`和`StreamingMovies`具有`No internet service`作为值，而不是`No`。让我们在那些列中用`No`代替`No internet service`。

以下代码块执行替换:

```
# Replace 'No internet service' to No for the following columns
```

```
replace_cols = ['OnlineSecurity', 'OnlineBackup', 
```

```
                'DeviceProtection', 'TechSupport',
```

```
                'StreamingTV', 'StreamingMovies']
```

```
for i in replace_cols : 
```

```
    churn_data[i] = churn_data[i].replace({'No internet service' : 'No'})
```

到目前为止，我们已经完成了一系列数据清理操作。让我们先预览一下数据集，然后再进行进一步的转换。

以下代码对`churn_data`数据帧进行采样:

```
churn_data.sample(5)
```

下面的代码输出一个示例预览，如下面的屏幕截图所示:

![Figure 8.2 – Churn dataset
](img/B18024_08_002.jpg)

图 8.2–流失数据集

正如您在*图 8.2* 中看到的，数据集是干净的，只有分类或数字列。下一步是将这些分类值转换成数字编码。让我们看看数据集，看看哪些是分类的，哪些是数字的。

以下代码计算每列中的唯一值:

```
churn_data.nunique()
```

上述代码显示以下输出:

![Figure 8.3 – Unique value count of every column
](img/B18024_08_003.jpg)

图 8.3–每列的唯一值计数

在*图 8.3* 中可以看到，除了`MonthlyCharges`、`tenure`和`TotalCharges`之外，所有其他列都是分类的。

在数据集中，有二进制列和多值类别列。让我们找出哪些是二进制，哪些是多值列。下面的代码块检查该列是否是列列表中的二进制列:

```
# filter all the col if unique values in the column is 2
```

```
bin_cols = churn_data.nunique()[churn_data.nunique() == 2].keys().tolist()
```

现在我们有了二进制列的列表，让我们使用标签编码器将它们转换成 0 和 1。

以下代码使用标签编码器对二进制列执行转换:

```
le = LabelEncoder()
```

```
for i in bin_cols :
```

```
    churn_data[i] = le.fit_transform(churn_data[i])
```

下一步是将多值分类列转换为 0 和 1。为此，让我们先过滤掉多值列名。

下面的代码块选择多值列:

```
all_categorical_cols = churn_data.nunique()[churn_data.nunique() <=4].keys().tolist()
```

```
multi_value_cols = [col for col in all_categorical_cols if col not in bin_cols]
```

前面的代码块首先过滤掉所有的分类列，然后过滤掉二进制列，这样我们只剩下多值列。

以下代码块将多值列转换为二进制编码:

```
churn_data = pd.get_dummies(data = churn_data, columns=multi_value_cols)
```

最后一部分是转换数值。由于数字列可以有不同的范围，将列缩放到标准范围对 ML 算法是有益的。这也有助于算法更快地收敛。因此，让我们将数字列缩放到一个标准范围。

下面的代码块使用`StandardScaler`将所有数字列缩放到一个标准范围:

```
numerical_cols = ['tenure','MonthlyCharges','TotalCharges']
```

```
std = StandardScaler()
```

```
churn_data[numerical_cols] = std.fit_transform(churn_data[numerical_cols])
```

前面的代码块缩放数字列:`tenure`、`MonthlyCharges`和`TotalCharges`。现在我们的特性工程已经完成，让我们预览最终的特性集，并将其导入 SageMaker 特性库。

以下代码块显示了功能集预览:

```
churn_data.columns = [slugify(col, lowercase=True, separator='_') for col in churn_data.columns]
```

```
churn_data.head()
```

前面的代码块将列名格式化为小写，并将字符串中的所有分隔符(如空格和连字符)替换为下划线。最终的功能显示在下面的截图中:

![Figure 8.4 – Feature set
](img/B18024_08_004.jpg)

图 8.4–功能集

最终特征集合有 33 列，如图*图 8.4* 所示。如果您还记得在 [*第 4 章*](B18024_04_ePub.xhtml#_idTextAnchor065) 、*将特征库添加到 ML 模型*中，在创建特征定义时，我们根据实体或逻辑组来识别实体和分组特征。虽然这些功能可以被分组到多个组中，但是我们将创建一个单一的功能组，并将所有的功能纳入其中。

在下一节中，让我们创建特性定义并接收数据。

# 特征组定义和特征摄取

既然我们已经为获取特性集做好了准备，让我们创建特性定义并将特性获取到特性库中。在这个练习中，如前所述，我们将使用 SageMaker 特性库。如果您还记得前面的章节，我们总是将特性定义保存在单独的笔记本中，因为这是一次性的活动。在本练习中，我们将尝试一种不同的方法，即使用条件语句创建一个不存在的功能组。这两种方法你都可以用。

让我们继续在同一个笔记本中初始化 boto3 会话，并检查我们的功能组是否已经存在:

```
import boto3
```

```
FEATURE_GROUP_NAME = "telcom-customer-features"
```

```
feature_group_exist = False
```

```
client = boto3.client('sagemaker')
```

```
response = client.list_feature_groups(
```

```
    NameContains=FEATURE_GROUP_NAME)
```

```
if FEATURE_GROUP_NAME in response["FeatureGroupSummaries"]:
```

```
  feature_group_exist = True
```

前面的代码块查询 SageMaker 来检查名为`telcom-customer-features`的特性组是否存在，并基于此设置一个布尔值。我们将使用这个布尔值来创建要素组，或者跳过创建，只将数据接收到要素存储中。

以下代码块初始化与 SageMaker 功能库交互所需的对象:

```
import sagemaker
```

```
from sagemaker.session import Session
```

```
import time
```

```
from sagemaker.feature_store.feature_definition import FeatureDefinition, FeatureTypeEnum
```

```
role = "arn:aws:iam::<account_number>:role/sagemaker-iam-role"
```

```
sagemaker_session = sagemaker.Session()
```

```
region = sagemaker_session.boto_region_name
```

```
s3_bucket_name = "feast-demo-mar-2022"
```

重要说明

在前面的代码块中，使用在前面的部分中创建的 IAM 角色。IAM 角色应该有**AmazonSageMakerFullAccess**和 **AmazonS3FullAccess** 。

下一个步骤是初始化`FeatureGroup`对象。以下代码初始化功能组对象:

```
from sagemaker.feature_store.feature_group import FeatureGroup
```

```
customers_feature_group = FeatureGroup(
```

```
    name=FEATURE_GROUP_NAME, 
```

```
    sagemaker_session=sagemaker_session
```

```
)
```

现在，如果特性组不存在，我们将使用之前设置的布尔值有条件地创建特性组。以下代码块加载特征定义，如果特征组不存在，则调用`create`:

```
churn_data["event_timestamp"] = float(round(time.time()))
```

```
if not feature_group_exist:
```

```
  customers_feature_group.load_feature_definitions(
```

```
      churn_data[[col 
```

```
                  for col in churn_data.columns 
```

```
                  if col not in ["customerid"]]]) 
```

```
  customer_id_def = FeatureDefinition(
```

```
      feature_name='customerid', 
```

```
      feature_type=FeatureTypeEnum.STRING)
```

```
  customers_feature_group.feature_definitions = [customer_id_def] + customers_feature_group.feature_definitions
```

```
  customers_feature_group.create(
```

```
    s3_uri=f"s3://{s3_bucket_name}/{FEATURE_GROUP_NAME}",
```

```
    record_identifier_name="customerid",
```

```
    event_time_feature_name="event_timestamp",
```

```
    role_arn=role,
```

```
    enable_online_store=False
```

```
    )
```

重要说明

在`load_feature_definitions`调用中，如果您注意到，我正在加载除了`customerid`列之外的所有特性定义列，并在下一行中手动将`customerid`列添加到特性定义列表中。这是因为`sagemaker`库无法计算出`string`的数据类型，因为`string`的熊猫`dtype`是`object`。

`create`功能组调用很简单。我通过将`enable_online_store`传递为`False`来禁用在线商店，因为我们将尝试批处理管道，我将在线模型作为练习。一旦前面的代码块执行，基于条件语句，它将首次创建特性组，对于后续运行，它将跳过特性组创建。

最后一步是接收数据帧。以下代码块执行接收并打印任何失败:

```
ingestion_results = customers_feature_group.ingest(
```

```
    churn_data, max_workers=1)
```

```
ingestion_results.failed_rows
```

重要说明

如果你只有批量用例，SageMaker 有一个 Spark 库，可以用来直接摄取到线下商店，也是性价比高的。

这就完成了特征工程和摄取。在下一节中，我们来看看模型训练。

# 模特培训

和之前一样，对于模型训练，特征库是来源。因此，让我们创建我们的模型培训笔记本，并安装和初始化查询特性存储所需的对象。以下是笔记本的链接:

[https://github . com/packt publishing/Feature-Store-for-Machine-Learning/blob/main/chapter 08/Ch8 _ model _ training . ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_model_training.ipynb)

以下代码块安装模型训练所需的库:

```
!pip install sagemaker==2.88.0 s3fs joblib scikit-learn==1.0.2 xgboost
```

安装所需的库之后，初始化 SageMaker 会话和所需的对象:

```
import sagemaker
```

```
from sagemaker.session import Session
```

```
from sagemaker.feature_store.feature_group import FeatureGroup
```

```
#import os
```

```
#os.environ["AWS_ACCESS_KEY_ID"] = "<aws_key_id>"
```

```
#os.environ["AWS_SECRET_ACCESS_KEY"] = "<aws_secret>"
```

```
#os.environ["AWS_DEFAULT_REGION"] = "us-east-1"
```

```
role = "arn:aws:iam::<account_number>:role/sagemaker-iam-role"
```

```
FEATURE_GROUP_NAME = "telcom-customer-features"
```

```
sagemaker_session = sagemaker.Session()
```

```
region = sagemaker_session.boto_region_name
```

```
s3_bucket_name = "feast-demo-mar-2022"
```

```
customers_feature_group = FeatureGroup(
```

```
    name=FEATURE_GROUP_NAME, 
```

```
    sagemaker_session=sagemaker_session
```

```
)
```

前面的代码块初始化 SageMaker 会话，并初始化 feature group 对象。特征组的`name`应该与我们在特征工程笔记本中创建的特征组的`name`相同。

重要说明

将之前创建的 IAM 角色分配给`role`变量。此外，如果您在 AWS 之外运行笔记本，您需要在前面的代码块中取消注释并设置 AWS 凭证。

下一部分是查询历史存储以生成训练数据。与 Feast 不同，这里我们不需要实体数据框架。相反，我们使用 SQL 查询来获取历史数据。它拥有和盛宴一样的时间旅行能力。在本练习中，我们使用与上一章 SageMaker 概述中类似的查询来获取所有客户的最新功能:

```
get_latest_snapshot_query = customers_feature_group.athena_query()
```

```
query = f"""SELECT *
```

```
FROM
```

```
    (SELECT *,
```

```
         row_number()
```

```
        OVER (PARTITION BY customerid
```

```
    ORDER BY  event_timestamp desc, Api_Invocation_Time DESC, write_time DESC) AS row_num
```

```
    FROM "{get_latest_snapshot_query.table_name}")
```

```
WHERE row_num = 1 and 
```

```
NOT is_deleted;"""
```

```
get_latest_snapshot_query.run(
```

```
    query_string=query, 
```

```
    output_location=f"s3://{s3_bucket_name}/output")
```

```
get_latest_snapshot_query.wait()
```

如果您记得没错，我们在上一章中使用了类似的嵌套查询。前面的代码块获取所有客户及其最新特性。查询的输出将被写入到一个特定的 S3 位置，正如在`run` API 调用中提到的。

一旦查询成功运行，就可以使用以下代码块提取数据集:

```
churn_data = get_latest_snapshot_query.as_dataframe()
```

```
churn_data = churn_data.drop(columns=["event_timestamp", "write_time", "api_invocation_time", "is_deleted", "row_num"])
```

重要说明

请注意，我们将从头至前一个代码块执行与本节相同的模型预测和特征监控步骤(*模型训练*)。

前面的代码块获取数据集并删除不需要的列。获取的数据集类似于图 8.4 中*所示的数据，增加了`write_time`、`api_invocation_time`、`is_deleted`和`row_num`列。前三个是 SageMaker 在摄取过程中添加的附加元数据列，而`row_num`是我们在查询中创建的用于获取每个客户的最新特性的列。*

现在我们已经有了数据集，让我们把它分开进行训练和测试。下面的代码块从数据集中删除不需要定型的列，并拆分用于定型和测试的数据:

```
from sklearn.model_selection import train_test_split
```

```
from sklearn.linear_model import LogisticRegression
```

```
from sklearn.metrics import confusion_matrix,accuracy_score,classification_report
```

```
from sklearn.metrics import roc_auc_score,roc_curve
```

```
from sklearn.metrics import precision_score,recall_score
```

```
Id_col = ["customerid"]
```

```
target_col = ["churn"]
```

```
# Split into a train and test set
```

```
train, test = train_test_split(churn_data,
```

```
                               test_size = .25,
```

```
                               random_state = 111)
```

```
cols    = [i for i in churn_data.columns if i not in Id_col + target_col]
```

```
training_x = train[cols]
```

```
training_y = train[target_col]
```

```
testing_x  = test[cols]
```

```
testing_y  = test[target_col]
```

前面的代码块省略了 ID 列，并为训练和测试执行了 75/25 分割。

剩下的部分很简单，基本上就是训练一个 XGBoost 模型、调整参数和比较性能。以下是用于培训、样本分析和记录模型的示例代码块:

```
import joblib
```

```
import boto3
```

```
model = XGBClassifier(max_depth=7, 
```

```
                      objective='binary:logistic')
```

```
model.fit(training_x, training_y)
```

```
predictions = model.predict(testing_x)
```

```
probabilities = model.predict_proba(testing_x)
```

```
print("\n Classification report : \n", 
```

```
      classification_report(testing_y, predictions))
```

```
print("Accuracy   Score : ", 
```

```
      accuracy_score(testing_y, predictions))
```

```
# confusion matrix
```

```
conf_matrix = confusion_matrix(testing_y, predictions)
```

```
model_roc_auc = roc_auc_score(testing_y, predictions)
```

```
print("Area under curve : ", model_roc_auc, "\n")
```

```
joblib.dump(model, '/content/customer-churn-v0.0')
```

```
s3_client = boto3.client('s3')
```

```
response = s3_client.upload_file('/content/customer-churn-v0.0', s3_bucket_name, "model-repo/customer-churn-v0.0")
```

前面的代码块也将模型记录到 S3 的一个特定位置。这是一种简单的方法。最好使用实验培训工具来记录性能和模型。

现在模型训练已经完成，让我们看看模型评分。

# 模型预测

正如上一节最后一个注释中提到的,由于这是一个批处理模型，因此从线下商店获取数据的模型评分步骤类似。但是，根据需要对哪些客户进行评分(可能是所有客户)，您可能会过滤掉数据集。一旦过滤掉数据集，剩下的步骤就简单了，即加载模型、运行预测并存储结果。

以下是用于加载模型、运行预测以及将结果存储回 S3 以供使用的示例代码块:

```
import boto3
```

```
from datetime import date
```

```
s3 = boto3.client('s3')
```

```
s3.download_file(s3_bucket_name, f"model-repo/customer-churn-v0.0", "customer-churn-v0.0")
```

```
features = churn_data.drop(['customerid', 'churn'], axis=1)
```

```
loaded_model = joblib.load('/content/customer-churn-v0.0')
```

```
prediction = loaded_model.predict(features)
```

```
prediction.tolist()
```

```
file_name = f"customer_churn_prediction_{date.today()}.parquet"
```

```
churn_data["predicted_churn"] = prediction.tolist()
```

```
s3_url = f's3://{s3_bucket_name}/prediction_results/{file_name}'
```

```
churn_data.to_parquet(s3_url)
```

前面的代码块从 S3 下载模型，加载模型，根据从历史存储中获取的数据对模型进行评分，并将结果存储在 S3 桶中以供使用。

注意

XGBoost、Joblib 和 scikit-learn 的库版本应该与保存模型时使用的版本相同，否则模型的加载可能会失败。

为了生产这个 ML 管道，我们可以使用类似于我们在 [*第 6 章*](B18024_06_ePub.xhtml#_idTextAnchor096) 、*模型到生产和超越*中所做的编排。我将把它作为练习，因为它是重复的内容。接下来让我们看一个特性监控的例子。

# 功能监控

我们已经在书中几次讨论了特征监控在 ML 系统中的重要性。我们还讨论了特性存储如何标准化特性监控。在这一节中，让我们看一个对任何模型都有用的特性监控的例子。由于要素监控正在计算要素数据的一组统计数据，并将变化通知数据科学家或数据工程师，因此它需要模型使用的最新要素。

在这个部分，让我们计算特性数据和特性相关性的汇总统计，它可以按计划运行，并定期发送给感兴趣的人，以便他们可以根据它采取行动。正如在*模型训练*部分的最后一个注释中提到的，获取特征的步骤与该部分中所做的相同。一旦您拥有了所有的特性，下一步就是计算所需的统计数据。

重要说明

请注意，您可能需要安装额外的库。下面是笔记本的网址:[https://github . com/packt publishing/Feature-Store-for-Machine-Learning/blob/main/chapter 08/Ch8 _ Feature _ monitoring . ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_feature_monitoring.ipynb)。

以下代码块计算要素数据的汇总统计信息，并绘制相关性指标:

```
import numpy as np
```

```
import warnings
```

```
warnings.filterwarnings("ignore")
```

```
import plotly.offline as py
```

```
import plotly.graph_objs as go
```

```
churn_data.describe(include='all').T
```

前面的代码行生成数据集的描述性统计数据，包括最小值、最大值、计数、标准差等。

除了描述性统计，特征的相关矩阵是另一个对所有 ML 模型有用的东西。以下代码块计算要素的相关矩阵并绘制热图:

```
corr = churn_data.corr()
```

```
cols = corr.columns.tolist()
```

```
trace = go.Heatmap(z=np.array(corr),
```

```
                   x=cols,
```

```
                   y=cols,
```

```
                   colorscale="Viridis",
```

```
                   colorbar=dict(
```

```
                       title="Pearson Coefficient",
```

```
                       titleside="right"
```

```
                       ),
```

```
                   )
```

```
layout = go.Layout(dict(title="Correlation Matrix",
```

```
                        height=720,
```

```
                        width=800,
```

```
                        margin=dict(r=0, l=210,
```

```
                                    t=25, b=210,
```

```
                                    ),
```

```
                        )
```

```
                   )
```

```
fig = go.Figure(data=[trace], layout=layout)
```

```
py.iplot(fig)
```

前面的代码块输出以下热图:

![Figure 8.5 – Feature correlation
](img/B18024_08_005.jpg)

图 8.5–特征相关性

与之前的运行相比，您可以添加更多的统计数据、通过电子邮件发送的警报、空闲通知等等。这可以在另一个笔记本/Python 脚本中，它可以以与特征工程笔记本相同或更低的频率进行调度，并自动向您发送报告。下面是完整笔记本的链接:[https://github . com/packt publishing/Feature-Store-for-Machine-Learning/blob/main/chapter 08/Ch8 _ Feature _ monitoring . ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_feature_monitoring.ipynb)。

重要说明

这只是特征监控的一个例子。还有更复杂的统计数据和指标可用于确定特性的健康状况。

接下来让我们看看模型监控。

# 模式监控

ML 的另一个重要方面是模型监控。模型监控有不同的方面:一个方面是在线模型的系统监控，您可以监控模型的延迟、CPU、内存使用、每分钟的请求等等。另一个方面是模型的性能监控。同样，衡量绩效有许多不同的方式。在本例中，我们将查看一个简单的分类报告和模型的准确性。

要生成分类报告并计算实时模型的准确性，您需要预测数据以及实时数据的基本事实。对于本例，假设客户流失模型每周运行一次，以生成客户流失预测，从模型运行之日起，每 4 周将提供一次基本事实。这意味着如果模型预测客户 x 的流失率为`True`，并且在接下来的 4 周内，如果我们因为任何原因失去客户，则模型预测正确；否则，预测是错误的。因此，对于模型预测的每一次运行，我们需要等待 4 周才能获得地面真实情况。

为了简化这个练习，我们还假设地面实况每周都被填充回特征存储中。这意味着功能商店总是有最新的地面真相。现在，我们的工作是获取与特性存储中的最新特性相对应的预测结果(4 周前运行的预测)并计算所需的指标。让我们接下来做那件事。

如前所述，从特性库中获取最新特性的步骤与我们在上三节中所做的相同。从要素存储中获取数据后，以下代码将获取相应的预测结果并合并数据集:

```
from datetime import date, timedelta
```

```
import pandas as pd
```

```
pred_date = date.today()-timedelta(weeks=4)
```

```
file_name = f"customer_churn_prediction_{pred_date}.parquet"
```

```
prediction_data = pd.read_parquet(f"s3://{s3_bucket_name}/prediction_results/{file_name}")
```

```
prediction_y = prediction_data[["customerid", 
```

```
                                "predicted_churn"]]
```

```
acutal_y = churn_data[["customerid", "churn"]]
```

```
merged_data = prediction_y.merge(acutal_y, on="customerid")
```

```
merged_data.head()
```

前面的代码块应该产生类似于下面的快照的输出。

![Figure 8.6 – Merged data for model monitoring
](img/B18024_08_006.jpg)

图 8.6–模型监控的合并数据

重要说明

因为我们假设预测和实际情况相差 4 周，所以前面的代码块试图获取从今天起 4 周的数据。在本练习中，您可以用预测输出 Parquet 文件替换`file_name`变量。

一旦有了*图 8.6* 中的数据帧，下面的代码块使用`predicted_churn`和`churn`列产生分类报告和精度:

```
testing_y = merged_data["churn"]
```

```
predictions = merged_data["predicted_churn"]
```

```
print("\n Classification report : \n", 
```

```
      classification_report(testing_y, predictions))
```

```
print("Accuracy   Score : ", 
```

```
      accuracy_score(testing_y, predictions))
```

```
# confusion matrix
```

```
conf_matrix = confusion_matrix(testing_y, predictions)
```

```
# roc_auc_score
```

```
model_roc_auc = roc_auc_score(testing_y, predictions)
```

```
print("Area under curve : ", model_roc_auc, "\n")
```

前面的代码块产生类似下面的输出。

![Figure 8.7 – Classification report
](img/B18024_08_007.jpg)

图 8.7–分类报告

如前所述，这是样本监测。这可以安排在与特征工程笔记本相同的时间间隔，尽管由于预测数据不可用，前四次迭代会失败。还要确保根据需要适当调整预测文件名。以下是完整笔记本的网址:[https://github . com/packt publishing/Feature-Store-for-Machine-Learning/blob/main/chapter 08/Ch8 _ model _ monitoring . ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_model_monitoring.ipynb)。

说到这里，让我们总结一下本章所学的内容。

# 总结

在这一章中，我们开始尝试一个用例，即使用 Kaggle 提供的数据集进行电信客户流失预测。对于这个用例，我们使用了一个托管的 SageMaker 特性库，这在上一章中已经介绍过了。在练习中，我们经历了 ML 的不同阶段，如数据处理、特征工程、模型训练和模型预测。我们还看了一个特性监控和模型监控的例子。本章的目的不是构建模型，而是展示如何使用托管功能存储来构建模型，以及它为监控带来的机遇。要了解更多关于特色商店的信息，应用大会(【https://www.applyconf.com/】)和特色商店论坛([【https://www.featurestore.org/】](https://www.featurestore.org/))都是很好的资源。为了了解 ML 的最新发展以及其他公司如何解决类似问题，有一些有趣的播客，如 TWIML AI([https://twimlai.com/](https://twimlai.com/))和 Data 怀疑论(https://dataskeptic.com/)。这些资源可以帮助你找到更多基于你对 ML 感兴趣的领域的资源。就这样，让我们结束这一章和这本书。我希望我在 ML 过程中有效地传达了特性存储的重要性，并且很好地利用了你和我的时间。谢谢大家！