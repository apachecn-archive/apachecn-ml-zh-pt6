

# 六、从模型到生产及其他

在上一章中，我们讨论了与**Feast**在线和批量模型的模型训练和预测。在本练习中，我们使用了在 [*第 4 章*](B18024_04_ePub.xhtml#_idTextAnchor065) 、*向 ML 模型添加特征存储*的练习中部署到 AWS 云的 Feast 基础设施。在这些练习中，我们研究了 Feast 如何将特征工程从模型训练和模型预测中分离出来。我们还学习了如何在批量和在线预测期间使用离线和在线存储。

在本章中，我们将重用特征工程管道和 [*第四章*](B18024_04_ePub.xhtml#_idTextAnchor065)*向 ML 模型添加特征库*， [*第五章*](B18024_05_ePub.xhtml#_idTextAnchor078) ，*模型训练和推理*中建立的模型，来生产**机器学习** ( **ML** )管道。本章的目标是重用我们在前面章节中构建的所有东西，例如 AWS 上的Feast基础设施、特征工程、模型训练和模型评分笔记本，以生产 ML 模型。在我们进行练习时，这将为我们提供一个机会来了解如何尽早采用 Feast 不仅可以分离 ML 管道阶段，还可以加速 ML 模型的生产准备。一旦我们将批量和在线 ML 管道生产化，我们将研究 Feast 的采用如何为 ML 生命周期的其他方面带来机遇，如特征监控、自动化模型再训练，以及它如何加速未来 ML 模型的开发。本章将帮助你理解如何生产使用 Feast 的批处理和在线模型，以及如何使用 Feast 进行特征漂移监控和模型再训练。

我们将依次讨论以下主题:

*   为编排设置气流
*   生产批量模型管道
*   生产在线模型管道
*   超越模型生产

# 技术要求

为了遵循本章中的代码示例，需要在 [*第 4 章*](B18024_04_ePub.xhtml#_idTextAnchor065) 、*向 ML 模型添加特征库*和 [*第 5 章*](B18024_05_ePub.xhtml#_idTextAnchor078) 、*模型训练和推理*中创建的资源。您需要熟悉 Docker 和任何笔记本环境，可以是本地设置，如 Jupyter，也可以是在线笔记本环境，如 Google Colab、Kaggle 或 SageMaker。您还需要一个对一些资源有完全访问权限的 AWS 帐户，如 Redshift、S3、Glue、DynamoDB 和 IAM 控制台。您可以创建一个新帐户，并在试用期内免费使用所有服务。您可以在下面的 GitHub 链接中找到书籍和特征库的代码示例:

*   [https://github . com/packt publishing/Feature-Store-for-Machine-Learning/tree/main/chapter 06](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter06)
*   [https://github . com/packt publishing/Feature-Store-for-Machine-Learning/tree/main/customer _ segmentation](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation)

# 为编排设置气流

为了将在线和批处理模型生产化，我们需要一个工作流程编排工具，它可以为我们按时运行 ML 管道。有很多可用的工具，比如 Apache Airflow、AWS Step Functions 和 SageMaker Pipelines。如果您愿意，也可以将其作为 GitHub 工作流运行。根据您熟悉的工具或您的组织提供的工具，编排可能会有所不同。对于这个练习，我们将使用亚马逊**管理的 Apache air flow**(**MWAA**)工作流。顾名思义，它是由 AWS 提供的 Apache Airflow 管理的服务。让我们在 AWS 中创建一个亚马逊 MWAA 环境。

重要说明

亚马逊 MWAA 没有免费试用。您可以在以下网址查看使用定价:[https://AWS . Amazon . com/managed-workflows-for-Apache-air flow/pricing/](https://aws.amazon.com/managed-workflows-for-apache-airflow/pricing/)。或者，您可以选择在本地或 EC2 实例上运行 Airflow(EC2 有空闲层资源)。您可以在此处找到在本地或 EC2 上运行气流的设置说明:

气流本地设置:[https://towardsdatascience . com/getting-started-with-air flow-local-and-remote-d 068 df 7 fc B4](https://towardsdatascience.com/getting-started-with-airflow-locally-and-remotely-d068df7fcb4)

EC2 上的气流:[https://christo-laga Li . medium . com/getting-air flow-up-and-running-on-an-EC2-instance-AE 4 F3 a 69441](https://christo-lagali.medium.com/getting-airflow-up-and-running-on-an-ec2-instance-ae4f3a69441)

## 气流元数据的 S3 桶

在我们创建一个环境之前，我们需要一个 S3 桶来存储气流相关性。在 S3 桶中，创建一个名为`dags`的文件夹。我们将使用这个文件夹来存储所有的气流 Dag。

亚马逊 MWAA 提供了多种不同的方式来配置要安装在气流环境中的附加插件和 Python 依赖项。因为我们需要安装一些 Python 依赖项来运行我们的项目，所以我们需要告诉 Airflow 安装这些必需的依赖项。一种方法是使用`requirements.txt`文件。下面的代码块显示了文件的内容:

```
papermill==2.3.4
```

```
boto3==1.21.41
```

```
ipython==8.2.0
```

```
ipykernel==6.13.0
```

```
apache-airflow-providers-papermill==2.2.3
```

将前面代码块的内容保存在`requirements.txt`文件中。我们将使用【https://papermill.readthedocs.io/en/latest/】()来运行 Python 笔记本。您还可以使用 Airflow 中可用的`bash`或`python`操作符提取代码并运行 Python 脚本。

重要说明

如果您在本地运行 Airflow，请确保库版本与 Airflow 版本兼容。撰写本文时的亚马逊 MWAA 气流版本是 2.2.2。

一旦你创建了`requirement.txt`文件，将它上传到我们创建的 S3 桶中。我们将在下一节的环境创建中使用它。

## 用于编排的亚马逊 MWAA 环境

现在我们已经有了创建亚马逊 MWAA 环境所需的资源，让我们按照以下步骤来创建环境:

1.  要创建新环境，请登录您的 AWS 帐户，并使用 AWS 控制台中的搜索栏导航到亚马逊 MWAA 控制台。或者，参观 https://us-east-1.console.aws.amazon.com/mwaa/home?region = us-east-1 #环境。将显示以下网页:

![Figure 6.1 – The Amazon MWAA environments console
](img/B18024_06_001.jpg)

图 6.1–亚马逊 MWAA 环境控制台

1.  在*图 6.1* 显示的页面上，点击**创建环境**按钮，将显示后的页面:

![Figure 6.2 – Amazon MWAA environment details
](img/B18024_06_002.jpg)

图 6.2–亚马逊 MWAA 环境详情

1.  在*图 6.2* 中显示的页面上提供亚马逊 MWAA 环境的名称。向下滚动到亚马逊 S3 部分的 **DAG 代码；您应该会在屏幕上看到以下参数:**

![Figure 6.3 – Amazon MWAA – the DAG code in S3 section
](img/B18024_06_003.jpg)

图 6.3-亚马逊 MWAA-S3 部分的 DAG 代码

1.  在图 6.3 显示的屏幕上，在文本框中输入 S3 桶或使用我们上传的`requirements.txt`文件或输入文件的路径。由于我们不需要任何插件来运行项目，我们可以将可选的**插件文件**字段留空。点击**下一个**按钮:

![Figure 6.4 – Amazon MWAA advanced settings
](img/B18024_06_004.jpg)

图 6.4-亚马逊 MWAA 高级设置

1.  显示的下一个页面如图*图 6.4* 所示。对于**虚拟私有云(VPC)** ，从下拉列表中选择可用的默认 VPC。这里需要注意的是，所选的 VPC 应该至少有两个私有子网。如果它没有私有子网，当您尝试选择**子网 1** 和**子网 2** 时，您会注意到所有选项都是灰色的。如果你遇到这种情况，点击**创建 MWAA·VPC**。它将带你到云形成控制台；在表格中填写所有参数后，继续操作并点击**创建堆栈**。它将创建一个可供亚马逊 MWAA 使用的 VPC。创建 VPC 后，返回此窗口并选择新的 VPC 和子网，然后继续。
2.  选择 VPC 后，对于 **Web 服务器访问**，选择**公共网络**；其他一切都保持默认，一路向下滚动。在**权限**部分，你会注意到它将为亚马逊 MWAA 创建一个新角色。记下角色名称。稍后我们将不得不为此角色添加权限。之后，点击下一个的**。**
3.  在下一页上，查看提供的所有输入，向下滚动，并单击**创建环境**。创建环境需要几分钟时间。
4.  一旦创建了环境，您应该能够在亚马逊 MWAA 环境页面上看到处于**可用**状态的环境。选择我们刚刚创建的环境，并单击 **Open Airflow UI** 链接。将显示一个气流主页，类似于下图:![Figure 6.5 – The Airflow UI
    ](img/B18024_06_005.jpg)

图 6.5–气流用户界面

1.  为了测试一切是否正常，让我们快速创建一个简单的 DAG 并看看它是如何工作的。下面的代码块创建了一个简单的 DAG，带有一个伪操作符和一个 Python 操作符:

    ```
    from datetime import datetime from airflow import DAG from airflow.operators.dummy_operator import DummyOperator from airflow.operators.python_operator import PythonOperator def print_hello():     return 'Hello world from first Airflow DAG!' dag = DAG('hello_world',            description='Hello World DAG',           schedule_interval='@daily',           start_date=datetime(2017, 3, 20),            catchup=False) start = DummyOperator(task_id="start", dag=dag) hello_operator = PythonOperator(     task_id='hello_task',      python_callable=print_hello,      dag=dag) start >> hello_operator
    ```

2.  在前面的代码中定义的 DAG非常简单；它有两个任务——`start`和`hello_operator`。`start`任务是一个`DummyOperator`，什么也不做，用于使 DAG 在 UI 上看起来漂亮。`hello_operator`任务只是调用一个返回消息的函数。在最后一行，我们定义了操作符之间的依赖关系。
3.  复制代码块前面的，保存为`example_dag.py`，上传到我们之前创建的 S3`dags`文件夹。(我的 S3 位置是`s3://airflow-for-ml-mar-2022/dags`。)一旦上传，它应该会在几秒钟内出现在气流 UI 中。下图显示了带有 DAG 的气流用户界面:

![Figure 6.6 – The Airflow UI with the example DAG
](img/B18024_06_006.jpg)

图 6.6–带有示例 DAG 的气流用户界面

1.  默认情况下，dag 处于禁用状态；因此，当您访问该页面时，您可能不会看到如图*图 6.6* 所示的确切页面。通过单击最左侧列中的切换按钮来启用 DAG。启用后，DAG 将首次运行并更新运行结果。您也可以使用**链接**栏中的图标触发 DAG。点击 UI 中 DAG 栏的 **hello_world** 超链接。您将看到带有不同选项卡的 DAG 详细信息页面。请随意在详细信息页面上体验和查看不同的选项。
2.  下图显示了 DAG 的图形视图:

![Figure 6.7 – The graph view of the DAG
](img/B18024_06_007.jpg)

图 6.7–DAG 的图形视图

1.  现在我们已经验证了气流设置正确，让我们添加气流运行 ML 管道所需的权限。
2.  如果您还记得，在环境创建的最后一步(图*6.4*之后的段落)，我们记下了气流环境用于运行 Dag 的角色名称。现在，我们需要向角色添加权限。为此，请使用搜索特征导航至 AWS IAM 角色控制台页面或访问 https://us-east-1.console.aws.amazon.com/iamv2/home?区域=美国东部-1 #/角色。在控制台中，您应该会看到与气流环境相关联的 IAM 角色。选择 IAM 角色；您应该会看到以下页面:

![Figure 6.8 – The Amazon MWAA IAM role
](img/B18024_06_008.jpg)

图 6.8-亚马逊 MWAA IAM 角色

重要说明

如果您没有做笔记，您可以在 AWS 控制台的环境详细信息页面中找到角色名称。

1.  在*图 6.8* 中，点击**上的添加权限**；从下拉菜单中，选择**附加策略**，您将进入以下页面:

![Figure 6.9 – IAM – Attach policies
](img/B18024_06_009.jpg)

图 6.9–IAM–附加策略

1.  On the web page, search and select the following policies – **AmazonS3FullAccess**, **AWSGlueConsoleFullAccess**, **AmazonRedshiftFullAccess**, and **AmazonDynamoDBFullAccess**. Once the policies are selected, scroll down and click on **Attach policies** to save the role with the new policies.

    重要说明

    无限制地分配对任何资源的完全访问权限从来都不是一个好主意。运行企业应用程序时，建议根据资源限制访问，例如对特定 S3 存储桶和 DynamoDB 表的只读访问。

    如果您在本地运行 Airflow，可以使用笔记本中的 IAM 用户凭据。

现在我们的编排系统已经准备好了，让我们看看如何使用它来生产 ML 管道。

# 批量模型流水线生产

在 [*第 4 章*](B18024_04_ePub.xhtml#_idTextAnchor065) ，*向 ML 模型添加特征库*中，为了模型训练，我们使用了特征工程笔记本摄取的特征。我们还创建了一个模型评分笔记本，它从 Feast 获取一组客户的特征，并使用训练好的模型为其运行预测。为了便于实验，我们假设原始数据的新鲜度延迟是一天。这意味着特征需要每天重新生成一次，模型需要每天根据这些特征对客户进行一次评分，并将结果存储在 S3 桶中以供消费。为了实现这一点，由于我们早期的组织和阶段分离，我们需要做的就是每天连续运行一次特征工程和模型评分笔记本/Python 脚本。现在我们也有了一个工具来执行这个任务，让我们继续在气流环境中安排这个工作流。

下图显示了我们将如何操作批量模型:

![Figure 6.10 – Operationalization of the batch model
](img/B18024_06_010.jpg)

图 6.10–批量模型的操作化

正如您在图中所看到的，为了操作工作流，我们将使用 Airflow 来编排特性工程和模型评分笔记本。在我们的例子中，特征工程的原始数据源是存储`online-retail.csv`的 S3 桶。因为我们已经设计了我们的评分笔记本来从模型回购加载生产模型(在我们的例子中，是 S3 存储桶)并将预测结果存储在 S3 存储桶中，所以我们将重用同一个笔记本。你可能注意到的一件事是，我们不是每次跑步都使用模型训练笔记本；原因是显而易见的——我们希望根据已经过验证、测试并符合测试数据性能标准的模型版本进行预测。

在安排这个工作流程之前，我对特征工程笔记本和模型预测笔记本做了一些小的改动。最终的笔记本可以在以下 GitHub 网址找到:[https://GitHub . com/packt publishing/Feature-Store-for-Machine-Learning/blob/main/chapter 06/notebooks/(ch6 _ Feature _ engineering . ipynb，ch6 _ model _ prediction . ipynb)](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter06/notebooks/(ch6_feature_engineering.ipynb,ch6_model_prediction.ipynb))。要安排工作流程，请从 GitHub 下载最终的笔记本，并将它们上传到我们之前创建的 S3 桶中，因为在运行期间，气流环境需要访问这些笔记本。我会上传到以下位置:`s3://airflow-for-ml-mar-2022/notebooks/`。

重要说明

AWS 秘密访问密钥和 S3 路径——我已经在两个笔记本中注释掉了 AWS 凭证，因为我们正在向 Amazon MWAA IAM 角色添加权限。如果你在本地气流中运行，请取消注释并添加秘密。此外，必要时更新 S3 的 URL，因为 S3 的 URL 指向我在练习中创建的私有存储桶。

Feature repo——正如我们之前看到的，我们必须克隆 feature repo，以便 Feast library 可以读取元数据。您可以遵循相同的`git clone`(假设安装了`git`)方法，或者设置 GitHub 工作流将回购推送到 S3，并将其下载到笔记本中。我已经在笔记本上留下了两个代码块和注释。你可以用任何方便的。

S3 方法—要使用 S3 下载方法，请在您的本地系统中克隆回购，并在 Linux 终端中运行以下命令，将其上传到特定的 S3 位置:

**导出 AWS _ ACCESS _ KEY _ ID =<AWS _ KEY>**

**导出 AWS _ SECRET _ ACCESS _ KEY =<AWS _ SECRET>**

**AWS _ DEFAULT _ REGION = us-east-1**

**aws s3 cp 客户 _ 细分 S3://<S3 _ bucket>/客户 _ 细分-递归**

成功上传后，您应该能够在 S3 存储桶中看到文件夹内容。

既然笔记本已经准备好了，让我们为批量模型管道编写气流 DAG。DAG 将按顺序执行以下任务—`start`(`dummy operator`)`feature_engineering`(`Papermill operator`)`model_prediction`(`Papermill operator`)和`end` ( `dummy operator`)。

以下代码块包含气流 DAG 的第一部分:

```
from datetime import datetime
```

```
from airflow import DAG
```

```
from airflow.operators.dummy_operator import DummyOperator
```

```
from airflow.providers.papermill.operators.papermill import PapermillOperator
```

```
import uuid
```

```
dag = DAG('customer_segmentation_batch_model', 
```

```
          description='Batch model pipeline', 
```

```
          schedule_interval='@daily', 
```

```
          start_date=datetime(2017, 3, 20), catchup=False)
```

在前面的代码块中，我们定义了 imports 和 DAG 参数，如`name`、`schedule_interval`和`start_date`。时间表显示 DAG 应该每天运行。

以下代码块定义了 DAG 的其余部分(第二部分)，其中包含所有任务以及它们之间的依赖关系:

```
start = DummyOperator(task_id="start", dag=dag)
```

```
run_id = str(uuid.uuid1())
```

```
feature_eng = PapermillOperator(
```

```
    task_id="feature_engineering",
```

```
    input_nb="s3://airflow-for-ml-mar-2022/notebooks/ch6_feature_engineering.ipynb",
```

```
    output_nb=f"s3://airflow-for-ml-mar-2022/notebooks/runs/ch6_feature_engineering_{ run_id }.ipynb",
```

```
    dag=dag,
```

```
    trigger_rule="all_success"
```

```
)
```

```
model_prediction = PapermillOperator(
```

```
    task_id="model_prediction",
```

```
    input_nb="s3://airflow-for-ml-mar-2022/notebooks/ch6_model_prediction.ipynb",
```

```
    output_nb=f"s3://airflow-for-ml-mar-2022/notebooks/runs/ch6_model_prediction_{run_id}.ipynb",
```

```
    dag=dag,
```

```
    trigger_rule="all_success"
```

```
)
```

```
end = DummyOperator(task_id="end", dag=dag, 
```

```
                    trigger_rule="all_success")
```

```
start >> feature_eng >> model_prediction >> end
```

正如您在代码块中看到的，有四个步骤将依次执行。使用`PapermillOperator`运行`feature_engineering`和`model_prediction`步骤。这将 S3 笔记本的路径作为输入。我还设置了到另一个 S3 位置的输出路径，这样我们就可以检查每次运行的输出记录。最后一行定义了任务之间的依赖关系。将前面的两个代码块(第一和第二部分)保存为一个 Python 文件，并将其命名为`batch-model-pipeline-dag.py`。保存文件后，导航到 S3 控制台将文件上传到我们在*图 6.3* 中指向我们的气流环境的`dags`文件夹中。上传的文件由气流调度程序处理。当您导航到 Airflow UI 时，您应该会在屏幕上看到名为**customer _ segmentation _ batch _ model**的新 DAG。

下图显示了带有 DAG 的气流用户界面:

![Figure 6.11 – The batch model DAG on Airflow
](img/B18024_06_011.jpg)

图 6.11–气流上的批次模型 DAG

由于我们在创建气流环境的过程中没有启用默认 DAG 选项(可以在亚马逊 MWAA 的气流配置变量中设置)，当 DAG 第一次出现在 UI 上时，它将被禁用。单击最左栏的切换按钮将其启用。启用后，DAG 将首次运行。单击**customer _ segmentation _ batch _ model**超链接导航到详细信息页面，并随意查看 DAG 的不同可视化和属性。如果您导航到**图形**选项卡，将显示 DAG，如以下截图所示:

![Figure 6.12 – The batch model DAG graph view
](img/B18024_06_012.jpg)

图 6.12–批处理模型 DAG 图形视图

在图 6.12 中，您可以看到 DAG 的图形视图。如果在上次运行中有任何失败，它们将出现在红色轮廓中。您还可以查看每个任务成功执行或失败的日志。由于所有的任务都是绿色的，这意味着一切进展顺利。你也可以在*图 6.11* 中看到最后几次运行的结果。Airflow 还为您提供了所有跑步的历史记录。

现在任务运行已完成，我们可以查看输出笔记本、新要素集的 S3 存储桶或新预测集的 S3 存储桶。在成功运行之后，这三个都应该可用。这里，我们将只验证预测结果文件夹，但也可以随意验证其他文件夹。

重要说明

如果出现任何失败，请验证失败任务的日志(在图形视图中单击失败的任务以查看可用信息)。检查亚马逊 MWAA 的权限、输入/输出的 S3 路径，以及是否所有的需求都安装在亚马逊 MWAA 环境中。

以下屏幕截图显示了 S3 时段中的新预测结果:

![Figure 6.13 – The prediction results in an S3 bucket
](img/B18024_06_013.jpg)

图 6.13–S3 桶中的预测结果

除此之外，你还可以用气流做各种花哨的事情，比如发送失败的邮件通知，日常运行的 Slack 通知，以及与 PagerDuty 的集成。请随意探索这些选项。下面是一个air flow 中支持的提供者列表:[https://air flow . Apache . org/docs/Apache-air flow-providers/packages-ref . html](https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.html)。

现在我们的批处理模型已经在生产中运行了，让我们看看如何使用 Feast 将在线模型生产化。

# 制作在线模型管道

在上一章中，对于在线模型，我们构建了 REST 端点来为客户细分提供按需预测服务。尽管在线模型作为 REST 端点托管，但它需要一个支持以下特征的基础设施:

*   实时提供特征服务(我们为此举办了Feast)
*   保持特征最新(为此，我们将使用带有气流协调的特征工程笔记本)

在本章中，我们将从我们停止的地方继续，并使用第 4 章*中内置的 [*特征工程笔记本，将特征存储添加到 ML 模型中，*与笔记本结合使用，将离线数据同步到 Feast 中的在线存储。](B18024_04_ePub.xhtml#_idTextAnchor065)*

下图显示了在线模型管道的操作化:

![Figure 6.14 – The operationalization of the online model
](img/B18024_06_014.jpg)

图 6.14–在线模型的可操作性

在*图 6.14* 中可以看到，我们将使用气流进行特征工程的编排；这里的数据新鲜度还是一天，可以做更短时长的调度。如果需要，Feast 还可以支持流数据。下面的网址有一个你可以使用的例子:[https://docs.Feast.dev/reference/data-sources/push](https://docs.Feast.dev/reference/data-sources/push)。在 [*第 5 章*](B18024_05_ePub.xhtml#_idTextAnchor078) 、*模型训练和推理*中开发的其余端点将作为 SageMaker 端点进行 docker 化和部署。

重要说明

一旦 Docker 化，Docker 映像就可以用于部署到任何容器化的环境中，比如 Elastic Container Service、Elastic BeanStalk 和 Kubernetes。我们使用 SageMaker，因为它的设置时间更短，并且具有开箱即用的数据捕获和 IAM 认证等优势。

## 特征工程作业的编排

由于我们已经有两台笔记本电脑(特征工程和离线同步到在线存储),并且我们熟悉 Airflow，所以让我们先安排特征工程工作流程。还是那句话，在笔记本上，我做了一些小改动。请在使用前验证更改。你可以在这里找到笔记本(`ch6_feature_engineering.ipynb` a [和`ch6_sync_offline_online.ipynb`):https://github.com/PacktPublishing/Feature-Store-for-Machin](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter06/notebooks)电子学习/tree/main/Chapter06/notebooks。就像我们对批量模型所做的那样，下载笔记本并上传到特定的 S3 位置。我会把它们上传到和以前一样的地方:`s3://airflow-for-ml-mar-2022/notebooks/`。现在笔记本已经准备好了，让我们为在线模型管道编写气流 DAG。DAG 将按顺序执行以下步骤—`start`(`dummy operator`)`feature_engineering`(`Papermill operator`)`sync_offline_to_online`(`Papermill operator`)和`end` ( `dummy operator`)。

以下代码块包含气流 DAG 的第一部分:

```
from datetime import datetime
```

```
from airflow import DAG
```

```
from airflow.operators.dummy_operator import DummyOperator
```

```
from airflow.providers.papermill.operators.papermill import PapermillOperator
```

```
dag = DAG('customer_segmentation_online_model', 
```

```
          description='Online model pipeline', 
```

```
          schedule_interval='@daily', 
```

```
          start_date=datetime(2017, 3, 20), catchup=False)
```

就像批处理模型管道 DAG 的情况一样，它包含 DAG 参数。

以下代码块定义了 DAG 的其余部分(第二部分)，其中包含所有任务以及它们之间的依赖关系:

```
start = DummyOperator(task_id="start")
```

```
run_time = datetime.now()
```

```
feature_eng = PapermillOperator(
```

```
    task_id="feature_engineering",
```

```
    input_nb="s3://airflow-for-ml-mar-2022/notebooks/ch6_feature_engineering.ipynb",
```

```
    output_nb=f"s3://airflow-for-ml-mar-2022/notebooks/runs/ch6_feature_engineering_{run_time}.ipynb",
```

```
    trigger_rule="all_success",
```

```
    dag=dag
```

```
)
```

```
sync_offline_to_online = PapermillOperator(
```

```
    task_id="sync_offline_to_online",
```

```
    input_nb="s3://airflow-for-ml-mar-2022/notebooks/ch6_sync_offline_online.ipynb",
```

```
    output_nb=f"s3://airflow-for-ml-mar-2022/notebooks/runs/ch6_sync_offline_online_{run_time}.ipynb",
```

```
    trigger_rule="all_success",
```

```
    dag=dag
```

```
)
```

```
end = DummyOperator(task_id="end", trigger_rule="all_success")
```

```
start >> feature_eng >> sync_offline_to_online >> end
```

气流 DAG 的结构类似于我们之前看的批量模型 DAG；唯一不同的是第三个任务，`sync_offline_to_online`。这款笔记本将最新特征从离线数据同步到在线数据。将前面的两个代码块(第一和第二部分)保存为一个 Python 文件，并将其命名为`online-model-pipeline-dag.py`。保存文件后，导航到 S3 控制台将文件上传到`dags`文件夹，我们在*图 6.3* 中将气流环境指向该文件夹。与批处理模型一样，上传的文件由 Airflow 调度程序处理，当您导航到 Airflow UI 时，您应该在屏幕上看到名为**customer _ segmentation _ online _ model**的新 DAG。

下面的截图显示了带有 DAG 的气流 UI:

![Figure 6.15 – The Airflow UI with both the online and batch models
](img/B18024_06_015.jpg)

图 6.15–在线和批量模型的气流用户界面

要启用 DAG，请单击最左侧栏中的切换按钮。启用后，DAG 将首次运行。单击**customer _ segmentation _ online _ model**超链接导航到详细信息页面，随意查看 DAG 的不同可视化和属性。如果您导航到**图形**选项卡，将显示 DAG，如以下截图所示:

![Figure 6.16 – The online model pipeline graph view
](img/B18024_06_016.jpg)

图 6.16–在线模型管道图视图

正如您在*图 6.16* 中看到的，在成功运行时，图表将为绿色。正如在批处理模型管道执行期间所讨论的，您可以验证输出笔记本、DynamoDB 表或 S3 桶，以确保一切工作正常，还可以在出现故障时检查日志。

现在，在线模型管道的第一部分已经准备好了，让我们将前一章开发的 REST 端点进行 Dockerize，并将其部署为 SageMaker 端点。

## 将模型部署为 SageMaker 端点

为了将模型部署到 SageMaker，我们需要首先将我们在 [*第 5 章*](B18024_05_ePub.xhtml#_idTextAnchor078) 、*模型训练和推理*中构建的 REST API 进行 dockerize。在此之前，让我们创建一个**弹性容器注册中心** ( **ECR** )，在这里我们可以保存模型的 Docker 图像并在 SageMaker 端点配置中使用它。

### Docker 图像的 ECR

要创建 ECR 资源，请从搜索栏导航至 ECR 控制台或使用以下 URL:[https://us-east-1.console.aws.amazon.com/ecr/repositories?地区=美国东部-1](https://us-east-1.console.aws.amazon.com/ecr/repositories?region=us-east-1) 。将显示以下页面:

![Figure 6.17 – The ECR home page
](img/B18024_06_017.jpg)

图 6.17–ECR 主页

在*图 6.17* 显示的页面上，可以选择**私有**或者**公共**存储库选项卡。然后，点击**创建存储库**按钮:

![Figure 6.18 – ECR – Create repository
](img/B18024_06_018.jpg)

图 6.18–ECR–创建存储库

我在这里选择了**私有**；取决于你是选择**私有**还是**公共**，选项将会改变，但无论哪种方式，都很简单。填写必填字段，向下滚动所有的，并点击**创建存储库**。一旦创建了存储库，进入存储库详细信息页面，您应该会看到类似于图 6.19 所示的页面。

重要说明

私有存储库由 IAM 保护，而公共存储库可以被互联网上的任何人访问。公共存储库主要用于与组织外的其他人共享/开源您的工作:

![ Figure 6.19 – ECR repository details
](img/B18024_06_019.jpg)

图 6.19–ECR 储存库详情

在上一页中，点击**查看推送命令**，您应该会看到一个弹出窗口，类似于图 6.20 中的*所示:*

![Figure 6.20 – ECR push commands
](img/B18024_06_020.jpg)

图 6.20–ECR 推送命令

根据您用于构建 Docker 映像的操作系统，保存必要的命令。我们将使用这些命令来构建 Docker 映像。

### 建立码头工人形象

如前所述，我们将在本节中使用前一章构建的 REST 端点。如果您记得没错，我们添加了两个 REST 端点，`ping`和`invocations`。这些端点不是随机的，尽管它们可以在任何容器环境中托管。要在 SageMaker 端点中托管 Docker 映像，要求是它应该有`ping`(这是`GET`方法)和`invocations`(这是`POST`方法)路由。我已经在同一个文件夹结构中添加了几个文件，这对构建 Docker 映像很有用。REST 代码和文件夹结构可从以下网址获得:[https://github . com/packt publishing/Feature-Store-for-Machine-Learning/tree/main/online-model-REST-API](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/online-model-rest-api)。

重要说明

附加文件有`Dockerfile`、`requirements.txt`和`serve`。

依次将其余代码克隆到本地系统，将特征库复制到项目的`root`目录中，导出凭证，然后运行*图 6.20* 中的命令。

重要说明

您可以使用在 [*第 4 章*](B18024_04_ePub.xhtml#_idTextAnchor065) 、*向 ML 模型添加特征库*中创建的相同用户凭证。然而，我们错过了向用户添加 ECR 权限。请导航到 IAM 控制台，并将**amazonec 2 containerregistrytryfull access**添加到用户。否则，您将得到一个访问错误。

以下是命令示例:

```
cd online-model-rest-api/
export AWS_ACCESS_KEY_ID=<AWS_KEY>
export AWS_SECRET_ACCESS_KEY=<AWS_SECRET>
export AWS_DEFAULT_REGION=us-east-1
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin <account_number>.dkr.ecr.us-east-1.amazonaws.com
docker build -t customer-segmentation .
docker tag customer-segmentation:latest <account_number>.dkr.ecr.us-east-1.amazonaws.com/customer-segmentation:latest
docker push <account_number>.dkr.ecr.us-east-1.amazonaws.com/customer-segmentation:latest
```

这些命令使用环境中设置的凭据登录到 ECR，构建 Docker 映像，标记 Docker 映像并将其推送到注册表。一旦图像被推送，如果您导航回*图 6.19* 中的屏幕，您应该会看到新的图像，如以下截图所示:

![Figure 6.21 – ECR with the pushed image
](img/B18024_06_021.jpg)

图 6.21–带有推送图像的 ECR

现在图像准备好了，通过点击**复制 URI** 旁边的图标复制图像**统一资源标识符(URI)** ，如图*图 6.21* 所示。接下来让我们将 Docker 映像部署为 SageMaker 端点。

### 创建 SageMaker 端点

Amazon SageMaker 的目标是为 ML 提供托管基础设施。在本节中，我们将只使用 SageMaker 推理组件。SageMaker 端点用于将模型部署为实时预测的 REST 端点。它支持 Docker 图像模型，也支持一些现成的风格。我们将使用我们在上一节中推入 ECR 的 Docker 映像。SageMaker 端点使用三个构建模块构建，即模型、端点配置和端点。让我们使用这些构建块，然后创建一个端点。

#### SageMaker 模型

模型用于定义模型参数，如模型的名称、位置和 IAM 角色。要定义模型，使用搜索栏导航到 SageMaker 控制台，并在**推理**部分中查找`Models`。或者，参观 https://us-east-1.console.aws.amazon.com/sagemaker/home?区域=美国东部-1 #/车型。将显示以下画面:

![Figure 6.22 – The SageMaker Models console
](img/B18024_06_022.jpg)

图 6.22–sage maker 型号控制台

在显示的页面上，点击**创建模型**导航至下一屏幕。将显示以下页面:

![Figure 6.23 – SageMaker – Create model
](img/B18024_06_023.jpg)

图 6.23–sage maker–创建模型

如图*图 6.23* 所示，输入型号名称，对于 IAM 角色，在下拉菜单中选择**新建角色**。将出现一个新的弹出窗口，如下面的屏幕截图所示:

![Figure 6.24 – The SageMaker model – Create an IAM role
](img/B18024_06_024.jpg)

图 6.24–sage maker 模型–创建一个 IAM 角色

在弹出窗口中，出于本练习的目的，将保留为默认值，并单击**创建角色**。AWS 将创建一个 IAM 角色，在同一个屏幕上，您应该会在对话框中看到一条消息，其中包含一个指向 IAM 角色的链接。下图显示了显示的消息:

![Figure 6.25 – The SageMaker model – the new execution role is created
](img/B18024_06_025.jpg)

图 6.25–sage maker 模型–新的执行角色被创建

现在，如果你没记错的话，我们使用 DynamoDB 作为在线存储；当我们按需从 DynamoDB 表中读取数据时，IAM 角色需要访问它们。因此，使用页面上显示的链接在新选项卡中导航到我们刚刚创建的 IAM 角色，添加**amazondynamodbullaccess**，然后返回到该选项卡。向下滚动到**容器定义 1** 部分，在这里您会看到以下参数:

![Figure 6.26 – The SageMaker model – the Container definition 1 section 
](img/B18024_06_026.jpg)

图 6.26–sage maker 模型–容器定义 1 部分

对于推理代码图像参数的**位置，粘贴我们从屏幕上复制的图像 URI，如图*图 6.21* 所示。将其他选项保留为默认值，并再次滚动到**网络**部分:**

![Figure 6.27 – The Sagemaker Model – the Network section
](img/B18024_06_027.jpg)

图 6.27-sage maker 模型-网络部分

在这里，选择 **VPC** 到**默认 vpc** ，从列表中选择一到两个子网，选择默认安全组。向下滚动到底部，点击**创建模型**。

重要说明

为生产部署选择默认安全组从来都不是一个好主意，因为入站规则没有限制。

既然模型已经准备好了，接下来让我们创建端点配置。

#### 端点配置

要设置端点配置，使用搜索栏导航到 SageMaker 控制台，并在**推理**部分中查找`Endpoint Configurations`。或者，参观 https://us-east-1.console.aws.amazon.com/sagemaker/home?region = us-east-1 #/endpoint config。将显示以下页面:

![Figure 6.28 – The Sagemaker Endpoint configuration console
](img/B18024_06_028.jpg)

图 6.28–sage maker 端点配置控制台

在显示的网页上，点击**创建端点配置**。您将被导航至以下页面:

![Figure 6.29 – SageMaker – Create endpoint configuration
](img/B18024_06_029.jpg)

图 6.29–sage maker–创建端点配置

在这个屏幕上，填写`customer-segmentation-config`。向下滚动到**数据采集**部分。这用于定义需要捕获的实时推理数据的百分比、位置(S3 位置)以及存储方式(JSON 或 CSV)。您可以选择启用或禁用它。在本练习中，我将其禁用。如果您启用它，它会要求您提供更多信息。在**数据采集**之后的部分是**生产变量**。这用于设置多个模型变量，以及模型的 A/B 测试。现在，因为我们只有一个变体，让我们在这里添加它。要添加变型，点击该部分中的**添加型号**链接；将出现以下弹出窗口:

![Figure 6.30 – SageMaker – adding a model to the endpoint config
](img/B18024_06_030.jpg)

图 6.30–sage maker–向端点配置添加模型

在弹出菜单中，选择我们之前创建的模型，向下滚动，并点击**创建端点配置**。

#### SageMaker 端点创建

最后一步是使用端点配置来创建一个端点。要创建 SageMaker 端点，使用搜索栏导航到 SageMaker 控制台，并在**推理**部分中查找`Endpoints`。或者，参观 https://us-east-1.console.aws.amazon.com/sagemaker/home?区域=美国东部-1 #/端点。将显示以下页面:

![Figure 6.31 – The SageMaker Endpoints console
](img/B18024_06_031.jpg)

图 6.31–sage maker 端点控制台

在*图 6.31* 所示的页面上，点击**创建端点**进入以下页面:

![Figure 6.31 – SageMaker – creating an endpoint
](img/B18024_06_032.jpg)

图 6.31–sage maker–创建端点

在*图 6.31* 显示的网页上，提供一个端点名称。我给了这个名字`customer-segmentation-endpoint`。向下滚动到**端点配置**部分，选择我们之前创建的端点配置，并点击**选择端点配置**按钮。一旦选中，点击**创建端点**。创建一个端点需要几分钟时间。当端点状态更改为**可用**时，您的模型将为实时流量提供服务。

#### 测试 SageMaker 端点

接下来我们需要知道的是如何消费模型。有不同的方法——您可以使用 SageMaker 库、Amazon SDK 客户端(Python、TypeScript 或任何其他可用的软件)或 SageMaker 端点 URL。所有这些方法都默认为 AWS IAM 身份验证。如果您有特殊的需求，并且想要在没有身份验证的情况下或者使用自定义身份验证的情况下公开模型，那么可以使用 API gateway 和 Lambda authorizer 来实现。出于本练习的目的，我们将使用`boto3`客户端来调用 API。不管我们如何调用端点，结果应该是相同的。

下面的代码块使用`boto3`客户端调用端点:

```
import json
```

```
import boto3
```

```
import os
```

```
os.environ["AWS_ACCESS_KEY_ID"] = "<aws_key>"
```

```
os.environ["AWS_SECRET_ACCESS_KEY"] = "<aws_secret>"
```

```
os.environ["AWS_DEFAULT_REGION"] = "us-east-1"
```

```
payload = json.dumps({"customer_list":["12747.0", "12841.0"]})
```

```
runtime = boto3.client("runtime.sagemaker")
```

```
response = runtime.invoke_endpoint(
```

```
    EndpointName= "customer-segmentation-endpoint", 
```

```
    ContentType="application/json", Body=payload
```

```
)
```

```
response = response["Body"].read()
```

```
result = json.loads(response.decode("utf-8"))
```

```
print(results)
```

在前面的代码块中，我们调用我们创建的端点来为 id 为`12747.0`和`12841.0`的两个客户运行预测。端点将在几毫秒内响应给定客户 id 的预测。现在，端点可以与模型消费者共享。

既然模型已经进入生产阶段，那么让我们来看看模型进入生产阶段后的几个方面。

# 超越模式生产

在这一部分，我们将讨论 ML 的后期制作方面，以及我们如何从采用特征存储中获益。

## 特征漂移监控和模型再训练

一旦模型投入生产，下一个经常出现的问题是该模型在生产中的表现如何。可能有不同的指标用于衡量模型的性能–例如，对于推荐模型，性能可以通过转化率来衡量，即推荐产品的购买频率。同样，预测一个客户的下一步行动，可能用错误率来衡量，等等。做这件事没有通用的方法。但是如果一个模型表现不好，就需要重新训练或者换一个新的。

定义模型何时应该被重新训练的另一个方面是特征何时开始偏离它被训练的值。例如，假设客户在初始模型训练期间的平均频率值是 10，但是现在，平均频率值是 25。同样，最低货币价值最初是 100.00 美元，现在是 500.00 美元。这就是所谓的**数据漂移**。

数据漂移监控测量数据统计分布的变化；在特征监控的情况下，它比较从 *t1* 时间到 *t2* 时间的特征统计分布的变化。位于以下 URL 的文章讨论了数据漂移监控的不同指标:[https://towards data science . com/automating-data-drift-thresholding-in-machine-learning-systems-524 e 6259 f 59 f](https://towardsdatascience.com/automating-data-drift-thresholding-in-machine-learning-systems-524e6259f59f)。

使用特征库，很容易从两个不同的时间点检索训练数据集，即用于模型训练的数据集和用于模型训练的所有特征的最新特征值。现在，我们需要做的就是按计划运行数据漂移监控，以生成漂移报告。Feast 带来的标准化是，由于数据是使用标准 API 存储和检索的，因此可以针对要素存储中的所有数据集按计划运行通用要素漂移监控。特征漂移报告可以作为模型再训练的指标之一。如果特性漂移影响了模型的性能，可以使用最新的数据集对其进行重新训练，并使用当前的生产模型进行部署和 AB 测试。

## 模型再现性和预测问题

如果你从 [*回忆起第一章*](B18024_01_ePub.xhtml#_idTextAnchor014)*机器学习生命周期的概述*，模型再现性是 ML 的常见问题之一。我们需要一种方法来一致地重现模型(或用于模型的训练数据)。在没有特征库的情况下，如果用于生成特征的基础原始数据发生变化，则不可能再现相同的训练数据集。但是，对于要素存储，正如我们之前所讨论的，要素使用时间戳进行版本化(要素数据帧中的一列是事件时间戳)。因此，我们可以查询历史数据来生成用于模型训练的相同特征集。如果用于训练该模型的算法不是随机的，则该模型也可以被复制。让我们试试这个。

由于我们已经在第五章 、*模型训练和推理*的*模型训练与特征库*部分做了类似的事情，我们将重用相同的代码来运行这个实验。复制并运行所有代码，直到创建实体数据帧，然后用一个旧的时间戳(模型被训练时的时间戳)替换`event_timestamp`列，如下所示。在这种情况下，模型在`2022-03-26 16:24:21`被训练，如 [*的*图 5.1* 第五章*](B18024_05_ePub.xhtml#_idTextAnchor078) 、*模型训练与推理*所示:

```
## replace timestamp to older time stamp.
```

```
entity_df["event_timestamp"] = pd.to_datetime("2022-03-26 16:24:21")
```

一旦你完成替换时间戳，继续运行第五章 、*模型训练和推理*的*迪伊模型训练实验*部分的代码。您应该能够生成在 Dee 的模型训练中使用的完全相同的数据集(在这种情况下，数据集在*图 5.2* 的 [*第 5 章*](B18024_05_ePub.xhtml#_idTextAnchor078) 、*模型训练和推理*)。因此，如果模型使用非随机算法，那么模型也可以使用特征集来再现。

特性库的另一个优势是调试预测问题。让我们考虑一个场景，其中您有一个面向网站的模型，该模型将交易分类为欺诈或非欺诈。在高峰时段，它将一些交易标记为欺诈，但这些交易是合法的。客户打电话到客服部门投诉，现在轮到数据科学家 Subbu 找出问题所在。如果项目中没有特征存储，要重现该问题，Subbu 必须进入原始数据，尝试生成特征，并查看行为是否仍然保持不变。如果没有，Subbu 将不得不进入应用程序日志，处理它，在事件发生前寻找用户行为，尝试从用户交互的角度再现它，并捕获所有这些试验的特征，希望该问题至少可以再现一次。

另一方面，利用项目中使用的特性库，Subbu 将计算出事件发生的大致时间、模型中使用的实体和特性，以及事件发生时生产中运行的模型版本。有了这些信息，Subbu 将连接到特征库，并获取问题发生时大致时间范围内所有实体的模型中使用的所有特征。假设事件发生在今天中午 12:00 到 12:15 之间，特征是流式的，刷新间隔大约为 30 秒。这意味着，平均而言，对于给定的实体，从任何给定的时间开始，在接下来的 30 秒内，特征有可能会发生变化。

为了重现该问题，Subbu 将形成一个实体数据帧，其中相同的实体 ID 在一列中重复 30 次，对于事件时间列，时间戳从 12:00pm 到 12:15pm，间隔为 30 秒。有了这个实体数据帧，Subbu 将使用 Feast API 查询历史存储，并对生成的要素运行预测。如果问题重现，Subbu 具有导致该问题的特征集。如果没有，使用实体数据帧，间隔将减少到小于 30 秒，可能减少到 10 秒，以确定特征变化的速度是否快于 30 秒。Subbu 可以继续这样做，直到她找到重现问题的特征集。

## 下一个模型的开端

现在模型已经生产出来，数据科学家 Subbu 开始处理下一个问题陈述。让我们假设下一个 ML 模型必须预测顾客的**下一个购买日** ( **NPD** )。这里的用例可能是，基于 NPD，我们想要为客户开展一项活动。如果客户的购买日在未来很久，我们想提供一个特殊的交易，这样我们可以鼓励购买更快。现在，在进入原始数据集之前，Subbu 可以根据搜索和可发现性如何集成到要素存储中来寻找可用的要素。由于 Feast 从面向服务转向面向 SDK/CLI，因此需要目录工具、包含所有特性库的 GitHub 库、数据网格门户等等。但是，对于 SageMaker 或 Databricks 等特征存储，用户可以连接到特征存储端点(使用 boto3 或 Databricks 工作区的 SageMaker 运行时),并使用 API 或从 UI 浏览可用的特征定义。我以前没有使用过 Tecton 特性库，但是 Tecton 也为它的特性库提供了一个 UI，可以用来浏览可用的特性。如你所见，这是 0.9.X 和 0.20.X 之间的不同版本的 Feast 的弊端之一(0.20 是编写时的版本)。

现在，让我们假设 Subbu 有办法定位所有的特性库。现在，她可以连接并浏览它们，找出在 NPD 模型中有用的项目和特性定义。到目前为止，我们只有一个特性存储库，其中包含我们一直在使用的客户 RFM 特性，这些特性在模型中非常有用。要使用这些特性，Subbu 所要做的就是获得对 AWS 资源的读取权限，最新的 RFM 特性每天都可以用于实验，如果模型进入生产，也可以使用这些特性。

为了了解特性存储在后续模型开发过程中的益处，我们应该尝试构建 NPD。我将通过最初的几个步骤让您开始使用这个模型。正如我们在第一个模型的开发过程中关注的博客一样，我们将关注同一博客系列的另一部分，可以在[https://towards data science . com/predicting-next-purchase-day-15 FAE 5548027](https://towardsdatascience.com/predicting-next-purchase-day-15fae5548027)找到。请通读这篇博客，因为它讨论了这种方法，以及为什么作者认为特定的特性会有用。在这里，我们将跳到特性工程部分。

我们将使用博客作者使用的特征集，包括以下内容:

*   RFM 特征和集群
*   最近三次购买之间的天数
*   采购差异的平均值和标准偏差

第一个特征集已经存在于特征库中；我们不需要为此做任何额外的事情。但是对于另外两个，我们需要从原始数据中进行特征工程。位于[https://github . com/packt publishing/Feature-Store-for-Machine-Learning/blob/main/chapter 06/notebooks/ch6 _ next _ purchase _ day _ Feature _ engineering . ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter06/notebooks/ch6_next_purchase_day_feature_engineering.ipynb)的笔记本具有生成前述第二和第三要点中的特征所需的特征工程。我将把这些特性吸收到一个特性库中，并结合使用以前模型中的特性(RFM)来训练一个新模型。当您开发和生产这个模型时，您将会看到特征库的好处，以及它是如何加速模型构建的。

接下来，让我们讨论如何在模型处于生产状态时更改特征定义。

## 生产后特征定义的变化

到目前为止，我们已经讨论了开发阶段的特性摄取、查询和特性集的变更。然而，我们还没有谈到模型在生产中对特性定义的改变。通常，一旦模型进入生产阶段，改变特征定义是很困难的。这样做的原因是，有可能多个模型正在使用特性定义，对它们的任何更改都会对模型产生级联效应。这也是为什么一些特性库还不支持特性定义更新的原因之一。我们需要一种方法来有效地处理这种变化。

这还是一个灰色地带；做这件事没有对错之分。我们可以采用我们在其他软件工程过程中使用的任何机制。一个简单的方法是特性视图的版本控制，类似于我们处理 REST APIs 或 Python 库的方式。每当生产特性集需要变更时，假设它正被其他人使用，一个新版本的特性视图(姑且称之为`customer-segemantion-v2`)将被创建和使用。然而，在所有模型迁移之前，仍然需要管理先前的版本。如果，由于某种原因，有需要旧版本的模型，并且不能迁移到新版本的特性表/视图，那么它可能必须被管理或者移交给需要它的团队。需要对特征和特征工程作业的所有权进行一些讨论。

这就是数据作为产品的概念非常有意义的地方。这里缺少的是生产者和消费者定义合同和通知变化的框架。数据生产者需要一种发布其数据产品的方式；在这里，数据产品是特征视图。产品的消费者可以订阅数据产品并使用它。在特征集更改期间，生产者可以定义数据产品的新版本，并降低旧版本的价值，以便消费者可以得到有关更改的通知。这只是我对一个解决方案的看法，但是我确信有更好的人可能已经在实施另一个解决方案了。

说到这里，让我们总结一下本章所学的内容，然后进入下一章。

# 总结

在这一章中，我们的目标是使用我们在前面章节中构建的所有东西，并为批处理和在线用例生产 ML 模型。为此，我们创建了一个亚马逊 MWAA 环境，并将其用于批处理模型管道的编排。对于在线模型，我们使用 Airflow 来编排特征工程管道和 SageMaker 推理组件，以部署 Docker 在线模型作为 SageMaker 端点。我们研究了特征存储如何促进 ML 的后期制作方面，例如特征漂移监控、模型再现性、调试预测问题，以及如何在模型生产时更改特征集。我们还研究了数据科学家如何通过使用特征库在新模型上取得领先。到目前为止，我们在所有的练习中都使用了 Feast 在下一章，我们将看看市场上的一些特征存储，以及它们与 Feast 的不同之处，还有一些例子。