

# 一、机器学习生命周期概述

**机器学习** ( **ML** )是计算机科学的子领域，涉及研究和探索能够使用统计分析学习数据结构的计算机算法。用于学习的数据集称为训练数据。训练的输出被称为模型,然后可以用来对模型以前没有见过的新数据集进行预测。机器学习有两大类:**监督学习**和**非监督学习**。在监督学习中，训练数据集被标记(数据集将有一个目标列)。该算法旨在学习如何根据数据集中的其他列(特征)来预测目标列。预测房价、股市变化和客户流失是一些监督学习的例子。另一方面，在无监督学习中，数据没有被标记(数据集没有目标列)。在这种情况下，该算法旨在识别数据集中的常见模式。为无标签数据集生成标签的方法之一是使用无监督学习算法。异常检测是无监督学习的用例之一。

1943 年，沃尔特·皮茨和沃伦·麦卡洛克提出了机器学习的第一个数学模型的想法。–[https://label yourdata . com/articles/history-of-machine-learning-how-do-it-all-start](https://labelyourdata.com/articles/history-of-machine-learning-how-did-it-all-start))。后来，在 20 世纪 50 年代，亚瑟·塞缪尔开发了一个玩冠军级别的计算机跳棋的程序。从那以后，我们在曼梯里走过了漫长的道路。如果你还没有读过这篇文章，我强烈推荐你去读一读。

今天，当我们试图向系统和设备教授实时决策时，ML 工程师和数据科学家职位是市场上最热门的工作。据预测，到 2027 年，全球机器学习市场将从 2019 年的 83 亿美元增长到 1179 亿美元。如下图所示，这是一个独特的技能集，与多个领域重叠:

![Figure 1.1 – ML/data science skill sets
](img/B18024_01_01.jpg)

图 1.1–ML/数据科学技能集

在 2007 年和 2008 年，DevOps 运动彻底改变了软件开发和操作的方式。它缩短了软件的生产时间:

![Figure 1.2 – DevOps 
](img/B18024_01_02.jpg)

图 1.2–DevOps

类似地，要将一个模型从实验转化为操作，我们需要一套标准化的流程，使这个流程无缝衔接。嗯，答案是**机器学习操作** ( **MLOps** )。业内的许多专家已经发现了一套可以缩短 ML 模型生产时间的模式。2021 年是 MLOps 年——有许多新的初创企业正在努力迎合那些仍落后于 ML 旅程的公司的 ML 需求。我们可以假设这将随着时间的推移而扩大，只会变得更好，就像任何其他过程一样。随着我们与它一起成长，将会有许多发现和工作方式、最佳实践，以及更多将会发展。在这本书里，我们将讨论一个用来标准化 ML 及其最佳实践的常用工具:特性库。

在我们讨论什么是特性库以及如何使用它之前，我们需要了解 ML 的生命周期及其常见的疏忽。我想用这一章来学习 ML 生命周期的不同阶段。作为本章的一部分，我们将进行一个 ML 模型构建练习。我们不会深究 ML 模型本身，比如它的算法或者如何做特征工程；相反，我们将把重点放在一个 ML 模型通常会经历的阶段，以及模型建立与模型操作化所涉及的困难。我们还将讨论耗时且重复的阶段。本章的目标是理解整个 ML 生命周期和操作模型所涉及的问题。这将为后面的章节做准备，我们将讨论特性管理，特性库在 ML 中的作用，以及特性库如何解决我们将在本章讨论的一些问题。

在本章中，我们将讨论以下主题:

*   实践中的 ML 生命周期
*   理想世界与现实世界的对比
*   ML 最耗时的阶段

事不宜迟，让我们用一个 ML 模型来弄脏我们的手。

# 技术要求

要理解本书中的代码示例，您需要熟悉 Python 和任何笔记本环境，这可以是 Jupyter 之类的本地设置，也可以是 Google Colab 或 Kaggle 之类的在线笔记本环境。我们将使用 Python3 解释器和 PIP3 来管理虚拟环境。你可以从下面的 GitHub 链接下载本章的代码示例:[https://GitHub . com/packt publishing/Feature-Store-for-Machine-Learning/tree/main/chapter 01](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter01)。

# 实践中的 ML 生命周期

正如杰夫·丹尼尔在 HBO 的《新闻编辑室》中扮演的角色曾经说过的，解决任何问题的第一步是认识到问题的存在。让我们跟随这些知识，看看它是否对我们有用。

在本节中，我们将挑选一个问题陈述，并逐步执行 ML 生命周期。完成后，我们将回顾并找出任何问题。下图显示了 ML 的不同阶段:

![Figure 1.3 – The ML life cycle
](img/B18024_01_03.jpg)

图 1.3–ML 生命周期

让我们看看我们的问题陈述。

## 问题陈述(计划和创建)

在这个练习中，让我们假设你拥有一家零售企业，想要改善客户体验。首先，你要找到你的客户群和客户**终身价值(LTV)** 。如果您在这个领域工作过，您可能知道解决这个问题的不同方法。我将关注一个名为*了解你的度量标准——了解如何使用 Python* 跟踪什么以及如何跟踪的中型博客系列，作者是 Bar Karaman(https://towards data science . com/data-driven-growth-with-Python-part-1-Know-Your-Metrics-812781 e66a 5b)。你可以通过文章了解更多细节。你可以自己尝试一下。数据集可在此获得:[https://www.kaggle.com/vijayuv/onlineretail](https://www.kaggle.com/vijayuv/onlineretail)。

## 数据(准备和清洗)

首先，让安装`pandas`包:

```
!pip install pandas
```

让我们将数据集用于我们的笔记本环境。为此，将数据集下载到您的本地系统，然后根据您的设置执行以下任一步骤:

*   文件，并将其作为输入提供给`pd.read_csv`方法。
*   **Google Colab** :点击文件夹图标，然后点击左侧导航菜单中的上传图标，上传数据集。

让我们预览数据集:

```
import pandas as pd
```

```
retail_data = pd.read_csv('/content/OnlineRetail.csv', 
```

```
                          encoding= 'unicode_escape')
```

```
retail_data.sample(5)
```

前面代码块的输出如下:

![Figure 1.4 – Dataset preview
](img/B18024_01_04.jpg)

图 1.4-数据集预览

如您所见，数据集包括客户交易数据。数据集由八列组成，除了未标记的索引列之外还有:

*   `InvoiceNo`:唯一的订单 ID；数据属于`integer`类型
*   `StockCode`:产品的唯一标识；数据属于`string`类型
*   `Description`:产品的描述；数据属于`string`类型
*   `Quantity`:已订购产品的单位数量
*   `InvoiceDate`:发票生成的日期
*   `UnitPrice`:单位产品的成本
*   `CustomerID`:订购产品的客户的唯一 ID
*   `Country`:订购产品的国家

一旦有了数据集，在进入特征工程和模型构建之前，数据科学家通常会执行一些探索性分析。这里的想法是检查您拥有的数据集是否足以解决问题，识别缺失的空白，检查数据集中是否有任何相关性，等等。

在本练习中，我们将计算月收入并查看其季节性。下面的代码块从`InvoiceDate`列中提取年和月(`yyyymm`)信息，通过将`UnitPrice`和`Quantity`列相乘来计算每个交易的`revenue`属性，并根据提取的年-月(`yyyymm`)列来合计收入。

让我们从前面的代码语句继续:

```
##Convert 'InvoiceDate' to of type datetime
```

```
retail_data['InvoiceDate'] = pd.to_datetime(
```

```
    retail_data['InvoiceDate'], errors = 'coerce')
```

```
##Extract year and month information from 'InvoiceDate'
```

```
retail_data['yyyymm']=retail_data['InvoiceDate'].dt.strftime('%Y%m')
```

```
##Calculate revenue generated per order
```

```
retail_data['revenue'] = retail_data['UnitPrice'] * retail_data['Quantity']
```

```
## Calculate monthly revenue by aggregating the revenue on year month column  
```

```
revenue_df = retail_data.groupby(['yyyymm'])['revenue'].sum().reset_index()
```

```
revenue_df.head()
```

前面的代码将输出以下数据帧:

![Figure 1.5 – Revenue DataFrame
](img/B18024_01_05.jpg)

图 1.5–收入数据框架

让我们想象一下`revenue`数据帧。我将使用一个名为`plotly`的库。以下命令将在您的笔记本环境中安装`plotly`:

```
!pip install plotly
```

让我们从`revenue`数据框中绘制一个条形图，其中`yyyymm`列在 *x* 轴上，`revenue`在 *y* 轴上:

```
import plotly.express as px
```

```
##Sort rows on year-month column
```

```
revenue_df.sort_values( by=['yyyymm'], inplace=True)
```

```
## plot a bar graph with year-month on x-axis and revenue on y-axis, update x-axis is of type category.
```

```
fig = px.bar(revenue_df, x="yyyymm", y="revenue", 
```

```
             title="Monthly Revenue") 
```

```
fig.update_xaxes(type='category')
```

```
fig.show()
```

前面的代码对`yyyymm`列上的收入数据帧进行排序，并根据年月(`yyyymm`)列绘制一个`revenue`条形图，如下面的屏幕截图所示。如您所见，9 月、10 月和 11 月是高收入月份。根据几年的数据来验证我们的假设是很好的，但不幸的是，我们没有这样的数据。在我们继续进行模型开发之前，让我们再看一个指标——月活跃客户数，看看它是否与月收入相关:

![Figure 1.6 – Monthly revenue
](img/B18024_01_06.jpg)

图 1.6-月收入

继续在同一个笔记本中，以下命令将通过在年月(`yyyymm`)列上合计唯一的`CustomerID`计数来计算每月活跃客户:

```
active_customer_df = retail_data.groupby(['yyyymm'])['CustomerID'].nunique().reset_index()
```

```
active_customer_df.columns = ['yyyymm', 
```

```
                              'No of Active customers']
```

```
active_customer_df.head()
```

上述代码将产生以下输出:

![Figure 1.7 – Monthly active customers DataFrame
](img/B18024_01_07.jpg)

图 1.7–每月活跃客户数据框架

让我们以绘制月收入的相同方式绘制前面的数据框架:

```
## Plot bar graph from revenue data frame with yyyymm column on x-axis and No. of active customers on the y-axis.
```

```
fig = px.bar(active_customer_df, x="yyyymm", 
```

```
             y="No of Active customers", 
```

```
             title="Monthly Active customers") 
```

```
fig.update_xaxes(type='category')
```

```
fig.show()
```

前面的命令绘制了一个针对年月(`yyyymm`)列的条形图`No of Active customers`。如下图所示，`Monthly Active customers`与上图所示的月收入正相关:

![Figure 1.8 – Monthly active customers
](img/B18024_01_08.jpg)

图 1.8–每月活跃客户

在下一节中，我们将构建一个客户 LTV 模型。

## 型号

现在我们已经完成了对数据的探索，让我们建立 LTV 模型。**客户终身价值** ( **CLTV** )被定义为*与公司客户生命周期相关的净盈利能力。简单来说，CLV/LTV 是对每个客户对企业价值的预测*(参考:[https://www . toolbox . com/marketing/customer-experience/articles/what-is-customer-lifetime-value-CLV/](https://www.toolbox.com/marketing/customer-experience/articles/what-is-customer-lifetime-value-clv/))。有不同的方法来预测终身价值。一种方法是预测客户的价值，这是一个回归问题，而另一种方法是预测客户群，这是一个分类问题。在本练习中，我们将使用后一种方法。

在本练习中，我们将客户分为以下几组:

*   **低 LTV** :不太活跃或低收入的客户
*   **LTV 中部**:相当活跃且收入中等的客户
*   **高 LTV** :高收入客户——我们不想失去的部分

我们将使用 3 个月的数据来计算客户的**新近度** ( **R** )、**频率** ( **F** )和**货币** ( **M** )指标，以生成特性。一旦我们有了这些特征，我们将使用 6 个月的数据来计算每个客户的收入，并生成 LTV 聚类标签(低 LTV、中 LTV 和高 LTV)。然后，生成的标签和特征将用于训练 XGBoost 模型，该模型可用于预测新客户群。

### 特征工程

让我们继续在同一个笔记本中的工作，计算客户的 R、F 和 M 值，并根据根据个人 R、F 和 M 得分计算出的值对客户进行分组:

*   **Recency(R)**:Recency 指标表示自客户最后一次购买以来已经过去了多少天。
*   **频率(F)** :顾名思义，F 代表客户购买了多少次。
*   货币(M) :某个特定客户带来了多少收入。

由于客户的消费和购买模式因人口统计位置而异，因此在本练习中，我们将只考虑属于英国的数据。我们来读取`OnlineRetails.csv`文件，过滤掉不属于英国的数据:

```
import pandas as pd
```

```
from datetime import datetime, timedelta, date
```

```
from sklearn.cluster import KMeans
```

```
##Read the data and filter out data that belongs to country other than UK
```

```
retail_data = pd.read_csv('/content/OnlineRetail.csv', 
```

```
                           encoding= 'unicode_escape')
```

```
retail_data['InvoiceDate'] = pd.to_datetime(
```

```
    retail_data['InvoiceDate'], errors = 'coerce')
```

```
uk_data = retail_data.query("Country=='United Kingdom'").reset_index(drop=True)
```

在下面的代码块中，我们将创建两个不同的数据帧。第一个(`uk_data_3m`)将用于`2011-03-01`和`2011-06-01`之间的`InvoiceDate`。该数据框架将用于生成 RFM 要素。第二个数据帧(`uk_data_6m`)将用于`2011-06-01`和`2011-12-01`之间的`InvoiceDate`。该数据框架将用于生成模型训练的目标列。在本练习中，目标列是 LTV 组/集群。因为我们正在计算客户 LTV 组，所以更大的时间间隔会给出更好的分组。因此，我们将使用 6 个月的数据来生成 LTV 集团标签:

```
## Create 3months and 6 months data frames
```

```
t1 = pd.Timestamp("2011-06-01 00:00:00.054000")
```

```
t2 = pd.Timestamp("2011-03-01 00:00:00.054000")
```

```
t3 = pd.Timestamp("2011-12-01 00:00:00.054000")
```

```
uk_data_3m = uk_data[(uk_data.InvoiceDate < t1) & (uk_data.InvoiceDate >= t2)].reset_index(drop=True)
```

```
uk_data_6m = uk_data[(uk_data.InvoiceDate >= t1) & (uk_data.InvoiceDate < t3)].reset_index(drop=True)
```

现在我们有了两个不同的数据帧，让我们使用`uk_data_3m`数据帧来计算 RFM 值。下面的代码块通过将`UnitPrice`乘以`Quantity`来计算`revenue`列。为了计算 RFM 值，代码块对`CustomerID`执行三次聚合:

*   要计算数据帧中的`max_date`，必须计算每个客户的`R = max_date – x.max()`，其中`x.max()`计算特定`CustomerID`的最新`InvoiceDate`。
*   计算`count`特定`CustomerID`的发票数量。
*   计算特定`CustomerID`的`revenue`的`sum`值。

以下代码片段执行此逻辑:

```
## Calculate RFM values.
```

```
uk_data_3m['revenue'] = uk_data_3m['UnitPrice'] * uk_data_3m['Quantity']
```

```
# Calculating the max invoice date in data (Adding additional day to avoid 0 recency value)
```

```
max_date = uk_data_3m['InvoiceDate'].max() + timedelta(days=1)
```

```
rfm_data = uk_data_3m.groupby(['CustomerID']).agg({
```

```
        'InvoiceDate': lambda x: (max_date - x.max()).days,
```

```
        'InvoiceNo': 'count',
```

```
        'revenue': 'sum'})
```

```
rfm_data.rename(columns={'InvoiceDate': 'Recency',
```

```
                         'InvoiceNo': 'Frequency',
```

```
                         'revenue': 'MonetaryValue'}, 
```

```
                         inplace=True)
```

这里，我们已经为客户计算了 R、F 和 M 值。接下来，我们需要将客户分为 R、F 和 M 组。这种分组定义了一个客户在 R、F 和 M 指标方面相对于其他客户所处的位置。为了计算 R、F 和 M 组，我们将分别根据客户的 R、F 和 M 值将他们分成大小相等的组。这些是在前面的代码块中计算的。为了实现这一点，我们将在 DataFrame 上使用一个名为`pd.qcut`([https://pandas . pydata . org/pandas-docs/stable/reference/API/pandas . qcut . html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html))的方法。或者，您可以使用任何*聚类*方法将客户分成不同的组。我们将把 R、F 和 M 组的值加在一起，生成一个名为`RFMScore`的值，范围从 0 到 9。

在本练习中，顾客将被分成四个小组。*肘方法*([https://towards data science . com/clustering-metrics-better-than-the-elbow-method-6926 E1 f 723 a6](https://towardsdatascience.com/clustering-metrics-better-than-the-elbow-method-6926e1f723a6))可用于计算任何数据集的最佳组数。前面的链接还包含了关于可用于计算最佳组数的替代方法的信息，所以请随意尝试。我会把它留给你做练习。

下面的代码块计算`RFMScore`:

```
## Calculate RFM groups of customers 
```

```
r_grp = pd.qcut(rfm_data['Recency'], q=4, 
```

```
                labels=range(3,-1,-1))
```

```
f_grp = pd.qcut(rfm_data['Frequency'], q=4, 
```

```
                labels=range(0,4))
```

```
m_grp = pd.qcut(rfm_data['MonetaryValue'], q=4, 
```

```
                labels=range(0,4))
```

```
rfm_data = rfm_data.assign(R=r_grp.values).assign(F=f_grp.values).assign(M=m_grp.values)
```

```
rfm_data['R'] = rfm_data['R'].astype(int)
```

```
rfm_data['F'] = rfm_data['F'].astype(int)
```

```
rfm_data['M'] = rfm_data['M'].astype(int)
```

```
rfm_data['RFMScore'] = rfm_data['R'] + rfm_data['F'] + rfm_data['M']
```

```
rfm_data.groupby('RFMScore')['Recency','Frequency','MonetaryValue'].mean()
```

前面的代码将生成以下输出:

![Figure 1.9 – RFM score summary
](img/B18024_01_09.jpg)

图 1.9-RFM 得分汇总

这些汇总数据让我们大致了解了`RFMScore`如何与`Recency`、`Frequency`和`MonetaryValue`指标成正比。例如，具有`RFMScore=0`的组具有最高的平均新近度(该组的最后购买日是过去最远的)、最低的平均频率和最低的平均货币值。另一方面，具有`RFMScore=9`的组具有最低的平均新近度、最高的平均频率和最高的平均货币值。

有鉴于此，我们明白`RFMScore`与客户给企业带来的价值正相关。因此，让我们对客户进行如下细分:

*   0-3 = >低值
*   4-6 = >中间值
*   7-9 = >高值

以下代码将客户标记为低值、中值或高值:

```
# segment customers.
```

```
rfm_data['Segment'] = 'Low-Value'
```

```
rfm_data.loc[rfm_data['RFMScore']>4,'Segment'] = 'Mid-Value' 
```

```
rfm_data.loc[rfm_data['RFMScore']>6,'Segment'] = 'High-Value' 
```

```
rfm_data = rfm_data.reset_index()
```

### 客户 LTV

现在，我们已经在包含 3 个月数据的数据框架中为客户准备好了 RFM 特征，让我们使用 6 个月的数据(`uk_data_6m)`来计算客户的收入，就像我们之前所做的那样，并将 RFM 特征与新创建的收入数据框架合并:

```
# Calculate revenue using the six month dataframe.
```

```
uk_data_6m['revenue'] = uk_data_6m['UnitPrice'] * uk_data_6m['Quantity']
```

```
revenue_6m = uk_data_6m.groupby(['CustomerID']).agg({
```

```
        'revenue': 'sum'})
```

```
revenue_6m.rename(columns={'revenue': 'Revenue_6m'}, 
```

```
                  inplace=True)
```

```
revenue_6m = revenue_6m.reset_index()
```

```
revenue_6m = revenue_6m.dropna()
```

```
# Merge the 6m revenue data frame with RFM data.
```

```
merged_data = pd.merge(rfm_data, revenue_6m, how="left")
```

```
merged_data.fillna(0)
```

随意让策划`revenue_6m`对抗`RFMScore`。你会看到两者之间的正相关关系。

在下面的代码块中，我们使用了`revenue_6m`列，这是客户的*生命周期值，并使用 K-means 聚类创建了三个组，称为*低 LTV* 、*中 LTV* 和*高 LTV* 。同样，您可以使用前面提到的*肘形方法*来验证集群的最佳数量:*

```
# Create LTV cluster groups
```

```
merged_data = merged_data[merged_data['Revenue_6m']<merged_data['Revenue_6m'].quantile(0.99)]
```

```
kmeans = KMeans(n_clusters=3)
```

```
kmeans.fit(merged_data[['Revenue_6m']])
```

```
merged_data['LTVCluster'] = kmeans.predict(merged_data[['Revenue_6m']])
```

```
merged_data.groupby('LTVCluster')['Revenue_6m'].describe()
```

前面的代码块产生以下输出:

![Figure 1.10 – LTV cluster summary
](img/B18024_01_10.jpg)

图 1.10-LTV 集群摘要

如您所见，标签为 1 的聚类包含一组终生价值非常高的客户，因为该组的平均收入为 14，123.309 美元，而这样的客户只有 21 个。标签为 0 的分类包含一组终生价值较低的客户，因为该组的平均收入仅为$828.67，而这样的客户有 1，170 个。这种分类让我们知道哪些客户应该一直保持满意。

### 特征集和模型

让我们使用到目前为止我们已经计算出的特征来构建一个 XGBoost 模型，以便该模型能够在给定输入特征的情况下预测客户的 LTV 群体。以下是将用作模型输入的最终特征集:

```
feature_data = pd.get_dummies(merged_data)
```

```
feature_data.head(5)
```

前面的代码块产生以下数据帧。这包括将用于训练模型的特征集:

![Figure 1.11 – Feature set for model training
](img/B18024_01_11.jpg)

图 1.11–模型训练的特征集

现在，让我们使用这个特征集来训练`Xgboost`模型。预测标签(`y`)是`LTVCluster`列；除了`Revenue_6m`和`CustomerID`列之外，数据集的其余部分是`X`值。当使用`Revenue_6m`计算`LTVCluster`列(`y`)时，将从特征集中删除`Revenue_6m`。对于新客户，我们可以计算其他特性，而不需要至少 6 个月的数据，还可以预测他们的`LTVCluster(y)`。

以下代码将训练`Xgboost`模型:

```
from sklearn.metrics import classification_report, confusion_matrix
```

```
import xgboost as xgb
```

```
from sklearn.model_selection import KFold, cross_val_score, train_test_split
```

```
#Splitting data into train and test data set.
```

```
X = feature_data.drop(['CustomerID', 'LTVCluster',
```

```
                       'Revenue_6m'], axis=1)
```

```
y = feature_data['LTVCluster']
```

```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)
```

```
xgb_classifier = xgb.XGBClassifier(max_depth=5, objective='multi:softprob')
```

```
xgb_model = xgb_classifier.fit(X_train, y_train)
```

```
y_pred = xgb_model.predict(X_test)
```

```
print(classification_report(y_test, y_pred))
```

前面的代码块将输出以下分类结果:

![Figure 1.12 – Classification report
](img/B18024_01_12.jpg)

图 1.12–分类报告

现在，让我们假设我们对这个模型很满意，并且想把它带到下一个阶段，也就是生产阶段。

## 包装、放行和监控

到目前为止，我们已经花了很多时间来研究数据分析、探索、清理和建模，因为这是数据科学家应该专注的事情。但是，一旦所有的工作都完成了，模型可以在没有任何额外工作的情况下部署吗？答案是否定的。我们离部署还很远。在部署模型之前，我们必须完成以下工作:

*   我们必须创建一个预定的数据管道来执行数据清理和特征工程。
*   我们需要一种在预测过程中提取特征的方法。如果是在线/交易模式，应该有一种方法以低延迟获取特性。由于客户的 R、F 和 M 值经常变化，假设我们想在网站上针对中值和高值细分市场开展两个不同的活动。将需要接近实时的客户评分。
*   使用历史数据找到重现模型的方法。
*   执行模型打包和版本化。
*   想办法 AB 测试模型。
*   找到监控模型和数据漂移的方法。

由于我们没有任何准备，让我们在这里停下来，回头看看我们已经做了什么，如果有一种方法可以做得更好，看看是否有任何共同的疏忽。

在下一部分，我们将看看*我们认为我们已经建立的(理想世界)与我们已经建立的(现实世界)*。

# 理想世界对现实世界

既然我们已经花了大量的时间来构建这个漂亮的数据产品，它可以帮助企业根据客户带来的价值来区别对待客户，那么让我们来看看我们对此有什么期望，以及它能做什么。

## 可重用性和共享性

可重用性是 IT 行业常见的问题之一。我们面前有产品的大量数据，我们在探索过程中构建的图表，以及我们为模型生成的特征。这些可以被其他数据科学家、分析师和数据工程师重用。以它目前的状态，它能被重用吗？答案是可能。数据科学家可以共享笔记本本身，可以创建演示文稿，等等。但是，人们没有办法发现他们是否正在寻找，比如说，客户细分或 RFM 特征，这在其他模型中可能非常有用。因此，如果另一个数据科学家或 ML 工程师正在构建一个需要相同特征的模型，他们剩下的唯一选择就是重新发明同一个轮子。根据数据科学家生成新模型的方式，新模型可能具有相同、更准确或不太准确的 RFM 要素。然而，如果有更好的方法来发现和重用工作，第二个模型的开发可能会加快。还有，俗话说，*三个臭皮匠胜过一个诸葛亮*。合作对数据科学家和企业都有好处。

## 笔记本里的一切

数据科学是不同于软件工程的独特技能。尽管一些数据科学家可能有软件工程师的背景，但角色本身的需求可能会使他们远离软件工程技能。随着数据科学家在数据探索和模型构建阶段花费更多时间，集成开发环境 ( **IDEs** )可能不够用，因为他们要处理的数据量非常大。如果我们不得不在我们的个人 Mac 或 PC 上探索、做特征工程和做模型构建，数据处理阶段将运行数天。此外，他们需要灵活地使用不同的编程语言，如 Python、Scala、R、SQL 等，以便在分析过程中动态添加命令。这就是为什么有这么多笔记本平台提供商的原因之一，包括 Jupyter、Databricks 和 SageMaker。

由于数据产品/模型开发不同于传统的软件开发，不做任何额外的工作就将实验代码交付生产总是不可能的。大多数数据科学家在笔记本上开始他们的工作，并以与我们在上一节中相同的方式构建一切。一些标准实践和工具(如 feature store)不仅可以帮助他们将模型构建过程分解为多个生产就绪的笔记本，还可以帮助他们避免重新处理数据、调试问题和代码重用。

现在我们已经了解了 ML 开发的现实，让我们简单地回顾一下 ML 最耗时的阶段。

# ML 最耗时的阶段

在本章的第一部分，我们经历了 ML 生命周期的不同阶段。让我们更详细地了解一些阶段，并考虑它们的复杂程度以及我们应该在每个阶段花费的时间。

## 计算出数据集

一旦我们有了问题陈述，下一步就是找出解决问题所需的数据集。在我们接下来的例子中，我们知道数据集在哪里，并且它是给定的。然而，在现实世界中，事情并没有那么简单。由于每个组织都有自己的数据仓库方式，找到您需要的数据可能很简单，也可能需要很长时间。大多数组织运行数据目录服务，如 Amundsen、Atlan 和 Azure Data Catalog，以使其数据集易于被发现。但是，工具的好坏取决于使用它们的方式或使用它们的人。所以，我在这里要说的是，找到你要找的数据总是很容易的。除此之外，考虑到数据的访问控制，即使您找出了解决问题所需的数据集，也很可能无法访问它，除非您以前使用过它。弄清楚如何进入将是另一个主要的障碍。

## 数据探索和特征工程

数据探索:一旦你弄清楚了数据集，下一个最大的任务就是再次弄清楚数据集！你没看错——对于数据科学家来说，下一个最大的任务是确保他们挑选的数据集是解决问题的正确数据集。这将涉及数据清理、增加缺失的数据、转换数据、绘制不同的图表、找到相关性、找出数据偏差等等。最好的部分是，如果数据科学家发现有些事情不对劲，他们会回到上一步，即寻找更多的数据集，再次尝试，然后返回。

特征工程也不容易；领域知识成为构建特征集以训练模型的关键。如果您是一名在过去几年中一直从事定价和促销模型工作的数据科学家，您会知道什么样的数据集和特征会产生比过去几年中一直从事客户价值模型工作的数据科学家更好的模型，反之亦然。让我们尝试一个练习，看看特征工程是否容易，领域知识是否起着关键作用。看看下面的截图，看看你能不能认出这些动物:

![Figure 1.13 – A person holding a dog and a cat
](img/B18024_01_13.jpg)

图 1.13-一个人抱着一只狗和一只猫

我相信你知道这些动物是什么，但是让我们后退一步，看看我们是如何正确识别这些动物的。当我们看这个图形的时候，我们的潜意识确实是以工程学为特征的。它本可以挑选一些特征，比如*它有一对耳朵*，一对眼睛，*一个鼻子*，*一个头*，以及*一条尾巴*。相反，它选择了更复杂的特征，比如脸的形状、眼睛的形状、鼻子的形状和皮毛的颜色和质地。如果它选择了第一组特征，这两只动物将会是一样的，这是一个糟糕的特征工程和糟糕的模型的例子。既然它选择了后者，我们就认定它是不同的动物。同样，这是一个好的特征工程和好的模型的例子。

但是我们需要回答的另一个问题是，我们是什么时候发展出动物识别的专业知识的？嗯，可能是我们幼儿园老师送的。我们都记得从老师、父母、兄弟姐妹那里学到的最初 100 种动物的一些版本。一开始我们没有把所有的都做对，但最终，我们做到了。随着时间的推移，我们获得了专业知识。

现在，如果不是一只猫和一只狗的图片，而是两条蛇的图片，我们的工作是识别它们中哪一条是有毒的，哪一条是无毒的。虽然我们所有人都能认出它们是蛇，但几乎没有人能分辨出哪一条是有毒的，哪一条是无毒的。除非这个人以前是耍蛇人。

因此，领域专业知识在特征工程中变得至关重要。就像数据探索阶段一样，如果我们对特征不满意，我们就回到起点，这涉及到寻找更多的数据和更好的特征。

## 建模到生产和监控

一旦我们搞清楚了前面提到的阶段，将模型投入生产是非常耗时的，除非合适的基础设施已经准备好并在等待。对于在生产中运行的模型，它需要一个运行数据清理和特征工程代码的处理平台。它还需要一个编排框架来以预定的或基于事件的方式运行特性工程管道。在某些情况下，我们还需要以低延迟安全地存储和检索特征。如果模型是事务性的，那么模型必须被打包，以便消费者可以安全地访问它，可能作为一个 REST 端点。此外，部署的模型应该是可伸缩的，以服务于传入的流量。

模型和数据监控也是非常重要的方面。由于模型性能直接影响业务，您必须知道哪些指标将决定模型需要提前重新训练。除了模型监控之外，还需要监控数据集的偏差。例如，在电子商务业务中，流量模式和购买模式可能会根据季节性、趋势和其他因素频繁变化。尽早发现这些变化将对业务产生积极影响。因此，数据和特征监控是将模型投入生产的关键。

# 总结

在这一章中，我们讨论了 ML 生命周期的不同阶段。我们选择了一个问题陈述，进行了数据探索，绘制了一些图表，进行了特征工程和客户细分，并建立了一个客户终身价值模型。我们查看了疏漏之处，并讨论了 ML 中最耗时的阶段。我想让你和我在同一页上，并为这本书的其余部分打下良好的基础。

在下一章中，我们将为特性库的需求以及它如何改进 ML 过程做准备。我们还将讨论将特征引入生产的必要性以及一些传统的实现方式。

第 1 章:机器学习生命周期概述

第 1 章:机器学习生命周期概述