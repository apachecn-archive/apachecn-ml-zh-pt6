<html><head/><body>









<title>5 </title>







<div><div><h1 class="chapter-number" id="_idParaDest-116"><a id="_idTextAnchor116"/> 5</h1>

<h1 id="_idParaDest-117"><a id="_idTextAnchor117"/>了解神经网络和深度学习</h1>

<p>自 2012 年首次亮相以来，<strong class="bold">深度学习</strong> ( <strong class="bold"> DL </strong>)取得了巨大的突破，并在包括计算机视觉、<strong class="bold">自然语言处理</strong> ( <strong class="bold"> NLP </strong>)等多个研究和工业领域得到应用。在本章中，我们将介绍基本概念，包括以下内容:</p>

<ul>

<li>神经网络和数字逻辑</li>

<li>成本函数</li>

<li>优化算法</li>

<li>激活功能</li>

</ul>

<p>掌握这些概念后，我们将讨论几种神经网络模型及其业务用例，包括以下内容:</p>

<ul>

<li><strong class="bold">卷积神经网络</strong>(<strong class="bold">CNN</strong>)</li>

<li><strong class="bold">递归神经网络</strong> ( <strong class="bold"> RNNs </strong>)</li>

<li><strong class="bold"> L </strong> <strong class="bold"> ong 短期记忆</strong> ( <strong class="bold"> LSTM </strong>)网络</li>

<li><strong class="bold">生成对抗网络</strong> ( <strong class="bold">甘斯</strong>)</li>

</ul>

<p>理解神经网络和 DL 概念、公共模型和业务用例在我们的云 ML 之旅中极其重要。让我们开始吧。</p>

<h1 id="_idParaDest-118"><a id="_idTextAnchor118"/>神经网络和 DL</h1>

<p>在我们人类的历史上，有许多有趣的里程碑，从视觉发展和语言发展到制造和使用工具。人类是如何进化的，我们怎样才能训练一台计算机让<em class="italic">看</em>，<em class="italic">说话</em>，<em class="italic">使用</em>工具？寻找这些问题的答案将我们带到了现代人工智能领域。</p>

<p>我们的大脑是如何工作的？现代科学揭示，在大脑中，有一个由一组神经元组成的分层神经网络<a id="_idIndexMarker301"/>。一个典型的神经元通过一个叫做<strong class="bold">树突</strong>的精细结构从其他神经元收集电信号，并通过一个叫做<strong class="bold">轴突</strong>的传导结构发出信号尖峰<a id="_idIndexMarker302"/>，轴突分裂成<a id="_idIndexMarker303"/>许多分支。在每个分支的末端，突触将来自轴突的信号转化为电效应，以刺激目标<a id="_idIndexMarker304"/>神经元的活动。<em class="italic">图 5.1 </em>显示了一个生物神经元的工作机制:</p>

<div><div><img alt="Figure 5.1 – How a biological neuron works " height="922" src="img/Figure_5.1.jpg" width="1627"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图 5.1-生物神经元如何工作</p>

<p>受生物神经网络模型的启发，<strong class="bold">人工神经网络</strong> ( <strong class="bold"> ANN </strong>)模型由被称为<strong class="bold">感知器</strong>的人工神经元<a id="_idIndexMarker305"/>组成。一个感知器从其他感知器接收<a id="_idIndexMarker306"/>加权输入，应用传递函数(加权输入之和)和激活函数(将非线性激活添加到和中)，并输出以激励下一个感知器。<em class="italic">图 5.2 </em>显示了人工神经元(感知器)的工作机制:</p>

<div><div><img alt="Figure 5.2 – How an artificial neuron (perceptron) works&#10;&#10;" height="596" src="img/Figure_5.2.jpg" width="1353"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图 5.2-人工神经元(感知器)如何工作</p>

<p>人工神经网络由通过层一起工作的感知器组成。图 5.3 显示了多层人工神经网络的结构，其中每个圆形节点代表一个感知器，一条线代表从一个感知器的输出到另一个感知器的输入的连接。神经网络中有三种类型的层:输入层、一个或多个隐藏层和输出层。<em class="italic">图 5.3 </em>中的神经网络<a id="_idIndexMarker308"/>有一个输入层、两个隐含层和一个输出层:</p>

<div><div><img alt="Figure 5.3 – A multilayer ANN&#10;&#10;" height="753" src="img/Figure_5.3.jpg" width="1198"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图 5.3-多层人工神经网络</p>

<p>使用神经<a id="_idIndexMarker309"/>网络执行 ML 模型<a id="_idIndexMarker310"/>训练，数据在网络中流动如下:</p>

<ol>

<li>一个数据集(<em class="italic"> x </em> 1 <em class="italic">，x </em> 2 <em class="italic">，x </em> 3 <em class="italic">，...，x </em> n)准备好并发送到输入层，输入层的感知器数量与数据集的特征数量相同。</li>

<li>然后数据移动到隐藏层。在每个隐藏层，感知器处理加权输入(如前所述，求和并激活)，并将输出发送到下一个隐藏层的神经元。</li>

<li>在隐藏层之后，数据最终移动到提供输出的输出层。</li>

</ol>

<p>神经网络的目标是确定最小化成本函数(数据集的平均预测误差)的权重。类似于我们在前面章节中讨论的回归模型训练过程，DL 模型训练通过两部分过程的迭代来实现，向前传播和向后传播，如下所示:</p>

<ul>

<li><strong class="bold">正向传播</strong>是信息通过隐藏层从输入层流向输出<a id="_idIndexMarker311"/>层的路径。在训练过程开始时，数据到达输入层，在那里与随机初始化的权重相乘，然后传递到第一个隐藏层。由于输入层有多个节点，每个节点都连接到第一个隐藏层中的每个节点；隐藏层中的节点对其加权值求和，并应用激活函数(增加非线性)。然后，它将输出发送到下一层的节点，在下一层的节点也这样做，直到最后一个隐藏层的输出乘以权重，并成为最终输出层的输入，在最终输出层，应用进一步的函数来生成输出。</li>

<li><strong class="bold">反向传播</strong>是信息从输出层一直流回输入层<a id="_idIndexMarker312"/>的路径。在此过程中，作为反向传播的第一步，神经网络将预测输出与实际输出进行比较，并计算成本函数或预测误差。如果成本函数不够好，它返回以基于算法<a id="_idIndexMarker313"/>如<strong class="bold">梯度下降</strong> ( <strong class="bold"> GD </strong>)调整权重，然后用新的权重再次开始前向传播。</li>

</ul>

<p>前向传播和后向传播重复多次——每次网络调整<a id="_idIndexMarker314"/>权重，试图获得更好的成本函数值——直到网络<a id="_idIndexMarker315"/>在输出层获得良好的成本函数(可接受的精度)。此时模型训练完成，我们得到了优化的权重，这就是训练的结果。</p>

<p>DL 正在用神经网络训练 ML 模型。如果您将前面使用<a id="_idIndexMarker316"/>神经网络的 DL 模型训练过程与我们在<a href="B18333_04.xhtml#_idTextAnchor094"> <em class="italic">第 4 章</em> </a>、<em class="italic">开发和部署 ML 模型</em>的<em class="italic">训练模型</em>部分中讨论的 ML 模型训练过程进行比较，您会发现 ML 和 DL 的概念非常相似。通过迭代的前向传播和后向传播，两者都试图最小化模型的成本函数——ML 更多地是关于计算机利用传统算法从数据中学习，而 DL 更多地是关于计算机从模拟人脑和神经网络的数据中学习。相对来说，ML 需要的计算能力更少，DL 需要的人工干预更少。在接下来的部分中，我们将仔细研究用于具有神经网络的 DL 的成本函数、优化器算法和激活函数。</p>

<h1 id="_idParaDest-119"><a id="_idTextAnchor119"/>成本函数</h1>

<p>我们在第 4 章 的<a href="B18333_04.xhtml#_idTextAnchor094"> <em class="italic">中的<em class="italic">线性回归</em>部分介绍了成本函数的概念。成本函数<a id="_idIndexMarker317"/>为我们提供了一种确定当前模型有多大误差的数学方法——它为做出不正确的预测分配了成本，并提供了一种测量模型性能的方法。成本函数是 ML 模型训练中的关键指标，选择正确的成本函数可以显著提高模型性能。</em></a></p>

<p>回归模型的常见成本函数是 MAE 和 MSE。正如我们在前面章节中所讨论的，MAE 定义了<a id="_idIndexMarker318"/>预测值和标签值之间的绝对差值的总和。MSE 定义了预测值和标签值之间的差的平方和<a id="_idIndexMarker319"/>。</p>

<p>分类模型的成本函数非常不同。从概念上讲，分类<a id="_idIndexMarker320"/>模型的成本函数是不同类别的概率分布之间的差异。对于二进制分类模型，其中模型输出<a id="_idIndexMarker321"/>是二进制的，1 表示是，0 表示否，我们使用<strong class="bold">二进制交叉熵</strong>。对于多类<a id="_idIndexMarker322"/>分类模型，根据数据集标签上的<a id="_idIndexMarker323"/>，我们使用<strong class="bold">分类交叉熵</strong>和<strong class="bold">稀疏分类交叉熵</strong>如下:</p>

<ul>

<li>如果标签是整数，例如，对一幅狗、猫或牛的图像进行分类，那么我们使用稀疏分类交叉熵，因为输出是一个唯一的类。</li>

<li>否则，如果标签被编码为每个类的一系列 0 和 1(对于我们在前面章节中讨论的一次热编码格式也是如此)，我们将使用分类交叉熵。例如，给定一幅图像，您需要检测是否存在驾照、护照或社会保障卡，我们将使用分类交叉熵作为成本函数，因为输出包含多个类。</li>

</ul>

<p>成本函数是一种测量模型的方法，因此我们可以调整模型参数以最小化它们——模型优化过程。在下一节中，我们将讨论最小化成本函数的优化器算法。</p>

<h1 id="_idParaDest-120"><a id="_idTextAnchor120"/>优化器算法</h1>

<p>在第四章 的<a href="B18333_04.xhtml#_idTextAnchor094"> <em class="italic">中的<em class="italic">线性回归</em>部分，我们讨论了<strong class="bold"> GD </strong>算法，其中<a id="_idIndexMarker324"/>优化了线性回归代价<a id="_idIndexMarker325"/>函数。在神经网络中，优化器是一种用于在模型训练中最小化<a id="_idIndexMarker326"/>成本函数的算法。常用的<a id="_idIndexMarker327"/>优化器<a id="_idIndexMarker328"/>有<strong class="bold">随机梯度下降</strong> ( <strong class="bold"> SGD </strong>)、<strong class="bold"> RMSprop </strong>和<strong class="bold"> Adam </strong>如下:</em></a></p>

<ul>

<li>SGD 对于非常大的数据集非常有用。与 GD 不同，SGD 使用一个训练样本或训练样本的子集，而 GD 会遍历<a id="_idIndexMarker330"/>训练数据集中的所有样本来更新参数。</li>

<li>RMSprop 通过引入可变学习率来改进 SGD。正如我们在<a href="B18333_04.xhtml#_idTextAnchor094"> <em class="italic">第 4 章</em> </a>中所讨论的，学习率会影响模型性能——较大的学习率可以<a id="_idIndexMarker331"/>减少训练时间，但可能导致模型振荡，并可能错过最佳模型参数值。较低的学习率会使训练过程变得更长。在 SGD 中，学习率是固定的。RMSprop 随着训练的进行调整学习率，因此当模型具有高成本函数时，它允许您以大的学习率开始，但是当成本函数降低时，它逐渐降低学习率。</li>

<li><strong class="bold"> Adam </strong>代表<strong class="bold">自适应矩估计</strong>，是应用最广泛的<a id="_idIndexMarker332"/>优化器之一。Adam 为 RMSprop 的自适应学习率增加了动力，因此它允许模型的变化加速，同时在训练期间沿同一方向移动，使模型训练过程更快更好。</li>

</ul>

<p>选择正确的<a id="_idIndexMarker333"/>成本函数和优化算法对于模型性能和训练速度非常重要。Google 的 TensorFlow 框架<a id="_idIndexMarker334"/>提供了很多优化器算法。详情请参考<a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers">https://www . tensor flow . org/API _ docs/python/TF/keras/optimizer</a>。</p>

<p>神经网络的其他重要特征是非线性和输出标准化，这是由激活函数提供的。我们将在下一节中研究它们。</p>

<h1 id="_idParaDest-121"><a id="_idTextAnchor121"/>激活功能</h1>

<p>从上一节可以看出，激活功能是培训过程的一部分。激活功能的目的<a id="_idIndexMarker335"/>是将加权和输入转换到节点:非线性化并改变输出范围。神经网络中有许多激活函数。我们将讨论一些最常用的函数:sigmoid 函数、tanh 激活函数、ReLu 函数和 LeakyReLU 函数。<em class="italic">图 5.4 </em>显示了这些函数的曲线:</p>

<div><div><img alt="Figure 5.4 – Activation functions&#10;&#10;" height="452" src="img/Figure_5.4.jpg" width="625"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图 5.4–激活功能</p>

<p>让我们按如下方式检查前面的每个激活功能:</p>

<ul>

<li>前面在成本函数章节中讨论了 sigmoid 激活函数。我们使用<a id="_idIndexMarker336"/>sigmoid<a id="_idIndexMarker337"/>函数将连续值更改为 0 到 1 之间的范围，这符合模型预测概率的输出。</li>

<li>双曲正切激活函数<a id="_idIndexMarker338"/>与 sigmoid 非常相似，但输出范围为-1 至+1，因此<a id="_idIndexMarker339"/>优于 sigmoid，因为输出以零为中心。</li>

<li>ReLU 激活功能代表整流线性单元。它被广泛使用，因为它<a id="_idIndexMarker340"/>将负<a id="_idIndexMarker341"/>值转换为 0，并保持正值不变。它的范围在 0 到无穷大之间。因为负区域中的梯度值为 0，所以在训练过程中，一些神经元的权重和偏差可能不会更新，从而导致死神经元永远不会被激活。</li>

<li>LeakyReLU 是 ReLU 函数的改进版本，用于解决垂死的 ReLU 问题<a id="_idIndexMarker342"/>，因为它在负区域有一个小的<a id="_idIndexMarker343"/>正斜率。LeakyReLU 的优势与 ReLU 的优势相同，此外，它甚至可以针对负输入值进行训练。</li>

</ul>

<p>另一个激活函数是<em class="italic"> softmax </em>函数，常用于多类<a id="_idIndexMarker344"/>分类的输出层。softmax 激活函数<a id="_idIndexMarker345"/>将输出层值转换为总和为 1 的概率，从而输出多类分类问题中每个类的概率。</p>

<p>在所有这些激活功能中，我们应该选择哪一个？答案取决于预测类型、网络架构、层数、网络中的当前<a id="_idIndexMarker346"/>层等因素。例如，sigmoid 更多地用于二元分类用例，而 softmax 通常用于多分类，回归问题可能使用也可能不使用激活函数。虽然开始时会有尝试和错误，但经验会建立良好的实践。</p>

<p>现在我们已经介绍了神经网络和激活函数的概念，让我们来考察一些常用于计算机视觉、<strong class="bold">自然语言处理</strong> ( <strong class="bold"> NLP </strong>)和其他领域的神经网络。</p>

<h1 id="_idParaDest-122"><a id="_idTextAnchor122"/>卷积神经网络</h1>

<p>现在我们已经了解了神经网络和 DL，让我们来看看一些业务用例。</p>

<p>第一种情况是图像<a id="_idIndexMarker347"/>识别。我们如何教会计算机识别图像？这对人类来说是一件容易的事，但对计算机来说却是一件非常困难的事。由于计算机只擅长处理 1 和 0，我们需要做的第一件事是使用像素将图像转换成数字矩阵。例如，<em class="italic">图 5.5 </em>显示了一个单位数字<em class="italic"> 8 </em>的黑白图像，由 28x28 像素矩阵表示。虽然人类可以通过我们眼中的一些<em class="italic">魔法传感器</em>轻松地将图像识别为数字<em class="italic"> 8 </em>，但计算机需要输入所有 28×28 = 784 个像素，每个像素都有一个<strong class="bold">像素值——一个代表像素亮度的</strong>单一数字。像素值有从 0 到 255 的可能值，0 为黑色，255 为<a id="_idIndexMarker348"/>白色。介于两者之间的值构成了不同的灰度。如果我们有一个<a id="_idIndexMarker349"/>彩色图像，像素将有三个数值 RGB 值(红色、绿色和蓝色)来代表它的颜色，而不是一个黑色值。</p>

<div><div><img alt="Figure 5.5 – Representing the number 8 with pixel values&#10;&#10;" height="498" src="img/Figure_5.5.jpg" width="719"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图 5.5–用像素值表示数字 8</p>

<p>在我们有了图像的像素矩阵表示之后，我们可以开始开发一个用于训练的<strong class="bold">多层感知器</strong> ( <strong class="bold"> MLP </strong>)网络。我们将用 784 个节点和<a id="_idIndexMarker350"/>输入 784 个像素值构建输入层，每个节点一个像素值。输入层的每个节点将输出到下一层(隐藏层)的每个节点，依此类推。当层数增加时，整个网络的总计算量将是巨大的。为了减少总计算量，特征过滤的想法开始发挥作用，并导致了<strong class="bold"> CNN </strong>的概念。</p>

<p>细胞神经网络广泛应用于计算机视觉，尤其是图像识别和处理。CNN 由三层组成:卷积层、池层和全连接层。卷积层卷积输入并过滤图像特征，汇集层压缩<a id="_idIndexMarker352"/>过滤后的特征，全连接层(基本上是 MLP)进行模型训练。让我们详细检查一下每一层。</p>

<h2 id="_idParaDest-123"><a id="_idTextAnchor123"/>卷积层</h2>

<p>一个<strong class="bold">卷积层</strong>执行<a id="_idIndexMarker353"/>卷积，该卷积应用于输入数据以过滤信息并且<a id="_idIndexMarker354"/>产生特征图。该滤波器用作滑动窗口来扫描整个图像，并自主识别图像中的特征。如图<em class="italic">图 5.6 </em>所示，一个 3x3 的滤波器，也就是<a id="_idIndexMarker355"/> <a id="_idIndexMarker356"/>也被称为<strong class="bold">内核</strong> ( <strong class="bold"> K </strong>)，扫描整个<strong class="bold">图像</strong> ( <strong class="bold"> I </strong>)并生成一个特征图，由于其元素来自于<em class="italic"> I </em>和<em class="italic"> K </em>的乘积(在<em class="italic">的例子中)</em></p>

<div><div><img alt=" Figure 5.6 – The convolution operation&#10;&#10;" height="353" src="img/Figure_5.6.jpg" width="750"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图 5.6–卷积运算</p>

<p>通过卷积过程提取图像特征，并生成仍然具有大量数据的特征图，这使得训练神经网络变得困难。为了压缩数据，我们要经过池层。</p>

<h2 id="_idParaDest-124"><a id="_idTextAnchor124"/>汇集层</h2>

<p><strong class="bold">池层</strong>接收来自卷积层的<a id="_idIndexMarker357"/>结果，即特征图，并且<a id="_idIndexMarker358"/>使用过滤器对其进行压缩。根据用于计算的函数，它可以是最大池或平均池。如图<em class="italic">图 5.7 </em>所示，一个 2x2 的滤镜面片扫描特征图并压缩。使用最大池，它从扫描窗口中获取最大值<a id="_idIndexMarker359"/>，<em class="italic"> max(15，8，20，9) = 20 </em>，依此类推。用<a id="_idIndexMarker360"/>平均池，取平均值，<em class="italic"> average(15，8，20，9) = 13 </em>。如您所见，池化图层的过滤器始终小于要素地图。</p>

<div><div><img alt="Figure 5.7 – The pooling layer&#10;&#10;" height="284" src="img/Figure_5.7.jpg" width="606"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图 5.7–池层</p>

<p>从输入图像开始，卷积和汇集的过程迭代，最终结果被输入到全连接层(MLP)进行处理。</p>

<h2 id="_idParaDest-125"><a id="_idTextAnchor125"/>全连接层</h2>

<p>在卷积<a id="_idIndexMarker361"/>和合并<a id="_idIndexMarker362"/>层之后，我们需要将结果扁平化，并将其传递给 MLP，一个完全连接的神经网络，用于分类。最终结果将通过 softmax 激活功能激活，以产生最终输出——对图像的理解。</p>

<h1 id="_idParaDest-126"><a id="_idTextAnchor126"/>递归神经网络</h1>

<p>第二种类型的神经网络是 RNN。rnn 广泛用于时间序列分析，如 NLP。RNN 的概念出现在 20 世纪 80 年代，但直到最近它才在 DL 中获得动力。</p>

<p>我们可以看到，在传统的前向神经网络(如 CNN)中，神经网络中的一个节点只对当前输入进行计数，而不会记住宝贵的输入。因此，它不能处理需要先前输入的时间序列数据。例如，要预测一个句子的下一个单词，将需要前面的单词来做推理。通过引入一个隐藏状态，它可以记住序列的一些信息，RNNs 解决了这个问题。</p>

<p>与前馈网络不同，RNNs 是一种神经网络，其中来自<a id="_idIndexMarker364"/>前一步骤的输出作为当前步骤的输入；使用循环结构来保存信息允许神经网络接受输入序列。如<em class="italic">图 5.8 </em>所示，展开节点<em class="italic"> A </em>的一个循环来说明其过程；首先，节点<em class="italic"> A </em>从输入序列中取出<em class="italic"> X </em> 0，然后输出<em class="italic"> h </em> 0，与<em class="italic"> X </em> 1 一起作为下一步的输入。同样，<em class="italic"> h </em> 1 和<em class="italic"> X </em> 2 是下一步的输入，以此类推。使用循环，网络在训练时保持记忆上下文:</p>

<div><div><img alt="Figure 5.8 – The RNN unrolled loop (source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/)&#10;&#10;" height="151" src="img/Figure_5.8.jpg" width="542"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图 5.8-https://colah.github.io/posts/2015-08-Understanding-LSTMs/ RNN 展开的环</p>

<p>简单的<a id="_idIndexMarker365"/> RNN 模型的缺点是消失梯度问题，这是由于在训练期间和反向传播期间，在每个时间步长使用相同的权重来计算节点的输出。当我们进一步向后移动时，错误信号变得更大或更小，从而导致记忆序列中更远的上下文的困难。为了克服这个缺点，开发了<strong class="bold"> LSTM </strong>神经网络。</p>

<h1 id="_idParaDest-127"><a id="_idTextAnchor127"/>长短期记忆网络</h1>

<p>一个 LSTM 网络<a id="_idIndexMarker367"/>被设计用来克服消失梯度问题。lstm 有反馈连接，lstm 的关键是单元状态——贯穿整个链的水平线，只有很小的线性交互，它保存上下文信息。LSTM 通过门向单元状态添加或移除信息，门由激活函数(如 sigmoid 或 tanh)和逐点乘法操作组成。</p>

<div><div><img alt="Figure 5.9 – An LSTM model (source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/)&#10;&#10;" height="266" src="img/Figure_5.9.jpg" width="725"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图 5.9-一个 LSTM 模型(来源:https://colah.github.io/posts/2015-08-Understanding-LSTMs/)</p>

<p><em class="italic">图 5.9 </em>显示了一个具有保护和控制单元状态的门的 LSTM。利用细胞状态，LSTM 解决了梯度消失的问题，因此特别擅长处理时间序列数据，如文本和语音推理。</p>

<h1 id="_idParaDest-128"><a id="_idTextAnchor128"/>生成性对抗网络</h1>

<p><strong class="bold"> GANs </strong>是算法架构，用于生成<a id="_idIndexMarker368"/>新的数据合成实例，可以冒充真实数据。如图<em class="italic">图 5.10 </em>所示，GAN 是一个创成式模型，同时训练以下两个模型:</p>

<ul>

<li>一个<strong class="bold">生成</strong> ( <strong class="bold"> G </strong>)模型，捕捉数据分布以生成可信数据。潜在的<a id="_idIndexMarker369"/>空间输入和随机噪声可以被采样并馈入生成器网络，以生成成为鉴别器的负训练样本的样本。</li>

<li><strong class="bold">鉴别</strong> ( <strong class="bold"> D </strong>)模型，将生成的图像与真实图像进行比较，并尝试<a id="_idIndexMarker370"/>识别给定图像是假的还是真实的。它估计样本来自训练数据而不是真实数据的概率，以区分生成器的假数据和真实数据。鉴别器对产生不可信结果的发生器<a id="_idTextAnchor129"/>进行处罚。</li>

</ul>

<div><div><img alt="Figure 5.10 – The GAN (source: https://developers.google.com/machine-learning/recommendation)&#10;&#10;" height="897" src="img/Figure_5.10.jpg" width="1316"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图 5.10–GAN(来源:https://developers . Google . com/machine-learning/recommendation)</p>

<p>模型训练<a id="_idIndexMarker371"/>从生成器生成假数据开始，鉴别器通过与真实样本进行比较来学习辨别假数据。然后，GAN 将结果发送给生成器和鉴别器，以更新模型。这个微调训练过程迭代，最终产生一些看起来极其真实的数据。GANs 可用于生成文本、图像、<a id="_idIndexMarker372"/>和视频，并对图像进行着色或去噪。</p>

<h1 id="_idParaDest-129"><a id="_idTextAnchor130"/>总结</h1>

<p>神经网络和 DL 为传统的 ML 光谱增添了现代色彩。在这一章中，我们首先通过检查成本函数、优化算法和激活函数来学习神经网络和 DL 的概念。然后，我们介绍了先进的神经网络，包括 CNN，，，和甘。正如我们所看到的，通过引入神经网络，DL 扩展了 ML 概念，并在许多应用中取得了突破，如计算机视觉、NLP 等。</p>

<p>本章总结了本书的第二部分:<em class="italic">机器学习和深度学习</em>。在第三部分，我们将关注<em class="italic">机器学习谷歌方式</em>，我们将讨论谷歌如何在谷歌云中进行 ML 和 DL。我们将在下一章从学习 BQML、Google TensorFlow 和 Keras 开始第三部分。</p>

<h1 id="_idParaDest-130"><a id="_idTextAnchor131"/>延伸阅读</h1>

<p>有关本章所学主题的更多信息，您可以参考以下链接:</p>

<ul>

<li><a href="https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/anatomy">https://developers . Google . com/machine-learning/crash-course/introduction-to-neural-networks/anatomy</a></li>

<li><a href="https://www.ibm.com/cloud/blog/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks">https://www . IBM . com/cloud/blog/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks</a></li>

<li><a href="https://aws.amazon.com/what-is/neural-network/">https://aws.amazon.com/what-is/neural-network/</a></li>

<li><a href="https://developers.google.com/machine-learning/gan">https://developers.google.com/machine-learning/gan</a></li>

</ul>

</div>

<div><div/>

</div>

</div>



</body></html>