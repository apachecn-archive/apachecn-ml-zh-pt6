

# 九、将 MLflow 用于部署和推理

在本章中，您将了解我们的**机器学习** ( **ML** )系统的端到端部署基础设施，包括使用 MLflow 的推理组件。然后，我们将把我们的模型部署到一个云原生 ML 系统(AWS SageMaker)和一个带有 Kubernetes 的混合环境中。接触这些不同环境的主要目的是让您具备在不同项目的不同环境(云本地和内部)约束下部署 ML 模型的技能。

本章的核心是部署 PsyStock 模型，根据您在整本书中到目前为止所研究的前 14 天的市场行为来预测比特币(BTC/美元)的价格。我们将借助工作流在多个环境中部署它。

具体来说，我们将了解本章的以下部分:

*   启动本地模型注册中心
*   设置批处理推理作业
*   创建用于推理的 API 流程
*   在 Kubernetes 中部署您的批量评分模型
*   使用 AWS SageMaker 进行云部署

# 技术要求

对于本章，您将需要以下先决条件:

*   最新版本的 Docker 安装在您的机器上。如果您还没有安装，请按照 https://docs.docker.com/get-docker/[的说明进行操作。](https://docs.docker.com/get-docker/)
*   安装了最新版本的`docker-compose` 。请按照 https://docs.docker.com/compose/install/.的指示
*   在命令行访问 Git，可以按照[https://Git-SCM . com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)中的描述进行安装。
*   访问 Bash 终端(Linux 或 Windows)。
*   访问浏览器。
*   Python 3.5 以上版本已安装。
*   如第 3 章 *、您的数据科学工作台*中所述，本地安装您的 ML 平台的最新版本。
*   配置为运行 MLflow 模型的 AWS 帐户。

# 启动本地模型注册中心

在执行本章的以下部分之前，您将需要设置一个集中式的模型注册和跟踪服务器。我们不需要完整的数据科学工作台，所以我们可以直接使用内置于模型中的工作台的一个更轻便的变体，我们将在下面的部分中部署它。你应该在本章代码的根文件夹中，可以在 https://github . com/packt publishing/Machine-Learning-Engineering-with-ml flow/tree/master/chapter 09 找到。

接下来，移动到`gradflow`目录，启动一个轻量级的环境来服务您的模型，如下所示:

```
$ cd gradflow
$ export MLFLOW_TRACKING_URI=http://localhost:5000 
$ make gradflow-light
```

在用 MLflow 和从 ML registry 中检索的模型建立了 API 部署的基础设施之后，我们接下来将转移到需要对一些批量输入数据进行评分的情况。我们将用 MLflow 为手头的预测问题准备一个批量推理作业。

# 设置批量推理作业

本节所需的代码在`pystock-inference-api folder`中。MLflow 基础设施在 Docker 镜像中提供，如下图所示:

![Figure 9.1 – Layout of a batch scoring deployment
](img/image0015.jpg)

图 9.1–批量评分部署的布局

如果您可以直接访问工件，那么您可以执行以下操作。代码在`pystock-inference-batch`目录下。为了设置一个批量推理作业，我们将遵循以下步骤:

1.  导入批处理作业的依赖项；在相关的依赖关系中，我们包括`pandas`、`mlflow,`和`xgboost` :

    ```
    import pandas as pd import mlflow import xgboost as xgb import mlflow.xgboost import mlflow.pyfunc
    ```

2.  接下来，我们将通过调用`mlflow.start_run`来加载`start_run`，并从`input.csv`得分输入文件:

    ```
    if __name__ == "__main__":     with mlflow.start_run(run_name="batch_scoring") as run:         data=pd.read_csv("data/input.csv",header=None)
    ```

    中加载数据
3.  接下来，我们通过指定`model_uri`值从注册表中加载模型，基于模型的细节:

    ```
            model_name = "training-model-psystock"         stage = 'Production'         model = mlflow.pyfunc.load_model(                 model_uri=f"models:/{model_name}/{stage}"         )
    ```

4.  我们现在准备好通过运行`model.predict` :

    ```
            y_probas=model.predict(data)
    ```

    来预测我们刚刚读取的数据集
5.  保存批量预测。这基本上包括将`y_preds`变量中的(市场上涨的)概率目标映射到一个从 0 到 1 的值:

    ```
        y_preds = [1 if  y_proba > 0.5 else 0 for y_proba in y_probas]              data[len(data.columns)] =y_preds              result = data     result.to_csv("data/output.csv")
    ```

6.  我们现在需要将作业打包成 Docker 映像，这样我们就可以轻松地在生产中运行它:

    ```
    FROM continuumio/miniconda3 WORKDIR /batch-scoring/ RUN pip install mlflow==1.16.0 RUN pip install pandas==1.2.4 COPY batch_scoring.py   /batch-scoring/ COPY MLproject          /batch-scoring/ ENV MLFLOW_TRACKING_URI=http://localhost:5000 ENTRYPOINT ["mlflow run . --no-conda"]
    ```

7.  构建您的 Docker 图像并标记它，以便您可以引用它:

    ```
    docker build . -t pystock-inference-batch
    ```

8.  通过执行以下命令运行 Docker 映像:

    ```
    docker run -i pystock-inference-batch
    ```

在这种情况下，Docker 映像为您提供了一种在任何支持云中或内部 Docker 映像的计算环境中运行批量评分作业的机制。

我们现在将说明 MLflow 的码头化 API 推理环境的生成。

# 创建用于推理的 API 流程

本节所需的代码在`pystock-inference-api folder`中。MLflow 基础设施在 Docker 映像中提供，伴随如下图所示的代码:

![Figure 9.2 – The structure of the API job
](img/image0026.jpg)

图 9.2–API 作业的结构

依靠 MLflow 内置的 REST API 环境，建立一个 API 系统相当容易。我们将依靠本地文件系统上的工件存储来测试 API。

使用下面这组命令，其核心是在 CLI 中使用`models serve`命令，我们可以为我们的模型提供服务:

```
cd /gradflow/
export MLFLOW_TRACKING_URI=http://localhost:5000
mlflow models serve -m "models:/training-model-psystock/Production" -p 6000
```

接下来，我们将把前面的命令打包到 Docker 映像中，这样它就可以在任何部署环境中使用。实现这一点的步骤如下:

1.  生成 Docker 镜像，指定工作目录和需要作为`entry point` :

    ```
    FROM continuumio/miniconda3 WORKDIR /batch-scoring/ RUN pip install mlflow==1.16.0 ENV MLFLOW_TRACKING_URI=http://localhost:5000 ENTRYPOINT ["mlflow models serve -m "models:/training-model-psystock/Production" -p 6000"]
    ```

    启动的命令
2.  建立你的码头工人形象:

    ```
    docker build . -t pystock-inference-api
    ```

3.  运行您的 Docker 映像:

    ```
    docker run -i pystock-inference-api -p 6000:6000
    ```

在这个阶段，您已经对 API 基础设施进行了 dockerized，并且可以将其部署到您方便的计算环境中。

在深入研究了 MLflow 和 AWS 平台上的云原生部署的交互之后，我们现在将看一个独立于任何提供者的部署。

# 在 Kubernetes 中部署您的批量评分模型

我们将使用 Kubernetes 来部署我们的批评分工作。我们需要做一些修改，使其符合Docker 格式，可接受通过 Kubernetes 在生产中部署 MLflow。本节的先决条件是您有权访问 Kubernetes 集群或者可以设置一个本地集群。这方面的指南可以在 https://kind.sigs.k8s.io/docs/user/quick-start/[或 https://minikube.sigs.k8s.io/docs/start/](https://kind.sigs.k8s.io/docs/user/quick-start/)找到。

现在，您将执行以下步骤，从 Kubernetes 的注册中心部署您的模型:

1.  先决条件:部署并配置`kubectl`(【https://kubernetes.io/docs/reference/kubectl/overview/】)并将其链接到您的 Kubernetes 集群。
2.  创建一个 Kubernetes 后端配置文件:

    ```
    {   "kube-context": "docker-for-desktop",   "repository-uri": "username/mlflow-kubernetes-example",   "kube-job-template-path": "/Users/username/path/to/kubernetes_job_template.yaml" }
    ```

3.  加载输入文件并运行模型:

    ```
    mlflow run . --backend kubernetes --backend-config kubernetes_config.json
    ```

看完了在 Kubernetes 中部署模型之后，我们现在将重点放在在云原生 ML 平台中部署我们的模型。

# 使用 AWS SageMaker 进行云部署

在过去的几年里，像 AWS SageMaker 这样的服务已经作为运行 ML 工作负载的引擎获得了一席之地。MLflow 提供集成和易于使用的命令，将您的模型部署到 SageMaker 基础设施中。由于需要构建大型 Docker 映像并将映像推送到 Docker 注册表，因此执行本部分需要几分钟时间(取决于您的连接，需要 5 到 10 分钟)。

以下是您需要遵循的一些关键先决条件的列表:

*   AWS CLI 使用默认配置文件在本地配置(更多详细信息，可以查看[https://docs . AWS . Amazon . com/CLI/latest/user guide/CLI-chap-configure . html](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html))。
*   帐户中对 SageMaker 及其依赖项的 AWS 访问。
*   AWS 访问账户中的推送亚马逊**弹性容器注册** ( **ECR** )服务。
*   您的 MLflow 服务器需要像第一个*启动本地模型注册中心*部分中提到的那样运行。

要将本地注册中心的模型部署到 AWS SageMaker 中，请执行以下步骤:

1.  建立你的`mlflow-pyfunc`形象。这是将与 SageMaker 兼容的基本图像。
2.  Build and push a container with an `mlflow pyfunc` message:

    ```
    mlflow sagemaker build-and-push-container
    ```

    这个命令将构建 MLflow 默认映像，并将其部署到 Amazon ECR 容器。

    为了确认该命令是否成功，您可以在控制台上检查您的 ECR 实例:

    ![Figure 9.3 – SageMaker deployed image
    ](img/image0036.jpg)

    图 9.3–sage maker 部署的映像

3.  在本地运行您的模型来测试SageMaker Docker 映像并导出跟踪 URI:

    ```
    7777.The output should look like the following excerpt and you should be able to test your model locally:

    ```
    Installing collected packages: mlflow   Attempting uninstall: mlflow     Found existing installation: mlflow 1.16.0     Uninstalling mlflow-1.16.0:       Successfully uninstalled mlflow-1.16.0 Successfully installed mlflow-1.15.0 pip 20.2.4 from /miniconda/lib/python3.8/site-packages/pip (python 3.8) Python 3.8.5 1.15.0 [2021-05-08 14:01:43 +0000] [354] [INFO] Starting gunicorn 20.1.0 [2021-05-08 14:01:43 +0000] [354] [INFO] Listening at: http://127.0.0.1:8000 (354)
    ```

     这将基本上确认映像按预期工作，并且您应该能够在 SageMaker 中运行您的 API。
    ```

4.  Double-check your image through the AWS `cli`:

    ```
    aws ecr describe-images --repository-name mlflow-pyfunc 
    ```

    您应该在映像列表中看到您部署的映像，并且可以运行了。

5.  您需要在 AWS 中配置一个指定的角色，以允许 SageMaker 代表您创建资源(您可以在 https://docs . data bricks . com/administration-guide/cloud-configuration s/AWS/sage maker . html # step-1-create-an-AWS-iam-role-and-attach-sage maker-permission-policy 找到更多详细信息)。
6.  接下来，您需要使用以下命令将您的区域和角色导出到`$REGION`和`$ROLE`环境变量中，指定您的环境的实际值:

    ```
    export $REGION=your-aws-region export $ROLE=your sagemaker-enabled-role
    ```

7.  To deploy your model to SageMaker, run the following command:

    ```
    mlflow sagemaker deploy -a pystock-api -m models:/training-model-psystock/Production –region-name $REGION -- $ROLE
    ```

    这个命令将把您的模型从本地注册中心作为内部表示加载到 SageMaker 中，并使用生成的 Docker 映像在 AWS SageMaker 基础设施引擎中为模型提供服务。设置所有基础设施需要几分钟时间。成功后，您应该会看到以下消息:

    ```
    2021/05/08 21:09:12 INFO mlflow.sagemaker: The deployment operation completed successfully with message: "The SageMaker endpoint was created successfully."
    ```

8.  Verify your SageMaker endpoint:

    ```
    aws sagemaker list-endpoints
    ```

    您可以查看以下输出消息类型的示例:

    ```
    {
        "Endpoints": [
            {
                "EndpointName": "pystock-api",
                "EndpointArn": "arn:aws:sagemaker:eu-west-1:123456789:endpoint/pystock-api",
                "CreationTime": "2021-05-08T21:01:13.130000+02:00",
                "LastModifiedTime": "2021-05-08T21:09:08.947000+02:00",
                "EndpointStatus": "InService"
            }
        ]
    }
    ```

9.  Next we need to consume our API with a simple script that basically the features, invokes the SageMaker endpoint using the Amazon Boto3 client, and prints the probablity of the market pricesgiven the feature vector:

    ```
    import pandas
    import boto3
    features = pd.DataFrame([[1,0,1,1,0,1,0,1,0,1,0,1,0,1]])
    payload = features.to_json(orient="split")
    result  = runtime.invoke_endpoint(
                EndpointName='pystock-api', Body=payload, 
                ContentType='application/json')
    preds = result['Body'].read().decode("ascii")
    print(preds)
    ```

    运行上述脚本后，您应该会看到以下输出:

    ```
    '[0.04279635474085808]
    ```

10.  Explore the SageMaker endpoint interface. In its monitoring component, you can look at different metrics related to your deployment environment and model as shown in *Figure 9.4*:![Figure 9.4 – SageMaker inference instance metrics
    ](img/image0046.jpg)

    图 9.4–sage maker 推理实例指标

11.  You can now easily tear down your deployed model, when in need to deploy the model or phase it out. All associated resources will be torn down:

    ```
    mlflow sagemaker delete -a pystock-api --region-name $REGION
    ```

    删除后，您应该会看到类似于以下摘录中的消息:

    ```
    2021/05/08 23:49:46 INFO mlflow.sagemaker: The deletion operation completed successfully with message: "The SageMaker endpoint was deleted successfully."
    2021/05/08 23:49:46 INFO mlflow.sagemaker: Cleaning up unused resources...
    2021/05/08 23:49:47 INFO mlflow.sagemaker: Deleted associated endpoint configuration with arn: arn:aws:sagemaker:eu-west-1:123456789:endpoint-config/pystock-api-config-v-hznm3ttxwx-g8uavbzia
    2021/05/08 23:49:48 INFO mlflow.sagemaker: Deleted associated model with arn: arn:aws:sagemaker:eu-west-1:123456789:model/pystock-api-model-4nly3634reqomejx1owtdg
    ```

在此部分，我们总结了与从您的本地机器在不同环境中使用 MLflow 在生产中部署 ML 模型相关的特性描述，包括 Docker 和`docker-compose`、公共云，以及使用 AWS SageMaker 的非常灵活的方法。

# 摘要

在这一章中，我们重点关注 ML 模型的生产部署、其背后的概念，以及可用于使用 MLflow 在多个环境中部署的不同特性。

我们解释了如何为部署准备 Docker 映像。我们还阐明了如何与 Kubernetes 和 AWS SageMaker 交互来部署模型。

在本书的下一章和后续章节中，我们将重点关注使用工具来帮助扩展我们的 MLflow 工作负载，以提高我们的模型基础架构的性能。

# 延伸阅读

为了加深您的知识，您可以参考以下链接中的文档:

*   [https://www . ml flow . org/docs/latest/python _ API/ml flow . sage maker . html](https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html)
*   [https://AWS . Amazon . com/blogs/machine-learning/managing-your-machine-learning-life cycle-with-ml flow-and-Amazon-sage maker/](https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/)