# 二、处理数据准备技巧

数据是任何机器学习项目的起点，将数据转化为可用于训练模型的数据集需要大量工作。这项工作通常包括注释数据集、运行定制脚本对其进行预处理，以及保存处理后的版本以备后用。正如你所猜测的，手动完成所有这些工作，或者构建工具来自动化这些工作，对于机器学习团队来说并不是一个令人兴奋的前景。

在本章中，您将了解帮助您构建和处理数据的 AWS 服务。我们将首先介绍**亚马逊 SageMaker 地面真相**，亚马逊 SageMaker 的一项功能，可以帮助您快速建立准确的训练数据集。然后，我们将介绍**Amazon sage maker Data Wrangler**，一种交互转换数据的新方法。接下来，我们将讨论 **Amazon SageMaker 处理**，这是另一种帮助您运行数据处理工作负载的功能，例如特征工程、数据验证、模型评估和模型解释。最后，我们将快速讨论其他可能有助于数据分析的 AWS 服务: **Amazon Elastic Map Reduce** 、 **AWS Glue** 和 **Amazon Athena** 。

本章包括以下主题:

*   用亚马逊 SageMaker 地面真相标注数据
*   使用 Amazon SageMaker Data Wrangler 转变数据
*   使用 Amazon SageMaker 处理运行批处理作业

# 技术要求

您将需要一个 AWS 帐户来运行本章中包含的示例。如果您还没有，请将浏览器指向[https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)来创建一个。您还应该熟悉 AWS 免费层，它允许您在一定的使用限制内免费使用许多 AWS 服务。

您需要为您的帐户([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))安装并配置 AWS **命令行界面** ( **CLI** )。

您将需要一个工作的 Python 3.x 环境。安装 Anaconda 发行版([https://www.anaconda.com/](https://www.anaconda.com/))不是强制性的，但是强烈建议安装，因为它包含了我们将需要的许多项目(Jupyter、`pandas`、`numpy`等等)。

书中包含的代码示例可在 GitHub 上获得，网址为[https://GitHub . com/packt publishing/Learn-Amazon-sage maker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition)。你需要安装一个 Git 客户端来访问它们([https://git-scm.com/](https://git-scm.com/))。

# 用亚马逊 SageMaker 地面真相标注数据

添加到亚马逊 sage maker2018 年末，亚马逊 SageMaker Ground Truth 帮助你快速建立精确的训练数据集。机器学习从业者可以将贴标工作分配给人类贴标机的公共和私人劳动力。得益于内置的工作流程和图形界面，贴标机可以立即完成常见的图像、视频和文本任务。此外，Ground Truth 可以实现自动标记，这是一种训练机器学习模型的技术，能够在没有额外人工干预的情况下标记数据。

在这一节中，你将学习如何使用地面真相来标记图像和文本。

## 使用劳动力

使用 Ground Truth 的第一步是创建一个劳动力，一组负责标记数据样本的工人。

让我们前往 SageMaker 控制台:在左侧垂直菜单中，我们点击**地面实况**，然后点击**贴标工作人员**。三种类型的劳动力可供选择:**亚马逊土耳其机械**、**供应商**和**私人**。让我们讨论一下它们是什么，以及何时应该使用它们。

### 亚马逊土耳其机械公司

**亚马逊土耳其机械公司**([https://www.mturk.com/](https://www.mturk.com/))使得很容易将大批量工作分解成小工作单元，这些小工作单元可以由分布式劳动力处理。

有了 Mechanical Turk，您可以招募遍布全球的数万甚至数十万名员工。当您需要标注非常大的数据集时，这是一个很好的选择。例如，考虑一个由 1000 小时视频组成的自动驾驶数据集:每一帧都需要进行处理，以识别其他车辆、行人、路标等。如果你想注释每一帧，你会看到 1000 小时 x 3,600 秒 x 每秒 24 帧=**8640 万张图像**！显然，你必须扩大你的标签劳动力来完成这项工作，机械土耳其人让你做到这一点。

### 供应商劳动力

尽管 Mechanical Turk 具有可扩展性，但有时您需要更多地控制与谁共享数据，以及注释的质量，尤其是在需要额外的领域知识的情况下。

出于这个目的，AWS 已经审查了许多数据标签公司，这些公司已经在他们的工作流程中集成了 Ground Truth。你可以在**AWS market place**([https://aws.amazon.com/marketplace/](https://aws.amazon.com/marketplace/))的**机器学习** | **数据标签服务** | **亚马逊 SageMaker 地面真相服务**下找到的公司名单。

### 私人劳动力

有时候，数据是无法被第三方处理的。可能只是太敏感，也可能需要只有贵公司员工才有的专业知识。在这种情况下，您可以创建一个由身份明确的个人组成的私有劳动力，他们将访问并标记您的数据。

## 创建私人劳动力

创建私人劳动力是最快也是最简单的选择。让我们看看它是如何做到的:

1.  Starting from the **Labeling workforces** entry in the SageMaker console, we select the **Private** tab, as seen in the following screenshot. Then, we click on **Create private team**:![Figure 2.1 – Creating a private workforce
    ](img/B17705_02_001.jpg)

    图 2.1–创建私人劳动力

2.  We give the team a name, then we have to decide whether we're going to invite workers by email, or whether we're going to import users that belong to an existing **Amazon Cognito** group.

    亚马逊 cogn ITO([https://aws.amazon.com/cognito/](https://aws.amazon.com/cognito/))是一个托管服务，让你建立和管理任何规模的用户目录。Cognito 既支持社交身份提供商(谷歌、脸书和亚马逊)，也支持企业身份提供商(微软活动目录、SAML)。

    这在企业环境中很有意义，但是让我们保持简单，使用电子邮件代替。在这里，我将使用一些示例电子邮件地址:请确保使用您自己的电子邮件地址，否则您将无法加入团队！

3.  然后，我们需要输入一个组织名称，更重要的是一个联系电子邮件，工人可以使用它来询问贴标工作的问题和反馈。这些对话对于微调标签说明、查明有问题的数据样本等极其重要。
4.  可选地，我们可以用**亚马逊简单通知服务**([https://aws.amazon.com/sns/](https://aws.amazon.com/sns/))设置通知，让工人知道他们有工作要做。
5.  The screen should look like in the following screenshot. Then, we click on **Create private team**:![Figure 2.2 – Setting up a private workforce
    ](img/B17705_02_002.jpg)

    图 2.2–建立私人劳动力

6.  A few seconds later, the team has been set up. Invitations have been sent to workers, requesting that they join the workforce by logging in to a specific URL. The invitation email looks like that shown in the following screenshot:![Figure 2.3 – Email invitation
    ](img/B17705_02_003.jpg)

    图 2.3–电子邮件邀请

7.  单击该链接会打开一个登录窗口。一旦我们登录并定义了一个新密码，我们就会进入一个显示可用工作的新屏幕，如下图所示。因为我们还没有定义一个，所以它显然是空的:

![Figure 2.4 – Worker console
](img/B17705_02_004.jpg)

图 2.4–工人控制台

让我们的工人保持忙碌，创造一个图像标签的工作。

## 上传标签数据

如你所料，亚马逊 SageMaker Ground Truth 使用亚马逊 S3 存储数据集:

1.  使用 AWS CLI，我们在运行 SageMaker 的同一个区域中创建一个 S3 bucket。存储桶名称是全局唯一的，因此请确保在创建存储桶时选择您自己的唯一名称。使用下面的代码(随意使用另一个 AWS 区域):

    ```
    $ aws s3 mb s3://sagemaker-book --region eu-west-1
    ```

2.  然后，我们复制位于 GitHub 库的`chapter2`文件夹中的猫图像如下:

    ```
    $ aws s3 cp --recursive cat/ s3://sagemaker-book/chapter2/cat/
    ```

现在我们有一些数据等待标注，让我们创建一个标注作业。

## 创建贴标作业

如您所料，我们需要定义数据的位置，我们想要为它标记什么类型的任务，以及我们的指令是什么:

1.  在 SageMaker 控制台的左侧垂直菜单中，我们点击**地面实况**，然后点击**贴标作业**，然后点击**创建贴标作业**按钮。
2.  首先，我们给作业起一个名字，比如'`my-cat-job`'。然后，我们定义数据在 S3 的位置。Ground Truth 期望一个**清单文件**:清单文件是一个 **JSON** 文件，让你过滤哪些对象需要被标记，哪些应该被忽略。一旦作业完成，一个名为增强清单的新文件将包含标签信息，我们将能够使用它向训练作业提供数据。
3.  Then, we define the location and the type of our input data, just like in the following screenshot:![Figure 2.5 – Configuring input data
    ](img/B17705_02_005.jpg)

    图 2.5–配置输入数据

4.  As is visible in the next screenshot, we select the IAM role that we created for SageMaker in the first chapter (your name will be different), and we then click on the **Complete data setup** button to validate this section:![Figure 2.6 – Validating input data
    ](img/B17705_02_006.jpg)

    图 2.6–验证输入数据

    点击**查看更多详情**，你可以了解到引擎盖下正在发生的事情。SageMaker Ground Truth 抓取你在 S3 的数据，并创建一个名为**清单文件**的 JSON 文件。如果你好奇，可以去 S3 下载。该文件指向 S3 中的对象(图像、文本文件等)。

5.  或者，我们可以决定使用完整的清单、随机样本或者基于 **SQL** 查询的过滤子集。我们还可以提供一个**亚马逊 KMS** 密钥来加密作业的输出。让我们坚持这里的默认值。
6.  The **Task type** section asks us what kind of job we'd like to run. Please take a minute to explore the different task categories that are available (text, image, video, point cloud, and custom). As shown in the next screenshot, let's select the **Image** task category and the **Semantic segmentation** task, and then click **Next**:![Figure 2.7 – Selecting a task type
    ](img/B17705_02_007.jpg)

    图 2.7–选择任务类型

7.  On the next screen, visible in the following screenshot, we first select our private team of workers:![Figure 2.8 – Selecting a team type
    ](img/B17705_02_008.jpg)

    图 2.8–选择团队类型

8.  如果我们有大量的样本(比如，数万或更多)，我们应该考虑启用**自动数据贴标**，因为该功能将减少贴标工作的持续时间和成本。事实上，随着工人开始标记数据样本，SageMaker Ground Truth 会在这些样本上训练一个机器学习模型。它会将它们作为一个数据集，用于监督学习问题。有了足够的工人标记的数据，这个模型将很快能够匹配并超过人类的准确性，在这一点上，它将取代工人并标记数据集的其余部分。如果你想了解更多关于这个功能的信息，请阅读 https://docs . AWS . Amazon . com/sage maker/latest/DG/SMS-automated-labeling . html 上的文档。
9.  The last step in configuring our training job is to enter instructions for the workers. This is an important step, especially if your job is distributed to third-party workers. The better our instructions, the higher the quality of the annotations. Here, let's explain what the job is about, and enter a "cat" label for workers to apply. In a real-life scenario, you should add detailed instructions, provide sample images for good and bad examples, explain what your expectations are, and so on. The following screenshot shows what our instructions look like:![Figure 2.9 – Setting up instructions
    ](img/B17705_02_009.jpg)

    图 2.9–设置说明

10.  一旦我们完成指令，我们点击**创建**启动贴标工作。几分钟后，工作就可以分配给工人了。

## 标记图像

登录到 worker URL，我们可以从下面的屏幕截图中看到我们有工作要做:

![Figure 2.10 – Worker console
](img/B17705_02_010.jpg)

图 2.10–工人控制台

我们将使用以下步骤:

1.  Clicking on **Start working** opens a new window, visible in the next picture. It displays instructions as well as a first image to work on:![Figure 2.11 – Labeling images
    ](img/B17705_02_011.jpg)

    图 2.11–标记图像

2.  使用工具栏中的图形工具，尤其是自动分段工具，我们可以非常快速地生成高质量的注释。请花几分钟时间练习，你很快就能做到。
3.  一旦我们完成了三个图像，工作就完成了，我们可以在 SageMaker 控制台中的**标签工作**下可视化标签图像。您的屏幕应该如下图所示:

![Figure 2.12 – Labeled images
](img/B17705_02_012.jpg)

图 2.12–带标签的图像

更重要的是，我们可以在 S3 输出位置找到标签信息。

在中，`output/my-cat-job/manifests/output/output.manifest`包含关于每个数据样本的注释信息，例如图像中存在的类别，以及到分段掩码的链接。

在 [*第 5 章*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091) *训练计算机视觉模型*中，我们将看到如何将这些信息直接馈送到亚马逊 SageMaker 中实现的内置计算机视觉算法。当然，我们也可以解析这些信息，并将其转换为我们用来训练计算机视觉模型的任何框架。

如您所见，SageMaker Ground Truth 使标注影像数据集变得非常容易。你只需要上传你的数据到 S3，然后创建一个劳动力。Ground Truth 将自动分发作品，并将结果存储在 S3。

我们刚刚看到了如何标记图像，但是文本任务呢？嗯，它们同样容易设置和运行。我们来看一个例子。

## 标注文本

这是一个为命名实体识别标记文本的的简单例子。该数据集由我的一篇博客文章的文本片段组成，我们希望在其中标注所有 AWS 服务名称。这些都可以在我们的 GitHub 库中找到。

我们将使用以下步骤开始标记文本:

1.  首先，让我们用下面一行代码将文本片段上传到 S3:

    ```
    $ aws s3 cp --recursive ner/ s3://sagemaker-book/chapter2/ner/
    ```

2.  Just like in the previous example, we configure a text labeling job, set up input data, and select an IAM role, as shown in the following screenshot:![Figure 2.13 – Creating a text labeling job
    ](img/B17705_02_013.jpg)

    图 2.13–创建文本标签作业

3.  然后，我们选择**文本**作为类别，**命名实体识别**作为任务。
4.  On the next screen, shown in the following screenshot, we simply select our private team again, add a label, and enter instructions:![Figure 2.14 – Setting up instructions
    ](img/B17705_02_014.jpg)

    图 2.14–设置说明

5.  Once the job is ready, we log in to the worker console and start labeling. You can see a labeled example in the following screenshot:![Figure 2.15 – Labeling text
    ](img/B17705_02_015.jpg)

    图 2.15–标签文本

6.  我们很快就完成了，我们可以在我们的 S3 桶中找到标签信息。对于每个样本，我们可以看到每个标记实体的起始偏移量、结束偏移量和标签。

亚马逊 SageMaker Ground Truth 真的让大规模标注数据集变得很容易。它有许多不错的功能，包括作业链和定制工作流程，我鼓励你在 https://docs.aws.amazon.com/sagemaker/latest/dg/sms.html[探索这些功能。](https://docs.aws.amazon.com/sagemaker/latest/dg/sms.html)

现在我们知道了如何标记数据集，让我们看看如何使用 Amazon SageMaker Data Wrangler 轻松地交互转换数据。

# 使用 Amazon SageMaker Data Wrangler 转换数据

收集和标记数据样本只是准备数据集的第一步。事实上，您很可能需要对数据集进行预处理，以便执行以下操作，例如:

*   将其转换为您正在使用的机器学习算法所期望的输入格式。
*   重新缩放或归一化数字特征。
*   设计更高级的功能，例如一键编码。
*   为自然语言处理应用清理和标记文本

在机器学习项目的早期阶段，并不总是很明显需要哪些转换，或者哪些转换最有效。因此，实践者经常需要试验许多不同的组合，以许多不同的方式转换数据，训练模型，并评估结果。

在本节中，我们将了解**Amazon sage maker Data Wrangler**，这是一个集成在 SageMaker Studio 中的图形界面，可以非常轻松地转换数据，并将结果导出到各种 Jupyter 笔记本电脑中。

## 在 SageMaker Data Wrangler 中加载数据集

首先，我们需要一个数据集。我们将使用 S. Moro、P. Cortez 和 P. Rita 于 2014 年 6 月在 Elsevier 出版的《预测银行电话营销成功的数据驱动方法》、*决策支持系统*中发布的直接营销数据集。

这个数据集描述了一个二元分类问题:一个客户是否会接受一个营销提议？它包含 41，000 多个客户样本，标签存储在 **y** 列中。

我们将从以下步骤开始:

1.  使用 AWS 命令行，让我们下载数据集，提取它，并将其复制到我们正在运行的区域的默认 SageMaker bucket 中(它应该是自动创建的)。您可以在本地机器或 Jupyter 终端上运行这个程序:

    ```
    $ aws s3 cp s3://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip . $ unzip bank-additional.zip $ aws s3 cp bank-additional/bank-additional-full.csv s3://sagemaker-ap-northeast-2-123456789012/direct-marketing/
    ```

2.  In SageMaker Studio, we create a new Data Wrangler flow with **File** | **New** | **Data Wrangler Flow** to create. The following screenshot shows the Data Wrangler image being loaded:![Figure 2.16 – Loading Data Wrangler
    ](img/B17705_02_016.jpg)

    图 2.16–加载数据牧马人

3.  Once Data Wrangler is ready, the **Import** screen opens. We also see the Data Wrangler image in the left-hand pane, as shown in the next screenshot:![Figure 2.17 – Opening Data Wrangler
    ](img/B17705_02_017.jpg)

    图 2.17–开始数据辩论

4.  我们可以从 S3、雅典娜或红移导入数据(通过点击**添加数据源**)。在这里，我们点击 S3。
5.  As shown in the following screenshot, we can easily locate the dataset that we just uploaded. Let's click on it.![Figure 2.18 – Locating a dataset
    ](img/B17705_02_018.jpg)

    图 2.18–定位数据集

6.  This opens a preview of the dataset, as shown in the next screenshot:![Figure 2.19 – Previewing a dataset
    ](img/B17705_02_019.jpg)

    图 2.19–预览数据集

7.  Let's just click on **Import**, which opens the **Prepare** view, as shown in the next screenshot:![Figure 2.20 – Previewing a dataset
    ](img/B17705_02_020.jpg)

    图 2.20–预览数据集

8.  Clicking on the **+** icon, we could add more data sources, joining them or concatenating them to our dataset. We could also edit data types for all columns, should Data Wrangler have detected them incorrectly. Instead, let's select **Add analysis** to visualize properties of our dataset. This opens the **Analyze view**, visible in the next screenshot:![Figure 2.21 – Visualizing a dataset
    ](img/B17705_02_021.jpg)

    图 2.21–可视化数据集

9.  The next screenshot shows a scatter plot on duration vs. age. See how easy this is? You can experiment by selecting different columns, click on **Preview** to see results, and click on **Save** to create the analysis and save it for further use.![Figure 2.22 – Building a scatter plot
    ](img/B17705_02_022.jpg)

    图 2.22–构建散点图

10.  On top of histograms and scatter plots, we can also build **Table Summary**, **Bias Analysis**, and **Target Leakage** reports. Let's build the latter to find out if certain columns are either leaking into the prediction, or not helpful at all. You can see the report in the next screenshot:![Figure 2.23 – Building a target leakage report
    ](img/B17705_02_023.jpg)

    图 2.23–构建目标泄漏报告

11.  这个报告告诉我们一栏都没有漏(所有分数都低于 1)。一些列在预测目标时也没有用(一些分数是 0.5 或更低):我们可能应该在数据处理过程中删除这些列。

我们也可以尝试**快速模型**报告，它使用一个用 Spark 实现的**随机森林**算法来训练一个模型，就在 SageMaker Studio 中。不幸的是，弹出一条错误消息，抱怨列名。的确，有些列名包含一个点，这是 Spark 所不允许的。没问题，我们可以在数据处理过程中轻松解决这个问题，并在以后构建报告。

事实上，让我们继续使用 Data Wrangler 来转换数据。

## 在 SageMaker Data Wrangler 中转换数据集

数据牧马人包括数百个内置变换，我们也可以添加自己的。

1.  Starting from the **Prepare** view visible in the next screenshot, we click on the **+** icon to add transforms.![Figure 2.24 – Adding a transform
    ](img/B17705_02_024.jpg)

    图 2.24–添加转换

2.  这将打开转换列表，如下一个屏幕截图所示。花一分钟来探索它们。
3.  Let's start by dropping the columns flagged as useless in the `marital`, `day of week`, `month`, `housing`, `cons.conf.idx`, `nr.employed`, `cons.price.idx`. We click on `marital` column. Your screen should look like the following screenshot:![Figure 2.25 – Dropping a column
    ](img/B17705_02_025.jpg)

    图 2.25–删除一列

4.  我们可以预览结果，并将转换添加到管道中。我们将对想要删除的其他列重复相同的操作。
5.  Now, let's remove these annoying dots in column names, replacing them with underscores. The easiest way to do this is to use a `df`.![Figure 2.26 – Applying a custom transform
    ](img/B17705_02_026.jpg)

    图 2.26–应用自定义转换

6.  Jumping back to the **Analyze** view, and clicking on **Steps**, we can see the list of transforms that we've already applied, as shown in the next screenshot. We could also delete each transform by clicking on the icon to the right of it.![Figure 2.27 – Viewing a pipeline
    ](img/B17705_02_027.jpg)

    图 2.27–查看管道

7.  Clicking on the `y` label, as shown in the next screenshot. The F1 score for this classification model is 0.881, and the most important features are `duration`, `euribor3m`, and `pdays`. By applying more transforms and building a quick model again, we can iteratively measure the positive impact (or the lack thereof) of our feature engineering steps.![Figure 2.28 – Building a quick model
    ](img/B17705_02_028.jpg)

    图 2.28–构建快速模型

8.  Coming back to the `job` and `education`. We decide to encode them to help algorithms understand that the different values are different dimensions to the problem. Starting with `job`, we apply the `job` column is automatically dropped.![Figure 2.29 – One-hot encoding a column
    ](img/B17705_02_029.jpg)

    图 2.29–一键编码列

9.  `job_admin.`列名包含一个点！我们可以用`education`列删除它…并删除列名中的点。我们可以应用**过程数值**转换来缩放和规范化数值列，但是让我们暂时停止。请随意探索和实验！
10.  最后一件事:数据管理器工作流存储在`.flow`文件中，在 Jupyter 文件视图中可见。这些是 JSON 文件，您可以(也应该)将它们存储在您的 Git 存储库中，以便以后重用它们并与其他团队成员共享。

现在我们的管道已经准备好了，让我们看看如何将它导出到 Python 代码。只需点击一下，我们就不用写一行代码了。

## 导出 SageMaker Data Wrangler 管道

Data Wrangler 可以通过四种方式轻松导出管道:

*   您可以在您的机器学习项目中轻松包含的普通 Python 代码。
*   运行 SageMaker 处理作业的 Jupyter 笔记本，该作业会将管道应用到数据集并将结果保存在 S3。该笔记本还包括训练模型的可选代码。
*   Jupyter 笔记本将处理后的数据集存储在 SageMaker 要素存储中。
*   一个 Jupyter 笔记本，创建一个 SageMaker Pipelines 工作流，包含处理数据集和在其上训练模型的步骤。

好吧，让我们开始吧:

1.  Starting from the **Export** view, we click on Steps and select the steps we'd like to export. Here, I selected them all, as shown in the next screenshot:![Figure 2.30 – Selecting steps to export
    ](img/B17705_02_030.jpg)

    图 2.30–选择要导出的步骤

2.  然后，我们只需点击**导出步骤**并选择四个选项之一。在这里，我选择**保存到 S3** ，以便运行一个 SageMaker 处理作业。
3.  This opens a new notebook. We'll discuss SageMaker Processing in the next section, but let's go ahead and run the job. Once the Job Status & S3 Output Location cell is complete, our dataset is available in S3, as visible in the next screenshot:![Figure 2.31 – Locating the processed dataset in S3
    ](img/B17705_02_031.jpg)

    图 2.31-在 S3 定位已处理的数据集

4.  下载并打开存储在该位置的 CSV 文件，我们看到它包含处理过的数据集，如下一个屏幕截图所示。在典型的机器学习工作流程中，我们将直接使用这些数据来训练模型。

![Figure 2.32 – Viewing the processed dataset
](img/B17705_02_032.jpg)

图 2.32–查看已处理的数据集

如您所见，SageMaker Data Wrangler 使得将变换应用到数据集变得非常容易(甚至有趣)。完成后，您可以立即将它们导出为 Python 代码，而无需编写任何代码。

在下一节中，我们将了解 Amazon SageMaker 处理，这是一种为数据处理和其他机器学习任务运行批处理作业的好方法。

# 使用 Amazon SageMaker 处理运行批处理作业

正如前面的部分所讨论的，数据集通常需要做大量的工作来为训练做好准备。训练完成后，您可能还需要运行其他作业来对预测数据进行后处理，并对不同数据集的模型进行评估。

一旦实验阶段完成，开始自动化所有这些工作是一个很好的实践，这样您就可以毫不费力地按需运行它们。

## 发现亚马逊 SageMaker 处理 API

亚马逊 SageMaker 处理 API 是 SageMaker SDK 的一部分，我们在 [*第一章*](B17705_01_Final_JM_ePub.xhtml#_idTextAnchor013) *中安装，介绍亚马逊 SageMaker* 。

SageMaker 处理作业在 Docker 容器中运行:

*   用于**scikit-learn**([https://scikit-learn.org](https://scikit-learn.org))的内置容器
*   **py spark**([https://spark.apache.org/docs/latest/api/python/](https://spark.apache.org/docs/latest/api/python/))的内置容器，支持分布式训练
*   您自己的定制容器

日志在`/aws/sagemaker/ProcessingJobs`日志组中可用。

让我们首先来看看如何使用 scikit-learn 和 SageMaker 处理来准备用于训练的数据集。

## 使用 scikit-learn 处理数据集

以下是高级流程:

*   将您的未处理数据集上传到亚马逊 S3。
*   使用 scikit-learn 编写一个脚本,以加载数据集、处理数据集并保存处理后的要素和标注。
*   在托管基础设施上运行带有 SageMaker 处理的脚本。

### 将数据集上传到亚马逊 S3

我们将重用前一节中介绍的直销数据集，并应用我们自己的转换。

1.  创建一个新的 Jupyter 笔记本，我们先下载并提取数据集:

    ```
    %%sh apt-get -y install unzip wget -N https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip unzip -o bank-additional.zip
    ```

2.  然后，我们用`pandas` :

    ```
    import pandas as pd data = pd.read_csv('./bank-additional/bank-additional-full.csv') print(data.shape) (41188, 21)
    ```

    加载它
3.  Now, let's display the first five lines:

    ```
    data[:5] 
    ```

    这将打印出在下图中可见的表格:

    ![Figure 2.33 – Viewing the dataset
    ](img/B17705_02_033.jpg)

    图 2.33–查看数据集

    向右滚动，我们可以看到一个名为 **y** 的列，存储标签。

4.  现在，让我们将数据集上传到亚马逊 S3。我们将使用 SageMaker 在我们正在运行的区域中自动创建的默认 bucket。我们将添加一个前缀来保持事物的整洁:

    ```
    import sagemaker prefix = 'sagemaker/DEMO-smprocessing/input' input_data = sagemaker.Session().upload_data(path='./bank-additional/bank-additional-full.csv', key_prefix=prefix)
    ```

### 使用 scikit-learn 编写处理脚本

由于 SageMaker 处理解决了所有基础设施问题，我们可以专注于脚本本身。SageMaker 处理还会自动将输入数据集从 S3 复制到容器中，并将经过处理的数据集从容器中复制到 S3。

当我们配置作业本身时，会提供容器路径。我们将使用以下方法:

*   输入数据集:`/opt/ml/processing/input`
*   处理后的训练集:`/opt/ml/processing/train`
*   已处理的测试集:`/opt/ml/processing/test`

在我们的 Jupyter 环境中，让我们开始编写一个名为`preprocessing.py`的新 Python 文件。如您所料，此脚本将加载数据集，执行基本的要素工程，并保存处理后的数据集:

1.  首先，我们用`argparse`库([https://docs.python.org/3/library/argparse.html](https://docs.python.org/3/library/argparse.html))读取我们的单个命令行参数:训练和测试数据集的比率。实际值将由 SageMaker 处理 SDK 传递给脚本:

    ```
    import argparse parser = argparse.ArgumentParser() parser.add_argument('--train-test-split-ratio',                      type=float, default=0.3) args, _ = parser.parse_known_args() print('Received arguments {}'.format(args)) split_ratio = args.train_test_split_ratio
    ```

2.  我们使用`pandas`加载输入数据集。启动时，SageMaker 处理自动将其从 S3 复制到容器内用户定义的位置，`/opt/ml/processing/input` :

    ```
    import os import pandas as pd input_data_path = os.path.join('/opt/ml/processing/input', 'bank-additional-full.csv') df = pd.read_csv(input_data_path) 
    ```

3.  然后，我们删除任何缺少值的行，以及重复的行:

    ```
    df.dropna(inplace=True) df.drop_duplicates(inplace=True)
    ```

4.  然后，统计正负样本，显示类比。这将告诉我们数据集有多不平衡:

    ```
    one_class = df[df['y']=='yes'] one_class_count = one_class.shape[0] zero_class = df[df['y']=='no'] zero_class_count = zero_class.shape[0] zero_to_one_ratio = zero_class_count/one_class_count print("Ratio: %.2f" % zero_to_one_ratio)
    ```

5.  查看数据集，我们可以看到一个名为`pdays`的列，它告诉我们多久以前联系过一个客户。有些行的值是 999，这看起来很可疑:实际上，这是一个占位符值，表示从未联系过客户。为了帮助模型理解这个假设，让我们添加一个新的列来明确地说明它:

    ```
    import numpy as np df['no_previous_contact'] =     np.where(df['pdays'] == 999, 1, 0)
    ```

6.  在“工作”列中，我们可以看到三个类别(`student`、`retired`和`unemployed`)，这三个类别可能应该进行分组，以表明这些客户没有全职工作。我们再加一栏:

    ```
    df['not_working'] = np.where(np.in1d(df['job'], ['student', 'retired', 'unemployed']), 1, 0)
    ```

7.  现在，让我们将数据集分成训练集和测试集。Scikit-learn 为此提供了一个方便的 API，我们根据传递给脚本的命令行参数来设置拆分比率:

    ```
    from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(         df.drop('y', axis=1),         df['y'],         test_size=split_ratio, random_state=0) 
    ```

8.  下一步是缩放数字特征并对分类特征进行一次性编码。我们将使用`StandardScaler`表示前者，使用`OneHotEncoder`表示后者:

    ```
    from sklearn.compose import make_column_transformer from sklearn.preprocessing import StandardScaler,OneHotEncoder preprocess = make_column_transformer(   (StandardScaler(), ['age', 'duration', 'campaign', 'pdays', 'previous']),   (OneHotEncoder(sparse=False), ['job', 'marital', 'education', 'default', 'housing', 'loan','contact', 'month', 'day_of_week', 'poutcome']) )
    ```

9.  然后，我们处理训练和测试数据集:

    ```
    train_features = preprocess.fit_transform(X_train) test_features = preprocess.transform(X_test)
    ```

10.  最后，我们保存处理过的数据集，将要素和标注分开。它们被保存到容器中用户定义的位置，SageMaker 处理会在终止作业之前自动将文件复制到 S3:

    ```
    train_features_output_path = os.path.join('/opt/ml/processing/train', 'train_features.csv') train_labels_output_path = os.path.join('/opt/ml/processing/train', 'train_labels.csv') test_features_output_path = os.path.join('/opt/ml/processing/test', 'test_features.csv') test_labels_output_path = os.path.join('/opt/ml/processing/test', 'test_labels.csv') pd.DataFrame(train_features).to_csv(train_features_output_path, header=False, index=False) pd.DataFrame(test_features).to_csv(test_features_output_path, header=False, index=False) y_train.to_csv(train_labels_output_path, header=False, index=False) y_test.to_csv(test_labels_output_path, header=False, index=False)
    ```

就是这样。如您所见，这段代码非常简单，因此改编您自己的脚本来处理 SageMaker 应该不难。现在让我们看看如何实际运行它。

### 运行处理脚本

回到我们的 Jupyter 笔记本，我们使用 SageMaker SDK 中的`SKLearnProcessor`对象来配置处理作业:

1.  首先，我们定义我们想要使用哪个版本的 scikit-learn，以及我们的基础设施需求是什么。这里，我们选择一个`ml.m5.xlarge`实例，一个全面的好选择:

    ```
    from sagemaker.sklearn.processing import SKLearnProcessor sklearn_processor = SKLearnProcessor(     framework_version='0.23-1',     role=sagemaker.get_execution_role(),     instance_type='ml.m5.xlarge',     instance_count=1)
    ```

2.  Then, we simply launch the job, passing the name of the script, the dataset input path in S3, the user-defined dataset paths inside the SageMaker Processing environment, and the command-line arguments:

    ```
    from sagemaker.processing import ProcessingInput, ProcessingOutput
    sklearn_processor.run(
        code='preprocessing.py',
        inputs=[ProcessingInput(
            source=input_data,   # Our data in S3                   
            destination='/opt/ml/processing/input')
        ],               
        outputs=[
            ProcessingOutput(
                source='/opt/ml/processing/train',                             
                output_name='train_data'),                                   
            ProcessingOutput(
                source='/opt/ml/processing/test',
                output_name='test_data'                                                 
                )
        ],
        arguments=['--train-test-split-ratio', '0.2']
    )
    ```

    当作业开始时，SageMaker 自动提供一个托管的`ml.m5.xlarge`实例，将适当的容器拉向它，并在容器中运行我们的脚本。一旦作业完成，实例就被终止，我们只为使用它的时间付费。零基础设施管理，我们永远不会让空闲的实例无缘无故地运行。

3.  After a few minutes, the job is complete, and we can see the output of the script as follows:

    ```
    Received arguments Namespace(train_test_split_ratio=0.2)
    Reading input data from /opt/ml/processing/input/bank-additional-full.csv
    Positive samples: 4639
    Negative samples: 36537
    Ratio: 7.88
    Splitting data into train and test sets with ratio 0.2
    Running preprocessing and feature engineering transformations
    Train data shape after preprocessing: (32940, 58)
    Test data shape after preprocessing: (8236, 58)
    Saving training features to /opt/ml/processing/train/train_features.csv
    Saving test features to /opt/ml/processing/test/test_features.csv
    Saving training labels to /opt/ml/processing/train/train_labels.csv
    Saving test labels to /opt/ml/processing/test/test_labels.csv
    ```

    下面的截图显示了 **CloudWatch** 中的相同日志:

    ![Figure 2.34 – Viewing the log in CloudWatch Logs
    ](img/B17705_02_034.jpg)

    图 2.34–查看 CloudWatch 日志中的日志

4.  Finally, we can describe the job and see the location of the processed datasets:

    ```
    preprocessing_job_description = 
       sklearn_processor.jobs[-1].describe()
    output_config = preprocessing_job_description['ProcessingOutputConfig']
    for output in output_config['Outputs']:
        print(output['S3Output']['S3Uri'])
    ```

    这会产生以下输出:

    ```
    s3://sagemaker-eu-west-1-123456789012/sagemaker-scikit-learn-2020-04-22-10-09-43-146/output/train_data
    s3://sagemaker-eu-west-1-123456789012/sagemaker-scikit-learn-2020-04-22-10-09-43-146/output/test_data
    ```

    在终端中，我们可以使用 AWS CLI 获取位于前面路径的经过处理的训练集，并查看第一个样本和标签:

    ```
    $ aws s3 cp s3://sagemaker-eu-west-1-123456789012/sagemaker-scikit-learn-2020-04-22-09-45-05-711/output/train_data/train_features.csv .
    $ aws s3 cp s3://sagemaker-eu-west-1-123456789012/sagemaker-scikit-learn-2020-04-22-09-45-05-711/output/train_data/train_labels.csv .
    $ head -1 train_features.csv
    0.09604515376959515,-0.6572847857673993,-0.20595554104907898,0.19603112301129622,-0.35090125695736246,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0
    $ head -1 train_labels.csv
    no
    ```

现在数据集已经用我们自己的代码处理过了，我们可以用它来训练一个机器学习模型。在现实生活中，我们也会自动执行这些步骤，而不是在笔记本电脑中手动运行。

重要说明

最后一件事:在这里，我们的工作将输出数据写入 S3。SageMaker 处理还支持直接写入到 **SageMaker 特征库**中的现有特征组(我们将在本书后面介绍)。API 详情可从[https://sagemaker . readthedocs . io/en/stable/API/training/processing . html # sagemaker . processing . processing output](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ProcessingOutput)获取。

## 用自己的代码处理数据集

在前面的示例中，我们使用了一个内置容器来运行我们的 scikit-learn 代码。SageMaker 处理也使得使用自己的容器成为可能。你可以在[https://docs . AWS . Amazon . com/sage maker/latest/DG/build-your-own-processing-container . html](https://docs.aws.amazon.com/sagemaker/latest/dg/build-your-own-processing-container.html)找到一个例子。

如您所见，SageMaker 处理使运行数据处理作业变得非常容易。您可以专注于编写和运行您的脚本，而不必担心配置和管理基础架构。

# 总结

在本章中，您了解了 Amazon SageMaker Ground Truth 如何使用图像和文本标注工作流帮助您构建高度准确的训练数据集。我们将在 [*第五章*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091)**中看到训练计算机视觉模型*，如何使用贴有地面真相的图像数据集。*

 *然后，您了解了 Amazon SageMaker Processing，这是一种帮助您在托管基础设施上运行自己的数据处理工作负载的功能:功能工程、数据验证、模型评估等等。

最后，我们讨论了其他三种 AWS 服务(Amazon EMR、AWS Glue 和 Amazon Athena)，以及它们如何适应您的分析和机器学习工作流。

在下一章中，我们将开始使用亚马逊 SageMaker 的内置机器学习模型来训练模型。*