# 五、训练 CV 模型

在前一章中，您学习了如何使用 SageMaker 的内置算法来解决传统的机器学习问题，包括分类、回归和异常检测。我们看到这些算法在表格数据上工作得很好，比如 CSV 文件。然而，它们不太适合图像数据集，并且它们通常在 **CV** ( **CV** )任务上表现很差。

几年来，CV 风靡全球，每个月都有从图像和视频中提取模式的新突破。在这一章中，你将学习三个专门为 CV 任务设计的内置算法。我们将讨论您可以用它们解决的问题类型。我们还将花大量时间解释如何准备图像数据集，因为这个至关重要的话题经常被莫名其妙地忽略。当然，我们也会训练和部署模型。

本章涵盖以下主题:

*   在 Amazon SageMaker 中发现 CV 内置算法
*   准备图像数据集
*   使用内置的 CV 算法:**图像分类**、**物体检测**和**语义分割**

# 技术要求

您将需要一个 AWS 帐户来运行本章中包含的示例。如果您还没有，请将浏览器指向[https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)来创建一个。您还应该熟悉 AWS 免费层([https://aws.amazon.com/free/](https://aws.amazon.com/free/))，它允许您在一定的使用限制内免费使用许多 AWS 服务。

您需要为您的帐户([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))安装和配置 AWS **命令行界面** ( **CLI** )。

您将需要一个工作的 Python 3.x 环境。安装 Anaconda 发行版([https://www.anaconda.com/](https://www.anaconda.com/))不是强制性的，但是强烈建议安装，因为它包含了我们将需要的许多项目(Jupyter、`pandas`、`numpy`等等)。

书中包含的代码示例可在 GitHub 上获得，网址为[https://GitHub . com/packt publishing/Learn-Amazon-sage maker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition)。你需要安装一个 Git 客户端来访问它们([https://git-scm.com/](https://git-scm.com/))。

# 发现亚马逊 SageMaker 中的 CV 内置算法

SageMaker 包括三个 CV 算法，基于经过验证的深度学习网络。在本节中，您将了解这些算法，它们可以帮助您解决什么样的问题，以及它们的训练场景是什么:

*   **图像分类**给图像分配一个或多个标签。
*   **物体检测**检测并对图像中的物体进行分类。
*   **语义分割**分配图像的每个像素到一个特定的类。

## 发现图像分类算法

从输入图像开始，**图像分类**算法预测训练数据集中存在的每个类别的概率。这个算法是基于**ResNet**卷积神经网络([https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385))。 **ResNet** 于 2015 年发布，同年([http://www.image-net.org/challenges/LSVRC/](http://www.image-net.org/challenges/LSVRC/))获得 ILSVRC 分类任务。从那时起，它成为图像分类的一种流行和通用的选择。

可以设置许多超参数，包括网络的深度，其范围可以从 18 层到 200 层。一般来说，网络的层数越多，学习效果就越好，但代价是增加了训练次数。

请注意**图像分类**算法支持**单标签**和**多标签**分类。在本章中，我们将重点讨论单标签分类。使用几个标签是非常相似的，你可以在[https://github . com/aw slabs/Amazon-sage maker-examples/blob/master/introduction _ to _ Amazon _ algorithms/image classification _ MSC oco _ multi _ label/](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_mscoco_multi_label/)找到一个完整的示例。

## 发现物体检测算法

从输入图像的开始，**对象检测**算法预测图像中每个对象的类别和位置。当然，该算法只能检测训练数据集中存在的对象类。每个对象的位置由一组四个坐标定义，称为**边界框**。

该算法是基于**单次多盒探测器** ( **SSD** )架构[https://arxiv.org/abs/1512.02325](https://arxiv.org/abs/1512.02325))的。分类的话，你可以从两个基地网络中挑选:**VGG-16**([https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556))或者**雷斯内特-50** 。

以下输出显示了对象检测的示例(来源:[https://www . dresagechien . net/WP-content/uploads/2017/11/chien-et-velo . jpg](https://www.dressagechien.net/wp-content/uploads/2017/11/chien-et-velo.jpg)):

![Figure 5.1 – Test image
](img/B17705_05_1.jpg)

图 5.1–测试图像

## 发现语义分割算法

从输入图像的开始，**语义分割**算法预测图像的每个像素的类别。这是一个比图像分类(仅考虑整个图像)或对象检测(仅关注图像的特定部分)困难得多的问题。使用包含在预测中的概率，可以构建覆盖图片中特定对象的**分割遮罩**。

三个神经网络可用于分割:

*   **全卷积网络**(**FCNs**):[https://arxiv.org/abs/1411.4038](https://arxiv.org/abs/1411.4038)
*   **金字塔场景解析**(**PSP**):[https://arxiv.org/abs/1612.01105](https://arxiv.org/abs/1612.01105)
*   **deep lab**v3:[https://arxiv.org/abs/1706.05587](https://arxiv.org/abs/1706.05587)

编码器网络为 **ResNet** ，有 50 层或 101 层。

以下输出显示了分割前一幅图像的结果。我们看到了分割蒙版，每个类被分配了一个独特的颜色；背景是黑色的，以此类推:

![Figure 5.2 – Segmented test image
](img/B17705_05_2.jpg)

图 5.2–分段测试图像

现在让我们看看如何在我们自己的数据上训练这些算法。

## 用 CV 算法训练

这三个算法都是基于**监督学习**的，所以我们的出发点会是一个带标签的数据集。当然，对于每个算法，这些标签的性质是不同的:

*   **图像分类的分类标签**
*   **对象检测的边界框和类别标签**
*   **语义分割的分割掩码和分类标签**

标注影像数据集是一项繁重的工作。如果需要建立自己的数据集，**亚马逊 SageMaker Ground Truth** 绝对可以提供帮助，我们在 [*第二章*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030) 、*处理数据准备技巧*中研究过。在本章的后面，我们将向您展示如何使用标有地面实况的影像数据集。

在打包数据集时，强烈推荐使用 **RecordIO** 文件([https://mxnet.apache.org/api/faq/recordio](https://mxnet.apache.org/api/faq/recordio))。将图像打包到少量记录结构的文件中，可以更容易地移动数据集，并将其拆分用于分布式训练。话虽如此，如果您愿意，您也可以在单个图像文件上进行训练。

一旦你的数据集在 S3 准备好了，你需要决定你是想从头开始训练，还是想从一个预先训练好的网络开始。

如果你有大量的数据，如果你确信用这些数据建立一个特定的模型是有价值的，从头开始训练是很好的。然而，这将花费大量时间，可能是数百个时期，并且超参数选择对于获得好的结果将是绝对关键的。

使用预训练网络通常是更好的选择，即使你有很多数据。多亏了**迁移学习**，你可以从一个在巨大的图像集合(想想数百万)上训练的模型开始，并在你的数据和类上微调它。训练时间会短得多，你会更快地得到准确率更高的模型。

鉴于模型的复杂性和数据集的规模，用 CPU 实例进行训练根本不是一个选项。我们将在所有例子中使用 GPU 实例。

最后但同样重要的是，这三种算法都是基于 Apache MXNet 的。这允许您将他们的模型导出到 SageMaker 之外，并部署到您喜欢的任何地方。

在接下来的部分中，我们将放大图像数据集，以及如何为训练做准备。

# 准备图像数据集

图像数据集的输入格式比表格数据集更复杂,我们需要确保它们完全正确。SageMaker 中的 CV 算法支持三种输入格式:

*   图像文件
*   **记录**文件
*   由 **SageMaker 地面真相**建立的扩充清单

在本节中，您将学习如何准备这些不同格式的数据集。据我所知，这个话题很少被如此详细地讨论过。准备好学很多东西吧！

## 处理图像文件

这是最简单的格式，三种算法都支持它。再来看看如何配合**图像分类**算法使用。

### 将影像分类数据集转换为影像格式

图像格式的数据集必须存储在 S3。图像文件不需要以任何方式排序，您可以简单地将它们存储在同一个桶中。

图像在一个**列表文件**中描述，一个文本文件包含每个图像的一行。对于**图像分类**，有三列:图像的唯一标识符、分类标签和路径。这里有一个例子:

```
1023  5  prefix/image2753.jpg
38    6  another_prefix/image72.jpg
983   2  yet_another_prefix/image863.jpg
```

第一行告诉我们`image2753.jpg`属于类 5，并被赋予了 ID 1023。

每个通道都需要一个列表文件，因此训练数据集需要一个列表文件，验证数据集需要一个列表文件，依此类推。你可以编写定制的代码来生成它们，或者你可以使用一个简单的程序，它是`im2rec`的一部分，可以用 Python 和 C++来实现。我们将使用 Python 版本。

让我们使用**ka ggle**([https://www.kaggle.com/c/dogs-vs-cats](https://www.kaggle.com/c/dogs-vs-cats))上可用的狗与猫的数据集。这个数据集是 812 MB。不出所料，它包含两类:狗和猫。它已经分为训练和测试(分别为 25，000 和 12，500 张图像)。方法如下:

1.  我们创建一个【https://github.com/Kaggle/kaggle-api】()CLI。
2.  在我们的本地机器上，我们下载并提取训练数据集(您可以忽略测试集，它只在比赛中需要):

    ```
    $ kaggle competitions download -c dogs-vs-cats $ sudo yum -y install zip unzip $ unzip dogs-vs-cats.zip $ unzip train.zip
    ```

3.  狗和猫的图片混在同一个文件夹里。我们为每个类创建一个子文件夹，并将适当的图片移到那里:

    ```
    $ cd train $ mkdir dog cat $ find . -name 'dog.*' -exec mv {} dog \; $ find . -name 'cat.*' -exec mv {} cat \;
    ```

4.  我们将需要验证图像，所以让我们将 1250 个随机的狗图像和 1250 个随机的猫图像移动到特定的目录。我在这里使用的是`bash`脚本，但是您可以随意使用任何您喜欢的工具:

    ```
    $ mkdir -p val/dog val/cat $ ls dog | sort -R | tail -1250 | while read file; do mv dog/$file val/dog; done $  ls cat | sort -R | tail -1250 | while read file; do mv cat/$file val/cat; done
    ```

5.  我们将剩余的 22，500 张图片移动到训练文件夹:

    ```
    $ mkdir train $ mv dog cat train
    ```

6.  我们的数据集现在看起来像这样:

    ```
    $ du -h 33M     ./val/dog 28M     ./val/cat 60M     ./val 289M    ./train/dog 248M    ./train/cat 537M    ./train 597M    .
    ```

7.  我们从 GitHub([https://GitHub . com/Apache/incubator-mxnet/blob/master/tools/im 2 rec . py](https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py))下载`im2rec`工具。它需要依赖项，我们需要安装这些依赖项(您可能需要根据您的自己的环境和 Linux 风格来修改命令):

    ```
    $ wget https://raw.githubusercontent.com/apache/incubator-mxnet/master/tools/im2rec.py $ sudo yum -y install python-devel python-pip opencv opencv-devel opencv-python $ pip3 install mxnet opencv-python
    ```

8.  我们运行`im2rec`来构建两个列表文件，一个用于训练数据，一个用于验证数据:

    ```
    dogscats-train.lst and dogscats-val.lst files. Their three columns are a unique image identifier, the class label (0 for cats, 1 for dogs), and the image path, as follows:

    ```
    3197  0.000000  cat/cat.1625.jpg 15084 1.000000  dog/dog.12322.jpg 1479  0.000000  cat/cat.11328.jpg 5262  0.000000  cat/cat.3484.jpg 20714 1.000000  dog/dog.6140.jpg
    ```

    ```

9.  我们将列表文件移动到特定的目录中。这是必需的，因为它们将作为两个新通道`train_lst`和`validation_lst` :

    ```
    $ mkdir train_lst val_lst $ mv dogscats-train.lst train_lst $ mv dogscats-val.lst val_lst
    ```

    传递给`Estimator`
10.  数据集现在看起来像这样:

    ```
    $ du -h 33M     ./val/dog 28M     ./val/cat 60M     ./val 700K    ./train_lst 80K     ./val_lst 289M    ./train/dog 248M    ./train/cat 537M    ./train 597M    .
    ```

11.  最后，我们将这个文件夹同步到 SageMaker 默认存储桶，以备将来使用。请确保只同步四个文件夹，不要同步其他内容:

    ```
    $ aws s3 sync .    s3://sagemaker-eu-west-1-123456789012/dogscats-images/input/
    ```

现在，让我们继续使用对象检测算法的图像格式。

### 将检测数据集转换为图像格式

一般的原理是一样的。我们需要建立一个文件树，代表四个通道:`train`、`validation`、`train_annotation`和`validation_annotation`。

主要区别在于标签信息的存储方式。我们需要构建 JSON 文件，而不是列表文件。

这是一个物体检测数据集中虚构图片的例子。对于图片中的每个对象，我们定义其边界框左上角的坐标、高度和宽度。我们还定义了类标识符，它指向一个也存储类名的类别数组:

```
{
   "file": " my-prefix/my-picture.jpg",
   "image_size": [{"width": 512,"height": 512,"depth": 3}],
   "annotations": [
      {
       "class_id": 1, 
       "left": 67, "top": 121, "width": 61, "height": 128
      },
      {
       "class_id": 5, 
       "left": 324, "top": 134, "width": 112, "height": 267
      }
   ],
   "categories": [
      { "class_id": 1, "name": "pizza" },
      { "class_id": 5, "name": "beer" }
   ]
}
```

我们需要为数据集中的每张图片做这件事，为训练集和验证集分别构建一个 JSON 文件。

最后，让我们看看如何使用语义分割算法的图像格式。

### 将分割数据集转换为图像格式

图像格式是图像分割算法唯一支持的格式。

这一次，我们需要建立一个代表四个通道的文件树:`train`、`validation`、`train_annotation`和`validation_annotation`。前两个通道包含源图像，后两个通道包含分段遮罩图像。

文件命名在将图像与其遮罩匹配时至关重要:源图像和遮罩图像在其各自的通道中必须具有相同的名称。这里有一个例子:

```
├── train
│   ├── image001.png
│   ├── image007.png
│   └── image042.png
├── train_annotation
│   ├── image001.png
│   ├── image007.png
│   └── image042.png 
├── validation
│   ├── image059.png
│   ├── image062.png
│   └── image078.png
└── validation_annotation
│   ├── image059.png
│   ├── image062.png
│   └── image078.png
```

您可以在下图中看到样本图片。左边的源图像将进入`train`文件夹，而右边的掩模图片将进入`train_annotation`文件夹。它们必须有完全相同的名称，这样算法才能匹配它们:

![Figure 5.3 – Sample image from the Pascal VOC dataset
](img/B17705_05_3.jpg)

图 5.3–来自 Pascal VOC 数据集的样本图像

这种格式的一个巧妙之处在于它如何将类标识符与遮罩颜色相匹配。蒙版图像是带有 256 色调色板的 PNG 文件。数据集中的每个类在调色板中都被分配了一个特定的条目。这些颜色是您在属于该类别的对象的遮罩中看到的颜色。

如果您的标注工具或现有数据集不支持此 PNG 功能，您可以添加自己的颜色映射文件。详情请参考 AWS 文档:[https://docs . AWS . Amazon . com/sage maker/latest/DG/semantic-segmentation . html](https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html)。

现在，让我们准备 Pascal VOC 数据集。该数据集经常用于基准对象检测和语义分割模型:

1.  我们首先下载并提取 2012 版本的数据集。我再次推荐使用 AWS 托管的实例来加速网络传输:

    ```
    $ wget https://data.deepai.org/PascalVOC2012.zip $ unzip PascalVOC2012.zip 
    ```

2.  我们创建一个工作目录，在这里我们将构建四个通道:

    ```
    $ mkdir input $ cd input $ mkdir train validation train_annotation validation_annotation
    ```

3.  使用数据集中定义的训练文件列表，我们将相应的图像复制到`train`文件夹。我在这里使用`bash`脚本；请随意使用您选择的工具:

    ```
    $ for file in 'cat ../ImageSets/Segmentation/train.txt | xargs'; do cp ../JPEGImages/$file".jpg" train; done
    ```

4.  然后，我们对验证图像、训练掩码和验证掩码进行同样的操作:

    ```
    $ for file in 'cat ../ImageSets/Segmentation/val.txt | xargs'; do cp ../JPEGImages/$file".jpg" validation; done $ for file in 'cat ../ImageSets/Segmentation/train.txt | xargs'; do cp ../SegmentationClass/$file".png" train_annotation; done $ for file in 'cat ../ImageSets/Segmentation/val.txt | xargs'; do cp ../SegmentationClass/$file".png" validation_annotation; done
    ```

5.  We check that we have the same number of images in the two training channels, and in the two validation channels:

    ```
    $ for dir in train train_annotation validation validation_annotation; do find $dir -type f | wc -l; done
    ```

    我们看到 1，464 个训练文件和掩码，以及 1，449 个验证文件和掩码。我们都准备好了:

    ```
    1464
    1464
    1449
    1449
    ```

6.  最后一步是将文件树同步到 S3 以备后用。同样，请确保只同步四个文件夹:

    ```
    $ aws s3 sync . s3://sagemaker-eu-west-1-123456789012/pascalvoc-segmentation/input/
    ```

我们知道如何准备图像格式的分类、检测和分割数据集。这是关键的一步，你必须把事情做好。

尽管如此，我相信您会觉得这一部分的步骤有点难。我也是！现在想象一下对数百万张图片做同样的事情。那听起来不太令人兴奋，是吗？

我们需要一种更简单的方法来准备图像数据集。让我们看看如何使用 **RecordIO** 文件简化数据集准备。

## 使用记录文件

记录文件更容易移动。对于一个算法来说，读取一个大的顺序文件比读取大量存储在随机磁盘位置的小文件要有效得多。

### 将影像分类数据集转换为记录

让我们将狗和猫的数据集转换成记录:

1.  从数据集的新提取的副本开始，我们将图像移动到适当的类文件夹:

    ```
    $ cd train $ mkdir dog cat $ find . -name 'dog.*' -exec mv {} dog \; $ find . -name 'cat.*' -exec mv {} cat \;
    ```

2.  我们运行`im2rec`来为训练数据集(90%)和验证数据集(10%)生成列表文件。不需要我们自己拆分数据集！

    ```
    $ python3 im2rec.py --list --recursive --train-ratio 0.9 dogscats .
    ```

3.  我们再次运行`im2rec`来生成记录文件:

    ```
    .rec) containing the packed images, and two index files (.idx) containing the offsets of these images inside the record files:

    ```
    $ ls dogscats* dogscats_train.idx dogscats_train.lst dogscats_train.rec dogscats_val.idx dogscats_val.lst dogscats_val.rec
    ```

    ```

4.  让我们将 RecordIO 文件存储在 S3，因为我们稍后会用到它们:

    ```
    $ aws s3 cp dogscats_train.rec s3://sagemaker-eu-west-1-123456789012/dogscats/input/train/ $ aws s3 cp dogscats_val.rec s3://sagemaker-eu-west-1-123456789012/dogscats/input/validation/
    ```

这要简单得多，不是吗？`im2rec`有附加选项来调整图像大小等。它还可以将数据集分成几个块，这是一种有用的技术用于**管道模式**和**分布式训练**。我们将在第 9 章 、*调整你的训练工作*中学习它们。

现在，让我们继续使用 RecordIO 文件进行对象检测。

### 将对象检测数据集转换为记录

过程与非常相似。一个主要的区别是列表文件的格式。我们不仅需要处理类标签，还需要存储边界框。

让我们看看这对 Pascal VOC 数据集意味着什么。以下图像取自数据集:

![Figure 5.4 – Sample image from the Pascal VOC dataset
](img/B17705_05_4.jpg)

图 5.4–来自 Pascal VOC 数据集的样本图像

它包含三把椅子。标签信息存储在一个单独的 **XML** 文件中，以略加缩写的形式显示:

```
<annotation>
        <folder>VOC2007</folder>
        <filename>003988.jpg</filename>
        . . .
        <object>
                <name>chair</name>
                <pose>Unspecified</pose>
                <truncated>1</truncated>
                <difficult>0</difficult>
                <bndbox>
                    <xmin>1</xmin>
                    <ymin>222</ymin>
                    <xmax>117</xmax>
                    <ymax>336</ymax>
                </bndbox>
        </object>
        <object>
                <name>chair</name>
                <pose>Unspecified</pose>
                <truncated>1</truncated>
                <difficult>1</difficult>
                <bndbox>
                    <xmin>429</xmin>
                    <ymin>216</ymin>
                    <xmax>448</xmax>
                    <ymax>297</ymax>
                </bndbox>
        </object>
        <object>
                <name>chair</name>
                <pose>Unspecified</pose>
                <truncated>0</truncated>
                <difficult>1</difficult>
                <bndbox>
                    <xmin>281</xmin>
                    <ymin>149</ymin>
                    <xmax>317</xmax>
                    <ymax>211</ymax>
                </bndbox>
        </object>
</annotation>
```

将转换成一个列表文件条目应该看起来像这样:

```
9404 2 6  8.0000  0.0022  0.6607  0.2612  1.0000  0.0000 8.0000  0.9576  0.6429  1.0000  0.8839  1.0000 8.0000  0.6272  0.4435  0.7076  0.6280  1.0000 VOC2007/JPEGImages/003988.jpg 
```

让我们解码每一列:

*   `9404`是唯一的图像标识符。
*   `2`是包含标题信息的列数，包括本列。
*   `6`是标注信息的列数。这六列是类标识符、四个边界框坐标和一个告诉我们对象是否难以看到的标志(我们不会使用它)。
*   The following is for the first object:

    a) `8`是类标识符。这里，`8`是`chair`类。

    b) `0.0022 0.6607 0.2612 1.0000`是相对坐标`0`意味着物体不困难。

*   For the second object, we have the following:

    a) `8`是类别标识符。

    b) `0.9576 0.6429 1.0000 0.8839`是第二个物体的坐标。

    c) `1`表示对象难。

*   The third object has the following:

    a) `8`是类别标识符。

    b) `0.6272 0.4435 0.7076 0.628`是第三个物体的坐标。

    c) `1`表示对象难。

*   `VOC2007/JPEGImages/003988.jpg`是通向图像的路径。

那么，我们如何将数千个 XML 文件转换成几个列表文件呢？除非您喜欢编写解析器，否则这不是一个非常令人兴奋的任务。

幸运的是，我们的工作已经为我们安排好了。Apache MXNet 包含一个 Python 脚本`prepare_dataset.py`，它将处理这个任务。让我们看看它是如何工作的:

1.  对于接下来的步骤，我推荐使用一个至少有 10 GB 存储空间的 Amazon Linux 2 EC2 实例。以下是设置步骤:

    ```
    $ sudo yum -y install git python3-devel python3-pip opencv opencv-devel opencv-python $ pip3 install mxnet opencv-python --user $ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/ec2-user/.local/lib/python3.7/site-packages/mxnet/ $ sudo ldconfig
    ```

2.  用`wget`下载 2007 年和 2012 年的 Pascal VOC 数据集，解压到同一个目录:

    ```
    $ mkdir pascalvoc $ cd pascalvoc $ wget https://data.deepai.org/PascalVOC2012.zip $ wget https://data.deepai.org/PASCALVOC2007.zip $ unzip PascalVOC2012.zip $ unzip PASCALVOC2007.zip  $ mv VOC2012 VOCtrainval_06-Nov-2007/VOCdevkit
    ```

3.  克隆 Apache MXNet 库([https://github.com/apache/incubator-mxnet/](https://github.com/apache/incubator-mxnet/)):

    ```
    $ git clone --single-branch --branch v1.4.x https://github.com/apache/incubator-mxnet
    ```

4.  运行`prepare_dataset.py`脚本来构建我们的训练数据集，合并 2007 和 2012 版本的训练和验证集:

    ```
    $ cd VOCtrainval_06-Nov-2007 $ python3 ../incubator-mxnet/example/ssd/tools/prepare_dataset.py --dataset pascal --year 2007,2012 --set trainval --root VOCdevkit --target VOCdevkit/train.lst $ mv VOCdevkit/train.* ..
    ```

5.  让我们遵循相似的步骤来生成我们的验证数据集，使用 2007 版本的测试集:

    ```
    $ cd ../VOCtest_06-Nov-2007 $ python3 ../incubator-mxnet/example/ssd/tools/prepare_dataset.py --dataset pascal --year 2007 --set test --root VOCdevkit --target VOCdevkit/val.lst $ mv VOCdevkit/val.* .. $ cd ..
    ```

6.  在顶层目录中，我们看到脚本生成的文件。请随意查看列表文件；他们应该有以前提出的格式:

    ```
    train.idx  train.lst  train.rec   val.idx  val.lst  val.rec  
    ```

7.  让我们将记录文件存储在 S3 中，因为我们稍后会用到它们:

    ```
    $ aws s3 cp train.rec s3://sagemaker-eu-west-1-123456789012/pascalvoc/input/train/ $ aws s3 cp val.rec s3://sagemaker-eu-west-1-123456789012/pascalvoc/input/validation/
    ```

`prepare_dataset.py`脚本真的让事情变得简单了。它还支持 **COCO** 数据集([http://cocodataset.org](http://cocodataset.org))，并且工作流程极其相似。

转换其他公共数据集呢？嗯，你的里程可能会有所不同。你可以在[https://gluon-cv . mxnet . io/build/examples _ datasets/index . html](https://gluon-cv.mxnet.io/build/examples_datasets/index.html)找到更多的例子。

RecordIO 无疑是一个进步。尽管如此，当使用自定义数据集时，很可能您必须编写自己的列表文件生成器。这没什么大不了的，但这是额外的工作。

标有**亚马逊 SageMaker 地面真相**的数据集完全解决了这些问题。让我们看看这是如何工作的！

## 使用 SageMaker 地面真相文件

在 [*第二章*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030) 、*处理数据的准备技巧*中，你了解到了 SageMaker Ground 的真相工作流程和他们的结局，一个**的增广清单**文件。该文件采用 **JSON Lines** 格式:每个 JSON 对象描述一个特定的注释。

下面是我们在第二章 、*处理数据准备技术*中运行的语义分段作业的一个例子(这个故事对于其他任务类型也是一样的)。我们看到了源图像和分割蒙版的路径，以及告诉我们如何将蒙版颜色与类别匹配的颜色映射信息:

```
{"source-ref":"s3://julien-sagemaker-book/chapter2/cat/cat1.jpg",
"my-cat-job-ref":"s3://julien-sagemaker-book/chapter2/cat/output/my-cat-job/annotations/consolidated-annotation/output/0_2020-04-21T13:48:00.091190.png",
"my-cat-job-ref-metadata":{
  "internal-color-map":{
   "0":{"class-name":"BACKGROUND","hex-color": "#ffffff", 
        "confidence": 0.8054600000000001}, 
   "1":{"class-name":"cat","hex-color": "#2ca02c", 
        "confidence":0.8054600000000001}
}, 
"type":"groundtruth/semantic-segmentation",
"human-annotated":"yes",
"creation-date":"2020-04-21T13:48:00.562419",
"job-name":"labeling-job/my-cat-job"}}
```

以下图像是前面的 JSON 文档中引用的图像:

![Figure 5.5 – Source image and segmented image
](img/B17705_05_5.jpg)

图 5.5-源图像和分割图像

这正是我们训练模型所需要的。事实上，我们可以将增强的清单原样传递给 SageMaker `Estimator`。无论如何都不需要数据处理。

要在 S3 使用一个指向标签图像的**增强清单**，我们只需传递它的位置和 JSON 属性的名称(在前一个例子中突出显示):

```
training_data_channel = sagemaker.s3_input(
    s3_data=augmented_manifest_file_path, 
    s3_data_type='AugmentedManifestFile',
    attribute_names=['source-ref', 'my-job-cat-ref'])
```

就是这样！这比我们以前见过的任何东西都简单。

你可以在[https://github . com/aw slabs/Amazon-sage maker-examples/tree/master/Ground _ Truth _ labeling _ jobs](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/ground_truth_labeling_jobs)找到更多使用 SageMaker Ground Truth 的例子。

既然我们知道了如何为训练准备图像数据集，让我们将 CV 算法投入使用。

# 使用内置的 CV 算法

在本节中，我们将使用公共图像数据集，使用所有三种算法来训练和部署模型。我们将涵盖从头开始的训练和迁移学习。

## 训练图像分类模型

在第一个示例中，让我们使用图像分类算法来构建一个模型，对我们在上一节中准备的狗和猫数据集进行分类。我们将首先使用图像格式进行训练，然后使用 RecordIO 格式。

### 图像格式训练

我们将通过以下步骤开始训练:

1.  在 Jupyter 笔记本中，我们定义了适当的数据路径:

    ```
    import sagemaker sess = sagemaker.Session() bucket = sess.default_bucket() prefix = 'dogscats-images' s3_train_path =    's3://{}/{}/input/train/'.format(bucket, prefix) s3_val_path =    's3://{}/{}/input/val/'.format(bucket, prefix) s3_train_lst_path =    's3://{}/{}/input/train_lst/'.format(bucket, prefix) s3_val_lst_path =    's3://{}/{}/input/val_lst/'.format(bucket, prefix) s3_output = 's3://{}/{}/output/'.format(bucket, prefix)
    ```

2.  We configure the `Estimator` for the image classification algorithm:

    ```
    from sagemaker.image_uris import retrieve
    region_name = sess.boto_session.boto_region_name
    container = retrieve('image-classification', region)
    ic = sagemaker.estimator.Estimator(container,
                  sagemaker.get_execution_role(),
                  instance_count=1,
                  instance_type='ml.p3.2xlarge', 
                  output_path=s3_output)
    ```

    我们使用一个名为`ml.p3.2xlarge`的 GPU 实例，它为这个数据集打包了足够多的 punch(在 eu-west-1 中为 4.131 美元/小时)。

    超参数([https://docs . AWS . Amazon . com/sage maker/latest/DG/IC-hyperparameter . html](https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html))呢？我们设置了类的数量(2)和训练样本的数量(22500)。由于我们使用的是图像格式，我们需要明确地调整图像的大小，将最小尺寸设置为 224 像素。因为我们有足够的数据，我们决定从头开始训练。为了保持较短的训练时间，我们满足于 18 层 **ResNet** 模型，我们只训练 10 个时期:

    ```
    ic.set_hyperparameters(num_layers=18,
                           use_pretrained_model=0,
                           num_classes=2,
                           num_training_samples=22500,
                           resize=224,
                           mini_batch_size=128,
                           epochs=10)
    ```

3.  我们定义了四个频道，将它们的内容类型设置为`application/x-image` :

    ```
    from sagemaker import TrainingInput train_data = TrainingInput (     s3_train_path,                                   content_type='application/x-image')                                        val_data = TrainingInput (     s3_val_path,                                                                       content_type='application/x-image') train_lst_data = TrainingInput (     s3_train_lst_path,                                       content_type='application/x-image') val_lst_data = TrainingInput (     s3_val_lst_path,                                         content_type='application/x-image')                                       s3_channels = {'train': train_data,                 'validation': val_data,                'train_lst': train_lst_data,                 'validation_lst': val_lst_data}
    ```

4.  We launch the training job as follows:

    ```
    ic.fit(inputs=s3_channels)
    ```

    在训练日志中，我们看到数据下载大约需要 3 分钟。惊喜，惊喜:我们还看到算法在训练前构建 RecordIO 文件。此步骤持续约 1 分钟:

    ```
    Searching for .lst files in /opt/ml/input/data/train_lst.
    Creating record files for dogscats-train.lst
    Done creating record files...
    Searching for .lst files in /opt/ml/input/data/validation_lst.
    Creating record files for dogscats-val.lst
    Done creating record files...
    ```

5.  随着训练开始，我们看到一个历元大约需要 22 秒:

    ```
    Epoch[0] Time cost=22.337 Epoch[0] Validation-accuracy=0.605859
    ```

6.  这项工作总共持续了 506 秒(大约 8 分钟)，花费了我们(506/3600)* 4.131 美元= 0.58 美元。它达到了 **91.2%** 的验证精度(希望您看到类似的东西)。考虑到我们还没有调整超参数，这已经很不错了。
7.  然后，我们在一个小型 CPU 实例上部署该模型，如下所示:

    ```
    ic_predictor = ic.deploy(initial_instance_count=1,                          instance_type='ml.t2.medium')
    ```

8.  We download the following test image and send it for prediction in `application/x-image` format.![Figure 5.6 – Test picture
    ](img/B17705_05_6.jpg)

    ```
    import boto3, json
    import numpy as np
    with open('test.jpg', 'rb') as f:
        payload = f.read()
        payload = bytearray(payload)
    runtime = boto3.Session().client(
        service_name='runtime.sagemaker')
    response = runtime.invoke_endpoint(
        EndpointName=ic_predictor.endpoint_name,                                  
        ContentType='application/x-image',
        Body=payload)
    result = response['Body'].read()
    result = json.loads(result)
    index = np.argmax(result)
    print(result[index], index)
    ```

    打印出概率和类别，我们的模型指示该图像是具有 99.997%置信度的狗，并且该图像属于类别 1:

    ```
    0.9999721050262451 1
    ```

9.  当完成后，我们删除端点，如下所示:

    ```
    ic_predictor.delete_endpoint()
    ```

现在，让我们使用 RecordIO 格式的数据集运行相同的训练作业。

### 记录格式的训练

唯一的区别是我们如何定义输入通道。我们只需要两个频道来服务我们上传到 S3 的记录文件。相应地，内容类型被设置为`application/x-recordio`:

```
from sagemaker import TrainingInput
prefix = 'dogscats'
s3_train_path=
  's3://{}/{}/input/train/'.format(bucket, prefix)
s3_val_path=
  's3://{}/{}/input/validation/'.format(bucket, prefix)
train_data = TrainingInput(
    s3_train_path,
    content_type='application/x-recordio')
validation_data = TrainingInput(
    s3_val_path,                                        
    content_type='application/x-recordio')
```

再次训练，我们看到数据下载需要 1 分钟，文件生成步骤已经消失。虽然很难从单次运行中得出任何结论，但使用 RecordIO 数据集通常会节省您的时间和金钱，即使是在单个实例上进行训练。

狗与猫的数据集每类有超过 10，000 个样本，这对于从头开始训练来说绰绰有余。现在，让我们尝试一个不是这种情况的数据集。

## 微调图像分类模型

请考虑一下 **Caltech-256** 数据集，这是一个流行的公共数据集，包含 256 个类中的 15，240 幅图像，外加一个杂波类([http://www.vision.caltech.edu/Image_Datasets/Caltech256/](http://www.vision.caltech.edu/Image_Datasets/Caltech256/))。浏览图片类别，我们看到所有的类都有少量的样本。例如，“鸭子”类只有 60 张图像:深度学习算法，无论多么复杂，都难以用这么少的数据提取鸭子独特的视觉特征。

在这种情况下，从头开始训练根本不是一个选项。相反，我们将使用一种称为**迁移学习**的技术，其中我们从一个已经在非常大和多样化的图像数据集上训练过的网络开始。**ImageNet**([http://www.image-net.org/](http://www.image-net.org/))大概是预训练最受欢迎的选择，有 1000 个类，几百万张图片。

经过预训练的网络已经学会了如何从复杂的图像中提取模式。假设我们数据集中的图像与预训练数据集中的图像足够相似，我们的模型应该能够继承这些知识。在我们的数据集上再训练几个时期，我们应该能够**微调**我们的数据和类的预训练模型。

让我们看看如何使用 SageMaker 轻松做到这一点。事实上，我们将重用前一个例子的代码，只做最小的改动。让我们开始吧:

1.  我们下载了 RecordIO 格式的 Caltech-256(如果您愿意，您可以下载图像格式的，并将其转换为 RecordIO:熟能生巧！):

    ```
    %%sh wget http://data.mxnet.io/data/caltech-256/caltech-256-60-train.rec wget http://data.mxnet.io/data/caltech-256/caltech-256-60-val.rec
    ```

2.  我们将数据集上传到 S3:

    ```
    import sagemaker session = sagemaker.Session() bucket = session.default_bucket() prefix = 'caltech256/' s3_train_path = session.upload_data(     path='caltech-256-60-train.rec',     bucket=bucket, key_prefix=prefix+'input/train') s3_val_path = session.upload_data(     path='caltech-256-60-val.rec',     bucket=bucket, key_prefix=prefix+'input/validation')
    ```

3.  我们为图像分类算法配置了`Estimator`功能。代码与上例中的*步骤 2* 完全相同。
4.  We use `use_pretrained_network` to 1\. The final fully connected layer of the pretrained network will be resized to the number of classes present in our dataset, and its weights will be assigned random values.

    我们设置正确的类数(256+1)和训练样本如下:

    ```
    ic.set_hyperparameters(num_layers=50,
                           use_pretrained_model=1,
                           num_classes=257,
                           num_training_samples=15240,
                           learning_rate=0.001,
                           epochs=5)
    ```

    由于我们正在微调，我们只训练 5 个纪元，学习率较小，为 0.001。

5.  我们配置频道并启动训练工作。该代码与上例中的*步骤 4* 完全相同。
6.  After 5 epochs and 272 seconds, we see the following metric in the training log:

    ```
    Epoch[4] Validation-accuracy=0.8119
    ```

    这对于仅仅几分钟的训练来说是相当不错的。即使有足够的数据，从头开始得到结果也需要更长的时间。

7.  为了部署并测试模型，我们将重用前面示例中的*步骤 7-9* 。

如你所见，迁移学习是一种非常强大的技术。即使数据很少，它也能提供出色的结果。你也将训练更少的纪元，在这个过程中节省时间和金钱。

现在，让我们进入下一个算法，**物体检测**。

## 训练对象检测模型

在本例中，我们将使用对象检测算法在我们在上一节准备的 Pascal VOC 数据集上构建一个模型:

1.  我们从定义数据路径开始:

    ```
    import sagemaker sess = sagemaker.Session() bucket = sess.default_bucket() prefix = 'pascalvoc' s3_train_data = 's3://{}/{}/input/train'.format(bucket, prefix) s3_validation_data = 's3://{}/{}/input/validation'.format(bucket, prefix) s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)
    ```

2.  我们选择的对象检测算法:

    ```
    from sagemaker.image_uris import retrieve region = sess.boto_region_name container = retrieve('object-detection', region)
    ```

3.  我们配置了`Estimator` :

    ```
    od = sagemaker.estimator.Estimator(          container,          sagemaker.get_execution_role(),          instance_count=1,          instance_type='ml.p3.2xlarge',          output_path=s3_output_location)
    ```

4.  我们设置所需的超参数。我们选择预训练的 ResNet-50 网络作为基础网络。我们设置班级和训练样本的数量。我们定在 30 个纪元，这应该足够开始看到结果:

    ```
    od.set_hyperparameters(base_network='resnet-50',                        use_pretrained_model=1,                        num_classes=20,                        num_training_samples=16551,                        epochs=30)
    ```

5.  We then configure the two channels, and we launch the training job:

    ```
    from sagemaker.session import TrainingInput
    train_data = TrainingInput (
          s3_train_data,
          content_type='application/x-recordio')
    validation_data = TrainingInput (
          s3_validation_data, 
          content_type='application/x-recordio')
    data_channels = {'train': train_data, 
                     'validation': validation_data}
    od.fit(inputs=data_channels)
    ```

    选择我们在**sage maker components and registry**|**Experiments and trials**中的工作，我们可以看到近乎实时的指标和图表。下一张图片显示了验证**平均精度指标(mAP)** ，这是对象检测模型的一个关键指标。

    ![Figure 5.7 – Validation mAP 
    ](img/B17705_05_7.jpg)

    图 5.7–验证图

    请浏览其他选项卡(**指标**、**参数**、**工件**等等)。它们包含了你需要知道的关于某一特定工作的一切。请注意右上角的**停止训练作业**按钮，您可以随时使用它来终止作业。

6.  训练持续 1 小时 40 分钟。这是一个相当重的模型！我们得到的**平均精度度量** ( **图**)为 0.5151。生产使用将需要更多的训练，但我们应该已经能够测试模型。
7.  鉴于其的复杂性，我们将该模型部署到一个更大的 CPU 实例:

    ```
    od_predictor = od.deploy(     initial_instance_count = 1,      instance_type= 'ml.c5.2xlarge')
    ```

8.  我们从维基百科下载了一个测试图像，并用我们的模型进行预测:

    ```
    import boto3,json with open('test.jpg', 'rb') as image:     payload = image.read()     payload = bytearray(payload) runtime = boto3.Session().client(     service_name='runtime.sagemaker') response = runtime.invoke_endpoint(     EndpointName=od_predictor.endpoint_name,                                       ContentType='image/jpeg',     Body=payload) response = response['Body'].read() response = json.loads(response)
    ```

9.  The response contains a list of predictions. Each individual prediction contains a class identifier, the confidence score, and the relative coordinates of the bounding box. Here are the first predictions in the response:

    ```
    {'prediction': 
    [[14.0, 0.7515302300453186, 0.39770469069480896, 0.37605002522468567, 0.5998836755752563, 1.0], 
    [14.0, 0.6490200161933899, 0.8020403385162354, 0.2027685046195984, 0.9918708801269531, 0.8575668931007385]
    ```

    使用这些信息，我们可以在源图像上绘制边界框。为了简洁起见，我不会在这里包含代码，但是您可以在本书的 GitHub 资源库中找到。以下输出显示了结果:

    ![Figure 5.8 – Test image
    ](img/B17705_05_8.jpg)

    图 5.8–测试图像

10.  完成后，我们删除端点，如下所示:

    ```
    od_predictor.delete_endpoint()
    ```

我们对物体检测的探索到此结束。我们还有一个算法要走:**语义分割**。

## 训练语义分割模型

在本例中，我们将使用语义分割算法在我们在上一节准备的 Pascal VOC 数据集上构建一个模型:

1.  像往常一样，我们定义数据路径如下:

    ```
    import sagemaker sess = sagemaker.Session() bucket = sess.default_bucket()   prefix = 'pascalvoc-segmentation' s3_train_data = 's3://{}/{}/input/train'.format(bucket, prefix) s3_validation_data = 's3://{}/{}/input/validation'.format(bucket, prefix) s3_train_annotation_data = 's3://{}/{}/input/train_annotation'.format(bucket, prefix) s3_validation_annotation_data = 's3://{}/{}/input/validation_annotation'.format(bucket, prefix) s3_output_location =  's3://{}/{}/output'.format(bucket, prefix)
    ```

2.  我们选择了语义切分算法，我们配置了`Estimator`功能:

    ```
    from sagemaker.image_uris import retrieve container = retrieve('semantic-segmentation', region) seg = sagemaker.estimator.Estimator(           container,           sagemaker.get_execution_role(),                                                instance_count = 1,           instance_type = 'ml.p3.2xlarge',           output_path = s3_output_location)
    ```

3.  我们定义所需的超参数。我们选择预训练的 ResNet-50 网络作为基础网络，并选择预训练的 **FCN** 用于检测。我们设置班级和训练样本的数量。同样，我们确定了 30 个纪元，这应该足以开始看到结果:

    ```
    seg.set_hyperparameters(backbone='resnet-50',                         algorithm='fcn',                         use_pretrained_model=True,                         num_classes=21,                         num_training_samples=1464,                         epochs=30)
    ```

4.  我们配置了四个通道，将源图像的内容类型设置为`image/jpeg`，将遮罩图像的内容类型设置为`image/png`。然后，我们启动训练作业:

    ```
    from sagemaker import TrainingInput train_data = TrainingInput(     s3_train_data,      content_type='image/jpeg') validation_data = TrainingInput(     s3_validation_data,     content_type='image/jpeg') train_annotation = TrainingInput(     s3_train_annotation_data,     content_type='image/png') validation_annotation = TrainingInput(     s3_validation_annotation_data,       content_type='image/png') data_channels = {   'train': train_data,   'validation': validation_data,   'train_annotation': train_annotation,              'validation_annotation':validation_annotation } seg.fit(inputs=data_channels)
    ```

5.  Training lasts about 32 minutes. We get a **mean intersection-over-union metric** (**mIOU**) of 0.4874, as shown in the following plot:![Figure 5.9 – Validation mIOU
    ](img/B17705_05_9.jpg)

    图 5.9–验证 mIOU

6.  我们将模型部署到一个 CPU 实例:

    ```
    seg_predictor = seg.deploy(     initial_instance_count=1,      instance_type='ml.c5.2xlarge')
    ```

7.  一旦端点投入使用，我们获取一个测试图像，并将其作为具有适当内容类型的字节数组发送以进行预测:

    ```
    !wget -O test.jpg https://bit.ly/3yhXB9l  filename = 'test.jpg' with open(filename, 'rb') as f:     payload = f.read()     payload = bytearray(payload) runtime = boto3.Session().client(     service_name='runtime.sagemaker') response = runtime.invoke_endpoint(     EndpointName=od_predictor.endpoint_name,     ContentType='image/jpeg',     Body=payload) response = response['Body'].read() response = json.loads(response)
    ```

8.  Using the **Python Imaging Library** (**PIL**), we process the response mask and display it:

    ```
    import PIL
    from PIL import Image
    import numpy as np
    import io
    num_classes = 21
    mask = np.array(Image.open(io.BytesIO(response)))
    plt.imshow(mask, vmin=0, vmax=num_classes-1, cmap='gray_r')
    plt.show()
    ```

    下图显示了源图像和预测遮罩。这个结果是有希望的，并且会随着更多的训练而改善:

    ![Figure 5.10 – Test image and segmented test image
    ](img/B17705_05_10.jpg)

    图 5.10–测试图像和分段测试图像

9.  使用`application/x-protobuf`接受类型再次预测，我们接收源图像中所有像素的分类概率。响应是一个 protobuf 缓冲区，我们将它保存到一个二进制文件:

    ```
    response = runtime.invoke_endpoint(     EndpointName=seg_predictor.endpoint_name,     ContentType='image/jpeg',     Accept='application/x-protobuf',     Body=payload) result = response['Body'].read() seg_predictor.accept = 'application/x-protobuf' response = seg_predictor.predict(img) results_file = 'results.rec' with open(results_file, 'wb') as f:     f.write(response)
    ```

10.  缓冲区包含两个张量:一个是概率张量的形状，另一个是实际概率。我们使用`values`张量来加载它们，张量描述了一个大小为 289x337 的图像，其中每个像素被分配了 21 个概率，每个概率对应一个 Pascal VOC 类。可以查一下 289 * 337 * 21 = 2045253。
11.  Knowing that, we can now reshape the `values` tensor, retrieve the 21 probabilities for the (0,0) pixel, and print the class identifier with the highest probability:

    ```
    mask = np.reshape(np.array(values), shape)
    pixel_probs = mask[0,:,0,0]
    print(pixel_probs)
    print(np.argmax(pixel_probs))
    ```

    以下是输出:

    ```
    [9.68291104e-01 3.72813665e-04 8.14868137e-04 1.22414716e-03
     4.57380433e-04 9.95167647e-04 4.29908326e-03 7.52388616e-04
     1.46311778e-03 2.03254796e-03 9.60668200e-04 1.51833100e-03
     9.39570891e-04 1.49350625e-03 1.51627266e-03 3.63648031e-03
     2.17934581e-03 7.69103528e-04 3.05095245e-03 2.10589729e-03
     1.12741732e-03]
    0
    ```

    最高的概率在索引 0 处:像素(0，0)的预测类是背景类 0。

12.  完成后，我们删除端点，如下所示:

    ```
    seg_predictor.delete_endpoint()
    ```

# 摘要

如你所见，这三种算法使得训练 CV 模型变得很容易。即使使用默认的超参数，我们也能很快得到好的结果。尽管如此，我们开始觉得有必要扩大我们的训练工作。不要担心，一旦相关的特性在以后的章节中被涉及到，我们将会重温一些我们的 CV 例子，我们将会从根本上扩展它们！

在本章中，您学习了图像分类、对象检测和语义分割算法。您还学习了如何准备 Image、RecordIO 和 SageMaker Ground Truth 格式的数据集。标记和准备数据是一个关键步骤，需要大量的工作，我们非常详细地讨论了它。最后，您学习了如何使用 SageMaker SDK 来训练和部署具有这三种算法的模型，以及如何解释结果。

在下一章，你将学习如何使用自然语言处理的内置算法。