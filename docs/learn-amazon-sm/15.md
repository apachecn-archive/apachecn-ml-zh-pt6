# 第十一章:部署机器学习模型

在前面的章节中，我们已经用最简单的方式部署了模型:通过配置一个估计器，调用`fit()` `deploy()` API 来创建一个实时端点。这是最简单的开发和测试场景，但不是唯一的场景。

可以导入模型。例如，您可以将您在本地机器上训练的现有模型导入到 SageMaker 中，并像在 SageMaker 上训练它一样部署它。

此外，模型可以部署在不同的配置中，如下所示:

*   一个实时端点上的单个模型，这是我们到目前为止所做的，以及同一个端点上的几个模型变体。
*   多达五个模型的序列，称为**推理管道**。
*   加载到上的任意数量的相关模型请求同一个端点，称为**多模型端点**。我们将在第 13 章 、*优化成本和性能*中检查这种配置。
*   通过称为**批处理转换**的特性，以批处理模式预测数据的单个模型或推理管道。

当然，型号也可以出口。你可以在**简单存储服务** ( **S3** )中抓取一个训练神器，提取模型，部署到你喜欢的任何地方。

在本章中，我们将讨论以下主题:

*   检查模型工件并导出模型
*   在实时端点上部署模型
*   在批处理转换器上部署模型
*   在推理管道上部署模型
*   使用 Amazon SageMaker 模型监视器监控预测质量
*   在容器服务上部署模型
*   我们开始吧！

# 技术要求

你需要一个**亚马逊网络服务** ( **AWS** )账户来运行本章中的例子。如果您还没有，请浏览到[https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)创建一个。您还应该熟悉 AWS 免费层([https://aws.amazon.com/free/](https://aws.amazon.com/free/))，它允许您在一定的使用限制内免费使用许多 AWS 服务。

您需要为您的帐户([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))安装和配置 AWS **命令行界面** ( **CLI** )。

您将需要一个工作的 Python 3.x 环境。安装 Anaconda 发行版([https://www.anaconda.com/](https://www.anaconda.com/))不是强制性的，但是强烈建议安装，因为它包含了许多我们需要的项目(Jupyter、`pandas`、`numpy`等等)。

本书中包含的代码示例可在 GitHub 上获得，网址为 https://GitHub . com/packt publishing/Learn-Amazon-sage maker-second-edition。你需要安装一个 Git 客户端来访问它们([https://git-scm.com/](https://git-scm.com/))。

# 检查模型工件并导出模型

一个模型工件包含一个或几个由培训作业生成的文件，并且是模型部署所需要的。这些文件的数量和性质取决于被训练的算法。正如我们多次看到的，模型工件被存储为一个`model.tar.gz`文件，位于估算器中定义的 S3 输出位置。

让我们来看看不同的例子，在这些例子中，我们重用了我们之前培训的工作中的工件。

## 检查和导出内置模型

几乎所有的内置算法都是用 **Apache MXNet** 实现，它们的工件都反映了这一点。关于 MXNet 的更多信息，请访问[https://mxnet.apache.org/](https://mxnet.apache.org/)。

让我们看看如何直接加载这些模型。另一个选择是使用**多模型服务器**(**MMS**)([https://github.com/awslabs/multi-model-server](https://github.com/awslabs/multi-model-server))，但是我们将如下进行:

1.  让我们从我们在 [*第 4 章*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069) ，*训练机器学习模型*中训练的**线性学习器**模型的工件开始，如下面的代码片段所示:

    ```
    $ tar xvfz model.tar.gz x model_algo-1 $ unzip model_algo-1 archive:  model_algo-1 extracting: additional-params.json extracting: manifest.json extracting: mx-mod-symbol.json extracting: mx-mod-0000.params
    ```

2.  我们加载符号文件，该文件包含一个 **JavaScript 对象符号** ( **JSON** )模型的定义，如下:

    ```
    import json sym_json = json.load(open('mx-mod-symbol.json')) sym_json_string = json.dumps(sym_json)
    ```

3.  我们使用这个 JSON 定义来实例化一个新的胶子模型。我们还定义了其输入符号的名称(`data`)，如下:

    ```
    import mxnet as mx from mxnet import gluon net = gluon.nn.SymbolBlock(     outputs=mx.sym.load_json(sym_json_string),     inputs=mx.sym.var('data')) 
    ```

4.  Now, we can easily plot the model, like this:

    ```
    mx.viz.plot_network(
        net(mx.sym.var('data'))[0],   
        node_attrs={'shape':'oval','fixedsize':'false'})
    ```

    该产生以下输出:

    ![Figure 11.1 – Linear Learner model
    ](img/B17705_11_1.jpg)

    图 11.1-线性学习者模型

5.  然后，我们加载训练时学习到的模型参数，如下:

    ```
    net.load_parameters('mx-mod-0000.params',                      allow_missing=True) net.collect_params().initialize()
    ```

6.  我们定义一个存储在 MXNet`NDArray`(https://MXNet . Apache . org/versions/1.6/API/python/docs/API/ndarray/index . html)中的测试样本如下:

    ```
    test_sample = mx.nd.array( [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98])
    ```

7.  Finally, we forward it through the model and read the output, as follows:

    ```
    response = net(test_sample)
    print(response)
    ```

    该房屋预计价格为**美元** ( **美元**)30173，如图所示:

    ```
    array([[30.173424]], dtype=float32)
    ```

这种技术应该适用于所有基于 MXNet 的算法。现在，我们来看看**计算机视觉** ( **CV** )的内置算法。

## 检查和导出内置 CV 模型

CV 的三个内置算法也是基于 Apache MXNet。流程完全相同，如下所示:

1.  以下是我们在 [*第五章*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091) ，*训练计算机视觉模型* :

    ```
    $ tar xvfz model.tar.gz x image-classification-0010.params x model-shapes.json x image-classification-symbol.json
    ```

    中对狗和猫训练的**图像分类**模型的神器
2.  加载模型及其参数，如下:

    ```
    import mxnet, json from mxnet import gluon sym_json = json.load(            open('image-classification-symbol.json')) sym_json_string = json.dumps(sym_json) net = gluon.nn.SymbolBlock(     outputs=mx.sym.load_json(sym_json_string),     inputs=mx.sym.var('data')) net.load_parameters(     'image-classification-0010.params',       allow_missing=True) net.collect_params().initialize()
    ```

3.  The input shape is a 300x300 color image with three channels (**red, green, and blue**, or **RGB**). Accordingly, we create a fake image using random values. We forward it through the model and read the results, as follows:

    ```
    test_sample = mx.ndarray.random.normal(
        shape=(1,3,300,300))
    response = net(test_sample)
    print(response)
    ```

    有趣的是，这个随机图像被归类为一只猫，如下面的代码片段中所定义的:

    ```
    array([[0.99126923, 0.00873081]], dtype=float32)
    ```

重用**对象检测**更加复杂，因为需要修改训练网络以进行预测。你可以在[https://github . com/AWS-samples/Amazon-sage maker-AWS-green grass-custom-object-detection-model/](https://github.com/aws-samples/amazon-sagemaker-aws-greengrass-custom-object-detection-model/)找到例子。

现在，我们来看看**极限梯度提升** ( **XGBoost** )神器。

## 检查并导出 XGBoost 模型

XGBoost 工件包含一个文件——模型本身。然而，模型的格式取决于如何使用 XGBoost。

使用内置算法，模型是一个存储`Booster`对象的酸洗文件。一旦工件被提取出来，我们只需简单地取消模型并加载它，如下所示:

```
$ tar xvfz model.tar.gz
x xgboost-model
$ python
>>> import pickle
>>> model = pickle.load(open('xgboost-model', 'rb'))
>>> type(model)
<class 'xgboost.core.Booster'>
```

有了内置的框架，模型只是一个保存的模型。一旦工件被提取出来，我们就直接加载模型，如下所示:

```
$ tar xvfz model.tar.gz
x xgb.model
$ python
>>> import xgboost as xgb
>>> bst = xgb.Booster({'nthread': 4})
>>> model = bst.load_model('xgb.model')
>>> type(bst)
<class 'xgboost.core.Booster'>
```

现在，我们来看看 **scikit-learn** 神器。

## 检查和导出 scikit-learn 模型

Scikit-learn 模型被保存并加载`joblib`(https://joblib . readthedocs . io)，如下面的代码片段所示。这个库包含了一组提供轻量级管道的工具，但是我们只使用它来保存模型:

```
$ tar xvfz model.tar.gz
x model.joblib
$ python
>>> import joblib
>>> model = joblib.load('model.joblib')
>>> type(model)
<class 'sklearn.linear_model._base.LinearRegression'>
```

最后，我们来看看 **TensorFlow** 神器。

## 检查和导出张量流模型

TensorFlow 和 **Keras** 模型以 **TensorFlow Serving** 格式保存，如以下代码片段所示:

```
$ mkdir /tmp/models
$ tar xvfz model.tar.gz -C /tmp/models
x 1/
x 1/saved_model.pb
x 1/assets/
x 1/variables/
x 1/variables/variables.index
x 1/variables/variables.data-00000-of-00002
x 1/variables/variables.data-00001-of-00002
```

为这种模型提供服务的最简单的方法是为 TensorFlow 服务运行 **Docker** 图像，如下面的代码片段所示。你可以在 https://www.tensorflow.org/tfx/serving/serving_basic 找到更多细节:

```
$ docker run -t --rm -p 8501:8501
  -v "/tmp/models:/models/fmnist"
  -e MODEL_NAME=fmnist
  tensorflow/serving
```

让我们看最后一个例子，我们导出一个拥抱脸模型。

## 检查并导出拥抱人脸模型

拥抱脸模型可以在 TensorFlow 或 PyTorch 上训练。让我们重新使用来自 [*第 7 章*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130) 、*使用内置框架扩展机器学习服务*的拥抱脸示例，其中我们使用 PyTorch 训练了一个情感分析模型，并进行如下操作:

1.  我们从 S3 复制模型工件并提取出来，像这样:

    ```
    $ tar xvfz model.tar.gz training_args.bin config.json pytorch_model.bin
    ```

2.  在 Jupyter 笔记本中，我们使用拥抱脸 API 来加载模型配置。然后我们使用一个`DistilBertForSequenceClassification`对象构建模型，它对应于我们在 SageMaker 上训练的模型。下面是实现这一点的代码:

    ```
    from transformers import AutoConfig, DistilBertForSequenceClassification config = AutoConfig.from_pretrained(          './model/config.json') model = DistilBertForSequenceClassification        .from_pretrained('./model/pytorch_model.bin',                           config=config)
    ```

3.  接下来，我们获取与模型相关联的标记器，如下:

    ```
    from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(             'distilbert-base-uncased') 
    ```

4.  我们编写一个简短的函数，将`softmax`应用于模型输出层返回的激活值，如下所示:

    ```
    import torch def probs(logits):     softmax = torch.nn.Softmax(dim=1)     pred = softmax(logits).detach().numpy()     return pred
    ```

5.  Finally, we define a sample and predict it with our model, as follows:

    ```
    inputs = tokenizer("The Phantom Menace was a really bad movie. What a waste of my life.", return_tensors='pt')
    outputs = model(**inputs)
    print(probs(outputs.logits))
    ```

    不出所料，情绪非常消极，正如我们在这里看到的:

    ```
    [[0.22012234 0.7798777 ]]
    ```

关于从 SageMaker 出口型号的章节到此结束。如你所见，这真的一点也不难。

现在，让我们学习如何在实时端点上部署模型。

# 在实时端点上部署模型

SageMaker 端点使用托管在完全托管的基础设施上的模型提供实时预测。它们可以用 SageMaker `boto3`创建和管理。

您可以在 SageMaker Studio 中的 **SageMaker 资源** / **端点**下找到关于您的端点的信息。

现在，让我们更详细地看看 SageMaker SDK。

## 使用 SageMaker SDK 管理端点

SageMaker SDK 允许您以多种方式使用端点，如下所述:

*   配置一个估计器，用`fit()`训练它，用`deploy()`部署一个端点，用`predict()`调用它
*   导入和部署模型
*   调用现有端点
*   更新现有端点

到目前为止，我们已经在很多例子中使用了第一种场景。让我们看看其他的。

### 导入和部署 XGBoost 模型

当您想要导入一个没有在 SageMaker 上训练过的模型，或者当您想要重新部署一个 SageMaker 模型时，这是非常有用的。在前面的部分中，我们看到了模型工件是什么样子的，以及我们应该如何使用它们来打包模型。我们现在将进行如下操作:

1.  从我们使用`save_model()`训练并保存在本地的 XGBoost 模型开始，我们首先通过运行下面的代码来创建一个模型工件:

    ```
    $ tar cvfz model-xgb.tar.gz xgboost-model
    ```

2.  在 Jupyter 笔记本中，我们将模型工件上传到我们的默认 bucket，就像这样:

    ```
    import sagemaker sess = sagemaker.Session() prefix = 'export-xgboost' model_path = sess.upload_data(     path=model-xgb.tar.gz',      key_prefix=prefix)
    ```

3.  然后，我们创建一个`XGBoostModel`对象，传递工件的位置和一个推理脚本(稍后会详细介绍)。我们还选择了一个框架版本，它应该与我们用来训练模型的版本相匹配。代码如下面的代码片段所示:

    ```
    from sagemaker.xgboost.model import XGBoostModel xgb_model = XGBoostModel(     model_data=model_path,     entry_point='xgb-script.py',     framework_version='1.3-1',     role=sagemaker.get_execution_role())
    ```

4.  推理脚本非常简单。它只需要包含一个模型加载函数，正如我们在第 7 章*中讨论部署框架模型时所解释的那样，用内置框架扩展机器学习服务*。代码如下面的代码片段所示:

    ```
    import os import xgboost as xgb def model_fn(model_dir):     model = xgb.Booster()     model.load_model(         os.path.join(model_dir,'xgboost-model'))     return model
    ```

5.  回到笔记本中，我们像往常一样进行部署和预测，如下:

    ```
    xgb_predictor = xgb_model.deploy(. . .) xgb_predictor.predict(. . .)
    ```

现在，让我们用张量流模型做同样的事情。

### 导入和部署张量流模型

这个过程非常相似，我们接下来会看到:

1.  我们首先使用`tar`来封装一个 TensorFlow 模型，我们将它训练并保存为 TensorFlow 服务格式。我们的工件应该是这样的(请不要忘记创建顶级目录！):

    ```
    $ tar tvfz model.tar.gz 1/ 1/saved_model.pb 1/assets/ 1/variables/ 1/variables/variables.index 1/variables/variables.data-00000-of-00002 1/variables/variables.data-00001-of-00002
    ```

2.  然后，我们将工件上传到 S3，如下:

    ```
    import sagemaker sess = sagemaker.Session() prefix = 'byo-tf' model_path = sess.upload_data(    path='model.tar.gz',     key_prefix=prefix)
    ```

3.  接下来，我们从工件中创建一个 SageMaker 模型。默认情况下，我们不必提供推理脚本。如果我们需要为功能工程、外来序列化等定制预处理和后处理程序，我们会通过。您可以在 https://sagemaker . readthe docs . io/en/stable/frameworks/tensor flow/using _ TF . html # deploying-from-an-estimator 找到更多信息。代码如下面的代码片段所示:

    ```
    from sagemaker.tensorflow.model import TensorFlowModel tf_model = TensorFlowModel(     model_data=model_path,     framework_version='2.3.1',     role=sagemaker.get_execution_role())
    ```

4.  然后我们照常部署和预测，感谢**深度学习容器** ( **DLC** )用于 TensorFlow。

现在，让我们做最后一个例子，我们用 PyTorch 的 DLC 和一个用于模型加载和定制处理的推理脚本导入和部署一个拥抱脸模型。

### 使用 PyTorch 导入和部署拥抱脸模型

让我们重用我们的拥抱脸例子，首先关注推理脚本。它包含四个功能:模型加载、预处理、预测和后处理。我们将如下进行:

1.  模型加载函数使用的代码与我们导出模型时使用的代码相同。唯一的区别是我们从`model_dir`加载文件，由 SageMaker 传递给 PyTorch 容器。我们还加载了一次记号赋予器。代码如下面的代码片段所示:

    ```
    tokenizer = AutoTokenizer.from_pretrained(             'distilbert-base-uncased') def model_fn(model_dir):   config_path='{}/config.json'.format(model_dir)   model_path='{}/pytorch_model.bin'.format(model_dir)   config=AutoConfig.from_pretrained(config_path)   model= DistilBertForSequenceClassification          .from_pretrained(model_path, config=config)   return model
    ```

2.  预处理和后处理功能很简单。它们只检查正确的内容并接受类型。您可以在下面的代码片段中看到这些:

    ```
    def input_fn(serialized_input_data,               content_type=JSON_CONTENT_TYPE):     if content_type == JSON_CONTENT_TYPE:     input_data = json.loads(serialized_input_data)     return input_data   else:     raise Exception('Unsupported input type: '                      + content_type) def output_fn(prediction_output,                accept=JSON_CONTENT_TYPE):   if accept == JSON_CONTENT_TYPE:     return json.dumps(prediction_output), accept   else:     raise Exception('Unsupported output type: '                     + accept)
    ```

3.  最后，预测函数对输入数据进行标记化、预测，并返回最可能的类的名称，如下所示:

    ```
    CLASS_NAMES = ['negative', 'positive'] def predict_fn(input_data, model):     inputs = tokenizer(input_data['text'],                         return_tensors='pt')     outputs = model(**inputs)     logits = outputs.logits     _, prediction = torch.max(logits, dim=1)     return CLASS_NAMES[prediction]
    ```

现在我们的推理脚本已经准备好了，让我们移到笔记本上，导入模型，并部署它，如下所示:

1.  我们创建一个`PyTorchModel`对象，传递模型工件在 S3 的位置和我们推理脚本的位置，如下:

    ```
    from sagemaker.pytorch import PyTorchModel model = PyTorchModel(     model_data=model_data_uri,     role=sagemaker.get_execution_role(),      entry_point='torchserve-predictor.py',     source_dir='src',     framework_version='1.6.0',     py_version='py36')
    ```

2.  We deploy with `model.deploy()`. Then, we create two samples and send them to our endpoint, as follows:

    ```
    positive_data = {'text': "This is a very nice camera, I'm super happy with it."}
    negative_data = {'text': "Terrible purchase, I want my money back!"}
    prediction = predictor.predict(positive_data)
    print(prediction)
    prediction = predictor.predict(negative_data)
    print(prediction)
    ```

    正如所料，输出是`positive`和`negative`。

关于导入和部署模型的部分到此结束。现在，让我们学习如何调用已经部署的端点。

### 调用现有端点

当您想要使用一个活动端点，但没有访问预测器的权限时，这很有用。我们只需要知道端点的名称。进行如下操作:

1.  为我们在前面的示例中部署的端点构建一个`TensorFlowPredictor`预测器。同样，该对象是特定于框架的。代码如下面的代码片段所示:

    ```
    from sagemaker.tensorflow.model import TensorFlowPredictor another_predictor = TensorFlowPredictor(     endpoint_name=tf_endpoint_name,     serializer=sagemaker.serializers.JSONSerializer() )
    ```

2.  然后，照常预测，如下:

    ```
    another_predictor.predict(…)
    ```

现在，让我们学习如何更新端点。

### 更新现有端点

`update_endpoint()` API 允许您以非破坏性的方式更新端点的配置。端点保持服务状态，您可以继续使用它进行预测。

让我们在 TensorFlow 端点上尝试一下，如下所示:

1.  我们将实例计数设置为`2`并更新端点，如下所示:

    ```
    another_predictor.update_endpoint(     initial_instance_count=2,     instance_type='ml.t2.medium')
    ```

2.  The endpoint is immediately updated, as shown in the following screenshot.![Figure 11.2 – Endpoint being updated
    ](img/B17705_11_2.jpg)

    图 11.2–正在更新的端点

3.  更新完成后，端点现在由两个实例支持，如下面的屏幕截图所示:

![Figure 11.3 – Endpoint backed by two instances
](img/B17705_11_3.jpg)

图 11.3–由两个实例支持的端点

如您所见，使用 SageMaker SDK 导入、部署、重新部署和更新模型非常容易。然而，有些操作要求我们使用较低级别的 API。它们在 AWS 语言 SDK 中可用，我们将使用我们的好朋友`boto3`来演示它们。

## 使用 boto3 SDK 管理端点

`boto3`是针对 Python 的AWS SDK([https://aws.amazon.com/sdk-for-python/](https://aws.amazon.com/sdk-for-python/))。它包括所有 AWS 服务的API(除非他们没有 API！).SageMaker API 可从 https://boto 3 . Amazon AWS . com/v1/documentation/API/latest/reference/services/sage maker . html 获得。

`boto3`API 是服务级 API，它们赋予我们对所有服务操作的完全控制权。让我们看看它们如何帮助我们以 SageMaker SDK 不允许的方式部署和管理端点。

### 使用 boto3 SDK 部署端点

使用`boto3`部署端点是一个四步操作，概述如下:

1.  用`create_model()` API 创建一个或多个模型。或者，我们可以使用现有的模型，这些模型已经被 SageMaker SDK 训练或导入。为了简洁起见，我们将在这里这样做。
2.  定义一个或多个**生产变量**，列出每个模型的基础设施要求。
3.  创建一个`create_endpoint_config()` API，传递先前定义的生产变量，并给每个变量分配一个权重。
4.  用`create_endpoint()` API 创建一个端点。

让我们将这些 API 投入使用，并部署运行我们在波士顿住房数据集上训练的 XGBoost 模型的两个变体的端点，如下所示:

1.  我们定义了两个变量:两者都由单个实例支持。然而，它们将分别接收十分之九和十分之一的传入请求——也就是说，“可变权重/权重之和”。如果我们想在生产中引入一个新的模型，并在发送流量之前确保它运行良好，我们可以使用这个设置。代码如下面的代码片段所示:

    ```
    production_variants = [   { 'VariantName': 'variant-1',     'ModelName': model_name_1,     'InitialInstanceCount': 1,     'InitialVariantWeight': 9,     'InstanceType': 'ml.t2.medium'},   { 'VariantName': 'variant-2',     'ModelName': model_name_2,     'InitialInstanceCount': 1,     'InitialVariantWeight': 1,     'InstanceType': 'ml.t2.medium'}]
    ```

2.  We create an endpoint configuration by passing our two variants and setting optional tags, as follows:

    ```
    import boto3
    sm = boto3.client('sagemaker')
    endpoint_config_name = 'xgboost-two-models-epc'
    response = sm.create_endpoint_config(
        EndpointConfigName=endpoint_config_name,
        ProductionVariants=production_variants,
        Tags=[{'Key': 'Name', 
               'Value': endpoint_config_name},
              {'Key': 'Algorithm', 'Value': 'xgboost'}])
    ```

    我们可以用`list_endpoint_configs()`列出所有的端点配置，并用`describe_endpoint_config()``boto3`API 描述特定的一个。

3.  We create an endpoint based on this configuration:

    ```
    endpoint_name = 'xgboost-two-models-ep'
    response = sm.create_endpoint(
        EndpointName=endpoint_name,
        EndpointConfigName=endpoint_config_name,
        Tags=[{'Key': 'Name','Value': endpoint_name},
              {'Key': 'Algorithm','Value': 'xgboost'},
              {'Key': 'Environment',
               'Value': 'development'}])
    ```

    我们可以用`list_endpoints()`列出所有端点，并用`describe_endpoint()``boto3`API 描述一个特定的端点。

4.  创建一个`boto3`服务者是一种等待端点服务的简便方法。您可以看到这里正在创建一个:

    ```
    waiter = sm.get_waiter('endpoint_in_service') waiter.wait(EndpointName=endpoint_name)
    ```

5.  After a few minutes, the endpoint is in service. As shown in the following screenshot, it now uses two production variants:![Figure 11.4 – Viewing production variants
    ](img/B17705_11_4.jpg)

    图 11.4–查看生产变量

6.  然后，我们调用端点，如下面的代码片段所示。默认情况下，预测请求根据其权重转发给变量:

    ```
    smrt = boto3.Session().client(     service_name='runtime.sagemaker')  response = smrt.invoke_endpoint(    EndpointName=endpoint_name,    ContentType='text/csv',    Body=test_sample)
    ```

7.  We can also select the variant that receives the prediction request. This is useful for A/B testing, where we need to stick users to a given model. The following code snippet shows you how to do this:

    ```
    variants = ['variant-1', 'variant-2']
    for v in variants:
      response = smrt.invoke_endpoint(
                     EndpointName=endpoint_name, 
                     ContentType='text/csv',
                     Body=test_sample,
                     TargetVariant=v)
      print(response['Body'].read())
    ```

    这会产生以下输出:

    ```
    b'[0.0013231043703854084]'
    b'[0.001262241625227034]'
    ```

8.  我们还可以更新权重，例如，赋予两个变量相同的权重，以便它们接收相同份额的传入流量，如下所示:

    ```
    response = sm.update_endpoint_weights_and_capacities(     EndpointName=endpoint_name,     DesiredWeightsAndCapacities=[         { 'VariantName': 'variant-1',            'DesiredWeight': 5},         { 'VariantName': 'variant-2',            'DesiredWeight': 5}])
    ```

9.  我们可以完全删除一个变体，并将所有流量发送给剩下的一个。在这里，端点也始终保持服务状态，不会丢失任何流量。代码如下面的代码片段所示:

    ```
    production_variants_2 = [   {'VariantName': 'variant-2',    'ModelName': model_name_2,    'InitialInstanceCount': 1,    'InitialVariantWeight': 1,    'InstanceType': 'ml.t2.medium'}] endpoint_config_name_2 = 'xgboost-one-model-epc' response = sm.create_endpoint_config(     EndpointConfigName=endpoint_config_name_2,     ProductionVariants=production_variants_2,     Tags=[{'Key': 'Name',            'Value': endpoint_config_name_2},           {'Key': 'Algorithm','Value': 'xgboost'}]) response = sm.update_endpoint(     EndpointName=endpoint_name,     EndpointConfigName=endpoint_config_name_2)
    ```

10.  最后，我们通过删除端点和两个端点配置进行清理，如下所示:

    ```
    sm.delete_endpoint(EndpointName=endpoint_name) sm.delete_endpoint_config(   EndpointConfigName=endpoint_config_name) sm.delete_endpoint_config(   EndpointConfigName=endpoint_config_name_2)
    ```

正如你所看到的，`boto3` API 更加冗长，但是它也给了我们进行**机器学习** ( **ML** )操作所需的灵活性。在下一章，我们将学习如何自动化这些。

# 在批量变压器上部署模型

一些使用的情况不需要实时端点。例如，您可能希望每周一次一次性预测 10 **千兆字节** ( **GB** )的数据，获得结果，并将其提供给下游应用程序。批量变压器是实现这一点的一种非常简单的方法。

在本例中，我们将使用我们在第 7 章 、*使用内置框架扩展机器学习服务*中的 [*波士顿住房数据集上训练的 scikit-learn 脚本。让我们开始吧，如下:*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130)

1.  像往常一样，通过运行下面的代码来配置估计器:

    ```
    from sagemaker.sklearn import SKLearn sk = SKLearn(entry_point='sklearn-boston-housing.py',    role=sagemaker.get_execution_role(),    instance_count=1,    instance_type='ml.m5.large',    output_path=output,    hyperparameters=        {'normalize': True, 'test-size': 0.1}) sk.fit({'training':training})
    ```

2.  让我们用批量模式预测训练集。我们删除目标值，将数据集保存到一个**逗号分隔值** ( **CSV** )文件，并上传到 S3，如下:

    ```
    import pandas as pd data = pd.read_csv('housing.csv') data.drop(['medv'], axis=1, inplace=True) data.to_csv('data.csv', header=False, index=False) batch_input = sess.upload_data(     path='data.csv',      key_prefix=prefix + '/batch')
    ```

3.  创建一个 transformer 对象并启动批处理，如下:

    ```
    sk_transformer = sk.transformer(     instance_count=1,      instance_type='ml.m5.large') sk_transformer.transform(     batch_input,      content_type='text/csv',      wait=True, logs=True)
    ```

4.  在训练日志中，我们可以看到 SageMaker 创建了一个临时端点，并使用它来预测数据。对于大规模作业，我们可以通过对用于预测的样本进行小批量处理来优化吞吐量(使用`strategy`参数)，增加预测并发性的级别(`max_concurrent_transforms`，并增加最大有效负载大小(`max_payload`)。
5.  一旦工作完成，预测在 S3 可用，如下所示:

    ```
    print(sk_transformer.output_path) s3://sagemaker-us-east-1-123456789012/sagemaker-scikit-learn-2020-06-12-08-28-30-978
    ```

6.  使用 AWS CLI，我们可以通过运行以下代码轻松检索这些预测:

    ```
    %%bash -s "$sk_transformer.output_path" aws s3 cp $1/data.csv.out . head -1 data.csv.out [[29.73828574177013], [24.920634119498292], …
    ```

7.  就像培训一样，工作一完成，transformer 使用的基础设施就关闭了，所以没有什么需要清理的。

在下一节中，我们将看看推理管道，以及如何使用它们来部署一系列相关的模型。

# 在推理管道上部署模型

现实生活中的 ML 场景通常涉及不止一个模型；例如，您可能需要对输入数据运行预处理步骤，或者使用**主成分分析** ( **PCA** )算法减少数据的维数。

当然，您可以将每个模型部署到一个专用的端点。然而，需要编排代码将预测请求按顺序传递给每个模型。增加端点也会带来额外的成本。

相反，**推理管道**允许您在同一个端点上部署多达五个模型，或者用于批量转换，并自动处理预测序列。

假设我们想运行 PCA，然后是线性学习器。构建推理管道将如下所示:

1.  在输入数据集上训练 PCA 模型。
2.  使用 PCA 处理训练和验证集，并将结果存储在 S3。批量转换是实现这一点的好方法。
3.  使用 PCA 处理的数据集作为输入来训练线性学习器模型。
4.  使用`create_model()` API 创建一个推理管道，如下所示:

    ```
    response = sagemaker.create_model(     ModelName='pca-linearlearner-pipeline',         Containers=[             {              'Image': pca_container,              'ModelDataUrl': pca_model_artifact,               . . .             },             {              'Image': ll_container,              'ModelDataUrl': ll_model_artifact,               . . .             }         ],         ExecutionRoleArn=role )
    ```

5.  以通常的方式创建端点配置和端点。我们也可以使用带有批量转换器的管道。

你可以在[https://github . com/aw slabs/Amazon-sage maker-examples/tree/master/sage maker-python-SDK/scikit _ learn _ inference _ pipeline](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_inference_pipeline)找到使用 scikit-learn 和线性学习器的完整示例。

**Spark** 是一个非常受欢迎的数据处理选择，SageMaker 允许您使用 **SparkML Serving** 内置容器(https://github . com/AWS/SageMaker-Spark ml-Serving-container)部署 Spark 模型，该容器使用 **mleap** 库([https://github.com/combust/mleap](https://github.com/combust/mleap))。当然，这些模型可以是**推理管道**的一部分。你可以在[https://github . com/aw slabs/Amazon-sage maker-examples/tree/master/advanced _ functionality](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality)找到几个例子。

我们对模型部署的讨论到此结束。在下一节中，我们将介绍一个 SageMaker 功能，它可以帮助我们检测影响预测质量的数据问题: **SageMaker 模型监视器**。

# 使用亚马逊 SageMaker 模型监控器监控预测质量

SageMaker 型号监视器有两个主要特点，在此概述如下:

*   捕获发送到端点的数据，以及端点返回的预测。这有助于进一步分析，或者在新模型的开发和测试过程中重放真实的流量。
*   将传入流量与根据训练集构建的基线进行比较，并发送有关数据质量问题的警报，如丢失的功能、输入错误的功能以及统计属性中的差异(也称为“数据漂移”)。

我们将使用来自第四章*的**线性学习器**示例，我们在 Boston Housing 数据集上训练了一个模型。首先，我们将向端点添加数据捕获。然后，我们将构建一个**基线**，并设置一个**监控时间表**，以定期将传入的数据与该基线进行比较。*

## 捕获数据

我们可以在部署端点时设置数据捕获流程。我们还可以在现有的端点上使用我们刚刚用于生产变体的`update_endpoint()` API 来启用它。

在撰写本文时，您应该注意以下几点:

*   如果您想执行模型监控，一次只能发送**一个样本**。将捕获小批量预测，但它们会导致监控作业失败。
*   同样，数据样本和预测必须是**扁平的表格数据**。结构化数据(比如列表的列表和嵌套的 JSON)将被捕获，但是模型监控作业将无法处理它。或者，您可以添加一个预处理脚本和一个后处理脚本来展平它。你可以在[https://docs . AWS . Amazon . com/sage maker/latest/DG/model-monitor-pre-and-post-processing . html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-pre-and-post-processing.html)找到更多信息。
*   内容类型和接受类型必须**相同**。CSV 和 JSON 都可以用，但是不能混用。
*   如果某个端点附加了监视计划，则不能删除该端点。您必须先**删除监控时间表**，然后删除端点。

了解了这一点，让我们捕捉一些数据吧！我们开始吧:

1.  训练照常进行。您可以在 GitHub 资源库中找到代码。
2.  我们为 100%的预测请求和响应创建一个数据捕获配置，将所有内容存储在 S3，如下所示:

    ```
    from sagemaker.model_monitor.data_capture_config import DataCaptureConfig capture_path = 's3://{}/{}/capture/'.format(bucket, prefix) ll_predictor = ll.deploy(     initial_instance_count=1,     instance_type='ml.t2.medium',     data_capture_config = DataCaptureConfig(               enable_capture = True,                              sampling_percentage = 100,                          capture_options = ['REQUEST', 'RESPONSE'],          destination_s3_uri = capture_path))
    ```

3.  一旦端点投入使用，我们就发送数据进行预测。在一两分钟内，我们看到在 S3 捕获的数据，然后复制到本地，如下:

    ```
    %%bash -s "$capture_path" aws s3 ls --recursive $1 aws s3 cp --recursive $1 .
    ```

4.  打开其中一个文件，我们看到了示例和预测，如下:

    ```
    {"captureData":{"endpointInput":{"observedContentType":"text/csv","mode":"INPUT","data":"0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98","encoding":"CSV"},"endpointOutput":{"observedContentType":"text/csv; charset=utf-8","mode":"OUTPUT","data":"30.4133586884","encoding":"CSV"}},"eventMetadata":{"eventId":"8f45e35c-fa44-40d2-8ed3-1bcab3a596f3","inferenceTime":"2020-07-30T13:36:30Z"},"eventVersion":"0"}
    ```

如果这个是实时数据，我们可以用它来测试新模型，以便将它们的性能与现有模型进行比较。

现在，让我们学习如何从训练集中创建基线。

## 创建基线

SageMaker Model Monitor 包含一个内置的容器，我们可以使用它来构建基线，并且我们可以将它直接用于`DefaultModelMonitor`对象。你也可以带上自己的容器，在这种情况下，你可以使用`ModelMonitor`对象。让我们开始吧，如下:

1.  A baseline can only be built on CSV datasets and JSON datasets. Our dataset is space-separated and needs to be converted into a CSV file, as follows. We can then upload it to S3:

    ```
    data.to_csv('housing.csv', sep=',', index=False)
    training = sess.upload_data(
       path='housing.csv', 
       key_prefix=prefix + "/baseline")
    ```

    注意

    这里有一个小警告:基线作业是在 **SageMaker 处理**中运行的 Spark 作业。因此，列名需要符合 Spark，否则您的工作将会以神秘的方式失败。特别是，列名中不允许有点。我们这里没有这个问题，但是请记住这一点。

2.  定义基础设施要求、训练集的位置及其格式，如下所示:

    ```
    from sagemaker.model_monitor import DefaultModelMonitor from sagemaker.model_monitor.dataset_format import DatasetFormat ll_monitor = DefaultModelMonitor(role=role,     instance_count=1, instance_type='ml.m5.large') ll_monitor.suggest_baseline(baseline_dataset=training,     dataset_format=DatasetFormat.csv(header=True))
    ```

3.  As you can guess, this is running as a SageMaker Processing job, and you can find its log in `/aws/sagemaker/ProcessingJobs` prefix.

    在其输出位置有两个 JSON 工件:`statistics.json`和`constraints.json`。我们可以通过运行下面的代码用`pandas`查看它们的内容:

    ```
    baseline = ll_monitor.latest_baselining_job
    constraints = pd.io.json.json_normalize(
        baseline.suggested_constraints()
        .body_dict["features"])
    schema = pd.io.json.json_normalize(
        baseline.baseline_statistics().body_dict["features"])
    ```

4.  As shown in the following screenshot, the `constraints` file gives us the inferred type of each feature, its completeness in the dataset, and whether it contains negative values or not:![Figure 11.5 – Viewing the inferred schema
    ](img/B17705_11_5.jpg)

    图 11.5–查看推断的模式

5.  `statistics`文件增加了基本的统计数据，如下图所示:

![Figure 11.6 – Viewing data statistics
](img/B17705_11_6.jpg)

图 11.6–查看数据统计

它还包括基于 KLL 草图(https://arxiv . org/ABS/1603.05346 v2)的分布信息，这是一种定义分位数的简洁方法。

一旦创建了基线，我们就可以设置一个监控计划，以便将传入流量与基线进行比较。

## 设置监控时间表

我们简单地传递端点的名称、统计数据、约束和分析运行的频率。我们将每小时一次，这是允许的最短频率。代码如下面的代码片段所示:

```
from sagemaker.model_monitor import CronExpressionGenerator
ll_monitor.create_monitoring_schedule(
    monitor_schedule_name='ll-housing-schedule',
    endpoint_input=ll_predictor.endpoint,
    statistics=ll_monitor.baseline_statistics(),
    constraints=ll_monitor.suggested_constraints(),
    schedule_cron_expression=CronExpressionGenerator.hourly())
```

这里，分析将由内置容器执行。可选地，我们可以提供我们自己的带有定制分析代码的容器。你可以在[https://docs . AWS . Amazon . com/sage maker/latest/DG/model-monitor-byoc-containers . html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-containers.html)找到更多信息。

现在，让我们向端点发送一些讨厌的数据，看看 SageMaker Model Monitor 是否会接收到这些数据。

## 发送错误数据

不幸的是，模型有时可能会收到不正确的数据。也许它在源代码中已经被破坏，也许负责调用端点的应用程序有问题，等等。让我们模拟一下，看看这对预测的质量有多大影响，如下所示:

1.  Starting from a valid sample, we get a correct prediction, as illustrated here:

    ```
    test_sample = '0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98'
    ll_predictor.serializer =    
        sagemaker.serializers.CSVSerializer()
    ll_predictor.deserializer =  
        sagemaker.deserializers.CSVDeserializer()
    response = ll_predictor.predict(test_sample)
    print(response)
    ```

    这栋房子的价格是 30，173 美元:

    ```
    [['30.1734218597']]
    ```

2.  Now, let's multiply the first feature by 10,000, as shown in the following code snippet. Scaling and unit errors are quite frequent in application code:

    ```
    bad_sample_1 = '632.0,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98'
    response = ll_predictor.predict(bad_sample_1)
    print(response)
    ```

    哎哟！价格是负的，正如我们在这里看到的。显然，这是一个虚假的预测:

    ```
    [['-35.7245635986']]
    ```

3.  Let's try negating the last feature by running the following code:

    ```
    bad_sample_2 = '0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,-4.98'
    response = ll_predictor.predict(bad_sample_2)
    print(response)
    ```

    正如我们在下面的代码片段中所看到的，预测值远远高于它应该达到的值。这是一个隐蔽的问题，这意味着它更难检测，并且可能会产生严重的业务后果:

    ```
    [['34.4245414734']]
    ```

您应该尝试用坏数据做实验，看看哪些特征最脆弱。所有这些流量都将被 SageMaker 模型监视器捕获。一旦监控作业运行，您应该会在其**违规报告**中看到条目。

## 审查违规举报

之前，我们创建了每小时一次的监控任务。如果需要 1 个多小时才能看到结果，也不用担心；作业执行由后端进行负载平衡，可能会有短暂的延迟:

1.  We can find more information about our monitoring job in the SageMaker console, in the `describe_schedule()` API and list executions with the `list_executions()` API, as follows:

    ```
    ll_executions = ll_monitor.list_executions()
    print(ll_executions)
    ```

    在这里，我们可以看到三个执行:

    ```
    [<sagemaker.model_monitor.model_monitoring.MonitoringExecution at 0x7fdd1d55a6d8>,
    <sagemaker.model_monitor.model_monitoring.MonitoringExecution at 0x7fdd1d581630>,
    <sagemaker.model_monitor.model_monitoring.MonitoringExecution at 0x7fdce4b1c860>]
    ```

2.  The violations report is stored as a JSON file in S3\. We can read it and display it with `pandas`, as follows:

    ```
    violations = ll_monitor.latest_monitoring_constraint_violations()
    violations = pd.io.json.json_normalize(
        violations.body_dict["violations"])
    violations
    ```

    这将打印出上一个监视作业检测到的违规，如以下屏幕截图所示:

    ![Figure 11.7 – Viewing violations
    ](img/B17705_11_7.jpg)

    图 11.7–查看违规

3.  Of course, we can also fetch the file in S3 and display its contents, as follows:

    ```
    %%bash -s "$report_path"
    echo $1
    aws s3 ls --recursive $1
    aws s3 cp --recursive $1 .
    ```

    这里有一个示例条目，警告我们该模型为`chas`特性接收了一个小数值，尽管它在模式中被定义为一个整数:

    ```
    {
        "feature_name" : "chas",
        "constraint_check_type" : "data_type_check",
        "description" : "Data type match requirement is not met.
            Expected data type: Integral, Expected match: 100.0%.  
            Observed: Only 0.0% of data is Integral."
    }
    ```

    我们还可以向 CloudWatch 指标发出这些违规，并触发警报来通知开发人员潜在的数据质量问题。你可以在[https://docs . AWS . Amazon . com/sage maker/latest/DG/model-monitor-interpreting-cloud watch . html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-cloudwatch.html)找到更多信息。

4.  完成后，不要忘记删除监控计划和端点本身，如下所示:

    ```
    response = ll_monitor.delete_monitoring_schedule() ll_predictor.delete_endpoint()
    ```

正如您所看到的，SageMaker Model Monitor 可以帮助您捕获输入数据和预测，这对于模型测试来说是一个非常有用的特性。此外，您还可以使用内置容器或您自己的容器来执行数据质量分析。

在下一节中，我们将离开端点，学习如何将模型部署到容器服务中。

# 将模型部署到容器服务

之前，我们看到了如何在 S3 获取模型工件以及如何从中提取实际的模型。知道了这一点，在容器服务上部署它就相当容易了，比如亚马逊弹性容器服务(Amazon Elastic Container Service)和 T21 的亚马逊弹性容器服务(Amazon Elastic Kubernetes Service)。

也许公司的政策是将所有东西都部署在容器中，也许你只是喜欢它们，或者两者都是！不管是什么原因，你肯定能做到。这里没有特别针对 SageMaker 的内容，这些服务的 AWS 文档会告诉您需要知道的一切。

一个示例高级流程可能如下所示:

1.  在 SageMaker 上训练一个模型。
2.  训练完成后，抓取工件，提取模型。
3.  将模型推送到 Git 存储库。
4.  编写任务定义(适用于 ECS 和 Fargate)或 pod 定义(适用于 EKS)。它可以使用一个内置容器或您自己的容器。然后，它可以运行模型服务器或您自己的代码，从您的 Git 存储库中克隆模型，加载它，并提供预测。
5.  使用这个定义，在集群上运行一个容器。

让我们将此应用于亚马逊 Fargate。

## 在 SageMaker 上培训并在 Amazon Fargate 上部署

**亚马逊 Fargate** 让你在完全托管的基础设施上运行容器([https://aws.amazon.com/fargate](https://aws.amazon.com/fargate))。不需要创建和管理集群，这对于不希望参与基础设施细节的用户来说非常理想。但是，请注意，在撰写本文时，Fargate 不支持**图形处理器** ( **GPU** )容器。

### 准备模型

我们使用以下步骤准备模型:

1.  首先，我们在时尚——MNIST 培训一名 TensorFlow 模特。一切照旧。
2.  我们在 S3 找到模型工件的位置，并将其设置为一个环境变量，如下:

    ```
    %env model_data {tf_estimator.model_data}
    ```

3.  我们从 S3 下载工件，并将其解压到本地目录，如下:

    ```
    %%sh aws s3 cp ${model_data} . mkdir test-models tar xvfz model.tar.gz -C test-models
    ```

4.  我们打开一个终端，将模型提交给一个公共的 Git 存储库，如下面的代码片段所示。我这里用的是我的一个([https://gitlab.com/juliensimon/test-models](https://gitlab.com/juliensimon/test-models))；你应该换成你的:

    ```
    <initialize git repository> $ cd test-models $ git add model $ git commit -m "New model" $ git push
    ```

### 配置 Fargate

既然模型在存储库中可用，我们需要配置 Fargate。这次我们将使用命令行。你可以用`boto3`或任何其他语言的 SDK 做同样的事情。我们将如下进行:

1.  `ecs-cli`是一个方便的 CLI 工具，用于管理集群。让我们通过运行下面的代码来安装它:

    ```
    %%sh sudo curl -o /usr/local/bin/ecs-cli https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-amd64-latest sudo chmod 755 /usr/local/bin/ecs-cli
    ```

2.  我们用它来“创建”一个 Fargate 集群。实际上，这并没有创造任何基础设施；我们只定义了一个将用于运行任务的集群名称。请确保您的`ecs:CreateCluster`。如果没有，请在继续之前添加它。代码如下面的代码片段所示:

    ```
    %%sh  aws ecs create-cluster --cluster-name fargate-demo ecs-cli configure --cluster fargate-demo --region eu-west-1
    ```

3.  我们在 **CloudWatch** 中创建一个日志组，我们的容器将在其中写入它的输出。我们只需要做一次。下面是实现这一点的代码:

    ```
    %%sh aws logs create-log-group --log-group-name awslogs-tf-ecs
    ```

4.  我们将需要一个用于 Google 远程过程调用的`8500`(`8501`用于**代表性状态转移**(**REST**API)。如果你还没有，你可以在**弹性计算云** ( **EC2** )控制台中轻松创建一个。在这里，我在我默认的**虚拟私有云** ( **VPC** )中创建了一个。看起来是这样的:

![Figure 11.8 – Viewing the security group
](img/B17705_11_8.jpg)

图 11.8–查看安全组

### 定义任务

现在，我们需要编写一个包含**任务定义**的 **JSON** 文件:要使用的容器映像、它的入口点以及它的系统和网络属性。让我们开始吧，如下:

1.  首先我们定义任务允许消耗的**中央处理器** ( **CPU** )和内存的数量。与 ECS 和 EKS 不同，Fargate 只允许有限的一组值，可从[https://docs . AWS . Amazon . com/AmazonECS/latest/developer guide/task-CPU-memory-error . html](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-cpu-memory-error.html)获得。我们将使用 4 个**虚拟 CPU**(**vcpu**)和 8 GB 的**随机存取内存** ( **RAM** )，如下面的代码片段所示:

    ```
    {   "requiresCompatibilities": ["FARGATE"],   "family": "inference-fargate-tf-230",   "memory": "8192",   "cpu": "4096",
    ```

2.  接下来，我们定义一个容器来加载我们的模型并运行预测。我们将在 TensorFlow 2.3.0 中使用 DLC。你可以在[https://github . com/AWS/deep-learning-containers/blob/master/available _ images . MD](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)找到完整的列表。代码如下面的代码片段所示:

    ```
      "containerDefinitions": [{     "name": "dlc-tf-inference",     "image": "763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.3.2-cpu-py37-ubuntu18.04",     "essential": true,
    ```

3.  它的入口点创建一个目录，克隆我们推送模型的存储库，启动 TensorFlow 服务，如下:

    ```
        "command": [        "mkdir -p /test && cd /test && git clone https://gitlab.com/juliensimon/test-models.git && tensorflow_model_server --port=8500  --rest_api_port=8501 --model_name=1  --model_base_path=/test/test-models/model"     ],     "entryPoint": ["sh","-c"],
    ```

4.  相应地，我们映射两个 TensorFlow 服务端口，如下所示:

    ```
        "portMappings": [         {           "hostPort": 8500,           "protocol": "tcp",           "containerPort": 8500         },         {           "hostPort": 8501,           "protocol": "tcp",           "containerPort": 8501         }     ],
    ```

5.  我们定义指向我们之前创建的 CloudWatch 日志组的日志配置，如下所示:

    ```
        "logConfiguration": {       "logDriver": "awslogs",         "options": {           "awslogs-group": "awslogs-tf-ecs",           "awslogs-region": "eu-west-1",           "awslogs-stream-prefix": "inference"         }     }   }],
    ```

6.  我们为容器设置了网络模式，如下面的代码片段所示。`awsvpc`是最灵活的选项，它将允许我们的容器被公开访问，如[https://docs . AWS . Amazon . com/AmazonECS/latest/developer guide/task-networking . html](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html)中所解释的。它将在我们选择的子网

    ```
      "networkMode": "awsvpc"
    ```

    中创建一个**弹性网络接口**
7.  最后，我们为任务定义一个 IAM 角色。如果这是您第一次使用 ECS，您应该在 IAM 控制台中创建这个角色。您可以在 https://docs . AWS . Amazon . com/AmazonECS/latest/developer guide/task _ execution _ IAM _ role . html 中找到相关说明

### 运行任务

现在，我们准备使用我们之前创建的安全组和默认 VPC 中的一个子网来运行我们的任务。让我们开始吧，如下:

1.  我们用`run-task` API 启动任务，传递任务定义的族名(不是文件名！).请注意版本号，因为它会在您每次注册新版本的任务定义时自动增加，所以请确保您使用的是最新版本。代码如下面的代码片段所示:

    ```
    %%sh aws ecs run-task   --cluster fargate-demo    --task-definition inference-fargate-tf-230:1    --count 1   --launch-type FARGATE   --network-configuration      "awsvpcConfiguration={subnets=[$SUBNET_ID],      securityGroups=[$SECURITY_GROUP_ID],      assignPublicIp=ENABLED}"
    ```

2.  几秒钟后，我们可以看到我们的预测容器正在运行(显示任务**标识符** ( **ID** )、状态、端口和任务定义)，如下所示:

    ```
    %%sh ecs-cli ps --desired-status RUNNING a9c9a3a8-8b7c-4dbb-9ec4-d20686ba5aec/dlc-tf-inference   RUNNING   52.49.238.243:8500->8500/tcp,  52.49.238.243:8501->8501/tcp                         inference-fargate-tf230:1
    ```

3.  使用容器的公共**互联网协议** ( **IP** )地址，我们用 10 个样本图像构建一个 TensorFlow 服务预测请求，并发送给我们的容器，如下:

    ```
    import random, json, requests inference_task_ip = '52.49.238.243' inference_url = 'http://' +                    inference_task_ip +                   ':8501/v1/models/1:predict' indices = random.sample(range(x_val.shape[0] - 1), 10) images = x_val[indices]/255 labels = y_val[indices] data = images.reshape(num_samples, 28, 28, 1) data = json.dumps(     {"signature_name": "serving_default",       "instances": data.tolist()}) headers = {"content-type": "application/json"} json_response = requests.post(     inference_url,      data=data,      headers=headers) predictions = json.loads(     json_response.text)['predictions'] predictions = np.array(predictions).argmax(axis=1) print("Labels     : ", labels) print("Predictions: ", predictions) Labels     :  [9 8 8 8 0 8 9 7 1 1] Predictions:  [9 8 8 8 0 8 9 7 1 1]
    ```

4.  当我们完成时，我们使用任务`run-task`API停止任务并删除集群，如下面的代码片段所示。当然，你也可以使用 ECS 控制台:

    ```
    %%sh aws ecs stop-task --cluster fargate-demo \                   --task $TASK_ARN ecs-cli down --force --cluster fargate-demo
    ```

ECS 和 EKS 的流程极其相似。你可以在 https://gitlab.com/juliensimon/dlcontainers 找到简单的例子。如果您希望构建自己的工作流，它们应该是一个很好的起点。

Kubernetes 粉丝也可以使用`kubectl`来训练和部署模型。详细教程见[https://sagemaker . readthe docs . io/en/stable/workflows/kubernetes/index . html](https://sagemaker.readthedocs.io/en/stable/workflows/kubernetes/index.html)。

# 总结

在本章中，您学习了模型工件，它们包含什么，以及如何使用它们在 SageMaker 之外导出模型。您还学习了如何导入和部署现有模型，以及如何使用 SageMaker SDK 和`boto3` SDK 详细管理端点。

然后，我们讨论了 SageMaker 的替代部署场景，使用批处理转换或推理管道，以及在 SageMaker 外部使用容器服务。

最后，您学习了如何使用 SageMaker Model Monitor 来捕获端点数据和监控数据质量。

在下一章中，我们将讨论使用三种不同的 AWS 服务来自动化 ML 工作流: **AWS CloudFormation** 、**AWS Cloud Development Kit**(**AWS CDK**)和**Amazon SageMaker Pipelines**。