

# 八、模型监控和管理解决方案

在 [*第六章*](B18638_06.xhtml#_idTextAnchor132) 、 *SageMaker 训练和调试解决方案*和 [*第七章*](B18638_07.xhtml#_idTextAnchor151) 、 *SageMaker 部署解决方案*中，我们重点使用 **SageMaker** 训练和部署**机器学习** ( **ML** )模型。如果您能够完成这些章节中介绍的动手解决方案，您应该能够使用其他算法和数据集执行类似类型的实验和部署。这两章是很好的起点，尤其是在开始使用托管服务时。然而，在某些时候，您将不得不使用它的其他功能来管理、排除故障和监视生产 ML 环境中的不同类型的资源。

使用 SageMaker 的一个明显优势是，数据科学家和 ML 实践者的许多经常执行的任务已经作为这种完全托管服务的一部分实现了自动化。这意味着我们通常不需要构建一个定制的解决方案，尤其是如果 SageMaker 已经有了那个能力或者特性的话。这些功能的例子包括 **SageMaker 调试器**、 **SageMaker 特征库**、 **SageMaker 训练编译器**、 **SageMaker 推理推荐器**、 **SageMaker 澄清**、 **SageMaker 处理**等等！如果我们需要使用这些功能中的一个或多个，我们需要做的就是使用 **boto3** 以及 **SageMaker Python SDK** ，运行几行代码，在几个小时(甚至几分钟)内获得想要的功能和结果！).

在本章中，我们将重点介绍如何使用 SageMaker 内置的**模型注册表**，我们将使用它来注册和管理经过训练的 ML 模型。我们还将展示如何将模型从模型注册中心部署到 ML 推理端点的快速演示。除了模型注册之外，我们将使用 **SageMaker 模型监视器**，这是另一个内置功能，我们将使用它来捕获和分析通过 ML 推理端点的数据。

在本章中，我们将讨论以下主题:

*   将模型注册到 SageMaker 模型注册中心
*   从 SageMaker 模型注册中心部署模型
*   启用数据捕获和模拟预测
*   使用 SageMaker 型号监视器进行定期监控
*   分析捕获的数据
*   使用监控计划删除端点
*   清理

一旦您完成了本章中的实践解决方案，您将更容易理解、使用和配置 SageMaker 的其他内置特性。考虑到这一点，我们开始吧！

# 技术先决条件

开始之前，我们必须准备好以下内容:

*   网络浏览器(最好是 Chrome 或 Firefox)
*   访问 AWS 账户和本书第一章使用的 **SageMaker Studio** 域名

Jupyter 笔记本、源代码和其他用于每章的文件都可以在本书的 GitHub 资源库中找到:[https://GitHub . com/packt publishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS)。

重要说明

运行本书中的示例时，建议使用具有有限权限的 IAM 用户，而不是 root 帐户。我们将在 [*第 9 章*](B18638_09.xhtml#_idTextAnchor187) 、*安全、治理和遵从策略*中详细讨论这一点以及其他安全最佳实践。如果您刚刚开始使用 AWS，您可以同时继续使用 root 帐户。

# 将模型注册到 SageMaker 模型注册中心

在 [*第六章*](B18638_06.xhtml#_idTextAnchor132) ， *SageMaker 训练和调试解决方案*中，我们使用`Estimator`实例的`deploy()`方法，在使用`fit()`方法训练模型之后，立即将我们的 ML 模型部署到推理端点。当在生产中执行 ML 实验和部署时，在进行部署步骤之前，可能必须首先对模型进行分析和评估。执行分析的个人或团队将审查输入配置参数、训练数据和用于训练模型的算法，以及其他可用的相关信息。一旦数据科学团队不得不处理多个模型，使用**模型注册中心**管理和组织所有这些模型将变得更加容易。

什么是模型注册中心？模型注册中心是一个简单的存储库，致力于帮助数据科学家和 ML 实践者管理、组织和编目 ML 模型。在训练步骤之后，数据科学团队可以将训练好的 ML 模型存储在模型注册表中，并将其状态标记为*以供审查*或*待批准*。这将允许评审团队轻松定位要评审的模型，以及与这些模型相关联的历史和信息:

![Figure 8.1 – Working with a model registry

](img/B18638_08_001.jpg)

图 8.1–使用模型注册中心

一旦评审团队完成了评审过程并批准了模型的部署，该模型的状态现在可以更改为*已批准*，类似于上图所示。一旦 ML 模型的状态变为*已批准*，就可以使用 **MLOps 管道**手动甚至自动部署。除此之外，还可以触发其他自动操作，如自动报告和通知。

注意

有关 MLOps 管道的更多信息，请随时查看第 10 章*亚马逊 EKS 上 Kubeflow 的*和 [*第 11 章*](B18638_11.xhtml#_idTextAnchor231)*机器学习管道和 SageMaker 管道*。

既然您对数据科学团队如何使用模型注册中心来简化他们的工作有了更好的了解，那么您可能已经计划从头开始编写模型注册中心了！停在那里——sage maker 已经为我们提供了一个！在本章的的后续页面中，我们将使用**bot O3**库 和 **SageMaker Python SDK** 来利用 SageMaker 中可用的模型注册表。

## 在 SageMaker Studio 中创建新笔记本

我们将通过打开 SageMaker Studio 并在新目录下创建一个新的 Jupyter 笔记本来开始本节的动手部分。

注意

在继续之前，请确保您已经完成了*第 1 章*、*AWS 上的 ML 工程介绍*的*sage maker 和 SageMaker Studio* 部分中的动手解决方案。请注意，本章中的实际操作部分是*而不是*我们在 [*第 6 章*](B18638_06.xhtml#_idTextAnchor132) 、 *SageMaker 训练和调试解决方案*和 [*第 7 章*](B18638_07.xhtml#_idTextAnchor151) 、 *SageMaker 部署解决方案*中完成的内容的延续。

按照以下步骤启动 SageMaker Studio，然后创建一个新的笔记本，用于运行本章中的 Python 脚本:

1.  导航到 AWS 管理控制台搜索栏中的`sagemaker studio`，并从**功能**下的结果列表中选择 **SageMaker Studio** 。

重要说明

本章假设我们在使用服务管理和创建不同类型的资源时使用了`us-west-2`区域。您可以使用不同的地区，但请确保进行必要的调整，以防某些资源需要转移到您选择的地区。

1.  接下来，点击侧边栏中 **SageMaker 域**下的 **Studio** 。
2.  点击**启动 app** ，如下图截图所示。从下拉选项的列表中选择**工作室**；

![Figure 8.2 – Opening SageMaker Studio

](img/B18638_08_002.jpg)

图 8.2–打开 SageMaker 工作室

这将把你重定向到 SageMaker 工作室。等待几秒钟，让界面加载。

1.  右键单击**文件浏览器**侧栏窗格中的空白区域，打开类似如下的上下文菜单:

![Figure 8.3 – Creating a new folder

](img/B18638_08_003.jpg)

图 8.3–创建新文件夹

选择`CH08`。

1.  双击侧边栏中相应的文件夹名称，导航到 **CH08** 目录。
2.  点击**文件**菜单，从**新建**子菜单下的选项列表中选择**笔记本**，新建一个笔记本:

![Figure 8.4 – Creating a new Notebook

](img/B18638_08_004.jpg)

图 8.4–创建新笔记本

在前面的截图中，我们还可以看到其他选项，包括创建一个新的`.ipynb`笔记本文件，它将用于运行不同的代码块。

1.  在`Data Science`(在 **SageMaker 图像**下找到的选项)
2.  `Python 3`
3.  `No script`
4.  之后点击**选择**按钮。

注意

等待内核启动。在配置 ML 实例以运行 Jupyter 笔记本单元时，此步骤可能需要大约 3 到 5 分钟。

1.  右键单击选项卡名称上的，如下图中突出显示的:

![Figure 8.5 – Renaming a notebook

](img/B18638_08_005.jpg)

图 8.5–重命名笔记本

从上下文菜单的选项列表中选择**重命名笔记本…** 。

1.  在`01 - Registering Models to the SageMaker Model Registry.ipynb`下**新名称**。之后点击**重命名**按钮。

现在我们的笔记本已经准备好了，我们可以继续向 SageMaker 模型注册中心注册预训练模型了！

## 使用 boto3 库将模型注册到 SageMaker 模型注册表

在这个部分，我们将使用存储在`.tar.gz`文件中的两个预训练模型。我们将在`.tar.gz`文件中存储和注册这些模型，这些文件是通过使用 SageMaker 的**K-最近邻**和**线性学习器**内置算法执行两个单独的 ML 训练作业生成的。这些模型接受 *x* 和 *y* 值作为输入，并返回一个预测的*标签*值作为输出。这些 *x* 和 *y* 值代表什么？让我们来看看:

![Figure 8.6 – Predicting the preferred vaccination site

](img/B18638_08_006.jpg)

图 8.6-预测首选疫苗接种地点

如前面的屏幕截图所示，这些 *x* 和 *y* 值对应于使用地图中的指定点作为参考的人口中某些成员居住的经过变换和缩放的坐标值。在第一轮疫苗接种中，这些成员中有几个选择了他们喜欢的疫苗接种地点。这些疫苗接种点标有适当的*标签*值-*0*、 *1* 和 *2* 。使用以前的疫苗接种地点数据作为我们的训练数据，我们能够生成两个模型，在给定一组坐标值的情况下，这两个模型可以自动预测未接种成员的首选疫苗接种地点——即 *x* 和 *y* 。

按照以下步骤下载提到的两个预训练模型的工件，并在我们在上一节准备的`01 - Registering Models to the SageMaker Model Registry.ipynb`笔记本中的 SageMaker 模型注册表中注册这些工件:

1.  我们将通过使用`wget`命令:

    ```py
    %%bash
    ```

    ```py
    mkdir -p tmp
    ```

    ```py
    wget -O tmp/knn.model.tar.gz https://bit.ly/3yZ6qHE
    ```

    ```py
    wget -O tmp/ll.model.tar.gz https://bit.ly/3ahj1fd
    ```

    将预先训练好的模型工件下载到`tmp`目录来启动

在这里，我们下载了两个`.tar.gz`文件:

*   `knn.model.tar.gz`:包含预训练 **K 近邻**模型的模型工件
*   `ll.model.tar.gz`:包含预训练**线性学习者**模型的模型工件

1.  指定唯一的 S3 时段名称和前缀。确保在运行下面的代码块之前，用一个惟一的 S3 存储桶名替换了`<INSERT S3 BUCKET HERE>`的值:

    ```py
    s3_bucket = "<INSERT S3 BUCKET HERE>"
    ```

    ```py
    prefix = "chapter08"
    ```

确保您为还不存在*和*的 S3 存储桶指定了存储桶名称。如果你想重复使用你在前面章节中创建的一个桶，你可以这样做，但是确保在设置和配置 **SageMaker Studio** 的同一区域使用 S3 桶。

1.  让我们创建一个 S3 桶，在这里我们将上传我们之前下载的`ll.model.tar.gz`和`knn.model.tar.gz`文件:

    ```py
    !aws s3 mb s3://{s3_bucket}
    ```

如果您打算重复使用您在前面章节中创建的现有 S3 铲斗，您可以跳过这一步。

1.  现在我们的 S3 存储桶已经准备好了，让我们准备 S3 路径，以便它们指向我们将要上传预先训练好的模型工件的地方:

    ```py
    ll_model_data = \
    ```

    ```py
    f's3://{s3_bucket}/{prefix}/models/ll.model.tar.gz'
    ```

    ```py
    knn_model_data = \
    ```

    ```py
    f's3://{s3_bucket}/{prefix}/models/knn.model.tar.gz'
    ```

注意，此时，`ll.model.tar.gz`和`knn.model.tar.gz`文件还不存在于`ll_model_data`和`knn_model_data`变量中存储的指定 S3 路径中。在这里，我们简单地准备了 S3 位置路径(字符串)来上传`.tar.gz`文件。

1.  现在，让使用`aws s3 cp`命令将`.tar.gz`文件复制并上传到它们对应的 S3 位置:

    ```py
    !aws s3 cp tmp/ll.model.tar.gz {ll_model_data}
    ```

    ```py
    !aws s3 cp tmp/knn.model.tar.gz {knn_model_data}
    ```

这将把`ll.model.tar.gz`和`knn.model.tar.gz`文件从`tmp`目录上传到 S3 存储桶。

1.  预训练的模型工件已经在 S3，让我们继续获取用于训练这些模型的 ML 算法的 ECR 容器图像 URI。我们将使用`retrieve()`函数为**线性学习器**和**K-最近邻**算法:

    ```py
    from sagemaker.image_uris import retrieve
    ```

    ```py
    ll_image_uri = retrieve(
    ```

    ```py
        "linear-learner", 
    ```

    ```py
        region="us-west-2", 
    ```

    ```py
        version="1"
    ```

    ```py
    )
    ```

    ```py
    knn_image_uri = retrieve(
    ```

    ```py
        "knn", 
    ```

    ```py
        region="us-west-2", 
    ```

    ```py
        version="1"
    ```

    ```py
    )
    ```

2.  为 SageMaker 初始化`boto3`客户端。我们将使用这个客户端来调用几个 SageMaker APIs，这将帮助我们在后续的步骤中创建模型包和模型包组:

    ```py
    import boto3
    ```

    ```py
    client = boto3.client(service_name="sagemaker")
    ```

3.  接下来，定义`generate_random_string()`功能:

    ```py
    import string 
    ```

    ```py
    import random
    ```

    ```py
    def generate_random_string():
    ```

    ```py
        return ''.join(
    ```

    ```py
            random.sample(
    ```

    ```py
            string.ascii_uppercase,12)
    ```

    ```py
        )
    ```

*这是干什么用的？*我们将在创建新资源时使用`generate_random_string()`功能(在后续步骤中)。这将帮助我们为将要创建的每个资源生成一个随机标识符或标签。

1.  准备好`generate_random_string()`函数后，让我们生成一个随机的`group_id`值。这将用于生成一个*包组名称* ( `package_group_name`)和一个*包组描述* ( `package_group_desc`)。然后，我们将使用 boto3 客户端的`create_model_package_group()`方法

    ```py
    group_id = generate_random_string()
    ```

    ```py
    package_group_name = f"group-{group_id}"
    ```

    ```py
    package_group_desc = f"Model package group {group_id}"
    ```

    ```py
        ModelPackageGroupName=package_group_name,
    ```

    ```py
        ModelPackageGroupDescription=package_group_desc
    ```

    ```py
    )
    ```

    ```py
    package_group_arn = response['ModelPackageGroupArn']
    ```

    ```py
    package_group_arn
    ```

    创建模型包组
2.  接下来，让我们定义`prepare_inference_specs()`函数，我们将在下一步使用它来配置和设置我们的模型包:

    ```py
    def prepare_inference_specs(image_uri, model_data):
    ```

    ```py
        return {
    ```

    ```py
            "Containers": [
    ```

    ```py
                {
    ```

    ```py
                    "Image": image_uri,
    ```

    ```py
                    "ModelDataUrl": model_data
    ```

    ```py
                }
    ```

    ```py
            ],
    ```

    ```py
            "SupportedContentTypes": [ 
    ```

    ```py
                "text/csv" 
    ```

    ```py
            ],
    ```

    ```py
            "SupportedResponseMIMETypes": [ 
    ```

    ```py
                "application/json" 
    ```

    ```py
            ],
    ```

    ```py
        }
    ```

这里，我们创建了一个函数，使用*ECR 容器图像 URI* 和*模型工件 S3 路径*作为输入参数，准备并返回必要的嵌套配置结构。

1.  接下来，让我们定义一个名为`create_model_package()`的自定义函数。该功能接受几个输入参数值，如下所示:
    *   模型包组的**亚马逊资源名称** ( **ARN** )
    **   *推理规范配置**   (可选)`boto3`客户端为 SageMaker:

        ```py
        def create_model_package(
        ```

        ```py
                package_group_arn, 
        ```

        ```py
                inference_specs, 
        ```

        ```py
                client=client):
        ```

        ```py
            input_dict = {
        ```

        ```py
                "ModelPackageGroupName" : package_group_arn,
        ```

        ```py
                "ModelPackageDescription" : "Description",
        ```

        ```py
                "ModelApprovalStatus" : "Approved",
        ```

        ```py
                "InferenceSpecification" : inference_specs
        ```

        ```py
            }
        ```

        ```py
            response = client.create_model_package(
        ```

        ```py
                **input_dict
        ```

        ```py
            )
        ```

        ```py
            return response["ModelPackageArn"]
        ``` 

 *这里，我们在创建模型包时自动将的`ModelApprovalStatus`值设置为`Approved`。请注意，我们可以选择在将其转换到`Approved`之前，先将值设置为`PendingManualApproval`。但是，我们将事情简化一点，直接将值设置为`Approved`。

注意

模型的批准状态可以用来标记和识别哪些模型可以部署到生产端点。理想情况下，ML 模型在部署之前首先经过评估和人工批准。如果模型通过评估步骤，我们可以将批准状态设置为`Approved`。否则，我们将状态设置为`Rejected`。

1.  使用`prepare_inference_specs()`函数为 **K 近邻**和**线性学习器**模型包:

    ```py
    knn_inference_specs = prepare_inference_specs(
    ```

    ```py
        image_uri=knn_image_uri,
    ```

    ```py
        model_data=knn_model_data
    ```

    ```py
    )
    ```

    ```py
    ll_inference_specs = prepare_inference_specs(
    ```

    ```py
        image_uri=ll_image_uri,
    ```

    ```py
        model_data=ll_model_data
    ```

    ```py
    )
    ```

    准备前提推理规范配置
2.  有了推论规格配置准备好了，让我们用`create_model_package()`来创建模型包:

    ```py
    knn_package_arn = create_model_package(
    ```

    ```py
        package_group_arn=package_group_arn,
    ```

    ```py
        inference_specs=knn_inference_specs
    ```

    ```py
    )
    ```

    ```py
    ll_package_arn = create_model_package(
    ```

    ```py
        package_group_arn=package_group_arn,
    ```

    ```py
        inference_specs=ll_inference_specs
    ```

    ```py
    )
    ```

3.  最后，让我们使用 IPython 的`%store`魔法来存储`knn_package_arn`、`ll_package_arn`、`s3_bucket`和`prefix` :

    ```py
    %store knn_package_arn
    ```

    、

    ```py
    %store ll_package_arn
    ```

    、

    ```py
    %store s3_bucket
    ```

    、

    ```py
    %store prefix
    ```

    的变量值

我们将在本章的后续章节中使用这些存储的变量值。

此时，两个模型包已经创建完毕，可以使用了。

注意

您可以使用`client.list_model_package_groups()`和`client.list_model_packages(ModelPackageGroupName='<INSERT GROUP NAME>')`查看已注册的模型包组和模型包列表。我们将把这个留给您作为练习！

# 从 SageMaker 模型注册中心部署模型

在一个 ML 模型被注册到一个模型注册中心之后，有许多可能的后续步骤可用。在本节中，我们将集中于将第一个注册的 ML 模型(预训练的**K-最近邻**模型)手动部署到新的推理端点。在部署了第一个注册的 ML 模型之后，我们将继续在已经部署了第一个 ML 模型的相同端点中部署第二个注册的模型(预训练的**线性学习器**模型)，类似于下图所示:

![Figure 8.7 – Deploying models from the model registry

](img/B18638_08_007.jpg)

图 8.7–从模型注册中心部署模型

在这里，我们可以看到我们可以直接替换正在运行的 ML 推断端点内的已部署 ML 模型，而无需创建新的单独的推断端点。这意味着我们不需要担心在我们的设置中改变“目标基础结构服务器”,因为模型替换操作是在幕后进行的。同时，SageMaker 已经为我们自动化了这个过程，所以我们需要做的就是调用正确的 API 来启动这个过程。

这里，我们将继续我们在*将模型注册到 SageMaker 模型注册表*部分中停止的地方，并将两个注册的模型部署到一个 ML 推理端点。也就是说，我们将执行以下一组步骤:

1.  点击**文件**菜单，从**新建**子菜单下的选项列表中选择**笔记本**，创建一个新笔记本。

注意

请注意，我们将在`CH08`目录中创建新的笔记本，放在我们在上一节中使用的`01 - Registering Models to the SageMaker Model Registry.ipynb`笔记本文件旁边。

1.  在`Data Science`(在 **SageMaker 图像**下找到的选项)
2.  `Python 3`
3.  `No script`

之后点击**选择**按钮。

1.  右键点击新笔记本的标签名称，选择**新名称**下的`02 - Deploying Models from the SageMaker Model Registry.ipynb`。点击**重命名**按钮。
2.  既然我们已经准备好了新笔记本，让我们继续使用 IPython:

    ```py
    %store -r knn_package_arn
    ```

    ```py
    %store -r ll_package_arn
    ```

    的`%store`魔法加载`knn_package_arn`和`ll_package_arn`的存储变量的值
3.  让我们使用下面的代码块初始化一个`ModelPackage`实例:

    ```py
    import sagemaker
    ```

    ```py
    from sagemaker import get_execution_role
    ```

    ```py
    from sagemaker import ModelPackage
    ```

    ```py
    from sagemaker.predictor import Predictor
    ```

    ```py
    session = sagemaker.Session()
    ```

    ```py
    role = get_execution_role()
    ```

    ```py
    model = ModelPackage(
    ```

    ```py
        role=role,
    ```

    ```py
        model_package_arn=knn_package_arn,
    ```

    ```py
        sagemaker_session=session
    ```

    ```py
    )
    ```

    ```py
    model.predictor_cls = Predictor
    ```

这里我们传递了 *IAM 执行角色*、*K-近邻模型包 ARN* 以及初始化`ModelPackage`实例时的`Session`实例。

1.  既然我们已经初始化了`ModelPackage`实例，我们将调用它的`deploy()`方法将预训练的模型部署到一个实时推理端点:

    ```py
    from sagemaker.serializers import JSONSerializer
    ```

    ```py
    from sagemaker.deserializers import JSONDeserializer
    ```

    ```py
    predictor = model.deploy(
    ```

    ```py
        instance_type='ml.m5.xlarge', 
    ```

    ```py
        initial_instance_count=1,
    ```

    ```py
        serializer=JSONSerializer(),
    ```

    ```py
        deserializer=JSONDeserializer()
    ```

    ```py
    )
    ```

由于我们在上一步中将`predictor_class`属性设置为`Predictor`，因此`deploy()`方法将返回一个`Predictor`实例，而不是`None`。

注意

完成模型部署大约需要 5 到 10 分钟。请随意喝杯咖啡或茶！

1.  一旦我们的 ML 推断端点准备就绪，我们将使用`Predictor`实例的`predict()`方法执行一个样本预测来测试我们的设置:

    ```py
    payload = {
    ```

    ```py
        'instances': [
    ```

    ```py
            {
    ```

    ```py
              "features": [ 1.5, 2 ]
    ```

    ```py
            },
    ```

    ```py
        ]
    ```

    ```py
    }
    ```

    ```py
    predictor.predict(data=payload)
    ```

该应产生等于或类似于`{'predictions': [{'predicted_label': 2.0}]}`的输出值。

1.  接下来，我们来定义一下`process_prediction_result()`功能:

    ```py
    def process_prediction_result(raw_result):
    ```

    ```py
        first = raw_result['predictions'][0]
    ```

    ```py
        return first['predicted_label']
    ```

这将从`Predictor`实例的`predict()`方法返回的嵌套结构中提取`label`值。当然，函数中的代码假设我们在调用`predict()`方法时一次只传递一个有效载荷。

1.  让我们定义一个自定义的`predict()`函数，它接受输入的`x`和`y`值，以及一个可选的`Predictor`实例参数值:

    ```py
    def predict(x, y, predictor=predictor):
    ```

    ```py
        payload = {
    ```

    ```py
            'instances': [
    ```

    ```py
                {
    ```

    ```py
                  "features": [ x, y ]
    ```

    ```py
                },
    ```

    ```py
            ]
    ```

    ```py
        }
    ```

    ```py
        raw_result = predictor.predict(
    ```

    ```py
            data=payload
    ```

    ```py
        )
    ```

    ```py
        return process_prediction_result(raw_result)
    ```

2.  让我们使用`x`和`y` :

    ```py
    predict(x=3, y=4)
    ```

    的一组样本值来测试我们的自定义`predict()`函数

这将返回等于或类似于`1.0`的预测`label`值。我们如何解释这个结果？居住在用指定输入值`x`和`y`表示的位置的顾客可能会去标记有标签`1`的接种点(即第二个接种点)。

注意

随意修改`process_prediction_result()`函数，将得到的预测`label`值的类型转换成一个*整数*而不是一个*浮点数*。

1.  接下来，我们来定义一下`test_different_values()`的功能:

    ```py
    from time import sleep
    ```

    ```py
    def test_different_values(predictor=predictor):
    ```

    ```py
        for x in range(-3, 3+1):
    ```

    ```py
            for y in range(-3, 3+1):
    ```

    ```py
                label = predict(
    ```

    ```py
                            x=x, 
    ```

    ```py
                            y=y, 
    ```

    ```py
                            predictor=predictor
    ```

    ```py
                        )
    ```

    ```py
                print(f"x={x}, y={y}, label={label}")
    ```

    ```py
                sleep(0.2)
    ```

这里，我们只是使用 *x* 和 *y* 的不同值组合多次调用我们的自定义`predict()`函数(每次预测请求之间有 200 毫秒的延迟)。

1.  在继续之前，让我们检查一下我们的`test_different_values()`功能是否按预期工作:

    ```py
    test_different_values()
    ```

这将向我们显示给定不同组合的 *x* 和 *y* 的预测`label`值。

1.  接下来，让我们定义一个定制的`create_model()`函数，该函数利用 boto3 客户端的`create_model()`方法与 SageMaker API 一起工作:

    ```py
    import boto3
    ```

    ```py
    client = boto3.client(service_name="sagemaker")
    ```

    ```py
    def create_model(model_package_arn, 
    ```

    ```py
                     model_name, 
    ```

    ```py
                     role=role, 
    ```

    ```py
                     client=client):
    ```

    ```py
        container_list = [
    ```

    ```py
            {'ModelPackageName': model_package_arn}
    ```

    ```py
        ]
    ```

    ```py
        response = client.create_model(
    ```

    ```py
            ModelName = model_name,
    ```

    ```py
            ExecutionRoleArn = role,
    ```

    ```py
            Containers = container_list
    ```

    ```py
        )
    ```

    ```py
        return response["ModelArn"]
    ```

2.  让我们定义`generate_random_string()`函数，我们将使用它来生成一个随机的模型名称。之后，我们将调用前面定义的自定义`create_model()`函数 we ，传递我们**线性学习器**模型的模型包 ARN 以及生成的模型名:

    ```py
    import string 
    ```

    ```py
    import random
    ```

    ```py
    def generate_random_string():
    ```

    ```py
        return ''.join(
    ```

    ```py
            random.sample(
    ```

    ```py
            string.ascii_uppercase,12)
    ```

    ```py
        )
    ```

    ```py
    model_name = f"ll-{generate_random_string()}"
    ```

    ```py
    model_arn = create_model(
    ```

    ```py
        model_package_arn=ll_package_arn,
    ```

    ```py
        model_name=model_name
    ```

    ```py
    )
    ```

3.  接下来，我们来定义一下`create_endpoint_config()`的功能:

    ```py
    def create_endpoint_config(
    ```

    ```py
            model_name, 
    ```

    ```py
            config_name, 
    ```

    ```py
            client=client):
    ```

    ```py
        response = client.create_endpoint_config(
    ```

    ```py
            EndpointConfigName = config_name,
    ```

    ```py
            ProductionVariants=[{
    ```

    ```py
                'InstanceType': "ml.m5.xlarge",
    ```

    ```py
                'InitialInstanceCount': 1,
    ```

    ```py
                'InitialVariantWeight': 1,
    ```

    ```py
                'ModelName': model_name,
    ```

    ```py
                'VariantName': 'AllTraffic'
    ```

    ```py
            }]
    ```

    ```py
        )
    ```

    ```py
        return response["EndpointConfigArn"]
    ```

这个函数简单地说就是利用 SageMaker 的 boto3 客户端的`create_endpoint_config()`方法来准备所需的端点配置。

1.  使用我们在上一步中定义的`create_endpoint_config()`函数，让我们创建一个 SageMaker ML 推断端点配置:

    ```py
    config_name = f"config-{generate_random_string()}"
    ```

    ```py
    config_arn = create_endpoint_config(
    ```

    ```py
        model_name=model_name,
    ```

    ```py
        config_name=config_name
    ```

    ```py
    )
    ```

2.  现在，让我们使用`update_endpoint()`方法更新端点配置:

    ```py
    response = client.update_endpoint(
    ```

    ```py
        EndpointName=predictor.endpoint_name,
    ```

    ```py
        EndpointConfigName=config_name
    ```

    ```py
    )
    ```

这里，我们使用了在上一步中创建的端点配置。

重要说明

这里会发生什么？一旦我们调用了`update_endpoint()`方法，SageMaker 将在后台执行所需的步骤来更新端点，并用最新端点配置中指定的新模型(**线性学习器**)替换旧的已部署模型( **K 近邻**)。注意，这只是我们可以使用 **SageMaker Python SDK** 和 **boto3** 库实现的可能解决方案之一。其他可能的部署解决方案包括**多模型端点**、 **A/B 测试**端点设置、使用**推理管道模型**的端点等等！我们不会深入研究这些其他变体和解决方案，所以请随意查看《亚马逊 SageMaker 烹饪书中的*机器学习中的部署方法。*

1.  在继续下一组步骤之前，让我们使用下面的代码块等待 5 分钟:

    ```py
    print('Wait for update operation to complete')
    ```

    ```py
    sleep(60*5)
    ```

这里，我们使用了`sleep()`函数，它接受一个输入值，该值等于我们希望代码等待或休眠的秒数。

注意

我们使用`sleep()`函数等待 5 分钟，以确保更新端点操作已经完成(假设大约需要 5 分钟或更少的时间来完成)。

1.  初始化一个`Predictor`对象，并将其附加到我们在本节前面准备的现有 ML 推理端点:

    ```py
    predictor = Predictor(
    ```

    ```py
        endpoint_name=predictor.endpoint_name,
    ```

    ```py
        sagemaker_session=session,
    ```

    ```py
        serializer=JSONSerializer(),
    ```

    ```py
        deserializer=JSONDeserializer()
    ```

    ```py
    )
    ```

2.  让我们通过使用样本有效载荷进行预测来测试我们的设置:

    ```py
    payload = {
    ```

    ```py
        'instances': [
    ```

    ```py
            {
    ```

    ```py
              "features": [ 1.5, 2 ]
    ```

    ```py
            },
    ```

    ```py
        ]
    ```

    ```py
    }
    ```

    ```py
    predictor.predict(data=payload)
    ```

这将产生一个结构类似于 `{'predictions': [{'score': [0.04544410854578018, 0.3947080075740814, 0.5598478317260742], 'predicted_label': 2}]}`的输出值。

我们如何解释这个结果？居住在用指定的输入`x`和`y`值(即 *x* = `1.5`和 *y* = `2`)表示的位置的客户有以下概率:

*   `4.5%`去第一个接种点的概率(标签= 0)
*   `39.5%`前往第二个接种点的概率(标签= 1)
*   `56%`前往第三个接种点的概率(标签= 2)

假设第三个接种点具有最高的概率值，模型将`predicted_label`值设置为`2`(假设计数从 0 开始)。

注意

请注意，部署的**线性学习器**模型返回了每个类别的*概率分数，以及*预测标签*，而我们在本节开始时部署的 **k 近邻**模型仅返回了*预测标签*。当用来自不同实例家族的模型替换已部署的模型时，我们需要小心(这可能需要使用不同的算法容器映像进行推断)，因为新的模型可能涉及一组不同的输入和输出结构和值。*

1.  与我们之前在托管我们的**K-最近邻**模型的 ML 推断端点上执行的类似，我们将使用不同的值 *x* 和 *y* :

    ```py
    test_different_values(predictor=predictor)
    ```

    来执行多个样本预测
2.  使用`%store`魔法为`endpoint_name` :

    ```py
    endpoint_name = predictor.endpoint_name
    ```

    ```py
    %store endpoint_name
    ```

    存储变量值

如果你想知道为什么我们还没有删除 ML 推断端点…我们将重用这个端点并在下一节用它来演示如何使用 SageMaker 的模型监控功能和特性！

# 实现数据捕捉和模拟预测

在一个 ML 模型被部署到一个推理端点之后，它的质量需要被监控和检查，这样我们就可以在检测到质量问题或偏差时很容易地执行纠正措施。这类似于 web 应用开发，即使质量保证团队已经花了几天(或几周)来测试应用的最终版本，仍然有其他问题只有在 web 应用已经运行时才能检测到:

![Figure 8.8 – Capturing the request and response data of the ML inference endpoint

](img/B18638_08_008.jpg)

图 8.8–捕获 ML 推理端点的请求和响应数据

如上图所示，模型监控从捕获请求和响应数据开始，这些数据通过一个正在运行的 ML 推断端点。收集的数据将在后续步骤中使用单独的自动化任务或作业进行处理和分析，该任务或作业可以生成报告并标记问题或异常。如果我们在定制的 web 应用端点中部署 ML 模型，我们可能需要自己构建这个数据捕获和模型监控设置。然而，如果我们使用 SageMaker，我们不需要从头开始编写任何代码，因为我们可以利用内置的模型监控功能，只需要启用和配置即可。

注意

在我们的“首选疫苗接种位置预测”示例中，捕获的数据(理想情况下)包括输入值( *x* 和 *y* 值)和输出值(预测的*标签*值)。

按照以下步骤在运行的 ML 推理端点中启用数据捕获，并使用随机生成的有效负载值模拟推理请求:

1.  点击**文件**菜单，从**新建**子菜单下的选项列表中选择**笔记本**，创建一个新笔记本。

注意

请注意，我们将在本章前面章节中使用的`01 - Registering Models to the SageMaker Model Registry.ipynb`和`02 - Deploying Models from the SageMaker Model Registry.ipynb`笔记本文件旁边的`CH08`目录中创建新笔记本。

1.  在`Data Science`(在 **SageMaker 图像**下找到的选项)
2.  `Python 3`
3.  `No script`

之后点击**选择**按钮。

1.  右键点击新笔记本的标签名称，选择**新名称**下的`03 - Enabling Data Capture and Simulating Predictions.ipynb`。点击**重命名**按钮。
2.  现在我们已经准备好了新的笔记本，让我们使用 IPython 的`%store`魔法来加载`s3_bucket`、`prefix`、`ll_package_arn`和`endpoint_name` :

    ```py
    %store -r s3_bucket
    ```

    、

    ```py
    %store -r prefix
    ```

    、

    ```py
    %store -r ll_package_arn
    ```

    、

    ```py
    %store -r endpoint_name
    ```

    的存储变量的值
3.  初始化一个`Predictor`对象并将其附加到我们在本章

    ```py
    import sagemaker
    ```

    ```py
    from sagemaker import get_execution_role
    ```

    ```py
    from sagemaker.predictor import Predictor
    ```

    ```py
    from sagemaker.serializers import CSVSerializer
    ```

    ```py
    from sagemaker.deserializers import CSVDeserializer
    ```

    ```py
    session = sagemaker.Session()
    ```

    ```py
    role = get_execution_role()
    ```

    ```py
    predictor = Predictor(
    ```

    ```py
        endpoint_name=endpoint_name,
    ```

    ```py
        sagemaker_session=session,
    ```

    ```py
        role=role,
    ```

    ```py
        serializer=CSVSerializer(),
    ```

    ```py
        deserializer=CSVDeserializer()
    ```

    ```py
    )
    ```

    的*从 SageMaker 模型注册表*部署模型中准备的
4.  接下来，让我们使用下面的代码块准备并初始化`DataCaptureConfig`实例:

    ```py
    from sagemaker.model_monitor import DataCaptureConfig
    ```

    ```py
    base = f"s3://{s3_bucket}/{prefix}"
    ```

    ```py
    capture_upload_path = f"{base}/data-capture"
    ```

    ```py
    capture_config_dict = {
    ```

    ```py
        'enable_capture': True,
    ```

    ```py
        'sampling_percentage': 100,
    ```

    ```py
        'destination_s3_uri': capture_upload_path,
    ```

    ```py
        'kms_key_id': None,
    ```

    ```py
        'capture_options': ["REQUEST", "RESPONSE"],
    ```

    ```py
        'csv_content_types': ["text/csv"],
    ```

    ```py
        'json_content_types': ["application/json"]
    ```

    ```py
    }
    ```

    ```py
    data_capture_config = DataCaptureConfig(
    ```

    ```py
        **capture_config_dict
    ```

    ```py
    )
    ```

这里，我们为指定了一个`100`的`sampling_percentage`值，这意味着所有的数据都将被捕获。我们还通过`capture_options`配置值指定，我们计划捕获通过 ML 推理端点的请求和响应数据。

1.  现在我们的配置已经准备好了，让我们调用`Predictor`实例的`update_data_capture_config()`方法:

    ```py
    %%time
    ```

    ```py
    predictor.update_data_capture_config(
    ```

    ```py
        data_capture_config=data_capture_config
    ```

    ```py
    )
    ```

注意

这大约需要 5 到 15 分钟才能完成。请随意喝杯咖啡或茶！

1.  使用`%store`魔术到为`capture_upload_path` :

    ```py
    %store capture_upload_path
    ```

    存储变量值
2.  定义`generate_random_payload()`功能:

    ```py
    import random
    ```

    ```py
    def generate_random_payload():
    ```

    ```py
        x = random.randint(-5,5)
    ```

    ```py
        y = random.randint(-5,5)
    ```

    ```py
        return f"{x},{y}"
    ```

3.  定义`perform_good_input()`和`perform_bad_input()`功能:

    ```py
    def perform_good_input(predictor):
    ```

    ```py
        print("> PERFORM REQUEST WITH GOOD INPUT")
    ```

    ```py
        payload = generate_random_payload()
    ```

    ```py
        result = predictor.predict(data=payload)
    ```

    ```py
        print(result)
    ```

    ```py
    def perform_bad_input(predictor):
    ```

    ```py
        print("> PERFORM REQUEST WITH BAD INPUT")
    ```

    ```py
        payload = generate_random_payload() + ".50"
    ```

    ```py
        result = predictor.predict(data=payload)
    ```

    ```py
        print(result)
    ```

重要说明

在这一点上，你可能想知道为什么我们把 *y* 输入有效载荷的浮点值视为*错误输入*。请注意，这只是出于演示目的，因为我们计划配置 **SageMaker 模型监视器**以便在配置*使用 SageMaker 模型监视器*部分中的约束时，将 *x* 和 *y* 的浮点输入值标记为无效值。

1.  使用`perform_good_input()`函数运行包含“有效值:”

    ```py
    perform_good_input(predictor)
    ```

    的示例推理请求
2.  使用`perform_bad_input()`功能运行包含“无效值:

    ```py
    perform_bad_input(predictor)
    ```

    的样本推断请求
3.  定义`generate_sample_requests()`函数，交替调用`perform_good_input()`和`perform_bad_input()`函数:

    ```py
    from time import sleep
    ```

    ```py
    def generate_sample_requests(predictor):
    ```

    ```py
        for i in range(0, 2 * 240):
    ```

    ```py
            print(f"ITERATION # {i}")
    ```

    ```py
            perform_good_input(predictor)
    ```

    ```py
            perform_bad_input(predictor)
    ```

    ```py
            print("> SLEEPING FOR 30 SECONDS")
    ```

    ```py
            sleep(30)
    ```

4.  一切准备就绪后，让我们使用`generate_sample_requests()`函数:

    ```py
    generate_sample_requests(predictor)
    ```

    向 ML 推理端点连续发送样本请求

重要说明

请注意，本节中的最后一步将每 30 秒连续发送一次样本推断请求，并循环 480 次。我们将让它继续运行，并继续下一部分。我们应该只在完成本章的*用 SageMaker 模型监视器*部分的预定监视后停止`generate_sample_requests()`功能的执行。

此时，您可能想知道数据存储在哪里，以及这些数据将如何用于分析。在接下来的几节中，我们将回答这些问题，并提供更多关于 SageMaker 中模型监控如何工作的细节。

# 使用 SageMaker 型号监控器进行计划监控

如果您已经在数据科学和 ML 行业工作了相当长一段时间，您可能知道 ML 模型在部署后的性能没有保证。必须实时(或接近实时)监控生产中部署的模型，以便一旦检测到任何**漂移**或与预期值集的偏差，我们可以替换部署的模型并修复任何问题:

![Figure 8.9 – Analyzing captured data and detecting violations using Model Monitor 

](img/B18638_08_009.jpg)

图 8.9–使用模型监视器分析捕获的数据并检测违规

在上图中，我们可以看到，我们可以通过一个监视(处理)作业来处理和分析捕获的数据。这项工作预计将生成一个自动化的报告，可用于分析部署的模型和数据。同时，任何检测到的违规都会被标记并作为报告的一部分进行报告。

注意

假设我们已经训练了一个 ML 模型，在给定专业人员的*年龄*、*工作经验年限*、*角色*和*子女数量*的情况下，该模型预测了专业人员的*工资*。一旦 ML 模型被部署到推理端点，各种应用就会向 ML 推理端点发送请求数据，以获得预测的工资值。如果其中一个应用开始发送错误的值，该怎么办？例如，为输入有效载荷中的*子节点数量*指定的值为负。鉴于该字段不可能为负数，监控作业应将该违规标记为**数据质量问题**。

在本节中，我们将配置 **SageMaker 模型监视器**来使用计划的每小时处理作业分析捕获的数据。一旦处理作业结果准备就绪，我们将看到监视作业已经标记了一个违规，该违规是由于将“错误输入”作为有效负载的一部分发送到上一节中的 ML 推断端点而导致的。模型监视器可以被配置为检测关于**数据质量**、**模型质量**、**偏差漂移**和**特征属性漂移**的违规。在本节的动手解决方案中，我们将只关注检测有关数据质量的违规行为。然而，检测其他类型的漂移和违规应该遵循一组类似的步骤，稍后将介绍这些步骤。

按照以下步骤配置 **SageMaker Model Monitor** 每小时运行一次监控任务，并分析通过 ML 推理端点捕获的数据:

1.  点击**文件**菜单，从**新建**子菜单下的选项列表中选择**笔记本**，创建一个新笔记本。

注意

请注意，我们将在本章前面几节中创建的其他笔记本文件旁边的`CH08`目录中创建新笔记本。

1.  在中的`Data Science`(在 **SageMaker 图像**下找到的选项)
2.  `Python 3`
3.  `No script`

之后点击**选择**按钮。

1.  右键点击新笔记本的标签名称，选择**新名称**下的`04 - Scheduled Monitoring with SageMaker Model Monitor.ipynb`。之后点击**重命名**按钮。
2.  现在我们已经准备好了新的笔记本，让我们使用 IPython 的`%store`魔法来加载`s3_bucket`、`prefix`、`ll_package_arn`、`endpoint_name`和`ll_package_arn` :

    ```py
    %store -r s3_bucket
    ```

    ```py
    %store -r prefix
    ```

    ```py
    %store -r ll_package_arn
    ```

    ```py
    %store -r endpoint_name
    ```

    ```py
    %store -r ll_package_arn
    ```

    的存储变量的值
3.  初始化一个`Predictor`对象，并将其附加到我们在*从 SageMaker 模型注册表*部分部署模型中部署的 ML 推理端点:

    ```py
    import sagemaker
    ```

    ```py
    from sagemaker import get_execution_role
    ```

    ```py
    from sagemaker.predictor import Predictor
    ```

    ```py
    session = sagemaker.Session()
    ```

    ```py
    role = get_execution_role()
    ```

    ```py
    predictor = Predictor(
    ```

    ```py
        endpoint_name=endpoint_name,
    ```

    ```py
        sagemaker_session=session,
    ```

    ```py
        role=role
    ```

    ```py
    )
    ```

4.  使用`wget`命令下载`baseline.csv`文件:

    ```py
    %%bash
    ```

    ```py
    mkdir -p tmp
    ```

    ```py
    wget -O tmp/baseline.csv https://bit.ly/3td5vjx
    ```

注意

`baseline.csv`文件是干什么用的？该 CSV 文件随后将作为**基线数据集**，由 **SageMaker 模型监视器**使用作为“参考”,检查采集数据的偏差和问题。

1.  让我们也准备好 S3 路径位置，我们将在这里存储基线分析输出文件:

    ```py
    base = f's3://{s3_bucket}/{prefix}'
    ```

    ```py
    baseline_source_uri = f'{base}/baseline.csv'
    ```

    ```py
    baseline_output_uri = f"{base}/baseline-output"
    ```

2.  使用`aws s3 cp`命令将`baseline.csv`文件从`tmp`目录上传到`baseline_source_uri` :

    ```py
    !aws s3 cp tmp/baseline.csv {baseline_source_uri}
    ```

    中存储的 S3 目标位置
3.  使用以下代码块初始化和配置`DefaultModelMonitor`实例:

    ```py
    from sagemaker.model_monitor import DefaultModelMonitor
    ```

    ```py
    monitor_dict = {
    ```

    ```py
        'role': role,
    ```

    ```py
        'instance_count': 1,
    ```

    ```py
        'instance_type': 'ml.m5.large',
    ```

    ```py
        'volume_size_in_gb': 10,
    ```

    ```py
        'max_runtime_in_seconds': 1800,
    ```

    ```py
    }
    ```

    ```py
    default_monitor = DefaultModelMonitor(
    ```

    ```py
        **monitor_dict
    ```

    ```py
    )
    ```

这里，我们在处理捕获的数据时配置了`ml.m5.large`实例。

注意

为了监控部署的 ML 模型和通过推理端点的数据，`monitor_dict`对应于用于监控 ML 模型和数据的 SageMaker 处理作业的配置。

1.  让我们使用下面的代码块运行基线作业:

    ```py
    %%time
    ```

    ```py
    from sagemaker.model_monitor import dataset_format
    ```

    ```py
    dataset_format = dataset_format.DatasetFormat.csv(
    ```

    ```py
        header=True
    ```

    ```py
    )
    ```

    ```py
    baseline_dict = {
    ```

    ```py
        'baseline_dataset': baseline_source_uri,
    ```

    ```py
        'dataset_format': dataset_format,
    ```

    ```py
        'output_s3_uri': baseline_output_uri,
    ```

    ```py
        'wait': True
    ```

    ```py
    }
    ```

    ```py
    default_monitor.suggest_baseline(
    ```

    ```py
        **baseline_dict
    ```

    ```py
    )
    ```

这里，我们使用`baseline.csv`文件作为将通过 ML 推断端点的数据的预期属性的参考。假设`baseline.csv`文件中的一列只包含正整数。使用这个 CSV 文件作为基线，我们将能够配置 **SageMaker 模型监视器**来标记负的或浮点输入值(对于所述列或特性)为“错误输入”

注意

当然，检测违规和问题只是故事的一半。解决问题是另一半。

1.  定义一个自定义的`flatten()`函数，它将帮助我们在数据帧

    ```py
    import pandas as pd
    ```

    ```py
    def flatten(input_dict):
    ```

    ```py
        df = pd.json_normalize(input_dict)
    ```

    ```py
        return df.head()
    ```

    中检查和查看字典对象
2.  下面我们来看一下由基线化作业生成的统计报表:

    ```py
    baseline_job = default_monitor.latest_baselining_job
    ```

    ```py
    stats = baseline_job.baseline_statistics()
    ```

    ```py
    schema_dict = stats.body_dict["features"]
    ```

    ```py
    flatten(schema_dict)
    ```

这将产生如下所示的数据帧:

![Figure 8.10 – DataFrame containing the baseline statistics

](img/B18638_08_010.jpg)

图 8.10–包含基线统计数据的数据帧

在这里，我们可以看到`baseline.csv`文件中每一列的`inferred_type`值，以及其他统计值。

1.  接下来，让我们查看由基线作业准备的建议约束:

    ```py
    constraints = baseline_job.suggested_constraints()
    ```

    ```py
    constraints_dict = constraints.body_dict["features"]
    ```

    ```py
    flatten(constraints_dict)
    ```

这将为我们提供一个类似如下的值的数据框架:

![Figure 8.11 – DataFrame with the suggested constraints of each of the features

](img/B18638_08_011.jpg)

图 8.11–每个特征的建议约束的数据框架

在这里，我们可以在分析所使用的基线数据集之后，看到基线作业推荐的约束。

注意

基准数据集中的`a`稍后将使用这些(建议的)约束。基准数据集中的约束应仅包含整数值，如果捕获的数据包含记录，处理作业将进行标记，其中列`a`值为浮点数。

1.  接下来，我们将修改列`a`和`b`的约束(包含输入的 *x* 和 *y* 值)，并假设这些值的有效值是整数类型，而不是浮点数或小数:

    ```py
    constraints.body_dict['features'][1]['inferred_type'] = 'Integral'
    ```

    ```py
    constraints.body_dict['features'][2]['inferred_type'] = 'Integral'
    ```

    ```py
    constraints.save()
    ```

一旦每小时处理作业分析了捕获的数据， **SageMaker 模型监视器**将把包含浮点值 *y* 的有效负载标记为“错误输入”

重要说明

如果我们将建议约束的列`a`和`b`的`inferred_type`值(分别包含 *x* 和 *y* 值)改为`'Fractional'`而不是`'Integral'`，会发生什么？由于*启用数据捕获和模拟预测*部分中的`generate_sample_requests()`函数生成的有效载荷值涉及整数和浮点值的组合，因此 **SageMaker 模型监视器**会将所有输入请求有效载荷标记为“良好输入”，并且不会报告任何检测到的违规。

1.  让我们定义`generate_label()`函数，它将帮助我们在后面的步骤中为监视计划名称生成一个随机的字符串标签:

    ```py
    from sagemaker.model_monitor import (
    ```

    ```py
        CronExpressionGenerator
    ```

    ```py
    )
    ```

    ```py
    from string import ascii_uppercase
    ```

    ```py
    import random
    ```

    ```py
    def generate_label():
    ```

    ```py
        chars = random.choices(ascii_uppercase, k=5)
    ```

    ```py
        output = 'monitor-' + ''.join(chars)
    ```

    ```py
        return output
    ```

2.  让我们分别使用`baseline_statistics()`和`suggested_constraints()`方法加载基线统计数据和建议的约束:

    ```py
    s3_report_path = f'{base}/report'
    ```

    ```py
    baseline_statistics = default_monitor.baseline_statistics()
    ```

    ```py
    constraints = default_monitor.suggested_constraints()
    ```

3.  让我们准备好 **cron 表达式**，我们将在后面的步骤

    ```py
    cron_expression = CronExpressionGenerator.hourly()
    ```

    中使用它来配置监控作业，使其每小时运行一次

注意

有关其他支持的 **cron 表达式**的更多详细信息，请随时查看[https://docs . AWS . Amazon . com/sagemaker/latest/DG/model-monitor-schedule-expression . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-schedule-expression.xhtml)。

1.  准备好先决条件后，让我们使用`DefaultModelMonitor`实例的`create_monitoring_schedule()`方法创建监视时间表:

    ```py
    schedule_dict = {
    ```

    ```py
        'monitor_schedule_name': generate_label(),
    ```

    ```py
        'endpoint_input': predictor.endpoint,
    ```

    ```py
        'output_s3_uri': s3_report_path,
    ```

    ```py
        'statistics': baseline_statistics,
    ```

    ```py
        'constraints': constraints,
    ```

    ```py
        'schedule_cron_expression': cron_expression,
    ```

    ```py
        'enable_cloudwatch_metrics': True
    ```

    ```py
    }
    ```

    ```py
    default_monitor.create_monitoring_schedule(
    ```

    ```py
        **schedule_dict
    ```

    ```py
    )
    ```

在运行这个代码块之后，`schedule`运行一个 **SageMaker 处理**作业(每小时一次),该作业处理并监控已经捕获的数据。

注意

如果您在使用`predictor.endpoint`时遇到不推荐使用的警告或问题，您可以用`predictor.endpoint_name`代替。有关使用 2.x 版本的 **SageMaker Python SDK** 时的弃用(以及破坏性和非破坏性更改)的更多信息，请随时查看[https://sagemaker.readthedocs.io/en/stable/v2.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-schedule-expression.xhtml)。

1.  让我们快速检查一下监视器的调度属性:

    ```py
    flatten(default_monitor.describe_schedule())
    ```

这将产生如下所示的数据帧:

![Figure 8.12 – DataFrame describing the properties of the monitoring schedule

](img/B18638_08_012.jpg)

图 8.12–描述监控计划属性的数据框架

在这里，我们可以看到`MonitoringScheduleStatus`的值仍然是`Pending`。

1.  使用`sleep()`功能等待 5 分钟后执行下一个单元格:

    ```py
    from time import sleep
    ```

    ```py
    sleep(300)
    ```

注意

在这里，我们等待几分钟，同时创建监视计划(假设它是在 5 分钟内创建的)。

1.  使用`DefaultModelMonitor`实例的

    ```py
    dm = default_monitor
    ```

    ```py
    monitoring_violations = \
    ```

    ```py
    dm.latest_monitoring_constraint_violations()
    ```

    ```py
    monitoring_statistics = \
    ```

    ```py
    dm.latest_monitoring_statistics()
    ```

    的`latest_monitoring_constraint_violations()` 和`latest_monitoring_statistics()`方法，测试并加载监视器的约束违反和统计的初始值集
2.  定义`get_violations()`和`load_and_load_violations()`功能:

    ```py
    %%time
    ```

    ```py
    from time import sleep
    ```

    ```py
    def get_violations():
    ```

    ```py
        return \
    ```

    ```py
        dm.latest_monitoring_constraint_violations()
    ```

    ```py
    def loop_and_load_violations():
    ```

    ```py
        for i in range(0, 2 * 120):
    ```

    ```py
            print(f"ITERATION # {i}")
    ```

    ```py
            print("> SLEEPING FOR 60 SECONDS")
    ```

    ```py
            sleep(60)
    ```

    ```py
            try:
    ```

    ```py
                v = get_violations()
    ```

    ```py
                violations = v
    ```

    ```py
                if violations:
    ```

    ```py
                    return violations
    ```

    ```py
            except:
    ```

    ```py
                pass
    ```

    ```py
        print("> DONE!")
    ```

    ```py
        return None              
    ```

3.  调用我们在上一步定义的`load_and_load_violations()`函数:

    ```py
    loop_and_load_violations()
    ```

这应该会生成一组日志，如下所示:

![Figure 8.13 – Logs generated while running the loop_and_load_violations() function

](img/B18638_08_013.jpg)

图 8.13–运行 loop_and_load_violations()函数时生成的日志

在这里，我们简单地迭代并等待预定的模型监视器处理作业生成包含检测到的违规的分析报告，以及从捕获的数据中计算出的其他统计值。

重要说明

完成此步骤可能需要一个小时或更长时间。随意拿一杯(更大的)咖啡或茶！在等待该步骤完成的同时，您可以继续本章下一节的动手解决方案，*分析捕获的数据*。

1.  一旦`loop_and_load_violations()`函数完成运行，您就可以使用`DefaultModelMonitor`实例的`latest_monitoring_constraint_violations()`方法:

    ```py
    violations = dm.latest_monitoring_constraint_violations()
    ```

    ```py
    violations.__dict__
    ```

    继续加载和检查检测到的违规

这将为我们提供一个嵌套的值字典，类似于下面的代码:

```py
{'body_dict': {'violations': [

  {'feature_name': 'b',

    'constraint_check_type': 'data_type_check',

    'description': 'Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 50.0% of data is Integral.'}]

  },

 'file_s3_uri': 's3://<BUCKET>/chapter08/report/1-2022-05-23-14-39-16-279/monitor-YTADH/2022/05/23/16/constraint_violations.json',

 'kms_key': None,

 'session': None

}
```

在这里，我们可以看到对于特性`b`(对应于 *y* 输入值)我们有几个检测到的违规。为了更好地了解这些检测到的违规是什么，我们可以查看可用的描述—`Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 50.0% of data is Integral`。

1.  使用`DefaultModelMonitor`实例

    ```py
    monitoring_statistics = dm.latest_monitoring_statistics()
    ```

    ```py
    monitoring_statistics.__dict__
    ```

    的`latest_monitoring_statistics()`方法加载并检查统计数据

这将为我们提供一个嵌套的值结构，如下所示:

```py
{'body_dict': {'version': 0.0,

  'dataset': {'item_count': 190},

  'features': [{'name': 'label',

    'inferred_type': 'Integral',

    'numerical_statistics': {'common': {'num_present': 190, 'num_missing': 0},

     'mean': 1.2052631578947368,

     'sum': 229.0,

     'std_dev': 0.7362591679068381,

     'min': 0.0,

     'max': 2.0,

      ... (and more) ...
```

那不是很容易吗？想象一下尝试自己构建这个！你可能要花几天时间自己从头开始编码和构建。

此时，您应该对如何配置和使用 **SageMaker 模型监视器**来检测模型和数据中的违规和潜在问题有了更好的了解。在清理我们在本章中创建和使用的资源之前，我们将看看关于如何分析和处理由模型监视器在 S3 桶中捕获和收集的数据的另一种方法。

# 分析捕获的数据

当然，还有其他方法来处理被捕获并存储在 S3 桶中的数据。除了使用前一节中讨论的内置模型监控功能和特性，我们还可以从 S3 桶下载收集的 ML 推断端点数据，并直接在笔记本中进行分析。

注意

仍然建议利用 SageMaker 的内置模型监控功能和特性。然而，了解这种方法将有助于我们解决在使用和运行 SageMaker 中可用的自动化解决方案时可能遇到的任何问题。

按照以下步骤使用各种 Python 库来处理、清理和分析在 S3 收集的 ML 推断数据:

1.  点击**文件**菜单，从**新建**子菜单下的选项列表中选择**笔记本**，创建一个新笔记本。

注意

请注意，我们将在本章前面几节中创建的其他笔记本文件旁边的`CH08`目录中创建新笔记本。

1.  在`Data Science`(在 **SageMaker 图像**下找到的选项)
2.  `Python 3`
3.  `No script`

之后点击**选择**按钮。

1.  右键点击新笔记本的标签名称，选择**新名称**下的`05 - Analyzing the Captured Data.ipynb`。点击**重命名**按钮。
2.  现在我们已经创建了我们的新笔记本，让我们使用来自`s3_bucket`和`capture_upload_path` :

    ```py
    %store -r s3_bucket
    ```

    ```py
    %store -r capture_upload_path
    ```

    的`%store`魔法

注意

等等！`capture_upload_path`从何而来？在*启用数据捕获和模拟预测*部分，我们初始化`capture_upload_path`并将其值设置为 S3 路径，捕获的数据(属于 **SageMaker 模型监视器**)将存储在该路径中。

1.  获取每个生成的包含推理请求输入输出数据的`jsonl`文件的 S3 路径:

    ```py
    results = !aws s3 ls {capture_upload_path} --recursive
    ```

    ```py
    processed = []
    ```

    ```py
    for result in results:
    ```

    ```py
        partial = result.split()[-1]
    ```

    ```py
        path = f"s3://{s3_bucket}/{partial}"
    ```

    ```py
        processed.append(path)
    ```

    ```py
    processed
    ```

2.  使用`mkdir`命令:

    ```py
    !mkdir -p captured
    ```

    创建`captured`目录
3.  接下来，使用`aws s3 cp`命令将每个生成的`jsonl`文件复制到我们在上一步刚刚创建的`captured`目录中:

    ```py
    for index, path in enumerate(processed):
    ```

    ```py
        print(index, path)
    ```

    ```py
        !aws s3 cp {path} captured/{index}.jsonl
    ```

4.  定义`load_json_file()`功能:

    ```py
    import json
    ```

    ```py
    def load_json_file(path):
    ```

    ```py
        output = []
    ```

    ```py
        with open(path) as f:
    ```

    ```py
            output = [json.loads(line) for line in f]
    ```

    ```py
        return output
    ```

5.  从`captured`目录下每个下载的`jsonl`文件中提取 JSON 值:

    ```py
    all_json = []
    ```

    ```py
    for index, _ in enumerate(processed):
    ```

    ```py
        print(f"INDEX: {index}")
    ```

    ```py
        new_records = load_json_file(
    ```

    ```py
            f"captured/{index}.jsonl"
    ```

    ```py
        )
    ```

    ```py
        all_json = all_json + new_records
    ```

    ```py
    all_json
    ```

6.  使用`pip`安装`flatten-dict`库:

    ```py
    !pip3 install flatten-dict
    ```

正如我们将在的后续步骤中看到的，`flatten-dict`包在“扁平化”任何嵌套的字典结构中是有用的。

1.  在`all_json`列表中存储的第一个条目上测试`flatten-dict`库中的`flatten()`函数:

    ```py
    from flatten_dict import flatten
    ```

    ```py
    first = flatten(all_json[0], reducer='dot')
    ```

    ```py
    first
    ```

这将为我们提供一个类似如下的扁平结构:

```py
{'captureData.endpointInput.observedContentType': 'text/csv',

 'captureData.endpointInput.mode': 'INPUT',

 'captureData.endpointInput.data': '0,0',

 'captureData.endpointInput.encoding': 'CSV',

 'captureData.endpointOutput.observedContentType': 'text/csv; charset=utf-8',

 'captureData.endpointOutput.mode': 'OUTPUT',

 'captureData.endpointOutput.data': '2\n',

 'captureData.endpointOutput.encoding': 'CSV',

 'eventMetadata.eventId': 'b73b5e15-06ad-48af-b53e-6b8800e98678',

 'eventMetadata.inferenceTime': '2022-05-23T18:43:42Z',

 'eventVersion': '0'}
```

注意

我们将很快使用`flatten()`将存储在`all_json`中的嵌套 JSON 值转换成“扁平的”JSON 值。这个“扁平化”的 JSON 值列表将被转换成一个**pandas**data frame**data frame**(我们将在后面的步骤中对其进行处理和分析)。

1.  使用以下代码块展平存储在`all_json`列表中的每个 JSON 值:

    ```py
    flattened_json = []
    ```

    ```py
    for entry in all_json:
    ```

    ```py
        result = flatten(entry, reducer='dot')
    ```

    ```py
        flattened_json.append(result)
    ```

    ```py
    flattened_json
    ```

2.  接下来，将展平的结构加载到 pandas DataFrame:

    ```py
    import pandas as pd
    ```

    ```py
    df = pd.DataFrame(flattened_json)
    ```

    ```py
    df
    ```

这将产生如下所示的数据帧:

![Figure 8.14 – DataFrame containing the collected monitoring data 

](img/B18638_08_014.jpg)

图 8.14–包含收集的监控数据的数据帧

在这里，我们可以看到收集到的端点数据在数据帧内变平。

1.  现在，让我们通过从 DataFrame 列`captureData.endpointInput.data`中提取 *x* 和 *y* 值来稍微清理一下的内容，data frame 列包含输入请求数据:

    ```py
    df[['x', 'y']] = df['captureData.endpointInput.data'].str.split(',', 1, expand=True)
    ```

2.  之后，让我们从 DataFrame 列`captureData.endpointOutput.data`中提取`label`值，它包含输出响应数据。将`label`值存储在名为`predicted_label` :

    ```py
    df['predicted_label'] = df['captureData.endpointOutput.data'].str.strip()
    ```

    的新列中
3.  让我们准备`clean_df`数据帧，它只包含原来`DataFrame`—`predicted_label`、`x`和`y` :

    ```py
    clean_df = df[['predicted_label', 'x', 'y']]
    ```

    、

    ```py
    clean_df.head()
    ```

    中的三列

这将为我们提供如下所示的数据框架:

![Figure 8.15 – DataFrame containing the values for predicted_label, x, and y

](img/B18638_08_015.jpg)

图 8.15–包含预测标签、x 和 y 值的数据帧

在这里，我们可以看到`y`列的一些值是整数，而一些值是浮点格式。

1.  接下来，让我们使用`astype`方法:

    ```py
    clean_df = clean_df.astype({
    ```

    ```py
        'predicted_label': 'int',
    ```

    ```py
        'x': 'float',
    ```

    ```py
        'y': 'float',
    ```

    ```py
    })
    ```

    ```py
    clean_df.head()
    ```

    对`clean_df`数据帧中存储的值进行类型转换

这将为我们提供如下所示的数据帧:

![Figure 8.16 – Values for x and y cast into floating-point values

](img/B18638_08_016.jpg)

图 8.16–x 和 y 值转换成浮点值

现在，`x`和`y`列下的所有内容都是浮点格式。

此时，我们可以运行不同类型的分析，例如手动计算不同类型的统计数据，类似于由 **SageMaker 模型监视器**自动执行的操作。我们也可以使用这种方法来解决模型监视器处理作业在分析收集的数据时遇到的数据编码问题，类似于我们在[https://github.com/aws/sagemaker-python-sdk/issues/1896](https://github.com/aws/sagemaker-python-sdk/issues/1896)所做的。

# 删除具有监控时间表的端点

既然我们已经使用完了我们的 ML 推理端点，让我们删除它，连同附加的监视器和监视时间表。

按照以下步骤列出我们的 ML 推理端点的所有连接的监视器，并删除任何连接的监视时间表以及端点:

1.  点击**文件**菜单，从**新建**子菜单下的选项列表中选择**笔记本**，创建一个新笔记本。

注意

请注意，我们将在本章前面几节中创建的其他笔记本文件旁边的`CH08`目录中创建新笔记本。

1.  在`Data Science`(在 **SageMaker 图像**下找到的选项)
2.  `Python 3`
3.  `No script`

之后点击**选择**按钮。

1.  右键点击新笔记本的标签名称，选择**新名称**下的`06 - Deleting an Endpoint with a Monitoring Schedule.ipynb`。点击**重命名**按钮。
2.  现在我们已经准备好了我们的新笔记本，让我们使用 IPython 的`%store`魔法来加载`endpoint_name` :

    ```py
    %store -r endpoint_name
    ```

    的存储变量值
3.  使用以下代码块初始化`Predictor`实例并将其附加到现有的 ML 推理端点:

    ```py
    import sagemaker
    ```

    ```py
    from sagemaker import get_execution_role
    ```

    ```py
    from sagemaker.predictor import Predictor
    ```

    ```py
    session = sagemaker.Session()
    ```

    ```py
    role = get_execution_role()
    ```

    ```py
    predictor = Predictor(
    ```

    ```py
        endpoint_name=endpoint_name,
    ```

    ```py
        sagemaker_session=session,
    ```

    ```py
        role=role
    ```

    ```py
    )
    ```

4.  在下一步删除之前，让我们快速列出所有连接的显示器:

    ```py
    monitors = predictor.list_monitors()
    ```

    ```py
    for monitor in monitors:
    ```

    ```py
        print(monitor.__dict__)
    ```

这里，我们使用了`__dict__`属性来检查监视器实例的属性。

1.  让我们使用`delete_monitoring_schedule()`方法删除每一个监视器:

    ```py
    for monitor in monitors:
    ```

    ```py
        monitor.delete_monitoring_schedule()
    ```

这将产生类似于`Deleting Monitoring Schedule with name: monitor-HWFEL`的输出。

1.  最后，让我们使用`delete_endpoint()`方法删除推理端点:

    ```py
    predictor.delete_endpoint()
    ```

确保您也停止了本章所用笔记本中任何正在运行的单元格的执行。

# 清理

现在我们已经完成了本章的动手解决方案，是时候清理并关闭我们不再使用的资源了。按照以下步骤定位并关闭 **SageMaker Studio** 中任何剩余的运行实例:

1.  点击侧边栏中的**运行实例和内核**图标，如下图所示:

![Figure 8.17 – Turning off the running instance

](img/B18638_08_017.jpg)

图 8.17–关闭正在运行的实例

点击**运行实例和内核**图标将打开并显示 SageMaker Studio 中的运行实例、应用和终端。

1.  通过单击每个实例的**关闭**按钮，关闭**运行实例**下的所有运行实例，如前面截图中突出显示的。点击**关闭**按钮将打开一个弹出窗口，验证实例关闭操作。点击**关闭所有**按钮继续。

重要说明

确保关闭**编辑器**窗格中打开的笔记本标签。在某些情况下，当 SageMaker 检测到有打开的笔记本标签时，它会自动打开一个实例。

1.  确保检查并删除 **SageMaker resources** 下所有正在运行的推理端点(如果有):

![Figure 8.18 – Checking the list of running inference endpoints

](img/B18638_08_018.jpg)

图 8.18–检查正在运行的推理端点列表

要检查是否有正在运行的推理端点，单击前面截图中突出显示的 **SageMaker resources** 图标，然后从下拉菜单的选项列表中选择**端点**。

1.  最后，打开**文件**菜单，从可用选项列表中选择**关闭**。这应该确保 SageMaker Studio 中所有正在运行的实例也已经关闭。

注意，该清理操作需要在使用 **SageMaker Studio** 后进行。SageMaker 不会自动关闭这些资源，即使在不活动期间也是如此。

# 总结

在这一章中，我们利用 SageMaker 中的模型注册中心来注册、组织和管理我们的 ML 模型。在部署了存储在注册表中的 ML 模型之后，我们使用 **SageMaker Model Monitor** 来捕获数据并运行处理作业，分析收集的数据并标记任何检测到的问题或偏差。

在下一章，我们将重点关注使用各种策略和解决方案来保护 ML 环境和系统。如果你真的想设计和构建安全的 ML 系统和环境，那么下一章就是为你准备的！

# 延伸阅读

有关本章涵盖的主题的更多信息，请随时查阅以下资源:

*   *SageMaker 模型注册表–查看部署历史*([https://docs . AWS . Amazon . com/sage maker/latest/DG/Model-Registry-deploy-History . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-deploy-history.xhtml))
*   *SageMaker 模型监视器——监视模型的数据和模型质量、偏差和可解释性*([https://docs . AWS . Amazon . com/sage maker/latest/DG/Model-Monitor . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.xhtml))
*   *SageMaker Python SDK —亚马逊 SageMaker 模型监视器*([https://SageMaker . readthe docs . io/en/stable/Amazon _ SageMaker _ Model _ monitoring . XHTML](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_monitoring.xhtml))*