<html><head/><body>


	
		<title>B17447_07_ePub_RK</title>
		
	
	
		<div><h1 id="_idParaDest-100"><em class="italic"> <a id="_idTextAnchor099"/>第 7 章</em>:在云中托管 ML 模型:最佳实践</h1>
			<p>在您成功地训练了一个模型之后，您想让这个模型可用于推理，不是吗？ML 模型通常是 ML 驱动的企业的产品。您的客户从您的模型中消费最大似然预测，而不是您的训练作业或处理数据。您如何提供令人满意的客户体验，从良好的 ML 模型体验开始？</p>
			<p>SageMaker 有几个 ML 托管和推理选项，这取决于您的用例。生活的许多方面都欢迎选择，但是很难找到最好的选择。本章将帮助您了解如何为批量推理和在线实时推理托管模型，如何使用多模型端点来节省成本，以及如何根据您的推理需求进行资源优化。</p>
			<p>在本章中，我们将讨论以下主题:</p>
			<ul>
				<li>培训后在云中部署模型</li>
				<li>使用批量转换进行批量推理</li>
				<li>托管实时端点</li>
				<li>优化您的模型部署</li>
			</ul>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor100"/>技术要求</h1>
			<p>对于本章，您需要访问位于<a href="https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter07">https://github . com/packt publishing/Getting-Started-with-Amazon-sage maker-Studio/tree/main/chapter 07</a>的代码。如果您没有运行上一章中的笔记本，请在继续之前运行存储库中的<a href="http://chapter05/02-tensorflow_sentiment_analysis.ipynb">chapter 05/02-tensor flow _ sensition _ analysis . ipynb</a>文件。</p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor101"/>培训后在云端部署模型</h1>
			<p>ML 模型<a id="_idIndexMarker441"/>主要可以通过两种<a id="_idIndexMarker443"/>方式<strong class="bold">批量推理</strong>和<strong class="bold">现场推理</strong>在云中消费。批量推理是指对批量数据执行的模型推理，通常是大批量的，本质上是异步的。它适合不经常收集数据的用例，关注组统计数据而不是个体推断，并且不需要为下游过程立即得到推断结果。例如，面向研究的项目不需要立即返回数据点的模型推断。研究人员通常收集大量数据用于测试和评估目的，并关心整体统计数据和性能，而不是单个预测。他们可以分批进行推断，并等待整批预测完成后再继续。</p>
			<p>另一方面，实时推理是指实时执行的模型推理。期望立即返回输入数据点的推断结果，以便它可以用于随后的决策过程。例如，交互式聊天机器人需要实时推理能力来支持这样的服务。没有人想等到对话结束时才得到聊天机器人模型的回应，也没有人想等上几秒钟。希望提供最佳客户体验的公司会希望做出一个推断，并立即返回给客户。</p>
			<p>考虑到不同的需求，批量推理和实时推理的架构和部署选择也有所不同。Amazon SageMaker 涵盖了这一点，因为它为您的推理用例提供了各种完全管理的选项。<strong class="bold">sage maker batch transform</strong>旨在大规模执行批量推理，并且具有成本效益，因为计算基础架构是完全托管的，并且在您的推理工作完成后会取消供应<a id="_idIndexMarker446"/>。<strong class="bold"> SageMaker 实时端点</strong>旨在为您的 ML 用例提供一个强大的实时托管选项。这两个 SageMaker 托管选项都是完全托管的，这意味着您不必太担心云基础架构。</p>
			<p>让我们先来看看 SageMaker batch transform，它是如何工作的，以及何时使用它。</p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor102"/>使用批量转换进行批量推理</h1>
			<p>SageMaker 批处理转换旨在为大型数据集提供离线推理。根据组织数据的方式，SageMaker batch transform 可以将 S3 中的一个大型文本文件按行分割成一个易于管理的小文件(mini-batch ),以便在对模型进行推断之前放入内存中；它还可以通过 S3 键将文件分发到计算实例中，以提高计算效率。例如，它可以将<code>test1.csv</code>发送到实例 1，将<code>test2.csv</code>发送到实例 2。</p>
			<p>为了演示 SageMaker 批处理转换，我们可以从上一章的训练示例中挑选。在<a href="B17447_06_ePub_RK.xhtml#_idTextAnchor090"> <em class="italic">第 6 章</em> </a>、<em class="italic">用 SageMaker Clarify 检测 ML 偏差和解释模型</em>中，我们向您展示了如何使用 SageMaker 管理的训练来训练 TensorFlow 模型，用于<code>Getting-Started-with-Amazon-SageMaker-Studio/chapter05/02-tensorflow_sentiment_analysis.ipynb.</code>中的电影评论情感预测用例。我们可以部署训练好的模型，在以下步骤中使用 SageMaker 批量转换进行批量推断:</p>
			<ol>
				<li>请打开<code>Getting-Started-with-Amazon-SageMaker-Studio/chapter07/01-tensorflow_sentiment_analysis_batch_transform.ipynb</code>笔记本，使用<strong class="bold">Python 3</strong>(<strong class="bold">tensor flow 2.3 Python 3.7 CPU 优化</strong>内核。</li>
				<li>Run the first three cells to set up the SageMaker SDK, import the libraries, and prepare the test dataset. There are 25,000 documents in the test dataset. We save the test data as a CSV file and upload the CSV file to our S3 bucket. The file is 27 MB.<p class="callout-heading">注意</p><p class="callout">SageMaker batch transform 期望输入 CSV 文件<em class="italic">而非</em>包含标题。也就是说，CSV 的第一行应该是第一个数据点。</p></li>
				<li>我们从我们在<a href="B17447_06_ePub_RK.xhtml#_idTextAnchor090"> <em class="italic">第 6 章</em> </a>、<em class="italic">中所做的训练工作中检索训练张量流估计器，检测 ML 偏差并用 SageMaker Clarify </em>解释模型。我们需要为<code>TensorFlow.attach()</code>方法获取培训工作名称。你可以在左边栏的<strong class="bold">实验和试验</strong>中找到，如图<em class="italic">图 7.1 </em>所示，这要感谢我们训练时用的<a id="_idIndexMarker448"/>实验。在<strong class="bold">实验和试验</strong>中，左键点击<strong class="bold">IMD b-情绪分析</strong>，你会在列表中看到你的培训工作是一个试验。</li>
			</ol>
			<div><div><img src="img/B17447_07_01.jpg" alt="Figure 7.1 – Obtaining training job name in Experiments and trials&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图 7.1-在实验和试验中获得培训工作名称</p>
			<p>您应该用自己的代码替换下面代码中的<code>training_job_name</code>:</p>
			<pre>from sagemaker.tensorflow import TensorFlow
training_job_name='&lt;your-training-job-name&gt;'
estimator = TensorFlow.attach(training_job_name)</pre>
			<p>一旦你替换了<code>training_job_name</code>并连接它来重新加载<code>estimator</code>，你应该在输出中看到打印的任务历史。</p>
			<ol>
				<li value="4">要运行 SageMaker 批量转换，您只需要两行 SageMaker API 代码:<pre>transformer = estimator.transformer(instance_count=1,                                      instance_type='ml.c5.xlarge',                                     max_payload = 2, # MB                                     accept = 'application/jsonlines',                                     output_path = s3_output_location,                                     assemble_with = 'Line') transformer.transform(test_data_s3,                        content_type='text/csv',                        split_type = 'Line',                        job_name = jobname,                       experiment_config = experiment_config)</pre></li>
			</ol>
			<p><code>estimator.transformer()</code>方法用推理所需的计算资源创建一个<code>Transformer</code>对象。这里我们请求一个<code>ml.c5.xlarge</code>实例来预测 25，000 条电影评论。<code>max_payload</code>参数允许我们控制 SageMaker Batch Transform 分割的每个小批量的大小。<code>accept</code>参数决定输出类型。SageMaker 托管 Tensorflow 服务容器支持'<code>application/json</code>'和' a <code>pplication/jsonlines</code>'。<code>assemble_with</code>控制如何组合小批量的推理结果。然后我们在<code>transformer.transform()</code>中提供测试数据<code>test_data_s3</code>的 S3 位置，并指出输入内容类型为“<code>text/csv</code>，因为文件是 CSV 格式。<code>split_type</code>确定 SageMaker 如何将输入文件分割成小批量转换。我们输入一个惟一的作业名和 SageMaker 实验配置，这样我们就可以在同一个试验中跟踪对相关训练作业的推断。完成批量转换作业大约需要 5 分钟。与培训作业一样，SageMaker 会在作业完成后管理实例的供应、计算和取消供应。</p>
			<ol>
				<li value="5">工作完成后，我们应该看看结果。SageMaker batch transform 将汇编后的结果保存到指定的 S3 位置，并在输入文件名后附加<code>.out</code>。您可以在<code>transformer.output_path</code>属性中访问完整的 S3 路径。SageMaker 使用 TensorFlow 开发的模型服务框架 TensorFlow Serving，对于模型服务，模型输出以 JSON 格式编写。输出将情感概率放在一个数组中，预测作为 JSON 键。我们可以用下面的代码检查批量转换的结果:<pre>output = transformer.output_path output_prefix = 'imdb_data/test_output' !mkdir -p {output_prefix} !aws s3 cp --recursive {output} {output_prefix} !head {output_prefix}/{csv_test_filename}.out <strong class="bold">{    "predictions": [[0.00371244829], [1.0], [1.0], [0.400452465], [1.0], [1.0], [0.163813606], [0.10115058], [0.793149233], [1.0], [1.0], [6.37737814e-14], [2.10463966e-08], [0.400452465], [1.0], [0.0], [1.0], [0.400452465], [2.65155926e-29], [4.04420768e-11], ……]}</strong></pre></li>
			</ol>
			<p>然后，我们将所有 25，000 个预测收集到一个<code>results</code>变量中:</p>
			<pre>results=[]
with open(f'{output_prefix}/{csv_test_filename}.out', 'r') as f:
    lines = f.readlines()
    for line in lines:
        print(line)
        json_output = json.loads(line)
        result = [float('%.3f'%(item)) for sublist in json_output['predictions'] 
                                       for item in sublist]
        results += result
print(results)</pre>
			<ol>
				<li value="6">笔记本的其余部分显示一个原始电影评论、预测情绪和相应的真实情绪。该模型返回评论为正面或负面的概率。我们采用一个<code>0.5</code>阈值，并将超过阈值的概率标记为正，将低于<code>0.5</code>的概率标记为负。</li>
				<li>由于我们在与培训作业相同的试验中记录了批量转换作业，我们可以在左侧栏的<strong class="bold">实验和试验</strong>中轻松找到它，如图<em class="italic">图 7.2 </em>所示。您可以在此条目中看到有关此批处理转换作业的更多信息。<div> <img src="img/B17447_07_02.jpg" alt="Figure 7.2 – The batch transform job is logged as a trial component alongside the training component&#13;&#10;"/> </div></li>
			</ol>
			<p class="figure-caption"> </p>
			<p class="figure-caption">图 7.2–批处理转换作业作为试验组件与培训组件一起记录</p>
			<p>这就是利用 SageMaker batch transform 在大型数据集上生成推论是多么容易。你可能会想，为什么我不能直接用笔记本来做推论呢？使用 SageMaker 批量转换有什么好处？是的，你可以用笔记本快速分析。SageMaker 批量转换的优点<a id="_idIndexMarker450"/>如下:</p>
			<ul>
				<li>完全托管的小型批处理有助于高效地对大型数据集进行推理。</li>
				<li>您可以使用不同于您笔记本实例的单独的 SageMaker 管理的计算基础设施。您可以使用实例集群轻松运行预测，以实现更快的预测。</li>
				<li>您只需为批量转换作业的运行时间付费，即使是使用大得多的计算集群。</li>
				<li>您可以使用 SageMaker batch transform 在云中独立地计划和启动模型预测。没有必要在 SageMaker Studio 中使用 Python 笔记本来启动预测作业。</li>
			</ul>
			<p>接下来，让我们看看如何在云中托管 ML 模型以用于实时用例。</p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor103"/>托管实时端点</h1>
			<p>SageMaker 实时推理是一个完全托管的特性，用于在 compute <a id="_idIndexMarker452"/>实例上托管您的模型，以实现实时低延迟推理。部署过程包括以下步骤:</p>
			<ol>
				<li value="1">在 SageMaker 中创建一个模型、容器和相关的推理代码。模型指的是训练神器<code>model.tar.gz</code>。容器是代码和模型的运行时环境。</li>
				<li>创建 HTTPS 端点配置。此配置将有关计算实例类型和数量、模型以及流量模式的信息传递给模型变体。</li>
				<li>创建 ML 实例和一个 HTTPS 端点。SageMaker 创建了一个 ML 实例群和一个处理流量和认证的 HTTPS 端点。最后一步是为一个可以与客户端请求交互的工作 HTTPS 端点做好准备。</li>
			</ol>
			<p>托管实时端点面临一个在托管网站或 web 应用程序时常见的特殊挑战:当端点流量达到峰值时，很难扩展计算实例。在特定的一个小时里，你可能每分钟有 1000 个客户访问你的网站，然后在接下来的一个小时里有 100000 个客户。如果您只在能够每分钟处理 5，000 个请求的端点后部署一个实例，那么它在第一个小时会工作得很好，而在下一个小时就会很困难。自动伸缩是云中的一种技术，可以帮助您在满足特定标准时自动扩展实例，以便您的应用程序可以随时处理负载。</p>
			<p>让我们看一个 SageMaker 实时端点的例子。像批量转换例子一样，我们继续<a href="B17447_05_ePub_RK.xhtml#_idTextAnchor077"> <em class="italic">第 5 章</em> </a> <em class="italic">中的 ML 用例，用 SageMaker Studio IDE </em>和 05/02-tensor flow _ 情操 _ 分析构建和训练 ML 模型。ipynb。请在<code>Getting-Started-with-Amazon-SageMaker-Studio/chapter07/02-tensorflow_sentiment_analysis_inference.ipynb</code>打开笔记本，使用<strong class="bold">Python 3</strong>(<strong class="bold">tensor flow 2.3 Python 3.7 CPU 优化</strong>)内核。我们将把一个经过训练的模型部署到<a id="_idIndexMarker453"/> SageMaker 作为一个实时端点，例如进行一些预测，最后应用一个自动扩展策略来帮助扩展端点后面的计算实例。请遵循以下步骤:</p>
			<ol>
				<li value="1">在前四个单元中，我们设置了 SageMaker 会话，加载 Python 库，加载我们在<code>01-tensorflow_sentiment_analysis_batch_transform.ipynb</code>中创建的测试数据，并检索我们之前使用其名称训练的训练作业。</li>
				<li>然后，我们将模型部署到一个端点:<pre>predictor = estimator.deploy(                  instance_type='ml.c5.xlarge',                  initial_instance_count=1)</pre></li>
			</ol>
			<p>这里，我们选择<code>ml.c5.xlarge</code>作为<code>instance_type</code>的自变量。<code>initial_instance_ count</code>参数是指当我们进行这个调用时，端点后面的 ML 实例的数量。稍后，我们将向您展示如何使用 autoscaling 特性，该特性旨在帮助我们在初始设置不足时扩展实例群。部署过程大约需要 5 分钟。</p>
			<ol>
				<li value="3">我们可以用一些样本数据来测试端点。容器中的 TensorFlow 服务框架处理数据接口，并将 NumPy 数组作为输入，因此我们可以直接将条目传递到模型中。我们可以从端点获得 JSON 格式的响应，该响应在<code>prediction</code>变量:<pre>prediction=predictor.predict(x_test[data_index]) print(prediction) <strong class="bold">{'predictions': [[1.80986511e-11]]}</strong></pre>中被转换成 Python 中的字典</li>
			</ol>
			<p>接下来的两个单元格以文本形式检索评论，并打印出基本事实情感和阈值为 0.5 的预测情感，就像批处理转换示例中一样。</p>
			<ol>
				<li value="4">(可选)您<a id="_idIndexMarker454"/>可能想知道:我可以要求端点预测 25，000 个数据点的全部<code>x_test</code>吗？要找到答案，请随意尝试下面的代码:<pre>predictor.predict(x_test)</pre></li>
			</ol>
			<p>这行代码将运行几秒钟，最终会失败。这是因为 SageMaker 端点被设计为一次接受一个 6 MB 大小的请求。您可以为多个数据点请求推断，例如，<code>x_test[:100]</code>，但不能一次调用 25，000 个数据点。相比之下，batch transform 会自动进行数据分割(小型批处理),更适合处理大型数据集。</p>
			<ol>
				<li value="5">接下来，我们可以使用来自<code>boto3</code> SDK: <pre>sagemaker_client = sess.boto_session.client('sagemaker') autoscaling_client = sess.boto_session.client('application-autoscaling')</pre>的<code>application-autoscaling</code>客户端将 SageMaker 的自动伸缩特性应用到这个端点</li>
				<li>在 AWS 中为计算实例配置自动缩放需要两个步骤。首先，我们运行<code>autoscaling_client.register_scalable_target()</code>来为我们的 SageMaker 端点注册具有所需最小/最大容量的目标:<pre>resource_id=f'endpoint/{endpoint_name}/variant/AllTraffic'  response = autoscaling_client.register_scalable_target(    ServiceNamespace='sagemaker',    ResourceId=resource_id, ScalableDimension='sagemaker:variant:DesiredInstanceCount',    MinCapacity=1,    MaxCapacity=4)</pre></li>
			</ol>
			<p>我们的目标，SageMaker 实时端点，用<code>resource_id</code>表示。我们将最小容量设置为<code>1</code>，最大容量设置为<code>4</code>，这意味着当负载最低时，至少会有一个实例在端点之后运行。我们的端点最多能够扩展到四个实例。</p>
			<ol>
				<li value="7">然后<a id="_idIndexMarker455"/>我们运行<code>autoscaling_client.put_scaling_policy()</code>来指示<em class="italic">我们想要如何</em>自动缩放:<pre>response = autoscaling_client.put_scaling_policy(    PolicyName='Invocations-ScalingPolicy',    ServiceNamespace='sagemaker',    ResourceId=resource_id,     ScalableDimension='sagemaker:variant:DesiredInstanceCount',     PolicyType='TargetTrackingScaling',     TargetTrackingScalingPolicyConfiguration={        'TargetValue': 4000.0,         'PredefinedMetricSpecification': {           'PredefinedMetricType':               'SageMakerVariantInvocationsPerInstance'},         'ScaleInCooldown': 600,          'ScaleOutCooldown': 300})</pre></li>
			</ol>
			<p>在这个例子中，我们在这个配置中采用了一个名为<code>SageMakerVariantInvocationsPerInstance</code>的扩展策略，以确保在向外扩展另一个实例之前，每个实例每分钟可以共享 4，000 个请求。<code>ScaleInCooldown</code>和<code>ScaleOutCooldown</code>是指在自动缩放可以再次缩放之前，最后一次缩放活动之后的时间段<a id="_idIndexMarker457"/>，以秒为单位。使用我们的配置，SageMaker 不会在上次向内扩展活动的 600 秒内向内扩展(删除一个实例)，也不会在上次向外扩展活动的 300 秒内向外扩展(添加一个实例)。</p>
			<p class="callout-heading">注意</p>
			<p class="callout"><code>PolicyType</code>常用的<a id="_idIndexMarker459"/>高级缩放策略有<a id="_idIndexMarker458"/>两种:<strong class="bold">步进缩放</strong>和<strong class="bold">预定缩放</strong>。在步进缩放中，您可以根据警报<a id="_idIndexMarker460"/>违反特定指标的大小，定义要缩放的实例数量。在<a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html">https://docs . AWS . Amazon . com/auto scaling/ec2/user guide/as-scaling-simple-step . html</a>阅读更多关于步进缩放的信息。在计划缩放中，您可以根据计划设置缩放。如果流量是可预测的或者有一定的季节性，这是特别有用的。在<a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html">https://docs . AWS . Amazon . com/auto scaling/ec2/user guide/schedule _ time . html</a>上阅读<a id="_idIndexMarker461"/>有关计划缩放的更多信息。</p>
			<ol>
				<li value="8">我们可以用下面的代码来验证自动缩放策略的配置:<pre>response = autoscaling_client.describe_scaling_policies(          ServiceNamespace='sagemaker') for i in response['ScalingPolicies']:     print('')     print(i['PolicyName'])     print('')     if('TargetTrackingScalingPolicyConfiguration' in i):         print(i['TargetTrackingS calingPolicyConfiguration'])      else:         print(i['StepScalingPolicyConfiguration'])     print('') <strong class="bold">Invocations-ScalingPolicy</strong> <strong class="bold">{'TargetValue': 4000.0, 'PredefinedMetricSpecification': {'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance'}, 'ScaleOutCooldown': 300, 'ScaleInCooldown': 600}</strong></pre></li>
				<li>在<strong class="bold"> Amazon SageMaker Studio </strong>中，你可以很容易地在左侧栏的<strong class="bold">端点</strong>注册表中找到一个端点的详细信息，如图<em class="italic">图 7.3 </em>所示。如果双击某个端点上的<a id="_idIndexMarker462"/>，您可以在主工作区看到更多信息:</li>
			</ol>
			<div><div><img src="img/B17447_07_03.jpg" alt="Figure 7.3 – Discovering endpoints in SageMaker Studio&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图 7.3–在 SageMaker Studio 中发现端点</p>
			<p>托管端点的目的是在云中服务 ML 模型，以便您可以将 ML 作为微服务集成到您的应用程序或网站中。只要你的主要产品或服务可用，你的模型就必须随时可用。您可以想象，您有很大的机会和动力来优化部署，从而在保持性能的同时最大限度地降低成本。我们刚刚学习了如何在云中部署 ML 模型；我们还应该学习如何优化部署。</p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor104"/>优化您的模型部署</h1>
			<p>优化模型部署对于企业来说是一个至关重要的话题。没有人想在不必要的时候多花一分钱。因为部署的端点会被持续使用，并持续产生费用，所以确保部署在成本和运行时性能方面得到优化可以为您节省大量资金。SageMaker 有几个选项可以帮助您在优化运行时性能的同时降低成本。在本节中，我们将讨论多模型端点部署，以及如何为您的用例选择实例类型和自动扩展策略。</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor105"/>托管多模式终端以节省成本</h2>
			<p>多模型端点是 SageMaker 中的一种实时端点，它允许在同一个端点后面部署多个模型。在许多用例中，您将为每个客户或每个地理区域构建模型，并且根据输入数据点的特征，您将应用相应的 ML 模型。以我们在<a href="B17447_03_ePub_RK.xhtml#_idTextAnchor043"> <em class="italic">第三章</em></a><em class="italic">用 SageMaker 数据牧马人</em>进行数据准备中攻克的电信客户流失预测用例为例。如果我们按州训练它们，我们可以获得更准确的 ML 模型，因为在本地电信提供商之间的竞争方面可能存在区域差异。如果我们为美国每个州训练 ML 模型，你也可以很容易地想象每个模型的利用率可能不完全相等。事实上，恰恰相反。</p>
			<p>模型利用率必然与每个州的人口成正比。你的纽约模型将会比你的阿拉斯加模型被更频繁地使用。在这种情况下，如果您为每个状态托管一个端点，那么您将不得不为实例付费，甚至为利用率最低的端点付费。借助多模型端点，SageMaker 通过减少您的用例所需的端点数量来帮助您降低成本。让我们看看它是如何与电信客户流失预测用例一起工作的。请打开装有 Python 3(数据科学)内核的<code>Getting-Started-with-Amazon-SageMaker-Studio/chapter07/03-multimodel-endpoint.ipynb</code>笔记本，并按照以下步骤操作:</p>
			<ol>
				<li value="1">我们定义 SageMaker 会话，加载 Python 库，并在前三个单元中加载 churn 数据集。</li>
				<li>我们做<a id="_idIndexMarker467"/>最小的预处理，将二进制列从字符串转换成<code>0</code>和<code>1</code> : <pre>df[["Int'l Plan", "VMail Plan"]] = df[["Int'l Plan", "VMail Plan"]].replace(to_replace=['yes', 'no'], value=[1, 0]) df['Churn?'] = df['Churn?'].replace(to_replace=['True.', 'False.'], value=[1, 0])</pre></li>
				<li>我们留下 10%的数据用于稍后的最大似然推断:<pre>from sklearn.model_selection import train_test_split df_train, df_test = train_test_split(df_processed,          test_size=0.1, random_state=42, shuffle=True,           stratify=df_processed['State'])</pre></li>
				<li>准备好数据后，我们在函数<code>launch_training_job()</code>中设置我们的状态模式训练过程，并集成 SageMaker 实验。我们使用的训练算法是 SageMaker 内置的 XGBoost 算法，对于像这样的结构化数据是快速准确的。对于二元分类，我们使用<code>num_round</code>设置为<code>20</code> : <pre>def launch_training_job(state, train_data_s3, val_data_s3):     ...     xgb = sagemaker.estimator.Estimator(image, role,           instance_count=train_instance_count,           instance_type=train_instance_type,           output_path=s3_output,           enable_sagemaker_metrics=True,           sagemaker_session=sess)     xgb.set_hyperparameters(           objective='binary:logistic',           num_round=20)          ...         xgb.fit(inputs=data_channels,              job_name=jobname,              experiment_config=experiment_config,              wait=False)     return xgb</pre>的<code>binary:logtistic</code>物镜</li>
				<li>使用<code>launch_training_job()</code>，我们可以轻松地在一个<code>for</code>循环<a id="_idIndexMarker468"/>中为各州创建多个培训工作。出于演示的目的，我们在这个例子中只训练五个状态:<pre>dict_estimator = {} for state in df_processed.State.unique()[:5]:     print(state)     output_dir = f's3://{bucket}/{prefix}/{local_prefix}/by_state'     df_state = df_train[df_train['State']==state].drop(labels='State', axis=1)     df_state_train, df_state_val = train_test_split(df_state, test_size=0.1, random_state=42,                                                      shuffle=True, stratify=df_state['Churn?'])          df_state_train.to_csv(f'{local_prefix}/churn_{state}_train.csv', index=False)     df_state_val.to_csv(f'{local_prefix}/churn_{state}_val.csv', index=False)     sagemaker.s3.S3Uploader.upload(f'{local_prefix}/churn_{state}_train.csv', output_dir)     sagemaker.s3.S3Uploader.upload(f'{local_prefix}/churn_{state}_val.csv', output_dir)          dict_estimator[state] = launch_training_job(state, out_train_csv_s3, out_val_csv_s3)     time.sleep(2)</pre></li>
			</ol>
			<p>每项培训工作不应超过 5 分钟。在继续使用<code>wait_for_training_job_to_complete()</code>功能之前，我们将等待它们全部完成。</p>
			<ol>
				<li value="6">在<a id="_idIndexMarker469"/>培训完成后，我们最终部署我们的多模型端点。这与从一个经过训练的评估者对象向一个端点部署一个单一的模型有点不同。我们使用<code>sagemaker.multidatamodel.MultiDataModel</code>类进行部署:<pre>model_PA = dict_estimator['PA'].create_model(        role=role, image_uri=image) mme = MultiDataModel(name=model_name,                       model_data_prefix=model_data_prefix,        model=model_PA,        sagemaker_session=sess)</pre></li>
			</ol>
			<p><code>MultiDataModel</code>初始化需要了解通用的模型配置，比如容器镜像和网络配置，来配置端点配置。我们为<code>PA</code>传递模型。随后，我们将模型部署到一个<code>ml.c5.xlarge</code>实例，并配置<code>serializer</code>和<code>deserializer</code>分别将 CSV 作为输入并生成 JSON 作为输出:</p>
			<pre>predictor = mme.deploy(
       initial_instance_count=hosting_instance_count, 
       instance_type=hosting_instance_type, 
       endpoint_name=endpoint_name,
       serializer = CSVSerializer(),
       deserializer = JSONDeserializer())</pre>
			<ol>
				<li value="7">然后，我们可以动态地向端点添加模型。注意，此时，没有模型部署在端点之后:<pre>for state, est in dict_estimator.items():     artifact_path = est.latest_training_job.describe()['ModelArtifacts']['S3ModelArtifacts']     model_name = f'{state}.tar.gz'     mme.add_model(model_data_source=artifact_path,                    model_data_path=model_name)</pre></li>
			</ol>
			<p>就是这样。我们可以验证有五个模型与此端点相关联:</p>
			<pre>list(mme.list_models())
<strong class="bold">['MO.tar.gz', 'PA.tar.gz', 'SC.tar.gz', 'VA.tar.gz', 'WY.tar.gz']</strong></pre>
			<ol>
				<li value="8">我们可以用每个状态的一些数据点来测试端点。您可以使用<code>predictor.predict()</code> : <pre>state='PA' test_data=sample_test_data(state) prediction = predictor.predict(data=test_data[0],                                 target_model=f'{state}.tar.gz')</pre>中的<code>target_model</code>参数指定要进行推理的模型</li>
			</ol>
			<p>在本单元及以后的单元中，我们还设置了一个计时器来测量模型对其他状态做出响应所需的时间，以说明模型从 S3 到端点的动态加载的性质。第一次创建端点时，端点后面没有模型。使用<code>add_model()</code>，它仅仅将模型上传到 S3 的位置<code>model_data_prefix</code>。当第一次请求一个模型时，SageMaker 动态地将所请求的模型从 S3 下载到 ML 实例，并将其加载到推理容器中。当我们第一次为每个状态模型运行预测时，这个过程具有较长的响应时间，长达 1000 毫秒。但是一旦模型被加载到端点后容器的内存中，响应时间就会大大减少，大约为 20 毫秒。当一个模型被加载时，它被保存在容器中，直到由于一次加载太多的模型而耗尽了实例的内存。然后 SageMaker 从内存中卸载不再使用的模型，同时仍然将实例中的<code>model.tar.gz</code>保存在磁盘上，以供下一个请求使用，从而避免从 S3 下载。</p>
			<p>在这个<a id="_idIndexMarker471"/>示例中，我们展示了如何托管灵活且经济高效的 SageMaker 多模型端点，因为它极大地减少了您的用例所需的端点数量。因此，我们将只托管和支付一个端点，而不是托管和支付五个端点。这可以轻松节省 80%的成本。通过在一个端点而不是 50 个端点为美国 50 个州培训托管模型，节省了 98%的成本！</p>
			<p>使用 SageMaker 多模型端点，您可以在一个 S3 存储桶位置托管尽可能多的模型。您可以在一个端点中同时加载的模型数量取决于模型的内存占用和计算实例上的 RAM 数量。多模型端点适用于在同一个框架中构建模型的用例(在本例中为 XGBoost ),以及在不常用的模型上允许有延迟的用例。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">如果您有从不同 ML 框架构建的模型，例如，TensorFlow、PyTorch 和 XGBoost 模型的混合，您可以使用多容器端点，它允许托管多达 15 个不同的框架容器。多容器端点的另一个好处是，由于所有容器同时运行，它们没有延迟损失。更多信息请访问<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html">https://docs . AWS . Amazon . com/sage maker/latest/DG/multi-container-endpoints . html</a>。</p>
			<p>另一种优化方法是使用一种称为负载测试的技术来帮助我们选择实例和自动伸缩策略。</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor106"/>通过负载测试优化实例类型和自动伸缩</h2>
			<p>负载测试是一种<a id="_idIndexMarker473"/>技术，它允许我们理解我们的 ML 模型在一个具有计算资源配置的端点上如何响应在线流量。模型大小、ML 框架、CPU 数量、RAM 数量<a id="_idIndexMarker474"/>、自动缩放策略和<a id="_idIndexMarker475"/>流量大小等因素都会影响您的 ML 模型在云中的表现。可以理解的是，预测一段时间内有多少请求会到达一个端点并不容易。理解您的模型和端点在这种复杂情况下的行为是谨慎的。负载测试为您的端点创建人工流量和请求，并根据模型延迟、实例 CPU 利用率、内存占用等对您的模型和端点如何响应进行压力测试。</p>
			<p>在本节中，让我们用一些场景对我们在<code>chapter07/02-tensorflow_sentiment_analysis_inference.ipynb</code>中创建的端点运行一些负载测试。在本例中，我们将一个基于 TensorFlow 的模型托管到一个<code>ml.c5.xlarge</code>实例，该实例有 4 个 vCPUs 和 8 GiB 内存。</p>
			<p>首先，我们需要了解模型的延迟和容量，它是实例类型和端点不可用之前的实例数量的函数。然后，我们改变实例配置和自动扩展配置，直到达到所需的延迟和流量容量。</p>
			<p>请打开带有<code>ml.t3.xlarge</code>实例的<code>Getting-Started-with-Amazon-SageMaker-Studio/chapter07/04-load_testing.ipynb</code>笔记本，并遵循以下步骤:</p>
			<ol>
				<li value="1">我们使用名为<strong class="bold"> locust </strong>的 Python <a id="_idIndexMarker476"/>负载测试框架在 SageMaker Studio 中执行负载测试。先在笔记本里下载库吧。你可以在 https://docs.locust.io/en/stable/index.html 了解更多关于图书馆的信息。</li>
				<li>像往常一样，我们在第二个单元中设置了 SageMaker 会话。</li>
				<li>创建一个负载测试配置脚本<code>load_testing/locustfile.py</code>，这是 locust 所需要的。该脚本也在存储库中提供。该单元格会覆盖文件。在这个配置中，我们指示 locust 创建模拟用户(<code>SMLoadTestUser</code>类)来运行 SageMaker 端点(<code>test_endpoint</code>类函数)的模型推断，该端点由环境变量提供，并带有从<code>imdb_data/test/test.csv</code>加载的数据点。这里，响应时间<code>total_time</code>是以<strong class="bold">毫秒</strong> ( <strong class="bold">毫秒</strong>)来测量的。</li>
				<li>在下一个单元中，我们用一个<code>ml.c5.xlarge</code>实例在已经部署的 SageMaker 端点上开始我们的第一个负载测试工作。还记得我们在<code>chapter07/02-tensorflow_sentiment_analysis_inference</code>中应用了自动缩放策略吗？让我们首先通过将<a id="_idIndexMarker478"/> <code>MaxCapacity</code>设置为<code>1</code>来反转策略，以确保<a id="_idIndexMarker479"/>在我们的第一次测试中端点不会扩展到多个实例:<pre>sagemaker_client = sess.boto_session.client('sagemaker') autoscaling_client = sess.boto_session.client('application-autoscaling') endpoint_name = '&lt;endpoint-with-ml.c5-xlarge-instance&gt;' resource_id = f'endpoint/{endpoint_name}/variant/AllTraffic'  response = autoscaling_client.register_scalable_target(    ServiceNamespace='sagemaker',    ResourceId=resource_id,    ScalableDimension='sagemaker:variant:   DesiredInstanceCount',    MinCapacity=1,    MaxCapacity=1)</pre></li>
				<li>然后我们用 locust 测试端点。在下面的代码片段中，我们在两个 CPU 内核上设置了两个工作线程的分布式负载测试。我们指示<code>locust</code>每秒创建 10 个用户(<code>-r 10</code>参数)，最多 500 个在线用户(<code>-u 500</code>)，每个用户呼叫我们的端点 60 秒(<code>-t 60s</code>)。请用您的 SageMaker 端点名称替换<code>ENDPOINT_NAME</code>字符串。您可以在<strong class="bold">端点</strong>注册表中找到端点名称，如图<em class="italic">图 7.3 </em> : <pre>%%sh --bg export ENDPOINT_NAME='&lt;endpoint-with-ml.c5-xlarge-instance&gt;' bind_port=5557 locust -f load_testing/locustfile.py --worker --loglevel ERROR --autostart --autoquit 10 --master-port ${bind_port} &amp;  locust -f load_testing/locustfile.py --worker --loglevel ERROR --autostart --autoquit 10 --master-port ${bind_port} &amp; locust -f load_testing/locustfile.py --headless -u 500 -r 10 -t 60s \        --print-stats --only-summary --loglevel ERROR \        --autostart --autoquit 10 --master --expect-workers 2 --master-bind-port ${bind_port}</pre>所示</li>
			</ol>
			<p>由于<a id="_idIndexMarker480"/>正在运行，让我们使用您的端点名称将<a id="_idIndexMarker481"/>导航到<code>&lt;endpoint-with-ml.c5-xlarge-instance&gt;</code>，如果您使用的区域不是 us-west-2:   <code>https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'InvocationsPerInstance~'EndpointName~'&lt;endpoint-with-ml.c5-xlarge-instance&gt;~'VariantName~'AllTraffic)~(~'.~'ModelLatency~'.~'.~'.~'.~(stat~'Average))~(~'.~'Invocations~'.~'.~'.~'.)~(~</code> <code>'.~'OverheadLatency~'.~'.~'.~'.~(stat~'Average))~(~'.~'Invoca tion5XXErrors~'.~'.~'.~'.)~(~'.~'Invocation4XXErrors~'.~'.~'.~'.))~view~'timeSeries~stacked~false~region~'us-west-2~stat~'Sum~period~60~start~'-PT3H~end~'P0D );query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20&lt;endpoint-with-ml.c5-xlarge-instance&gt;</code>，请替换该区域</p>
			<p>在图 7.4 中可以看到一个仪表板。仪表板已经捕获了关于 SageMaker 端点的健康和状态的最重要的指标。<strong class="bold">调用</strong>和<strong class="bold">invocationspeinstance</strong>显示了调用总数和每个实例的计数。<strong class="bold"> Invocation5XXErrors </strong>和<strong class="bold"> Invocation4XXErrors </strong>分别是 HTTP 代码为 5XX 和 4XX 的错误计数。<strong class="bold"> ModelLatency </strong>(以微秒计)是 SageMaker 端点后面的容器中的模型返回响应所用的时间。<strong class="bold"> OverheadLatency </strong>(以微秒计)是我们的 SageMaker 端点传输一个请求和一个响应所花费的时间。请求的总等待时间是<strong class="bold">模型等待时间</strong>加上<strong class="bold">开销等待时间</strong>。这些指标由我们的 SageMaker 端点发送到 Amazon CloudWatch。</p>
			<div><div><img src="img/B17447_07_04.jpg" alt="Figure 7.4 – Viewing load testing results on one ml.c5.xlarge instance in Amazon CloudWatch&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图 7.4–在 Amazon CloudWatch 中查看一个 ml.c5.xlarge 实例的负载测试结果</p>
			<p>在第一个负载测试中(<em class="italic">图 7.4 </em>，我们可以看到每分钟大约有 8，221 次调用，0 次错误，平均<strong class="bold">模型延迟</strong>为<strong class="bold"> 53，825 </strong>微秒，即 53.8 毫秒。</p>
			<p>以<a id="_idIndexMarker483"/>这些数字为基准，让我们扩大实例，也就是说，让我们使用一个更大的实例。</p>
			<ol>
				<li value="6">我们加载之前的 IMDb 情感分析培训作业，并将 TensorFlow 模型部署到另一个具有一个<code>ml.c5.2xlarge</code>实例的端点，该实例具有 8 个 vCPU 和 16 GiB 内存，是<code>ml.c5.xlarge</code> : <pre>from sagemaker.tensorflow import TensorFlow training_job_name='&lt;your-training-job-name&gt;' estimator = TensorFlow.attach(training_job_name) predictor_c5_2xl = estimator.deploy(           initial_instance_count=1,            instance_type='ml.c5.2xlarge')</pre>的两倍</li>
			</ol>
			<p><a id="_idIndexMarker485"/>部署过程需要<a id="_idIndexMarker486"/>几分钟。然后，我们用下一个单元格<code>predictor_c5_2xl.endpoint_name</code>检索端点名称。</p>
			<ol>
				<li value="7">将<code>ENDPOINT_NAME</code>替换为<code>predictor_c5_2xl.endpoint_name</code>的输出，并运行单元来启动针对新端点的另一个负载测试:<pre>export ENDPOINT_NAME='&lt;endpoint-with-ml.c5-2xlarge-instance&gt;'</pre></li>
				<li>在 Amazon CloudWatch 中(替换<em class="italic">步骤 4 </em>中长 URL 中的<code>&lt;endpoint-with-ml.c5-xlarge- instance&gt;</code>或者点击笔记本中下一个单元格中生成的超链接)，我们可以看到<em class="italic">图 7.5 </em>中端点是如何响应流量的:</li>
			</ol>
			<div><div><img src="img/B17447_07_05.jpg" alt="Figure 7.5 – Viewing load testing results on one ml.c5.2xlarge instance in Amazon CloudWatch&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图 7.5–在 Amazon CloudWatch 中查看一个 ml . c 5.2 x 大型实例的负载测试结果</p>
			<p>类似地，locust 能够生成的流量大约是每分钟 8000 次调用(<code>ml.c5.xlarge</code>实例。</p>
			<ol>
				<li value="9">接下来，我们<a id="_idIndexMarker487"/>将同一个模型<a id="_idIndexMarker488"/>部署到一个<code>ml.g4dn.xlarge</code>实例，这是一个专用于模型推理用例的 GPU 实例。G4dn 实例配备英伟达 T4 GPU，对于 ML 推理和小型神经网络训练工作来说性价比很高:<pre>predictor_g4dn_xl = estimator.deploy(          initial_instance_count=1,                  instance_type='ml.g4dn.xlarge')</pre></li>
				<li>我们设置了一个与前面类似的负载测试作业。通过替换<em class="italic">步骤 4 </em>中的长 URL 中的<code>&lt;endpoint-with-ml.c5-xlarge- instance&gt;</code>，或者点击笔记本中下一个单元格中生成的超链接，也可以在 Amazon CloudWatch 仪表盘上找到结果。如<em class="italic">图 7.6 </em>所示，每分钟大约有 6000 次调用，平均<code>ml.g4dn.xlarge</code>实例做出推断的速度要快得多。</li>
			</ol>
			<div><div><img src="img/B17447_07_06.jpg" alt="Figure 7.6 – Viewing the load test results on one ml.g4dn.xlarge instance in Amazon CloudWatch&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图 7.6–在 Amazon CloudWatch 中查看一个 ml.g4dn.xlarge 实例的负载测试结果</p>
			<ol>
				<li value="11">我们应该尝试的最后一种方法是自动缩放。自动伸缩允许我们跨实例分布负载，这反过来有助于提高 CPU 利用率和模型延迟。我们再次使用以下单元格将自动缩放设置为<code>MaxCapacity=4</code>:<pre>endpoint_name = '&lt;endpoint-with-ml.c5-xlarge-instance&gt;' resource_id=f'endpoint/{endpoint_name}/variant/AllTraffic' response = autoscaling_client.register_scalable_target(    ServiceNamespace='sagemaker',    ResourceId=resource_id,    ScalableDimension='sagemaker:variant:DesiredInstanceCount',    MinCapacity=1,    MaxCapacity=4)</pre></li>
			</ol>
			<p>您可以确认笔记本中下一个单元格附带的缩放策略。</p>
			<ol>
				<li value="12">我们已经<a id="_idIndexMarker491"/>准备好执行我们最后的<a id="_idIndexMarker492"/>负载测试实验。用<code>&lt;endpoint-with-ml.c5-xlarge-instance&gt;</code>替换<code>ENDPOINT_NAME</code>，并运行下一个单元，开始对现在能够向外扩展到四个实例的端点进行负载测试。这个负载测试需要运行更长时间才能看到自动缩放的效果。这是因为 SageMaker 首先需要观察调用的数量，以根据我们的目标指标<code>SageMakerVariantInvocationsPerInstance=4000</code>来决定有多少新实例。我们的流量大约为每分钟 8，000 次调用，SageMaker 将增加一个实例，使每个实例的调用达到期望值 4，000 次。启动新实例大约需要 5 分钟才能完成。</li>
			</ol>
			<div><div><img src="img/B17447_07_07.jpg" alt="Figure 7.7 – Viewing load testing results on an ml.c5.xlarge instance with autoscaling in Amazon CloudWatch&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图 7.7–在 Amazon CloudWatch 中查看带有自动缩放的 ml.c5.xlarge 实例上的负载测试结果</p>
			<p>我们可以在亚马逊 CloudWatch 仪表盘上看到负载测试结果，如图<em class="italic">图 7.7 </em>所示。我们可以在图表中看到一个有趣的模式。我们可以清楚地看到<code>18:48</code>和<code>18:49</code>之间发生的事情<a id="_idIndexMarker494"/>。<code>33,839</code>微秒(33.8 毫秒)。和<code>SageMakerVariantInvocationsPerInstance=4000</code>并将流量分成两个实例。较低的<strong class="bold">模型延迟</strong>是让多个实例分担负载的首选结果。</p>
			<p>在四个负载测试实验之后，我们可以得出结论，在大约每分钟 6，000 到 8，000 次调用的负载下，会发生以下情况:</p>
			<ul>
				<li>单实例性能通过 1 个 GPU 和 4 个 vcpu 的平均<code>ml.g4dn.xlarge</code>来衡量，最小的<code>ml.c5.2xlarge</code>实例使用 8 个 vcpu，耗时 45.8 毫秒。最后是具有 4 个 vCPUs 的<code>ml.c5.xlarge</code>实例，耗时 53.8 毫秒。</li>
				<li>通过自动缩放，两个具有 8 个 vcpu 的<code>ml.c5.xlarge</code>实例在具有相同数量的 vcpu 的情况下实现了 33.8 毫秒的'<code>ml.c5.2xlarge</code>。</li>
			</ul>
			<p>如果我们<a id="_idIndexMarker495"/>考虑另一个维度，实例的成本<a id="_idIndexMarker496"/>，我们可以得出一个更有趣的情况，如图<em class="italic">图 7.8 </em>所示。在表中，我们创建了一个简单的复合指标来衡量配置的性价比，方法是将<strong class="bold">模型延迟</strong>乘以实例配置的每小时价格。</p>
			<div><div><img src="img/B17447_07_08_table.jpg" alt="Figure 7.8 – Cost-performance comparisons&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图 7.8-性价比比较</p>
			<p>如果我们受到成本的限制，我们应该考虑使用最后一种配置(d 行)，这种配置的月成本最低，性价比第二高，同时牺牲了一些模型延迟。如果我们需要大约 40 毫秒或更低的模型延迟，通过支付相同的月成本，我们将从我们的<a id="_idIndexMarker497"/>降压中获得更大的收益，并且与第二配置(b 行)相比，<a id="_idIndexMarker498"/>第三配置(c 行)的延迟更低。第一种配置(a 行)提供了最佳的模型延迟和最佳的性价比。但这也是最昂贵的选择。除非有严格的个位数模型延迟要求，否则我们可能不想使用这个选项。</p>
			<p>为了降低成本，当您完成示例时，请确保取消注释并运行<code>02-tensorflow_sentiment_analysis_inference.ipynb</code>、<code>03-multimodel-endpoint.ipynb</code>和<code>04-load_testing.ipynb</code>中的最后一个单元格，以删除端点，从而停止向您的 AWS 帐户收取费用。</p>
			<p>这个讨论基于我们使用的例子，这个例子假设了许多因素，比如模型框架、流量模式和实例类型。您应该遵循我们为您的用例介绍的最佳实践，并测试更多的实例类型和自动伸缩策略，以找到您的用例的最佳解决方案。你可以在 https://aws.amazon.com/sagemaker/pricing/<a href="https://aws.amazon.com/sagemaker/pricing/">的<strong class="bold">实时推断</strong>选项卡中找到实例、规格和每小时价格的完整列表，以得出你自己的性价比效率分析。</a></p>
			<p>SageMaker 中还有其他优化功能可以帮助您减少延迟，例如亚马逊<a id="_idIndexMarker499"/>弹性推理、SageMaker Neo 和亚马逊 EC2 Inf1 实例。<strong class="bold">弹性推理</strong>(<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/ei-endpoints.html">https://docs . AWS . Amazon . com/sagemaker/latest/DG/ei-endpoints . html</a>)将分数 GPU 附加到 sage maker 托管的端点。它增加了推理吞吐量，并减少了深度学习模型的模型延迟，可以从 GPU <a id="_idIndexMarker500"/>加速中受益。<strong class="bold">sage maker Neo</strong>(<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html">https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html</a>)优化了一个 ML 模型，用于云中的推理和边缘支持的设备，而不会损失准确性。SageMaker Neo 通过 SageMaker hosted endpoint 中的编译模型和优化容器加快预测速度并降低成本。<strong class="bold">亚马逊 EC2 Inf1 实例</strong>(<a href="https://aws.amazon.com/ec2/instance-types/inf1/">https://aws.amazon.com/ec2/instance-types/inf1/</a>)通过<strong class="bold"> AWS 推理</strong>芯片在云中提供高性能<a id="_idIndexMarker501"/>和低成本，该芯片由<a id="_idIndexMarker502"/>设计并由 AWS 构建，用于 ML 推理目的。您可以使用 SageMaker Neo 编译支持的 ML 模型，并选择 Inf1 实例以在 SageMaker 托管端点中部署编译的模型。</p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor107"/>总结</h1>
			<p>在这一章中，我们学习了如何使用 Amazon SageMaker 在云中高效地进行 ML 推理。我们继续使用我们在上一章中培训的内容——IMDb 电影评论情绪预测——来演示 SageMaker 的批量转换和实时托管。更重要的是，我们学会了如何通过负载测试优化成本和模拟延迟。我们还了解了另一个节省成本的好机会，即使用 SageMaker 多模型端点在一个端点上托管多个 ML 模型。一旦您为您的用例选择了最佳的推理选项和实例类型，SageMaker 就会使部署您的模型变得简单明了。有了这些循序渐进的指导和这次讨论，你将能够把你所学到的转化为你自己的 ML 用例。</p>
			<p>在下一章中，我们将采取不同的路线来学习如何使用 SageMaker 的 JumpStart 和 Autopilot 来快速启动您的 ML 之旅。SageMaker JumpStart 提供解决方案，帮助您了解最佳实践和 ML 用例是如何处理的。JumpStart 模型动物园收集了大量预先训练好的深度学习模型，用于自然语言处理和计算机视觉用例。SageMaker Autopilot 是一个 autoML 功能，它处理数据并训练一个性能模型，而无需您担心数据、编码或建模。在我们了解了 SageMaker 的基础知识(完全管理的模型训练和模型托管)之后，我们可以更好地理解 SageMaker JumpStart 和 Autopilot 的工作原理。</p>
		</div>
	

</body></html>