<title>Decision Trees and Ensemble Learning</title>  

# 决策树和集成学习

在这一章中，我们将讨论二元决策树和集成方法。尽管它们可能不是最常见的分类方法，但它们提供了很好的简单性，可以用于许多不需要很高复杂度的任务。当需要展示决策过程如何运作时，它们也非常有用，因为它们基于一种结构，可以很容易地在演示中显示出来，并一步一步地描述。

集成方法是复杂算法的强大替代方法，因为它们试图利用多数投票的统计概念。许多弱学习者可以被训练来捕捉不同的元素并做出他们自己的预测，这不是全局最优的，但使用足够数量的元素，从统计上看，大多数人可能会正确评估。特别是，我们将讨论决策树的随机森林和一些用于略有不同的算法的提升方法，这些算法可以通过关注错误分类的样本或通过持续最小化目标损失函数来优化学习过程。

特别是，我们将讨论以下内容:

*   二叉决策树的主要结构
*   最常见的杂质测量
*   决策树回归
*   集成学习介绍(随机森林、AdaBoost、梯度树提升和投票分类器)

<title>Binary Decision Trees</title>  

# 二元决策树

二叉决策树是一种基于顺序决策过程的结构。从根开始，评估一个特征并选择两个分支中的一个。重复这个过程，直到到达最后一片叶子，这通常代表我们正在寻找的分类目标。决策树的第一个公式被称为**迭代二分法 3 (ID3)** ，它需要分类特征。这种情况限制了它的使用，并导致了 **C4.5** 的发展，它也可以管理连续的(但装箱和离散的)值。此外，C4.5 也因其将树转换成一系列条件表达式的能力而为人所知(`if <condition>`然后<...> `else <...>`)。在本书中，我们将讨论最新的发展，它被称为**分类和回归树** ( **CART** )。这些类型的树可以管理分类和数字特征，可以在分类或回归任务中使用，并且不使用任何规则集作为内部表示。

考虑到其他算法，决策树在动力学上似乎更简单。然而，如果数据集在保持内部平衡的同时是可分裂的，那么整个过程是直观的，并且在预测中相当快。此外，决策树可以有效地处理非规范化数据集，因为它们的内部结构不受每个特征所采用的值的影响。在下图中，有来自非标准化二维数据集的图，以及使用逻辑回归和决策树获得的交叉验证分数:

![](img/7abfdf84-77e5-4f5e-a900-316228c74775.png)

具有不同方差(左)的数据集示例，以及逻辑回归和决策树的交叉验证分数(右)

决策树总是达到接近 **1.0** 的分数，而逻辑回归的平均值略大于 **0.6** 。然而，如果没有适当的限制，决策树可能会增长到每个节点中只存在一个样本(或者数量非常少)。这种情况导致模型过度拟合，树变得无法正确概括。使用一致的测试集或交叉验证，以及最大允许深度，可以帮助避免这个问题。在专门讨论用 scikit-learn 进行*决策树分类的部分*我们将讨论如何限制树的增长。另一个需要考虑的重要因素是阶级平衡。决策树对不平衡的类很敏感，当一个类占主导地位时会产生很差的准确性。为了缓解这个问题，可以采用在[第 2 章](dd32b61f-bc53-4e94-8c17-d5910a8e528d.xhtml)、*机器学习中的重要元素*中讨论的重采样方法之一，或者使用由 scikit-learn 实现提供的`class_weight`参数。这样，占主导地位的阶层可以受到相应的惩罚，从而避免偏见。

<title>Binary decisions</title>  

# 二元决策

让我们考虑一个输入数据集， *X* :

![](img/01198bf3-654d-4039-a1d4-3f0ea343d73c.png)

每个向量由 *m* 个特征组成，因此它们中的每一个都是基于元组(特征，阈值)创建节点的候选:

![](img/e6900ac2-6073-4527-b0bf-31b085831cb5.png)

单一分裂节点

根据特征和阈值，树的结构将改变。凭直觉，我们应该选择最能区分数据的特征。换句话说，一个完全分离的特性将只存在于一个节点中，而两个后续的分支将不再基于它。这些条件保证了树向最终叶子的收敛，其不确定性被最小化。然而，在实际问题中，这通常是不可能的，因此有必要找到一个特性，以最大限度地减少后续决策步骤的数量。

例如，让我们考虑这样一个学生班级，其中所有的男生都是深色头发，所有的女生都是金黄色头发，而两个子集都有不同大小的样本。如果我们的任务是确定类的组成，我们可以从以下细分开始:

![](img/9f260c99-92a5-4976-9133-e2a45f20b948.png)

具有包含另一个分割节点(深色)的分支的分割节点(长度)

不过，**深色？** block 将包含男性和女性(这是我们要分类的目标)。这个概念用术语*纯度*来表达(或者，更常见的是，它的反义词*杂质*)。理想的情况是基于杂质为零的节点，因此所有后续决策将仅针对剩余特征做出。在我们的例子中，我们可以简单地从色块开始:

![](img/8cb77c15-8268-4ab5-aa31-b6429090b342.png)

最佳分裂节点的例子——子节点的杂质被最小化

根据颜色特征，两个结果集现在是纯的，这对于我们的任务来说就足够了。如果我们需要更进一步的细节，比如头发长度，必须添加其他节点；他们的不纯不会是空的，因为我们知道，比如说，既有**男**又有**女**长头发的学生。

更正式地说，假设我们将选择元组定义如下:

![](img/980083ab-0955-4829-b81f-213eeacd2e1d.png)

在这里，第一个元素是我们希望用来在某个节点分割我们的数据集的特征的索引(它将是整个数据集，只有在开始时；每一步后，样本数减少)，而第二步是确定左右分支的阈值。最佳阈值的选择是一个基本要素，因为它决定了树的结构，从而决定了树的性能。目标是以最少的分裂次数减少残留杂质，从而在样本数据和分类结果之间形成非常短的决策路径。

我们还可以通过考虑以下两个分支来定义总杂质含量:

![](img/3cd4e3dc-be53-4332-b29f-b34bdaa8e680.png)

这里， *D* 是所选节点的整个数据集， *D [左]* 和 *D [右]* 是结果子集(通过应用选择元组)，而 *I* 表示杂质度量。

<title>Impurity measures</title>  

# 杂质测量

为了定义最常用的杂质测量，我们需要考虑目标类别的总数:

![](img/588d19a4-0956-4ce1-8869-e8c3b5d9d873.png)

在某个节点 *j* 中，我们可以定义概率*P(y = I | Node = j)***其中 *i* 是与每个类相关联的索引*【0，P-1】*。换句话说，根据 frequentist 方法，该值是属于类别 *i* 并分配给节点 *j* 的样本数量与属于所选节点的样本总数之间的比率:**

 **![](img/506507e6-f4c8-4cdc-bfc9-0225df177d74.png)

<title>Gini impurity index</title>  

# 基尼杂质指数

基尼不纯指数定义如下:

![](img/e49094f2-6ac6-4ef5-bb3e-28033899e86a.png)

在这里，总和总是扩展到所有类。这是一个非常常见的度量，它被 scikit-learn 用作默认值。给定一个样本，如果使用分支的概率分布随机选择一个标签，基尼系数衡量错误分类的概率。当一个节点的所有样本被分类到一个类别中时，该指数达到其最小值 *(0.0)* 。

<title>Cross-entropy impurity index</title>  

# 交叉熵杂质指数

交叉熵度量定义如下:

![](img/192af3c4-ffa6-455d-9dda-18674b806e2e.png)

该度量基于信息论，当属于单个类的样本出现在分割中时假设为空值，而当类之间均匀分布时该度量最大(这是决策树中最差的情况之一，因为这意味着在最终分类之前仍有许多决策步骤)。该指数与基尼系数非常相似，尽管更正式地说，交叉熵允许您选择使分类不确定性最小化的分割，而基尼系数使错误分类的概率最小化。

在第二章、*机器学习中的重要元素*中，我们定义了互信息的概念，*I(X；Y) = H(X) - H(X|Y)* ，作为两个变量共享的信息量，或者说我们通过 *Y* 的知识可以获得的关于 *X* 的信息量。考虑数据生成过程 *p(x)* 和条件概率 *p(x|y)* 也很有帮助。相对于条件变量 *y* ，很容易证明互信息是它们之间的 Kullback-Leibler 散度的期望值:

![](img/2b1d47a0-dca0-44c7-8ba0-86449ee5608e.png)

因此，这就是当*我(X；Y) → 0* ， *p(x) ≈ p(x|y)* 和 *Y* 的知识并不能提供任何关于 *X* 的有用信息。反之，更高的*I(X；Y)* 值意味着边际分布 *p(x)* 和条件分布 *p(x|y)* 之间的平均强背离。条件概率的定义可以提供更好的洞察力:

![](img/6d122315-a135-4278-baf4-ae6b6e20cf3a.png)

这里，这两个变量在统计上不是独立的，并且关于 *Y* 的知识必须提供关于 *X* 的比例信息。我们可以用这个概念来定义分裂提供的**信息增益**(形式上相当于互信息):

![](img/8d0a50b3-6341-4981-a140-5ded22cf99dc.png)

在这种情况下，我们感兴趣的是父节点和子节点的分布。因此，当生长一棵树时，我们从选择提供最高信息增益的分裂开始。这保证了最大限度地减少后续节点的杂质，因为一般来说，最相关的特征在开始时被选择，而下一个特征被保留用于微调。树会继续生长，直到满足以下条件之一:

*   所有节点都是纯的
*   信息增益为零
*   已达到最大深度

<title>Misclassification impurity index</title>  

# 错误分类杂质指数

错误分类杂质指数是最简单的指数，定义如下:

![](img/e7f5347e-0898-476f-85af-127fba65fa5f.png)

解释很简单，但不幸的是，就质量性能而言，这个指数不是最佳选择，因为它对不同的概率分布不是特别敏感。在所有这些情况下，基尼系数或交叉熵指数是最自然的选择。

<title>Feature importance</title>  

# 特征重要性

使用多维数据集生成决策树时，评估每个要素在预测输出值中的重要性会很有用。在第 3 章、*特征选择和特征工程*中，我们讨论了一些通过仅选择最重要的特征来降低数据集维度的方法。决策树提供了一种不同的方法，它基于由每个单一特征确定的杂质减少。特别地，考虑一个特征， *x ^((i))* ，其重要性可以确定如下:

![](img/d8671e5f-fb21-4731-b686-9743f79b6a9e.png)

求和扩展到使用 *x ^((i))* 的所有节点，并且 *N [k]* 是到达该节点的样本数， *k* 。因此，重要性是计算的所有杂质减少量的加权和，仅考虑使用特征分割它们的节点。如果采用基尼不纯指数，这个指标也叫做**基尼重要度**。

<title>Decision Tree classification with scikit-learn</title>  

# 用 scikit-learn 进行决策树分类

scikit-learn 库包含了`DecisionTreeClassifier`类，它可以训练一个带有基尼和交叉熵杂质度量的二叉决策树。在我们的示例中，让我们考虑一个具有`3`要素和`3`类的数据集:

```
from sklearn.datasets import make_classification

nb_samples = 500

X, Y = make_classification(n_samples=nb_samples, n_features=3, n_informative=3, n_redundant=0, n_classes=3, n_clusters_per_class=1)
```

首先，让我们考虑一个带有默认基尼系数的分类:

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score

dt = DecisionTreeClassifier()
print(cross_val_score(dt, X, Y, scoring='accuracy', cv=10).mean())
0.970
```

一个非常有趣的特性是能够以`graphviz`格式导出树并将其转换成 PDF。

Graphviz 是一款免费工具，可以从 http://www.graphviz.org[下载。或者，也可以使用免费网站](http://www.graphviz.org)[http://www.webgraphviz.com](http://www.webgraphviz.com)，它可以让你即时绘制图表。然而，在这种情况下，需要将导出文件的内容(明文)粘贴到文本框中。

要导出训练好的树，需要使用内置的`export_graphviz()`函数:

```
from sklearn.tree import export_graphviz

dt.fit(X, Y)
with open('dt.dot', 'w') as df:
    df = export_graphviz(dt, out_file=df, 
                         feature_names=['A','B','C'], 
                         class_names=['C1', 'C2', 'C3'])
```

在这种情况下，我们使用`A`、`B`和`C`作为特性名，使用`C1`、`C2`和`C3`作为类名。创建文件后，可以使用以下命令行工具将其转换为 PDF(我们假设可执行文件已经安装在`<Graphviz Home>`文件夹中):

```
<Graphviz Home>/bindot -Tpdf dt.dot -o dt.pdf
```

我们示例中的图形相当大，因此在下图中，您只能看到分支的一部分:

![](img/ac129354-8ce8-4a1a-90b6-1d03a535267d.png)

使用 Graphviz 可视化的决策树示例

如您所见，有两种节点:

*   非终端，包含分裂元组(作为 *< =阈值*特征)和正杂质测量
*   终端，其中杂质测量为空，并且存在最终目标类别

在这两种情况下，您都可以随时检查样本数量。这种图表对于理解需要多少决策步骤非常有用。不幸的是，即使过程相当简单，数据集结构也可能导致非常复杂的树，而其他方法可以立即找出最合适的类。当然，并不是所有的特性都同样重要。如果我们考虑树的根和第一个节点，我们会发现分离大量样本的特征；因此，它们的重要性必须高于所有终端节点，在终端节点，样本的剩余数量是最小的。在 scikit-learn 中，可以在训练模型后评估每个特征的基尼系数重要性:

```
print(dt.feature_importances_)
[ 0.12066952,  0.12532507,  0.0577379 ,  0.14402762,  0.14382398,
  0.12418921,  0.14638565,  0.13784106]

print(np.argsort(dt.feature_importances_))
[2, 0, 5, 1, 7, 4, 3, 6]
```

下图显示了每个特征的重要性:

![](img/beb266a4-e491-466e-8081-90502d7675ee.png)

所有特征的重要性图

最重要的特征是 **6** 、 **3** 、 **4** 和 **7** ，而例如特征 **2** 分离非常少量的样本，并且可以被认为对于分类任务是不提供信息的。

就效率而言，也可以使用`max_depth`参数修剪一棵树。然而，理解哪个值是最好的并不总是简单的(网格搜索和交叉验证可以帮助完成这项任务)。当然，避免训练集上的过度专门化(这会导致模型的过度拟合)是极其重要的，因此通常需要在某个级别上砍掉这棵树。通过这种方式，更容易在训练和验证准确性之间找到一个良好的平衡(如果模型不是太大，可以手动测试不同的值并选择最佳值)。下图显示了训练和验证分数的常见行为:

![](img/f10863e7-639b-4871-b141-f4d365463a37.png)

作为树的最大深度的函数绘制的训练和验证分数(R ² 系数)

很容易理解树容易适配过度。实际上，当**最大深度**达到 **14** 时，训练精度为 **1.0** 。不幸的是，这种情况对应于验证准确性的成比例下降。最佳值(给定整体现有配置)是`max_depth=3`，这导致最高的验证精度。每当性能不可接受时，数据科学家应该测试不同的杂质测量，增加训练集大小(也使用数据扩充技术)，并调整所有其他超参数(可能使用网格搜索)。如果这些策略都没有产生预期的结果，就应该考虑另一种模式。

有时，与其直接处理最大深度，不如决定每次分割时要考虑的最大要素数。`max_features`参数可用于此目的:

*   如果它是一个数字，则每次分割时都会直接考虑该值
*   如果是`'auto'`或`'sqrt'`，将采用特征数量的平方根
*   如果是`'log2'`，将使用对数(以 2 为底)
*   如果是`'None'`，将使用所有功能(这是默认值)

一般来说，当功能总数不太高时，默认值是最佳选择，尽管当太多的功能会相互干扰，降低效率时，引入小的压缩(通过`sqrt`或`log2`)是有用的。另一个对控制性能和效率都有用的参数是`min_samples_split`，它指定了一次分割要考虑的最小样本数。下面的代码片段显示了一些示例:

```
print(cross_val_score(DecisionTreeClassifier(), X, Y, scoring='accuracy', cv=10).mean())
0.77308070807080698

print(cross_val_score(DecisionTreeClassifier(max_features='auto'), X, Y, scoring='accuracy', cv=10).mean())
0.76410071007100711

print(cross_val_score(DecisionTreeClassifier(min_samples_split=100), X, Y, scoring='accuracy', cv=10).mean())
0.72999969996999692
```

正如我们已经解释过的，寻找最佳参数通常是一项困难的任务，执行这项任务的最佳方式是执行网格搜索，同时包括所有可能影响精度的值。

对之前的集合使用逻辑回归(仅用于比较)，我们得到以下结果:

```
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
print(cross_val_score(lr, X, Y, scoring='accuracy', cv=10).mean())
0.9053368347338937
```

所以分数更高，正如我们所料。然而，最初的数据集非常简单，并且基于每个类有一个聚类的概念。这允许更简单和更精确的线性分离。如果我们考虑具有更多变量和更复杂结构(很难被线性分类器捕获)的稍微不同的场景，我们可以比较线性回归和决策树的**接收器操作特性** ( **ROC** )曲线:

```
nb_samples = 1000

X, Y = make_classification(n_samples=nb_samples, n_features=8, n_informative=6, n_redundant=2, n_classes=2, n_clusters_per_class=4)
```

得到的 ROC 曲线如下面的屏幕截图所示:

![](img/21c7f0e8-25fc-4552-a016-b45e1a45900b.png)

比较决策树和逻辑回归性能的 ROC 曲线

使用 MNIST 数字数据集中最常见的参数进行网格搜索，我们可以得到以下结果:

```
from sklearn.model_selection import GridSearchCV

param_grid = [
 { 
   'criterion': ['gini', 'entropy'],
   'max_features': ['auto', 'log2', None],
   'min_samples_split': [ 2, 10, 25, 100, 200 ],
   'max_depth': [5, 10, 15, None]
 }
]

gs = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=param_grid,
                  scoring='accuracy', cv=10, n_jobs=multiprocessing.cpu_count())

gs.fit(digits.data, digits.target)
GridSearchCV(cv=10, error_score='raise',
       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini',      max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter='best'),
       fit_params={}, iid=True, n_jobs=8,
       param_grid=[{'max_features': ['auto', 'log2', None], 'min_samples_split': [2, 10, 25, 100, 200], 'criterion': ['gini', 'entropy'], 'max_depth': [5, 10, 15, None]}],
       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,
       scoring='accuracy', verbose=0)

print(gs.best_estimator_)
DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter='best')

print(gs.best_score_)
0.8380634390651085
```

在这种情况下，对准确性影响最大的因素是要考虑拆分的最小样本数。这是合理的，考虑到这个数据集的结构和需要有许多分支来捕捉甚至很小的变化。

<title>Decision Tree regression</title>  

# 决策树回归

决策树也可以用来解决回归问题。然而，在这种情况下，有必要考虑一种稍微不同的分割节点的方式。考虑到节点的平均预测，最常见的选择之一不是考虑杂质测量，而是选择最小化**均方误差** ( **MSE** )的特性。让我们假设一个节点， *i* ，包含 *m* 个样本。平均预测如下:

![](img/77010d34-da7c-431e-bf96-e43a8380d536.png)

此时，该算法必须寻找所有的二进制分裂，以便找到最小化目标函数的分裂:

![](img/ee1fc4a6-3e85-40d5-8794-322407b3fc39.png)

类似于分类树，重复该过程，直到 MSE 低于固定阈值 *λ* 。即使不正确，当一个节点的预测精度较低时，我们也可以考虑不可接受的杂质水平。事实上，在分类树中，不纯节点包含不止一个类，并且不确定性可能太高而无法做出合理的决定。同样，一个节点的 *MSE >* *λ* 对于正确的输出仍然过于*不确定*意味着需要进一步的分裂。另一种方法是基于**平均绝对误差** ( **MAE** )，但在大多数情况下，MSE 是最佳选择。

像往常一样，我总是推荐使用网格搜索和交叉验证来寻找最优的超参数。事实上，当特性数量很大时，有时很难立即找到最佳配置。此外，根据选定的训练子集，每个数据集的结构可能导致非常不同的验证准确性(正如我们将要看到的)。在特定问题中，了解训练集是否代表实际的数据生成过程(例如，在领域专家的帮助下)是有帮助的，以避免导致不同训练和测试分布的排除。另一个需要解决的常见问题是过度拟合。非常深的树可以完美地映射训练集，同时显示出较差的验证性能。因此，不要忘记在 CV 网格搜索中包含`max_depth`参数，并选择提供最佳折衷的解决方案。

<title>Example of Decision Tree regression with the Concrete Compressive Strength dataset</title>  

# 混凝土抗压强度数据集的决策树回归示例

在本例中，我们将使用由 Yeh 免费提供给 UCI 知识库的混凝土抗压强度数据集(最初用于论文*使用人工神经网络对***高性能混凝土的强度建模*，*叶宜成*，*水泥与混凝土研究*，*第 28 卷*，*第 12 期*，*第 17 页)数据集由 1030 个样本组成，具有八个自变量(`Cement`、`Blast furnace slag`、`Fly ash`、`Water`、`Superplasticizer`、`Coarse aggregate`、`Fine aggregate`和`Age`)和一个因变量(混凝土抗压强度)。**

 *一旦下载了 Excel 文件(假设它已经存储在`<DATA_HOME>`文件夹中)，就可以使用`pandas`对其进行解析，以获得实际的数据集(`X`、`Y`):

```
import pandas as pd

file_path = '<DATA_HOME>/Concrete_Data.xls'

df = pd.read_excel(file_path, header=0)

X = df.iloc[:, 0:8].values
Y = df.iloc[:, 8].values
```

前五个样本以漂亮的格式显示在下表中:

![](img/1fd383ff-7ab9-4db6-b236-adeeaf10613a.png)

混凝土抗压强度数据集的前五个样本

用户可以自由检查统计属性，但我们已经讨论了决策树对不同尺度的不敏感性，因此我们不打算对数据进行预处理。该数据集包含非常具体的*区域*，其特征在于不常见的行为，因此第一步是使用带有`max_depth=11`(我邀请读者测试其他值)的`DecisionTreeRegressor`实例和 MSE 标准(默认标准)检查 CV 分数(*R²，因为这是一个回归):*

```
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score

print(cross_val_score(DecisionTreeRegressor(criterion='mse', max_depth=11, random_state=1000), X, Y, cv=20))

[ 0.20764059  0.55004291  0.13532372  0.94144166  0.53860366  0.67753093

  0.48176233  0.39555753  0.15537892  0.05613209  0.68861262  0.27333756

  0.05999872  0.74014659  0.78379972  0.8200266   0.88995849  0.9468295

  0.8325872   0.98061205]
```

正如可能看到的，12 个褶皱有*R²0.5*(8 个有*R²0.7*，而其余的则表明预测更不准确(最差的情况是两个褶皱有 *R ² ≈ 0.05* )。这意味着并非所有随机测试子集选择都具有与训练集相同的分布。在这些情况下，深入了解数据集的结构非常重要。因此，有必要进行统计汇总(使用在数据帧上调用的 pandas `describe()`命令获得):

![](img/eb004ea5-21e7-46f9-9848-7b27af190cbb.png) 

混凝土抗压强度数据集的统计汇总

很明显，在很大一部分样本中，有一些成分是零。例如，**粉煤灰**的第 50 个^(百分位数等于 0，而**高炉渣**和**高效减水剂**的第 25 个^(百分位数等于 0。简单的统计表明，566 个样品具有零**飞灰**特征，466 个样品具有零**高炉渣**。因此，对于任何随机分割，几乎不可能有统一的行为，在一些极端的场景中，一个或多个特征经常为空，因此预测是错误的。正如我们已经指出的，在这些情况下，有必要获得一个代表原始数据生成过程的测试。对于我们的例子，我们将使用`random_state=1000`选择 200 个测试样本(不同的值可以产生其他结果)，并且我们将使用`max_depth=11`训练一个`DecisionTreeRegressor`实例:))

```
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=200, random_state=1000)

dtr = DecisionTreeRegressor(criterion='mse', max_depth=11, random_state=1000)
dtr.fit(X_train, Y_train)
```

在评估模型之前，使用 Graphviz 可视化树的一小部分是有帮助的(我们将输出文件存储在`<DATA_HOME>`文件夹中):

```
from sklearn.tree import export_graphviz

graphviz_path = '<DATA_HOME>/Concrete_Data.dot'

export_graphviz(dtr, out_file=graphviz_path, 
                feature_names=['Cement','Blast furnace slag', 'Fly ash','Water',
                               'Superplasticizer','Coarse Aggregate','Fine Aggregate','Age'])
```

该树非常宽，但下图显示了一个完整的分支:

![](img/492b479d-c6cd-4cbf-87a3-48ad28233363.png)

最终决策树的分支

如你所见，顶层节点(让我们把它当作根)有一个 **mse = 2.572** 。所有后续分割都试图减少 MSE 或属于该节点的样本数。在理想情况下(没有过度拟合的风险)，每片叶子应该包含一个 MSE ≈ 0 的样本。在这种情况下，有许多叶子满足这个条件，但也有其他叶子的 MSE 为空，但它们包含 **2** 和 **4** 样本(在这些情况下，取平均值)。这个图还应该显示决策树的实际内部复杂性(这是它的特征之一)。根只有 16 个样本，需要 5 级才能到达叶。这就是为什么这些类型的模型很容易过度拟合:如果没有明确控制它们的增长，它们可能会变得非常深，几乎没有泛化能力。

此时，我们可以开始评估模型的性能:

```
print(dtr.score(X_train, Y_train))
0.988068139046

print(dtr.score(X_test, Y_test))
0.821126631614
```

*R ²* 训练分数极高(几乎 99%)，而验证分数约为 82%(这是相当正面的结果)。**原始数据集** ( `Y`)与**预测**的对比如下图所示:

![](img/e1ca69c1-dbe5-4eb8-9aaf-fd052343841b.png)

原始抗压强度(上图)和预测值(下图)

很明显，差异很小，无法通过目视检查发现。下图显示了每个样本的绝对误差:

![](img/f2cccb3b-8940-4b7e-8679-20db36725e1b.png)

绝对预测误差

在大多数情况下，误差低于 **5** ，只有少数高峰的相应预测误差约为 20-50%。为了完成这个例子，让我们绘制绝对误差的直方图(用 *y* 对数标度)(它可以为我们提供最有价值的信息):

![](img/2e98fe17-879a-40e3-806a-22bd50990466.png)

绝对误差直方图

正如预期的那样，分布是指数分布(也可以是具有小方差的半高斯分布)，具有非常强的峰值，对应于*绝对误差≈ 0* (在图中，由于对数标度，它看起来更短)。大多数误差小于 **5** 并且尾部仅包含少量样本。这证实了回归(基于 200 个测试样本)成功地学习了数据集的结构，并且仅在非常有限的情况下失败。我邀请读者使用网格搜索和不同的测试集维度重复这个例子。

数据集可以从[https://archive . ics . UCI . edu/ml/datasets/Concrete+抗压+强度](https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength)下载。它由一个 Excel 文件组成，可以使用 pandas 轻松解析。

<title>Introduction to Ensemble Learning</title>  

# 集成学习简介

到目前为止，我们已经在单个实例上训练模型，迭代算法以最小化目标损失函数。这种方法基于所谓的**强学习者**，或者通过寻找最佳可能解决方案(最高准确性)来优化解决特定问题的方法。另一种方法是基于一组**弱学习器**，形式上，它们是能够达到略高于 0.5 的精确度的估计器。在现实世界中，集成学习中使用的实际估计器比它们的理论对应物精确得多，但是通常它们能够专门化样本空间的单个区域，并且在考虑整个数据集时表现不佳。此外，可以并行或顺序训练它们(对参数稍加修改)，并基于多数投票或结果的平均作为一个整体(组)使用。在这种情况下，我们假设弱学习器总是决策树，但这决不能被视为限制或约束(关于算法和理论部分的进一步细节，请参考*掌握机器学习算法*、 *Bonaccorso G* 。、 *Packt 发布*、 *2018* )。

这些方法可以分为三大类:

*   **Bagging(或 bootstrapping)** :在这种情况下，完整地构建了系综。每个分类器被独立训练，考虑原始数据集 *(X，Y)* 的子集 *(X [i] ，Y [i] )* 。有许多可能的实现策略；然而，主要的目标是避免两个或更多的评估者专注于同一个子集。一般来说，根据该算法，在该过程中包括不同程度的随机性。这可能导致次优的部分解决方案，但它允许集合有效地探索整个样本空间。装袋最常见的例子是随机森林算法。
*   **增强**:在这种情况下，集合是按顺序建立的，集中在先前被错误分类的样本上。这一过程通常通过对数据集重新加权来实现。如果在开始时，每个样本具有相同的被选择概率，则在一次迭代之后，分布变得更加峰值化，从而不会增加对需要更多特殊化的样本进行采样的概率。增强树的例子是 AdaBoost 和梯度树增强。
*   **堆叠**:这种方法基于一组异质的弱学习者(例如，SVM、逻辑回归和决策树)。每个分类器都是自主训练的，通过采用多数投票、平均结果或使用另一个辅助分类器做出最终选择，该辅助分类器获取所有中间预测并输出最终预测。在接下来的部分中，我们将讨论投票分类器的概念。

<title>Random Forests</title>  

# 随机森林

随机森林是一种基于一组决策树的打包集成方法。如果我们有 *N [c]* 个分类器，原始数据集被分割成 *N [c]* 个子集(带有替换)，称为引导样本:

![](img/10170e65-c4a7-462b-9c90-610c7b63a4c7.png)

与单个决策树相反，在随机森林中，分割策略基于中等水平的随机性。事实上，不是寻找最佳选择，而是使用特征的随机子集(针对每棵树)(通常，使用*sqrt()*或*log()*计算特征的数量)，试图找到最佳分离数据的阈值。因此，将有许多以较弱方式训练的树，并且它们中的每一个都将产生不同的预测。与此同时，每棵树在样本空间的一部分上更加专门化，而在其他区域产生不准确的预测。

有两种方法可以解释这些结果。最常见的方法是基于多数投票(通过 binned*arg max()*获得的投票最多的类将被认为是正确的):

![](img/772dc6d4-a539-4c5c-a0a4-df7bee759d07.png)

然而，scikit-learn 实现了一种基于平均结果的算法，这种算法可以产生非常准确的预测:

![](img/a679dc36-e625-4b90-bd6d-b074b4eb70b0.png)

即使它们在理论上是不同的，经过训练的随机森林的概率平均值也不可能与大多数预测有很大不同(否则，应该有不同的稳定点)，因此这两种方法通常会产生可比较的结果。

例如，让我们考虑由不同数量的树组成的随机森林的 MNIST 数据集:

```
from sklearn.ensemble import RandomForestClassifier

nb_classifications = 100
accuracy = []

for i in range(1, nb_classifications):
    a = cross_val_score(RandomForestClassifier(n_estimators=i), digits.data, digits.target,  scoring='accuracy', cv=10).mean()
    rf_accuracy.append(a)
```

结果图如下图所示:

![](img/8cc50f93-81d4-45c7-a773-496bfff10de7.png)

随机森林的准确性是决策树数量的函数

正如我们所预料的，当树的数量低于最小阈值时，精确度较低。然而，当少于 10 棵树时，它开始迅速增加。20 到 30 棵树之间的值产生最佳结果(95%)，这高于单个决策树的结果。当树的数量较低时，模型的方差非常高，并且平均过程产生许多不正确的结果；但是，增加树的数量会减少方差，并允许模型收敛到非常稳定的解。

scikit-learn 还提供了一个方差来增强选择最佳阈值的随机性。使用`ExtraTreesClassifier`类，可以实现一个随机计算阈值并选择最佳阈值的模型。这种选择使您可以进一步减少差异，并且通常可以获得更好的最终验证准确性:

```
from sklearn.ensemble import ExtraTreesClassifier

nb_classifications = 100

for i in range(1, nb_classifications):
    a = cross_val_score(ExtraTreesClassifier(n_estimators=i), digits.data, digits.target,  scoring='accuracy', cv=10).mean()
    et_accuracy.append(a)
```

结果(具有相同数量的树)在准确性方面稍好，如下图所示:

![](img/33b79d8e-cc98-4a88-87aa-3c445ca19ff7.png)

随机森林与超随机森林的准确性比较

<title>Feature importance in Random Forests</title>  

# 随机森林中的特征重要性

我们之前介绍的特征重要性的概念也可以应用于随机森林，计算森林中所有树的平均值:

![](img/9204f8a0-65bb-4709-aff0-f09cfbf33789.png)

我们可以使用虚拟数据集轻松测试重要性评估，该数据集包含带有`20`非信息元素的`50`要素:

```
nb_samples = 1000

X, Y = make_classification(n_samples=nb_samples, n_features=50, n_informative=30, n_redundant=20, n_classes=2, n_clusters_per_class=5)
```

根据带有`20`树的随机森林，第一个`50`特征的重要性绘制在下图中:

![](img/d430ebd8-438c-42cf-b7b1-46816a1d33d9.png)

随机森林中的要素重要性

正如所料，有几个*非常*重要的特性，一组具有中等重要性的特性，以及一个包含对预测几乎没有影响的特性的尾部。这种类型的图在分析阶段也很有用，可以更好地理解决策过程的结构。对于多维数据集，很难理解每个因素的影响，有时许多重要的业务决策是在没有完全意识到其潜在影响的情况下做出的。使用决策树或随机森林，可以评估所有特性的真正重要性，并排除固定阈值下的所有元素。这样，一个复杂的决策过程可以被简化，同时，部分降噪。

<title>AdaBoost</title>  

# adaboost 算法

另一种技术叫做 **AdaBoost** (是**自适应增强**的缩写)，它的工作方式与许多其他分类器略有不同。这背后的基本结构可以是决策树，但是用于训练的数据集被不断调整，以迫使模型关注那些被错误分类的样本。此外，分类器是按顺序添加的，因此新的分类器通过提高那些不如预期准确的领域的性能来增强前一个分类器。在每次迭代中，将权重因子应用于每个样本，以增加被错误预测的样本的重要性，并降低其他样本的重要性。换句话说，模型被反复增强，从非常弱的学习者开始，直到达到最大的`n_estimators`数。在这种情况下，预测总是通过多数票获得的。

这个算法的最初版本叫做 **AdaBoost。M1** ，并且基于动态更新的权重集(考虑 *n* 个样本):

![](img/4260f491-a8f1-4384-915e-88fddae29aa5.png)

对于所有权重，初始值 *W ^((0))* 被设置为等于 *1/n* ，从而不对任何样本表示偏好。在开始训练过程之前，考虑现有的权重，设置每个分类器样本。在每个训练步骤之后，计算指示函数:

![](img/ad544188-06ac-48ba-8be2-f38a7218c27f.png)

如果没有发生错误分类， *ε ^((t)) = 1* ，如果所有样本都被分配到错误的类别，则等于 *0* 。为了在随后的迭代中重新加权数据集，使用了一个特殊的函数:

![](img/b4ad8e8b-3898-4800-bbcb-d13d37d1d6d0.png)

通常添加常数 *ν* 以提高数值稳定性(避免被零除)；但是，对于我们的讨论，它可以被认为是空的。当*ε^((t))**→0*， *α ^((t)) → +∞* ， *ε ^((t)) → 1* ， *α ^((t)) → -∞* 。在二进制随机猜测的情况下，*ε^((t))= 0.5**α^((t))= 0*。即使不直观，我们的主要目标还是排除那些随机神谕 *(ε ^((t)) = 0.5)* 的分类器，它们的预测是完全排除的。当 *ε ^((t)) → 1* 时，分类器应该接收到负提升，但是这将与算法的初始目的相反。因此，在这种情况下，输出被反转(如果是误分类也是如此)，boost 变为正(例如，如果 *ε ^((t)) = 0.75* ，则转换为 *ε ^((t)) = 0.25* )，这不会改变 *α ^((t))* 的绝对值

这可以通过使用加权全局决策函数来实现:

![](img/f46003b5-929d-4415-bfea-70f7d4f638dd.png)

以这种方式，保持在接近 *0.5* 精度的分类器被自动丢弃，而所有其他的被提升。提升过程非常简单，它基于每个分类的结果。我们来考虑样本*x[I]。我们可以引入辅助变量 *o [i]* :*

![](img/d2041880-0bfd-4fa0-bfc7-a2bd39449a24.png)

通过考虑以下规则来更新相应的权重*w[I]:*

![](img/e820341d-cbcd-4b43-ba2c-4b68979cee6a.png)

显而易见，只有在出现错误分类时，权重才会增加，而如果样本被正确分类，权重就会减少。此外，参数 *α ^((t))* 考虑了估计器的整体行为。如果*ε^((t))**→0.5*，则重新加权不会改变现有配置，而当获得更大的 *α ^((t))* 值时，重新加权会变得更加激进。该过程的结果是新的分布，其中最有问题的样本将变得越来越有可能被采样，而最简单的样本在第一次迭代后可能被丢弃。

在 scikit-learn 实现中(即 **AdaBoost。SAMME** 用于分类和 **AdaBoost。SAMME.R** 用于回归，因为两者都被设计为在多类场景中工作)，还有一个名为`learning_rate`的参数来衡量每个分类器的效果。默认值为 1.0，因此所有估计值都被认为具有相同的重要性。但是，正如我们在 MNIST 数据集上看到的那样，减小该值会有助于削弱每个贡献:

```
from sklearn.ensemble import AdaBoostClassifier

accuracy = []

nb_classifications = 100

for i in range(1, nb_classifications):
    a = cross_val_score(AdaBoostClassifier(n_estimators=i, learning_rate=0.1), digits.data, digits.target, scoring='accuracy', cv=10).mean()
    ab_accuracy.append(a)
```

结果如下图所示:

![](img/8421ed1d-0dca-4415-8a62-efc647d0c87e.png)

AdaBoost 精度与决策树数量的函数关系

精确度没有前面的例子高。然而，可以看到，当提升增加大约 20-30 棵树时，它达到一个稳定的值。在`learning_rate`上进行网格搜索可以让您找到最佳值；然而，在这种情况下，顺序方法并不可取。经典的随机森林，从第一次迭代开始就使用固定数量的树，性能更好。这很可能是由于 AdaBoost 采用的策略。在这个集合中，增加正确分类的样本的权重和降低错误分类的强度会在损失函数中产生振荡，最终结果不是最佳的最小点。用 Iris 数据集(结构上简单得多)重复实验产生了更好的结果:

```
from sklearn.datasets import load_iris

iris = load_iris()

ada = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)
print(cross_val_score(ada, iris.data, iris.target, scoring='accuracy', cv=10).mean())
0.94666666666666666
```

在这种情况下，1.0 的学习率是最佳选择，很容易理解在几次迭代后可以停止升压过程。在下图中，您可以看到一个显示该数据集准确性的图:

![](img/c5360be7-e6e6-45eb-9566-549611859f3b.png)

基于 Iris 数据集的 AdaBoost 的准确性与决策树数量的关系

在大约 10 次迭代之后，精度变得稳定(残余振荡可以被丢弃)，达到与该数据集兼容的值。使用 AdaBoost 的优势可以在资源方面得到体现；它不适用于完全配置的分类器集和整个样本集。因此，在大型数据集上训练时，它可以帮助节省时间。

<title>Gradient Tree Boosting</title>  

# 梯度树提升

梯度树提升是一种允许您逐步构建树集合的技术(该方法也被称为**正向逐级加法建模**)，目标是最小化目标损失函数。集合的一般输出可以表示如下:

![](img/cc849f7e-e508-4a65-b3d5-1d49dff6a954.png)

这里，c *[j] (x)* 是表示弱学习者的函数(在这种特殊情况下，它总是一个决策树，可以建模为单个参数化函数*f()*，其中向量θ [i] 对 i ^(th) 树的所有分裂元组进行分组)。该算法基于在每一步添加一个新的决策树的概念，以使用最速梯度下降法最小化全局成本函数(基于预定义的损失*L()*)(更多信息，参见[https://en.wikipedia.org/wiki/Method_of_steepest_descent](https://en.wikipedia.org/wiki/Method_of_steepest_descent))。考虑到使用向量θ对分类器进行参数化，并且所有加权系数被分组到单个数组中，全局成本函数被定义如下:

![](img/03e23c09-0b39-48cd-a968-6a9fc423950c.png)

因此，目标是找到最佳元组，以便:

![](img/eba27624-9475-4800-9bef-b639c8ecd7be.png)

考虑到这个目标，增量过程可以重写如下:

![](img/d161d053-d82d-4bbf-8ef3-7e06fe8e2726.png)

在之前的表达式中，通过考虑之前的贡献来计算损失，并且相对于新的分类器来优化损失。不幸的是，尽管形式上很清楚，但这个问题极其复杂，并且需要不可接受的计算成本。然而，通过引入梯度，我们可以重写之前的表达式，将加法模型转换为更简单的优化过程:

![](img/2d08912f-9c26-417f-8be6-c44a8aa57380.png)

熟悉**随机梯度下降** ( **SGD** )算法的读者可以马上明白，每一步的目标都是最小化全局代价函数。换句话说，通过在与梯度相反的方向上移动，新的分类器被建立，目的是相对于它的前一个减少全局成本函数。通常，在这类任务中，参数η是学习速率，必须使用网格搜索来选择它，以避免非常慢的收敛或不稳定，从而导致次优。取而代之的是，在计算梯度(将α视为附加变量)之后，使用线搜索算法(在计算上是可承受的)来计算权重α [i] :

![](img/34df2317-9b44-4e85-b945-ae9987eab6ba.png)

这个算法的理念和 AdaBoost 差别不大；然而，在这种情况下，我们关注的是一个全局目标，而没有明确地对数据集进行重新加权。选择一个新的估计量来改进前一个估计量(在理想的情况下，成本函数应该总是下降到最小值)，但是我们不知道哪个样本区域更有问题。

scikit-learn 实现了`GradientBoostingClassifier`类，它支持两个分类损失函数:

*   二项式/多项式负对数似然(默认选择)
*   指数(类似于 AdaBoost)

让我们使用一个更复杂的虚拟数据集来评估这种方法的准确性，该数据集由 500 个样本组成，具有四个特征(三个信息性特征和一个冗余特征)和三个类别:

```
from sklearn.datasets import make_classification

nb_samples = 500

X, Y = make_classification(n_samples=nb_samples, n_features=4, n_informative=3, n_redundant=1, n_classes=3)
```

现在，我们可以收集区间(1，50)中许多估计量的交叉验证平均准确度。损失函数是默认函数(多项式负对数似然):

```
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score

a = []

max_estimators = 50

for i in range(1, max_estimators):
    score = cross_val_score(GradientBoostingClassifier(n_estimators=i, learning_rate=10.0/float(i)), X, Y, cv=10, scoring='accuracy').mean()
    a.append(score)
```

在增加估计器数量(通过使用`n_estimators`参数)的同时，降低学习率(通过使用`learning_rate`参数)也很重要。最佳值不容易预测；因此，执行网格搜索通常很有用。在我们的例子中，我在开始时设置了一个非常高的学习率(5.0)，当估计器的数量等于 100 时，学习率收敛到 0.05。这不是一个完美的选择(在大多数真实情况下无法接受！)，它只是为了显示精度性能的差异。结果如下图所示:

![](img/370a6a37-ffc8-477a-a0ed-9889fc609176.png)

梯度树提升的平均 CV 精度与估计数的函数关系

如您所见，估计器的最佳数量约为 50，学习率为 0.1。读者可以尝试不同的组合，并将该算法的性能与其他集成方法进行比较。

<title>Voting classifier</title>  

# 投票分类器

`VotingClassifier`类提供了一个非常有趣的集成解决方案(可以认为是堆叠子集的一部分)，它不是一个实际的分类器，而是一组不同分类器的包装器，这些分类器被并行训练和评估，以便利用每个算法的不同特性。

根据两种不同的策略，通过多数投票对预测做出最终决定:

*   **硬投票**:在这种情况下，将选择获得最高票数的阶级*N[c](y[t])*:

![](img/ebcbd784-5d7f-41f4-b8c7-ebcf1f263424.png)

*   **软投票**:在这种情况下，对每个预测类别(所有分类器)的概率向量求和并平均。获胜类是对应于最高值的类:

![](img/3d6d038a-2099-4930-96a3-db9f41d4c8bd.png)

让我们考虑一个虚拟数据集，并使用硬投票策略来计算准确度:

```
from sklearn.datasets import make_classification

nb_samples = 500

X, Y = make_classification(n_samples=nb_samples, n_features=2, n_redundant=0, n_classes=2)
```

对于我们的例子，我们将考虑三个分类器:逻辑回归、决策树(具有默认的基尼系数)和 SVM(具有多项式内核和`probability=True`以生成概率向量)。这种选择只是出于教学目的，可能不是最好的选择。创建集成时，考虑每个分类器的不同特征并避免“重复”算法是很有用的(例如，逻辑回归和线性 SVM 或感知器可能会产生非常相似的性能)。在许多情况下，将非线性分类器与随机森林或 AdaBoost 分类器混合使用会很有用。读者可以用其他组合重复这个实验，比较每个估计器的性能和投票分类器的准确性:

```
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import VotingClassifier

lr = LogisticRegression()
svc = SVC(kernel='poly', probability=True)
dt = DecisionTreeClassifier()

classifiers = [('lr', lr),
               ('dt', dt),
               ('svc', svc)]

vc = VotingClassifier(estimators=classifiers, voting='hard')
```

计算交叉验证准确度，我们得到以下结果:

```
from sklearn.model_selection import cross_val_score

a = []

a.append(cross_val_score(lr, X, Y, scoring='accuracy', cv=10).mean())
a.append(cross_val_score(dt, X, Y, scoring='accuracy', cv=10).mean())
a.append(cross_val_score(svc, X, Y, scoring='accuracy', cv=10).mean())
a.append(cross_val_score(vc, X, Y, scoring='accuracy', cv=10).mean())

print(np.array(a))
[ 0.90182873  0.84990876  0.87386955  0.89982873]
```

每个单个分类器和集合的平均 CV 精度绘制在下图中:

![](img/b5ab6a49-2917-4cbb-ad9e-ac749513b5b3.png)

属于集成的每个分类器的平均 CV 准确度

正如预期的那样，集成利用了不同的算法，比任何一个算法都有更好的性能。我们现在可以用软投票重复这个实验，考虑到也可以引入一个权重向量(通过`weights`参数)来给予每个分类器或多或少的重要性:

![](img/9201c413-5838-493b-8991-662625596a0f.png)

例如，考虑到前面的图表，我们可以决定更重视逻辑回归，而不太重视决策树和 SVM:

```
weights = [1.5, 0.5, 0.75]

vc = VotingClassifier(estimators=classifiers, weights=weights, voting='soft')
```

对交叉验证准确性重复相同的计算，我们得到以下结果:

```
print(np.array(a))
[ 0.90182873  0.85386795  0.87386955  0.89578952]
```

结果图如下图所示:

![](img/a5a04389-cb5c-4095-bf5e-c76c81b65fbd.png)

属于加权集成的每个分类器的平均 CV 准确度

加权并不局限于软策略。它也可以应用于硬投票，但在这种情况下，它将用于过滤(减少或增加)实际发生的次数:

![](img/0c569797-911c-4519-81a3-d5a65f057d29.png)

这里， *N [c] (y [t] ，w)* 是每个目标类的投票数，其中每个都乘以相应的分类器加权因子。

每当单个策略不能达到期望的准确度阈值时，投票分类器可以是一个好的选择。当利用不同的方法时，只使用一小组强有力的(但有时有限的)学习者就有可能捕捉到许多微观趋势。

<title>Summary</title>  

# 摘要

在这一章中，我们介绍了决策树作为一种特殊的分类器。这个概念背后的基本思想是，通过使用分裂节点，决策过程可以变得连续，根据我们使用的示例，选择一个分支，直到到达最后一片叶子。为了构建这样的树，引入了杂质的概念；从完整的数据集开始，我们的目标是找到一个分割点来创建两个不同的集合，这两个集合应该共享最少数量的要素，并且在该过程结束时，应该与单个目标类相关联。一棵树的复杂程度取决于内在的纯度；换句话说，当确定最能区分集合的特征总是很容易时，深度就会减小。然而，在许多情况下，这几乎是不可能的，所以产生的树需要许多中间节点来减少杂质，直到它到达最终的叶子。

我们还讨论了一些集成学习方法:随机森林，AdaBoost，梯度树提升和投票分类器。它们都是基于这样的想法:训练几个弱学习者，并使用多数投票或平均值来评估他们的预测。然而，虽然随机森林创建了一组部分随机训练的决策树，但 AdaBoost 和梯度提升树采用了通过一步一步地添加新模型来提升模型的技术，并且只关注那些先前被错误分类的样本或特定损失函数的最小化。相反，投票分类器允许混合不同的分类器，采用多数投票来决定在预测期间哪个类必须被认为是胜出的。

在下一章中，[第 9 章](ed25eb07-f07d-4084-96e9-64ee87c94331.xhtml)，*聚类基础*我们将介绍第一种无监督学习方法，k-means，这是最扩散的聚类算法之一。我们将集中讨论它的优点和缺点，并探索 scikit-learn 提供的一些替代方案。***