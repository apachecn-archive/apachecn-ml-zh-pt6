

# 十三、游戏的人工智能——成为蛇的主人

这是最后一个实用的章节；恭喜你完成了前面几个！我希望你真的喜欢它们。现在，让我们抛开商业问题和自动驾驶汽车。让我们来玩一个叫做 Snake 的流行游戏，并制作一个可以自学玩这个游戏的 AI，来找点乐子吧！

这正是我们在本章要做的。我们将实现的模型被称为深度卷积 Q 学习，使用一个**卷积神经网络** ( **CNN** )。

我们的 AI 不会完美，也不会填满整个地图，但经过一些训练后，它会开始在与人类相当的水平上玩耍。

让我们从游戏的样子和目标开始着手解决这个问题。

## 要解决的问题

首先，让我们来看一下游戏本身:

![](img/B14110_13_01.png)

图 1:贪吃蛇游戏

你看起来是不是有点熟悉？

我很确信它会的；每个人一生中至少玩过一次蛇。

这个游戏非常简单。它由一条蛇和一个苹果组成。我们控制这条蛇，我们的目标是吃掉尽可能多的苹果。

听起来很容易？嗯，有个小问题。我们的蛇每吃一个苹果，我们的蛇就变大一片。这意味着这个游戏在开始的时候非常简单，但是逐渐变得越来越难，以至于它变成了一个战略游戏。

还有，控制我们的蛇的时候，不能打自己，也不能打棋盘的边框。可以预见，这将导致我们的失败。

既然我们理解了这个问题，我们就可以前进到创建人工智能的第一步——构建环境！

## 建筑环境

这一次，与本书中的其他实用章节不同，我们不需要指定任何变量或做出任何假设。我们可以直接进入每个深度 Q 学习项目中的三个关键步骤:

1.  定义状态
2.  定义行动
3.  定义奖励

我们开始吧！

### 定义状态

在前面的每个例子中，我们的状态是一个 1D 向量，代表定义环境的一些值。例如，对于我们的自动驾驶汽车，我们从汽车周围的三个传感器和汽车的位置收集信息。所有这些都放在一个 1D 数组中。

但是如果我们想做一些稍微更真实的东西呢？如果我们希望人工智能像我们一样从相同的来源看到和收集信息，会怎么样？这就是我们在本章要做的。我们的 AI 会看到和我们玩贪吃蛇时看到的完全一样的棋盘！

游戏的状态应该是一个代表棋盘的 2D 数组，和我们看到的一模一样。

这个解决方案只有一个问题。看看下面的图片，看看你能否回答这个问题:我们的蛇现在在向哪个方向移动？

![](img/B14110_13_02.png)

图 2:贪吃蛇游戏

如果你说“我不知道”，那么你完全正确。

根据单一画面，我们无法判断我们的蛇往哪个方向走。因此，我们需要堆叠多个图像，然后一次性将它们全部输入到卷积神经网络中。这将导致我们有三维国家，而不是 2D。

所以，简单回顾一下:

![](img/B14110_13_03.png)

图 3:人工智能的愿景

我们将有一个 3D 数组，包含彼此堆叠的下一个游戏帧，其中最上面的一个是从我们的游戏中获得的最新帧。现在，我们可以清楚地看到我们的 AI 在向哪个方向移动；在这种情况下，它是向上的，朝向苹果。

既然我们已经定义了状态，我们可以进行下一步:定义动作！

### 定义行动

当我们在手机或网站上玩贪吃蛇游戏时，有四种行为可供我们选择:

1.  增长
2.  下降
3.  走右边
4.  向左走

然而，如果我们采取的行动需要蛇直接向自己转过 180 度，那么游戏会阻止这一行动，蛇继续向当前方向前进。

在前面的示例中，如果我们选择动作 2——向下——我们的蛇仍然会继续向上，因为向下是不可能的，因为蛇不能直接 180°转身。

值得注意的是，所有这些动作都是相对于棋盘的，而不是相对于蛇的；它们不受蛇当前运动的影响。向上、向下、向右或向左总是指相对于棋盘向上、向下、向右或向左，而不是相对于蛇当前的运动方向。

好的，那么现在在决定我们在人工智能中模拟什么动作时，你可能属于这两组中的一组:

1.  我们可以为我们的人工智能使用这四个相同的动作。
2.  我们不能使用这些相同的动作，因为阻止某些动作会让我们的 AI 感到困惑。相反，我们应该发明一种方法来告诉蛇向左走，向右走，或者继续走。

我们实际上可以对我们的人工智能使用这些相同的动作！

为什么不会让我们的经纪人感到困惑？这是因为，只要我们的人工智能代理因为它选择的动作而获得奖励，而不是因为蛇最终执行的动作，那么深度 Q 学习将会工作，我们的人工智能将会理解，在上面的例子中，选择*上升*或*下降*会导致相同的结果。

举个例子，假设 AI 控制的蛇目前向左走。它选择动作 3，向右走；因为这将导致蛇在自身上做 180 度的回转，相反，蛇继续向左。假设这个动作意味着这条蛇撞到了墙上，结果死了。为了不让我们的代理感到困惑，我们需要做的就是告诉它*向右*的动作导致它崩溃，尽管这条蛇一直向左移动。

你可以把它想象成教一个人工智能去玩手机上的实际按键。如果你一遍又一遍地按下向右按钮，试图让你的蛇在向左移动时折回，游戏会忽略你告诉它做的不可能的移动，继续向左移动，最终崩溃。这就是人工智能需要学习的全部。

这是因为，记住，在深度 Q 学习中，我们只更新 AI 采取的动作的 Q 值。如果我们的蛇向左走，而 AI 决定向右走，蛇死了，它需要明白的是*向右走*的动作导致它获得了负奖励，而不是蛇向左移动的事实；即使选择动作*向左*会导致相同的结果。

我希望你明白人工智能可以使用和我们玩游戏时一样的动作。我们可以继续下一步，也是最后一步，定义奖励！

### 定义奖励

这最后一步非常简单；我们只需要三个奖励:

1.  吃苹果的奖励
2.  死亡奖励
3.  无期徒刑

前两个很容易理解。毕竟，我们想鼓励我们的代理人尽可能多吃苹果，因此我们将它的奖励设置为正。准确的说:**吃一个苹果= +2**

同时，我们想阻止我们的蛇死亡。这就是为什么我们把奖励设为负数。准确的说:**垂死= -1**

然后是最后的奖励:活的惩罚。

那是什么，为什么有必要？我们必须说服我们的代理人，在不死亡的情况下尽快收集苹果是一个好主意。如果我们只有已经定义好的两个奖励，我们的代理将简单地遍历整个地图，希望在某个点找到一个苹果。它不会明白它需要尽可能快地收集苹果。

这就是我们引入活刑的原因。它会轻微惩罚我们的 AI 所做的每一个动作，除非这个动作导致死亡或收集一个苹果。这将向我们的代理显示它需要快速收集苹果，因为只有收集苹果的动作才能获得积极的奖励。那么，这个奖励应该有多大呢？好吧，我们不想惩罚它太多。准确的说:**活罚=-0.03**

如果你想修改这些奖励，这个奖励的绝对值应该总是比其他奖励相对较小，比如死亡(-1)和收集一个苹果(+2)。

## 人工智能解决方案

与往常一样，深度 Q 学习的人工智能解决方案由两部分组成:

1.  **大脑**——会学习和采取行动的神经网络
2.  **经验回放记忆**——将储存我们经验的记忆；神经网络将从这个记忆中学习

让我们现在就解决这些问题吧！

### 大脑

人工智能解决方案的这一部分将负责教授、存储和评估我们的神经网络。为了建造它，我们将使用 CNN！

为什么是 CNN？在解释它们背后的理论时，我提到它们经常在“我们的环境作为状态返回图像”时使用，这正是我们在这里处理的。我们已经确定游戏状态将是一个堆叠的 3D 数组，包含最后几个游戏帧。

在前一章中，我们讨论了 CNN 采用 2D 图像作为输入，而不是堆叠的 3D 图像阵列；但是你还记得这张图片吗？

![https://lh5.googleusercontent.com/qjfDY_d7Dvn92gkZ2KDpPAoy-SM_7AO8RExLTjtj-FYCQcCDVIrfSjvgslPBBT5kAneqJMRbJKAOikeslS-1T5TQaPDDxX338ko4DWQxi5xPggLbosb-p3tR8y5DDGp-blxs1aqj](img/B14110_13_04.png)

图 4: RGB 图像

在这里，我告诉你，RGB 图像是由 3D 阵列表示的，包含该图像的每个 2D 通道。听起来熟悉吗？我们可以用完全相同的方法来解决我们的问题。就像 RGB 结构中的每种颜色一样，我们将简单地输入每个游戏帧作为一个新的通道，这将为我们提供一个 3D 数组，我们将能够将其输入到 CNN 中。

实际上，CNN 通常只支持 3D 阵列作为输入。为了输入 2D 数组，您需要创建一个伪单通道，将 2D 数组转换为 3D 数组。

对于 CNN 架构，我们将有两个卷积层，由一个池层分隔开。一个卷积层将有 32 个 3×3 过滤器，另一个将有 64 个 2×2 过滤器。池层将缩小 2 倍，因为池窗口大小将为 2x2。为什么是这样的架构？这是一个经典的方法，在许多研究论文中都能找到，我随意地选择了它作为常用的方法，结果证明它非常有效。

我们的神经网络将具有一个具有 256 个神经元的隐藏层，以及一个具有 4 个神经元的输出层；一个代表我们可能的结果行动。

我们还需要为 CNN 设置最后两个参数——学习速率和输入形状。

在前面的示例中使用的学习率是一个参数，它指定了我们更新神经网络中的权重的程度。太小，它不会学习，太大，它不会学习的原因不同；对于任何优化来说，变化都太大了。我通过实验发现，这个例子的一个好的学习率是 0.0001。

我们已经同意输入应该是一个 3D 数组，包含从我们的游戏中获得的最后帧。确切地说，我们不会从屏幕上读取像素。相反，我们将读取在特定时间代表我们游戏屏幕的直接 2D 数组。

你可能已经注意到了，我们的游戏是建立在网格上的。在我们使用的例子中，网格是 10x10。然后，环境中有一个大小相同(10x10)的数组，从数学上告诉我们电路板的样子。例如，如果我们在一个单元格中有蛇的一部分，那么我们在 2D 数组的相应单元格中放置值 0.5，我们将读取该值。在这个数组中，苹果被描述为值 1。

现在我们知道了我们将如何看到一帧，我们需要决定当我们描述当前游戏状态时我们将使用多少先前的帧。两个应该足够了，因为我们可以从中辨别出蛇的走向，但是为了确保万无一失，我们需要四个。

你能确切地告诉我我们对 CNN 的投入会是什么样的吗？

它将是 10x10x4，这给了我们一个三维数组！

### 体验回放记忆

正如深度 Q 学习的理论章节中所定义的，我们需要有一个存储训练期间收集的经验的记忆。

我们将存储以下数据:

*   当前状态——AI 执行动作时所处的游戏状态(我们输入到 CNN 的内容)
*   行动——采取了什么行动
*   奖励–在当前状态下执行此操作获得的奖励
*   下一个状态——执行行动后发生了什么(状态如何)
*   游戏结束——关于我们是否输了的信息

此外，我们必须为每个体验回放记忆指定两个参数:

*   内存大小——我们内存的最大大小
*   伽马——存在于贝尔曼方程中的贴现因子

我们将内存大小设置为 60，000，gamma 参数设置为 0.9。

这里还有最后一件事需要说明。

我告诉过你我们的 AI 会从这个记忆中学习，这是真的；但是人工智能不会从整个记忆中学习。相反，它会从从中抽取的一小批样本中学习。指定这个大小的参数称为批处理大小，在本例中，我们将它的值设置为 32。这意味着我们的人工智能将从经验回放记忆中获取的一批这样大小的数据中学习每一次迭代。

现在，您已经了解了需要编码的所有内容，可以开始了！

## 实施

您将在五个文件中实现整个 AI 代码和贪吃蛇游戏:

1.  `environment.py`文件——包含环境(贪吃蛇游戏)的文件
2.  `brain.py`文件——我们构建 CNN 的文件
3.  `DQN.py`–建立体验回放记忆的文件
4.  `train.py`–我们将训练我们的人工智能玩蛇的文件
5.  `test.py`——我们将在这个文件中测试我们的人工智能，看看它的表现如何

您可以在 GitHub 页面上找到所有这些工具以及一个预先训练好的模型。为此，在主页上选择`Chapter 13`文件夹。

我们将以同样的顺序浏览每个文件。让我们开始营造环境吧！

### 第一步——构建环境

通过导入您需要的库，开始这第一步，也是重要的一步。像这样:

```
# Importing the libraries   #4

import numpy as np   #5

import pygame as pg   #6 
```

您将只使用两个库:NumPy 和 PyGame。前者在处理列表或数组时非常有用，后者将用于构建整个游戏——绘制蛇和苹果，并更新屏幕。

现在，让我们创建一个`Environment`类，它将包含游戏所需的所有信息、变量和方法。为什么要上课？这是因为它让你以后的事情变得更容易。您将能够从这个类的对象中调用特定的方法或变量。

您必须拥有的第一个方法是`__init__`方法，当在主代码中创建该类的新对象时，总是会调用该方法。要创建这个类和这个`__init__`方法，您需要编写:

```
# Initializing the Environment class   #8

class Environment():   #9

    #10

    def __init__(self, waitTime):   #11

        #12

        # Defining the parameters   #13

        self.width = 880            # width of the game window   #14

        self.height = 880           # height of the game window   #15

        self.nRows = 10             # number of rows in our board   #16

        self.nColumns = 10          # number of columns in our board   #17

        self.initSnakeLen = 2       # initial length of the snake   #18

        self.defReward = -0.03      # reward for taking an action - The Living Penalty   #19

        self.negReward = -1\.        # reward for dying   #20

        self.posReward = 2\.         # reward for collecting an apple   #21

        self.waitTime = waitTime    # slowdown after taking an action   #22

        #23

        if self.initSnakeLen > self.nRows / 2:   #24

            self.initSnakeLen = int(self.nRows / 2)   #25

        #26

        self.screen = pg.display.set_mode((self.width, self.height))   #27

        #28

        self.snakePos = list()   #29

        #30

        # Creating the array that contains mathematical representation of the game's board   #31

        self.screenMap = np.zeros((self.nRows, self.nColumns))   #32

        #33

        for i in range(self.initSnakeLen):   #34

            self.snakePos.append((int(self.nRows / 2) + i, int(self.nColumns / 2)))   #35

            self.screenMap[int(self.nRows / 2) + i][int(self.nColumns / 2)] = 0.5   #36

            #37

        self.applePos = self.placeApple()   #38

        #39

        self.drawScreen()   #40

        #41

        self.collected = False   #42

        self.lastMove = 0   #43 
```

您创建了一个新类，`Environment()`类，以及它的`__init__`方法。这个方法只取一个参数，就是`waitTime`。然后在定义方法之后，创建一个常量列表，每个常量都在行内注释中有解释。之后，您执行一些初始化。在第 24 行和第 25 行，确保蛇的长度不超过屏幕长度的一半，并在第 27 行设置屏幕。需要注意的一点是，您在第 32 行创建了`screenMap`数组，它更精确地表示了棋盘。单元格中的 0.5 表示这个单元格被蛇占了，单元格中的 1 表示这个单元格被苹果占了。

在第 34 到 36 行，你把蛇放在屏幕中间，面朝上，然后在剩下的行中，你用`placeapple()`方法(我们将要定义)放一个苹果，画出屏幕，设置苹果没有被收集，并设置没有最后的移动。

这是第一个完成的方法。现在你可以进行下一个了:

```
 # Building a method that gets new, random position of an apple

    def placeApple(self):

        posx = np.random.randint(0, self.nColumns)

        posy = np.random.randint(0, self.nRows)

        while self.screenMap[posy][posx] == 0.5:

            posx = np.random.randint(0, self.nColumns)

            posy = np.random.randint(0, self.nRows)

        self.screenMap[posy][posx] = 1

        return (posy, posx) 
```

这个简短的方法将一个苹果放在你的`screenMap`数组中一个新的随机点上。当我们的蛇收集苹果并且需要放置一个新的苹果时，您将需要这个方法。它还返回新苹果的随机位置。

然后，您将需要一个函数来绘制所有内容供您查看:

```
 # Making a function that draws everything for us to see

    def drawScreen(self):

        self.screen.fill((0, 0, 0))

        cellWidth = self.width / self.nColumns

        cellHeight = self.height / self.nRows

        for i in range(self.nRows):

            for j in range(self.nColumns):

                if self.screenMap[i][j] == 0.5:

                    pg.draw.rect(self.screen, (255, 255, 255), (j*cellWidth + 1, i*cellHeight + 1, cellWidth - 2, cellHeight - 2))

                elif self.screenMap[i][j] == 1:

                    pg.draw.rect(self.screen, (255, 0, 0), (j*cellWidth + 1, i*cellHeight + 1, cellWidth - 2, cellHeight - 2))

        pg.display.flip() 
```

如你所见，这个方法的名字是`drawScreen`，它没有任何参数。在这里，你只需清空整个屏幕，然后在蛇所在的地方填充白色瓷砖，在苹果所在的地方填充红色瓷砖。最后，用`pg.display.flip()`更新屏幕。

现在，您需要一个函数来更新蛇的位置，而不是整个环境:

```
 # A method that updates the snake's position

    def moveSnake(self, nextPos, col):

        self.snakePos.insert(0, nextPos)

        if not col:

            self.snakePos.pop(len(self.snakePos) - 1)

        self.screenMap = np.zeros((self.nRows, self.nColumns))

        for i in range(len(self.snakePos)):

            self.screenMap[self.snakePos[i][0]][self.snakePos[i][1]] = 0.5

        if col:

            self.applePos = self.placeApple()

            self.collected = True

        self.screenMap[self.applePos[0]][self.applePos[1]] = 1 
```

可以看到这个新方法有两个参数:`nextPos`和`col`。前者告诉你执行某个动作后，蛇头会在哪里。后者会通知你这条蛇是否通过这个动作收集了一个苹果，或者没有。记住，如果蛇收集了一个苹果，那么蛇的长度增加 1。如果你深入这个代码，你可以看到，但我们不会在这里深入细节，因为它不是那么相关的人工智能。你还可以看到，如果蛇收集了一个苹果，一个新的苹果就会在一个新的地方产生。

现在，让我们转到代码中最重要的部分。您定义了一个将更新整个环境的函数。它会移动你的蛇，计算奖励，检查你是否输了，并返回一个新的游戏框架。事情是这样开始的:

```
 # The main method that updates the environment

    def step(self, action):

        # action = 0 -> up

        # action = 1 -> down

        # action = 2 -> right

        # action = 3 -> left

        # Resetting these parameters and setting the reward to the living penalty

        gameOver = False

        reward = self.defReward

        self.collected = False

        for event in pg.event.get():

            if event.type == pg.QUIT:

                return

        snakeX = self.snakePos[0][1]

        snakeY = self.snakePos[0][0]

        # Checking if an action is playable and if not then it is changed to the playable one

        if action == 1 and self.lastMove == 0:

            action = 0

        if action == 0 and self.lastMove == 1:

            action = 1

        if action == 3 and self.lastMove == 2:

            action = 2

        if action == 2 and self.lastMove == 3:

            action = 3 
```

正如你所看到的，这个方法叫做`step`，它有一个参数:告诉你想要蛇走哪条路的动作。就在方法定义的下面，在注释中，您可以看到哪个动作意味着哪个方向。

然后你重置一些变量。您将`gameOver`设置为`False`,因为这个布尔变量会告诉您执行此操作后是否失败。你设置`reward`到`defReward`，因为这是活罚；如果我们收集了一个苹果或稍后死去，它会改变。

然后就是一个`for`循环。它的存在是为了确保 PyGame 窗口不会冻结；这是 PyGame 库的要求。它必须在那里。

`snakeX`和`snakeY`告诉你蛇的头部位置。稍后算法会使用它来确定头部移动后会发生什么。

在最后几行，你可以看到阻止不可能动作的算法。简单回顾一下，一个不可能的动作是要求蛇原地转 180 度。`lastMove`告诉你这条蛇现在往哪个方向走，并与`action`进行比较。如果这些导致矛盾，那么`action`被设置为`lastMove`。

仍然在这个方法中，您更新蛇的位置，检查游戏是否结束，并计算奖励，如下所示:

```
 # Checking what happens when we take this action

        if action == 0:

            if snakeY > 0:

                if self.screenMap[snakeY - 1][snakeX] == 0.5:

                    gameOver = True

                    reward = self.negReward

                elif self.screenMap[snakeY - 1][snakeX] == 1:

                    reward = self.posReward

                    self.moveSnake((snakeY - 1, snakeX), True)

                elif self.screenMap[snakeY - 1][snakeX] == 0:

                    self.moveSnake((snakeY - 1, snakeX), False)

            else:

                gameOver = True

                reward = self.negReward 
```

在这里，您可以检查如果蛇上升会发生什么。如果蛇的头已经在第一行(第`0`行)，那么你显然已经输了，因为蛇撞到了墙上。因此，`reward`被设置为`negReward`，而`gameOver`被设置为`True`。否则，你就去看看蛇前面有什么。

如果前面的单元格已经包含了蛇的身体的一部分，那么你就输了。在第一个`if`语句中检查，然后将`gameOver`设置为`True`并将`reward`设置为`negReward`。

否则，如果前面的单元格是苹果，则将`reward`设置为`posReward`。您还可以通过调用在这个方法之前创建的方法来更新蛇的位置。

否则，如果前面的单元格是空的，那么您不会以任何方式更新`reward`。您再次调用相同的方法，但是这次将`col`参数设置为`False`，因为蛇还没有收集到苹果。你每做一个动作都要经历同样的过程。我不会遍历每一行，但是看一下代码:

```
 elif action == 1:

            if snakeY < self.nRows - 1:

                if self.screenMap[snakeY + 1][snakeX] == 0.5:

                    gameOver = True

                    reward = self.negReward

                elif self.screenMap[snakeY + 1][snakeX] == 1:

                    reward = self.posReward

                    self.moveSnake((snakeY + 1, snakeX), True)

                elif self.screenMap[snakeY + 1][snakeX] == 0:

                    self.moveSnake((snakeY + 1, snakeX), False)

            else:

                gameOver = True

                reward = self.negReward

        elif action == 2:

            if snakeX < self.nColumns - 1:

                if self.screenMap[snakeY][snakeX + 1] == 0.5:

                    gameOver = True

                    reward = self.negReward

                elif self.screenMap[snakeY][snakeX + 1] == 1:

                    reward = self.posReward

                    self.moveSnake((snakeY, snakeX + 1), True)

                elif self.screenMap[snakeY][snakeX + 1] == 0:

                    self.moveSnake((snakeY, snakeX + 1), False)

            else:

                gameOver = True

                reward = self.negReward 

        elif action == 3:

            if snakeX > 0:

                if self.screenMap[snakeY][snakeX - 1] == 0.5:

                    gameOver = True

                    reward = self.negReward

                elif self.screenMap[snakeY][snakeX - 1] == 1:

                    reward = self.posReward

                    self.moveSnake((snakeY, snakeX - 1), True)

                elif self.screenMap[snakeY][snakeX - 1] == 0:

                    self.moveSnake((snakeY, snakeX - 1), False)

            else:

                gameOver = True

                reward = self.negReward 
```

简单地处理每一个单个动作，就像你处理向上的动作一样。检查蛇是否没有撞到墙壁，检查蛇前面有什么，并相应地更新蛇的位置、`reward`和`gameOver`。

在这个方法中还有两个步骤；让我们直接进入第一个问题:

```
 # Drawing the screen, updating last move and waiting the wait time specified

        self.drawScreen()

        self.lastMove = action

        pg.time.wait(self.waitTime) 
```

你通过在上面画蛇和苹果来更新我们的屏幕，然后将`lastMove`改为`action`，因为你的蛇已经移动了，现在它正在向`action`方向移动。

这个方法的最后一步是返回游戏现在的样子，获得的奖励是什么，以及你是否输了，就像这样:

```
 # Returning the new frame of the game, the reward obtained and whether the game has ended or not

        return self.screenMap, reward, gameOver 
```

`screenMap`给你你需要的关于游戏在完成一个动作后是什么样子的信息，`reward`给你完成这个动作收集到的奖励，`gameOver`告诉你你是否输了。

这个方法就是这样！要拥有一个完整的`Environment`类，你只需要创建一个将重置环境的函数，就像这个`reset`方法:

```
 # Making a function that resets the environment

    def reset(self):

        self.screenMap  = np.zeros((self.nRows, self.nColumns))

        self.snakePos = list()

        for i in range(self.initSnakeLen):

            self.snakePos.append((int(self.nRows / 2) + i, int(self.nColumns / 2)))

            self.screenMap[int(self.nRows / 2) + i][int(self.nColumns / 2)] = 0.5

        self.screenMap[self.applePos[0]][self.applePos[1]] = 1

        self.lastMove = 0 
```

它只是将游戏棋盘(`screenMap`)以及蛇的位置重置为默认位置，即棋盘的中间位置。它还将苹果的位置设置为与上一轮相同。

恭喜你！您刚刚完成了环境的构建。现在，我们将进行第二步，建造大脑。

### 第二步——打造大脑

这是你用卷积神经网络构建我们的大脑的地方。您还将为它的训练设置一些参数，并定义一个加载预训练模型进行测试的方法。

我们开始吧！

像往常一样，首先导入将要使用的库，如下所示:

```
# Importing the libraries

import keras

from keras.models import Sequential, load_model

from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten

from keras.optimizers import Adam 
```

正如你可能已经注意到的，所有的类都是 Keras 库的一部分，这也是你在本章将要用到的。Keras 实际上是您将在该文件中使用的唯一库。现在让我们来看一下这些类和方法:

1.  `Sequential`–允许您初始化神经网络的类，并定义该网络的一般结构。
2.  `load_model`–从文件加载模型的功能。
3.  `Dense`–在人工神经网络(ANN)中创建完全连接层的类。
4.  `Dropout`——一个让我们的网络辍学的班级。你已经在第八章、*物流人工智能——仓库里的机器人*中看到过它的使用。
5.  `Conv2D`–构建卷积层的类。
6.  `MaxPooling2D`–构建最大池层的类。
7.  `Flatten`–一个执行扁平化的类，这样你就有了一个经典 ANN 的输入。
8.  一个优化器，它将优化你的神经网络。这是训练 CNN 时用的。

现在您已经导入了您的库，您可以继续创建一个名为`Brain`的类，其中使用了所有这些类和方法。首先定义一个类和`__init__`方法，就像这样:

```
# Creating the Brain class

class Brain():

    def __init__(self, iS = (100,100,3), lr = 0.0005):

        self.learningRate = lr

        self.inputShape = iS

        self.numOutputs = 4

        self.model = Sequential() 
```

可以看到`__init__`方法带了两个参数:`iS`(输入形状)和`lr`(学习速率)。然后你定义一些将与这个类关联的变量:`learningRate`、`inputShape`、`numOutputs`。将`numOutputs`设置为`4`，因为这是我们的 AI 可以采取的动作数量。然后，在最后一行，创建一个空模型。为此，使用我们之前导入的`Sequential`类。

这样做将允许您将所有需要的层添加到模型中。这正是你对这些线所做的:

```
 # Adding layers to the model   #20

        self.model.add(Conv2D(32, (3,3), activation = 'relu', input_shape = self.inputShape))   #21

        #22

        self.model.add(MaxPooling2D((2,2)))   #23

        #24

        self.model.add(Conv2D(64, (2,2), activation = 'relu'))   #25

        #26

        self.model.add(Flatten())   #27

        #28

        self.model.add(Dense(units = 256, activation = 'relu'))   #29

        #30

        self.model.add(Dense(units = self.numOutputs))   #31 
```

让我们将这段代码分成几行:

**第 21 行**:给你的模型添加一个新的卷积层。它有 32 个带 ReLU 激活功能的 3x3 过滤器。您还需要在这里指定输入形状。记住，输入形状是该函数的参数之一，保存在`inputShape`变量下。

**第 23 行**:你添加一个最大池层。窗口的大小是 2x2，这将使我们的特征地图缩小`2`。

第 25 行:你添加第二个卷积层。这次它有 64 个 2x2 过滤器，具有相同的 ReLU 激活功能。为什么这次要重新开始？我实验性地尝试了一些其他的激活功能，结果证明对于这个 AI ReLU 效果最好。

应用卷积后，你得到了新的特征图，你将其展平为 1D 矢量。这正是这条线所做的——它将 2D 图像展平为 1D 向量，然后您可以将它用作神经网络的输入。

**第 29 行**:现在，你处于完整的连接步骤——你正在构建传统的人工神经网络。这条特定的线为我们的模型添加了一个带有`256`神经元和 ReLU 激活函数的新隐藏层。

**第 31 行**:你创建了神经网络的最后一层——输出层。它有多大？嗯，它必须有尽可能多的神经元，因为你可以采取行动。你之前把那个值放在`numOutputs`变量下，值等于`4`。这里没有指定激活函数，这意味着默认情况下激活函数将是线性的。事实证明，在这种情况下，在训练期间，使用线性输出比 Softmax 输出效果更好；这使得培训更加有效。

你还必须`compile`你的模型。这将告诉您的代码如何计算误差，以及在训练您的模型时使用哪个优化器。您可以用这一行代码来完成:

```
 # Compiling the model

        self.model.compile(loss = 'mean_squared_error', optimizer = Adam(lr = self.learningRate)) 
```

在这里，你使用一个方法来做这件事，这个方法是`Sequential`类的一部分(这就是为什么你可以使用你的模型来调用它)。该方法被称为`compile`，在本例中，它有两个参数。`loss`是一个告诉 AI 如何计算你的神经网络的误差的函数；你会用`mean_squared_error`。第二个参数是优化器。您已经导入了`Adam`优化器，并且在这里使用了它。这个优化器的学习率是这个类的`__init__`方法的参数之一，它的值由`learningRate`变量表示。

在这个类中只剩下一步要做——创建一个从文件中加载模型的函数。您可以使用以下代码来完成:

```
 # Making a function that will load a model from a file

    def loadModel(self, filepath):

        self.model = load_model(filepath)

        return self.model 
```

您可以看到您已经创建了一个名为`loadModel`的新函数，它有一个参数——`filepath`。此参数是预训练模型的文件路径。一旦定义了函数，就可以从这个文件路径加载模型了。为此，您可以使用前面导入的`load_model`方法。这个函数采用相同的参数–`filepath`。然后在最后一行，您返回加载的模型。

恭喜你！你刚刚完成大脑的构建。

让我们在我们的道路上前进，并建立经验回放记忆。

### 第三步——建立体验回放记忆

现在，您将构建这个记忆，稍后，您将从这个记忆的小批量中训练您的模型。该存储器将包含关于采取动作之前的游戏状态、采取的动作、获得的奖励以及执行动作之后的游戏状态的信息。

我有一些好消息要告诉你——你记得这个代码吗？

```
# AI for Games - Beat the Snake game

# Implementing Deep Q 学习with Experience Replay

# Importing the libraries

import numpy as np

# IMPLEMENTING DEEP Q 学习WITH EXPERIENCE REPLAY

class Dqn(object):

    # INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE DQN

    def __init__(self, max_memory = 100, discount = 0.9):

        self.memory = list()

        self.max_memory = max_memory

        self.discount = discount

    # MAKING A METHOD THAT BUILDS THE MEMORY IN EXPERIENCE REPLAY

    def remember(self, transition, game_over):

        self.memory.append([transition, game_over])

        if len(self.memory) > self.max_memory:

            del self.memory[0]

    # MAKING A METHOD THAT BUILDS TWO BATCHES OF INPUTS AND TARGETS BY EXTRACTING TRANSITIONS FROM THE MEMORY

    def get_batch(self, model, batch_size = 10):

        len_memory = len(self.memory)

        num_inputs = self.memory[0][0][0].shape[1]

        num_outputs = model.output_shape[-1]

        inputs = np.zeros((min(len_memory, batch_size), num_inputs))

        targets = np.zeros((min(len_memory, batch_size), num_outputs))

        for i, idx in enumerate(np.random.randint(0, len_memory, size = min(len_memory, batch_size))):

            current_state, action, reward, next_state = self.memory[idx][0]

            game_over = self.memory[idx][1]

            inputs[i] = current_state

            targets[i] = model.predict(current_state)[0]

            Q_sa = np.max(model.predict(next_state)[0])

            if game_over:

                targets[i, action] = reward

            else:

                targets[i, action] = reward + self.discount * Q_sa

        return inputs, targets 
```

您将使用几乎相同的代码，只有两处小的改动。

首先，你去掉这一行:

```
 num_inputs = self.memory[0][0][0].shape[1] 
```

然后修改这一行:

```
 inputs = np.zeros((min(len_memory, batch_size), num_inputs)) 
```

对这一个:

```
 inputs = np.zeros((min(len_memory, batch_size), self.memory[0][0][0].shape[1],self.memory[0][0][0].shape[2],self.memory[0][0][0].shape[3])) 
```

你为什么要做这件事？你去掉了第一行，因为你不再有输入的 1D 向量。现在你有了一个 3D 数组。

然后，如果你仔细看，你会发现你实际上并没有改变`inputs`。之前，您有一个 2D 数组，其中一个维度是批量大小，另一个维度是输入数量。现在，事情很相似；第一个维度同样是批量大小，最后三个维度也对应于输入的大小！

因为我们的输入现在是一个 3D 数组，所以您编写了`.shape[1]`、`.shape[2]`和`.shape[3]`。那些形状到底是什么？

`.shape[1]`是游戏中的行数(在您的例子中是 10)。`.shape[2]`是游戏中的列数(在您的例子中是 10)。`.shape[3]`是最后堆叠在一起的帧的数量(在您的例子中是 4)。

如你所见，你并没有真正改变什么。你只是让代码为我们的 3D 输入工作。

我还将这个`dqn.py`文件重命名为`DQN.py`，并将类`DQN`重命名为`Dqn`。

就是这样！这可能比你们大多数人想象的要简单得多。

你终于可以开始训练你的模型了。我们将在下一节——训练人工智能中完成。

### 第四步——训练人工智能

这是迄今为止最重要的一步。这里我们终于教会我们的 AI 玩蛇了！

像往常一样，首先导入您需要的库:

```
# Importing the libraries

from environment import Environment

from brain import Brain

from DQN import Dqn

import numpy as np

import matplotlib.pyplot as plt 
```

在前三行中，您导入先前创建的工具，包括`Brain`、`Environment`和体验回放记忆。

然后，在接下来的两行中，导入将要使用的库。其中包括 NumPy 和 Matplotlib。你已经认出了前者。后者将用于显示您的模型的性能。具体来说，它将帮助您显示一个图表，每 100 场游戏，将显示您收集的苹果的平均数量。

这一步就到此为止。现在，为您的代码定义一些超参数:

```
# Defining the parameters

memSize = 60000

batchSize = 32

learningRate = 0.0001

gamma = 0.9

nLastStates = 4

epsilon = 1.

epsilonDecayRate = 0.0002

minEpsilon = 0.05

filepathToSave = 'model2.h5' 
```

我将在这个列表中解释它们:

1.  `memSize`–体验回放记忆的最大容量。
2.  `batchSize`–您在每次迭代中从您的经验回放记忆中获得的一批输入和目标的大小，以供您的模型进行训练。
3.  `learningRate`–您的`Adam`优化器在`Brain`中的学习率。
4.  `gamma`–体验回放记忆的折扣系数。
5.  `nLastStates`–将最后几帧保存为游戏的当前状态。请记住，您将在`Brain`中向 CNN 输入一个大小为`nRows` x `nColumns` x `nLastStates`的 3D 数组。
6.  `epsilon`–初始ε，采取随机行动的几率。
7.  `epsilonDecayRate`–每一场游戏/每一个时代后，你减少多少。
8.  `minEpsilon`–可能的最低ε，在此之后不能再调整得更低。
9.  `filepathToSave`–您想要保存模型的位置。

这就对了——您已经定义了超参数。您将在以后编写其余代码时使用它们。现在，你必须创造一个环境，一个大脑，和一个经验回放记忆:

```
# Creating the Environment, the Brain and the Experience Replay Memory

env = Environment(0)

brain = Brain((env.nRows, env.nColumns, nLastStates), learningRate)

model = brain.model

dqn = Dqn(memSize, gamma) 
```

您可以看到在第一行中您创建了一个`Environment`类的对象。您需要在这里指定一个变量，它是您的环境的减速(移动之间的等待时间)。你不想在训练过程中有任何减速，所以你在这里输入`0`。

在下一行中，您创建了一个`Brain`类的对象。它需要两个参数——输入形状和学习速率。正如我多次提到的，输入形状将是一个大小为`nRows` x `nColumns` x `nLastStates`的 3D 数组，所以这就是你在这里输入的内容。第二个参数是学习率，因为您已经为此创建了一个变量，您只需输入这个变量的名称—`learningRate`。在这一行之后，您获取这个`Brain`类的模型，并在您的代码中创建这个模型的一个实例。把事情简单化，称之为`model`。

在最后一行中，您创建了一个`Dqn`类的对象。它有两个参数——内存的最大大小和内存的折扣系数。您已经为此指定了两个变量，`memSize`和`gamma`，所以在这里使用它们。

现在，你需要写一个函数来重置你的 AI 的状态。您需要它，因为状态非常复杂，在主代码中重置它们会把事情搞得一团糟。它看起来是这样的:

```
# Making a function that will initialize game states   #30

def resetStates():   #31

    currentState = np.zeros((1, env.nRows, env.nColumns, nLastStates))   #32

    #33

    for i in range(nLastStates):   #34

        currentState[:,:,:,i] = env.screenMap   #35

    #36

    return currentState, currentState   #37 
```

让我们把它分成几行:

**第 31 行**:你定义了一个名为`resetStates`的新函数。这不需要任何争论。

**第 32 行**:你创建了一个名为`currentState`的新数组。里面全是零，但你可能会问为什么是 4D；输入不应该是我们说的 3D 吗？你说得太对了，以后也是。第一个维度称为批量大小，简单地说就是你一次向神经网络输入多少输入。您一次只能输入一个数组，所以第一个大小是`1`。接下来的三个尺寸对应输入的尺寸。

**第 34-35 行**:在一个将被执行`nLastStates`次的`for`循环中，你将你的 3D 状态中的每一层的棋盘设置为你的环境中游戏棋盘的当前初始外观。你的状态中的每一帧最初看起来都是一样的，就像你开始游戏时棋盘的样子。

**第 37 行**:该函数将返回两个`currentStates`。为什么？这是因为你需要两个游戏状态数组。一个在你采取行动前代表董事会，另一个在你采取行动后代表董事会。

现在，您可以开始编写整个培训的代码了。首先，创建几个有用的变量，如下所示:

```
# Starting the main loop

epoch = 0

scores = list()

maxNCollected = 0

nCollected = 0.

totNCollected = 0 
```

会告诉你现在处于哪个时代/游戏。`scores`是你每 100 场/时代后保存每场平均分数的列表。`maxNCollected`告诉你训练到目前为止获得的最高分，而`nCollected`是每场比赛/每一个时期的分数。最后一个变量，`totNCollected`，告诉你在 100 个时代/游戏中你收集了多少苹果。

现在你开始一个重要的、无限的`while`循环，就像这样:

```
while True:

    # Resetting the environment and game states

    env.reset()

    currentState, nextState = resetStates()

    epoch += 1

    gameOver = False 
```

在这里，你迭代每个游戏，每个时代。这就是为什么你在第一行重启环境，在下一行创建新的`currentState`和`nextState`，将`epoch`加 1，并将`gameOver`设置为`False`，因为你显然还没有输。

注意这个循环没有结束；因此，训练从未停止。我们这样做是因为我们没有设定何时停止训练的目标，因为我们还没有定义对我们的 AI 来说什么是令人满意的结果。我们可以计算平均结果，或者类似的度量，但是训练可能需要太长时间。我更喜欢继续训练，你可以随时停止训练。停止的好时机是当 AI 达到平均每场 6 个苹果时，或者如果你想要更好的性能，你甚至可以达到每场 12 个苹果。

您已经开始了第一个循环，它将遍历每个时期。现在你需要创建第二个循环，人工智能执行动作，更新环境，并训练你的 CNN。从下面这几行开始:

```
 # Starting the second loop in which we play the game and teach our AI

    while not gameOver: 

        # Choosing an action to play

        if np.random.rand() < epsilon:

            action = np.random.randint(0, 4)

        else:

            qvalues = model.predict(currentState)[0]

            action = np.argmax(qvalues) 
```

正如我提到的，这是你的人工智能做出决定，移动和更新环境的循环。首先初始化一个`while`循环，只要没有失败，这个循环就会被执行；即只要`gameOver`设置为`False`即可。

然后，可以看到`if`条件。这是你的人工智能做决定的地方。如果范围(0，1)中的随机值低于ε，则将执行随机操作。否则，你可以根据游戏的当前状态来预测 Q 值，并从这些 Q 值中选择 Q 值最高的指数。这将是你的人工智能执行的动作。

然后，您必须更新您的环境:

```
 # Updating the environment

        state, reward, gameOver = env.step(action) 
```

您从您的`Environment`类对象中使用`step`方法。它有一个参数，即您执行的动作。它还会返回执行此操作后从游戏中获得的新画面，以及获得的奖励和游戏结束信息。你很快就会用到这些变量。

记住，这个方法从你的游戏中返回一个 2D 帧。这意味着您必须将这个新框架添加到您的`nextState`中，并移除最后一个框架。您可以用这些行来完成:

```
 # Adding new game frame to the next state and deleting the oldest frame from next state

        state = np.reshape(state, (1, env.nRows, env.nColumns, 1))

        nextState = np.append(nextState, state, axis = 3)

        nextState = np.delete(nextState, 0, axis = 3) 
```

如你所见，首先你重塑了`state`，因为它是 2D，而`currentState`和`nextState`都是 4D。然后你添加这个新的，重新塑造的框架到第三轴的`nextState`。为什么是第三？这是因为第三个索引引用了这个数组的第四维，它将 2D 帧保存在里面。在最后一行中，您只需从索引为 0 的`nextState`中删除第一帧(最老的帧保留在最低的索引上)。

现在，你可以`remember`在你的经历中回放记忆中的这种转变，并从这种记忆中随机抽取一批来训练你的模型。你可以用这几行来做:

```
 # Remembering the transition and training our AI

        dqn.remember([currentState, action, reward, nextState], gameOver)

        inputs, targets = dqn.get_batch(model, batchSize)

        model.train_on_batch(inputs, targets) 
```

在第一行中，您将这个转换添加到内存中。它包含关于采取行动前的游戏状态(`currentState`)、采取的行动(`action`)、获得的奖励(`reward`)和采取该行动后的游戏状态(`nextState`)的信息。你也记住了`gameOver`的身份。在下面两行中，您从内存中随机取出一批输入和目标，并根据它们训练您的模型。

完成后，您可以检查您的蛇是否收集了一个苹果，并更新`currentState`。您可以使用以下代码行来实现:

```
 # Checking whether we have collected an apple and updating the current state

        if env.collected:

            nCollected += 1

        currentState = nextState 
```

在前两行中，您检查蛇是否收集了一个苹果，如果收集了，您增加`nCollected`。然后通过将它的值设置为`nextState`的值来更新`currentState`。

现在，你可以退出这个循环了。你还有几件事要做:

```
 # Checking if a record of apples eaten in a around was beaten and if yes then saving the model

    if nCollected > maxNCollected and nCollected > 2:

        maxNCollected = nCollected

        model.save(filepathToSave)

    totNCollected += nCollected

    nCollected = 0 
```

你检查你是否打破了一轮吃苹果数量的记录(这个数字必须大于 2)，如果你打破了，你更新记录并将你当前的模型保存到你之前指定的文件路径。你也增加`totNCollected`并将`nCollected`重置为 0 用于下一个游戏。

然后，在 100 场比赛后，你显示平均分数，像这样:

```
 # Showing the results each 100 games

    if epoch % 100 == 0 and epoch != 0:

        scores.append(totNCollected / 100)

        totNCollected = 0

        plt.plot(scores)

        plt.xlabel('Epoch / 100')

        plt.ylabel('Average Score')

        plt.savefig('stats.png')

        plt.close() 
```

您有一个名为`scores`的列表，其中存储了 100 场比赛后的平均得分。您向它追加一个新值，然后重置该值。然后使用之前导入的 Matplotlib 库，在图形上显示`scores`。该图形每 100 个游戏/时期保存在`stats.png`中。

然后你降低ε，像这样:

```
 # Lowering the epsilon

    if epsilon > minEpsilon:

        epsilon -= epsilonDecayRate 
```

在`if`条件下，确保ε不会低于最小阈值。

在最后一行中，您显示了关于每个游戏的一些附加信息，如下所示:

```
 # Showing the results each game

    print('Epoch: ' + str(epoch) + ' Current Best: ' + str(maxNCollected) + ' Epsilon: {:.5f}'.format(epsilon)) 
```

你显示当前的纪元(游戏)，当前在一个游戏中收集苹果数量的记录，以及当前的 epsilon。

就是这样！恭喜你！您刚刚构建了一个将训练模型的函数。记住，这种训练会无限持续下去，直到你决定结束为止。当你对它满意时，你会想要测试它。为此，您需要一个短文件来测试您的模型。我们开始吧！

### 步骤 5–测试人工智能

这将是很短的一段，所以不用担心。您将马上运行这段代码！

和往常一样，首先导入您需要的库:

```
# Importing the libraries

from environment import Environment

from brain import Brain

import numpy as np 
```

这一次您将不会使用 DQN 内存或 Matplotlib 库，因此您不会导入它们。

您还需要指定一些超参数，如下所示:

```
# Defining the parameters

nLastStates = 4

filepathToOpen = 'model.h5'

slowdown = 75 
```

在这段代码的后面，您将需要`nLastStates`。您还为将要测试的模型创建了一个文件路径。最后，还有一个变量，您将使用它来指定每次移动后的等待时间，这样您就可以清楚地看到您的 AI 如何执行。

同样，您创建了一些有用的对象，比如一个`Environment`和一个`Brain`:

```
# Creating the Environment and the Brain

env = Environment(slowdown)

brain = Brain((env.nRows, env.nColumns, nLastStates))

model = brain.loadModel(filepathToOpen) 
```

在`Environment`的括号中，输入`slowdown`，因为这是这个类的参数。您还创建了一个`Brain`类的对象，但是这一次，您没有指定学习率，因为您不会训练您的模型。在最后一行，你使用`Brain`类中的`loadModel`方法加载一个预先训练好的模型。该方法接受一个参数，即加载模型的文件路径。

同样，您需要一个函数来重置状态。您可以使用与之前相同的代码，因此只需复制并粘贴以下代码行:

```
# Making a function that will reset game states

def resetStates():

    currentState = np.zeros((1, env.nRows, env.nColumns, nLastStates))

    for i in range(nLastStates):

        currentState[:,:,:,i] = env.screenMap

    return currentState, currentState 
```

现在，你可以像以前一样进入主`while`循环。然而，这一次，您将不定义任何变量，因为您不需要任何变量:

```
# Starting the main loop

while True:

    # Resetting the game and the game states

    env.reset()

    currentState, nextState = resetStates()

    gameOver = False 
```

如你所见，你已经开始了这个无限的循环。再一次，你必须重启环境、状态和游戏，每一次迭代。

现在，你可以进入游戏的`while`循环，在那里你采取行动，更新环境，等等:

```
 # Playing the game

    while not gameOver: 

        # Choosing an action to play

        qvalues = model.predict(currentState)[0]

        action = np.argmax(qvalues) 
```

这一次，你不需要任何`if`语句。毕竟，你在测试你的人工智能，所以你不能在这里有任何随机的行动。

您再次更新了环境:

```
 # Updating the environment

        state, _, gameOver = env.step(action) 
```

你并不真正关心奖励，所以只需放置`_`而不是`reward`。在采取行动后，环境仍然返回帧，以及游戏结束的信息。

由于这个事实，你需要重塑你的`state`并以和以前一样的方式更新`nextState`:

```
 # Adding new game frame to next state and deleting the oldest one from next state

        state = np.reshape(state, (1, env.nRows, env.nColumns, 1))

        nextState = np.append(nextState, state, axis = 3)

        nextState = np.delete(nextState, 0, axis = 3) 
```

在最后一行，您需要像在另一个文件中一样更新`currentState`:

```
 # Updating current state

        currentState = nextState 
```

这一节的编码到此结束！然而，这并不是本章的结束。你仍然需要运行代码。

## 演示

不幸的是，由于 PyGame 不被 Google Colab 支持，你需要使用 Anaconda。

幸运的是，你应该在*第 10 章*、*自动驾驶汽车的人工智能——建造一辆自动驾驶汽车*之后安装它，这样安装所需的包和库会更容易。

### 安装

首先，在 Anaconda 中创建一个新的虚拟环境。这一次，我将从 PC 上带您完成 Anaconda 提示符下的安装，这样您就可以看到它是如何在任何系统上完成的。

Windows 用户请打开 PC 上的 Anaconda 提示符，Mac/Linux 用户请打开 Mac/Linux 上的终端。然后键入:

```
conda create -n snake python=3.6 
```

就像这样:

![](img/B14110_13_05.png)

然后，点击键盘上的`Enter`。您应该得到或多或少像这样的东西:

![](img/B14110_13_06.png)

在键盘上键入`y`并再次点击`Enter`。安装好所有东西后，在 Anaconda 提示符下键入:

```
conda activate snake 
```

![](img/B14110_13_07.png)

又打了一次`Enter`又打了一次。现在在左边，你应该看到**蛇**写成了**基**。这意味着您处于新创建的 Anaconda 环境中。

现在您需要安装所需的库。第一个我 s Keras:

```
conda install -c conda-forge keras 
```

![](img/B14110_13_08.png)

写完之后，点击`Enter`。当你听到这个的时候:

![](img/B14110_13_09.png)

再次键入`y`并再次点击`Enter`。一旦你安装了它，你需要安装 PyGame 和 Matplotlib。

第一个可以通过输入`pip install pygame`安装，第二个可以通过输入`pip install matplotlib`安装。安装过程与您刚才安装 Keras 的过程相同。

好，现在你可以运行你的代码了！

如果您出于某种原因意外关闭了 Anaconda 提示符/终端，请重新打开它并键入以下内容来激活我们刚刚创建的`snake`环境:

```
conda activate snake 
```

![](img/B14110_13_10.png)

然后点击`Enter`。在这样做之后，我得到了一堆警告，你可能也会看到类似的警告，但是不要担心:

![](img/B14110_13_11.png)

现在，您需要将该控制台导航到包含您想要运行的文件的文件夹，在本例中为`train.py`。我建议你把`Chapter 13`的所有代码放在你桌面上一个叫做`Snake`的文件夹里。然后你就可以完全按照我现在给你的指示去做了。要导航到该文件夹，您需要使用`cd`命令。

首先，通过运行`cd Desktop`导航到桌面，就像这样:

![](img/B14110_13_12.png)

然后进入你创建的`Snake`文件夹。就像前面的命令一样，运行`cd Snake`，就像这样:

![](img/B14110_13_13.png)

你越来越接近了。要训练新模型，您需要键入:

```
python train.py 
```

![](img/B14110_13_14.png)

然后打`Enter`。这或多或少是你应该看到的:

![](img/B14110_13_15.png)

你既有一个在游戏左边的窗口，也有一个在终端右边的窗口，通知你每一个游戏(每一个纪元)。

恭喜你！你只是把这一章的代码砸了，给 Snake 造了一个 AI。请耐心等待！训练它可能需要几个小时。

那么，你能期待什么样的结果呢？

### 结果

首先，确保在您的 Anaconda 提示符/终端上一个接一个地跟踪结果。一个时代是一场游戏。经过数千次游戏(历元)，你会看到分数增加，以及蛇的大小增加。

经过数千代的训练，虽然蛇不会填满整个地图，但你的人工智能可以与人类媲美。下面是 25000 个纪元后的一些图片。

![](img/B14110_13_16.png)

图 5:结果示例 1

![](img/B14110_13_17.png)

图 6:结果示例 2

您还将获得一个在文件夹(`stats.png`)中创建的图表，显示各个时期的平均分数。这是我在训练我们的人工智能超过 25，000 个时期时得到的图表:

![](img/B14110_13_18.png)

图 7:超过 25，000 个时期的平均分数

你可以看到我们的 AI 达到了场均 10-11 分。考虑到训练前它对游戏一无所知，这并不坏。

如果您使用 GitHub 本章附带的预训练模型`model.h5`运行`test.py`文件，您也可以看到相同的结果。要做到这一点，您只需要在 Anaconda 提示符/终端中输入(仍然在您桌面上包含所有`Chapter 13`代码的同一个`Snake`文件夹中，仍然在`snake`虚拟环境中):

```
python test.py 
```

如果你想在训练后测试你的模型，你只需要在`test.py`文件中用`model2.h5`替换`model.h5`。这是因为在训练过程中，你的人工智能神经网络的权重将被保存在一个名为`model2.h5`的文件中。然后在 Anaconda 提示符/终端中重新输入`python test.py`，享受自己的结果。

## 总结

在本书的最后一个实用章节中，我们为 Snake 构建了一个深度卷积 Q 学习模型。在我们建造任何东西之前，我们必须定义我们的人工智能将会看到什么。我们确定我们需要堆叠多个帧，这样我们的 AI 就可以看到它移动的连续性。这是我们卷积神经网络的输入。输出是对应于四种可能移动的 Q 值:向上、向下、向左和向右。我们奖励我们的 AI 吃了一个苹果，惩罚它输了，稍微惩罚它执行了任何动作(活罚)。在运行了 25，000 场游戏后，我们可以看到我们的 AI 每场比赛能够吃 10-11 个苹果。

我希望你喜欢它！