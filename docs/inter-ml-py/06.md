

# 四、特征重要性和影响的基础

在本书的第一部分，我们介绍了机器学习解释的概念、挑战和目的。本章从第二部分开始，深入到用于诊断模型和理解其底层数据的大量方法中。解释方法回答的一个最大的问题是:*什么对模型最重要，有多重要？*准确地说，解释方法可以阐明特征的整体重要性，以及它们如何单独或组合影响模型的结果。本章将为解决这些问题提供理论和实践基础。

在本章中，我们将首先使用几个 scikit-learn 模型的内在参数来推导最重要的特征。然后，意识到这些结果是多么的不一致，我们将学习如何使用**排列特征重要性** ( **PFI** )来直观可靠地排列特征。此外，为了传达单个特征对预测的边际影响，我们将研究如何渲染和解释**部分依赖图**(**PDP**)。最后，我们将探索**个人条件期望** ( **ICE** )图，以解释当一个特征改变时的预测变化。

以下是我们将在本章中涉及的主要话题:

*   衡量功能对结果的影响
*   练习 PFI
*   解释 PDP
*   解释冰图

# 技术要求

本章的例子使用了`mldatasets`、`pandas`、`numpy`、`sklearn`、`matplotlib`和`PDPbox`库。关于如何安装所有这些库的说明在本书的序言中。本章的代码位于以下位置:

[https://github . com/packt publishing/Interpretable-Machine-Learning-with-Python/tree/master/chapter 04](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python/tree/master/Chapter04)

# 使命

我们都听说过这样的刻板印象:长子非常负责和专横；最小的被宠坏了，无忧无虑；而且老二是个嫉妒内向的人！事实证明，著名的心理学研究人员已经联系了你的数据科学咨询公司，并就出生顺序如何影响人格进行了几项小型实证研究。但是他们刚刚从*开源心理测量项目*获得了超过 40，000 个在线测验条目的数据集。他们持怀疑态度，因为它是在网上提交的，他们从未进行过如此大规模的研究，所以这是一个未知的领域。出于这些原因，他们希望有一个精通机器学习的第三方用新鲜的眼光来处理这个问题。他们希望了解测验答案和出生顺序之间的关系，并确定他们是否可以在实证研究中使用任何问题，或者在线测验是否是一种可靠的开始方法。你的公司已经同意解释这些问题。

## 性格和出生顺序

一个多世纪以来，关于兄弟姐妹动态如何影响不同的性格特征的理论一直在流传，在某种程度上，父母的教养方式本身在很大程度上是由出生顺序决定的。这些理论大多是在“西方”国家形成和研究的，从英国人弗朗西斯·高尔顿(1874)将长子与更高的智力联系起来，到荷兰人布拉姆·布恩克(1997)将次子与更高的嫉妒联系起来的研究。最近，更细致的研究将性别、年龄差距和社会经济地位纳入性格差异。即便如此，这些理论也很少得到广泛的认同。此外，众所周知，文化对养育方式和兄弟姐妹关系有影响，所以西方理论在其他文化中并不适用。

另一方面，已经有了一系列用于评估人格的心理测量方法，使用问卷将个人分为不同的类别和等级。该数据集包括这些方法之一的答案，即**国际人格项目库** ( **IPIP** )“大五”测试。“大五”测试是学术心理学中广泛接受的人格评估模型。该数据集还包括 26 个专门设计的问题，旨在找到与不同出生顺序相关的特征，尽管他们有确切的出生顺序，但研究人员只对以下三类感兴趣:

*   **长子**:参与者是一个以上孩子中的第一个。
*   中间的孩子:参与者既不是一个以上孩子中的第一个也不是最后一个。
*   最后出生的:参与者是一个以上孩子中的最后一个。

原始数据集包括来自世界各地的条目，这就是为什么研究人员要求特别关注以英语为主要语言的国家，因为这些问题是英语的。他们无法证实这些问题没有文化偏见。

# 走近

眼下的任务是找出哪些特征——无论是测验答案、技术还是人口统计细节——最能表明出生顺序，以及它们是否能可靠地用于这一目的。一种方法是通过创建分类模型来预测出生顺序，然后执行以下操作:

*   使用模型的内在参数来发现哪些特征对模型的影响最大。这个概念叫做**特征重要性**，是一种**全局模块化解释方法**。这在第二章[](B16383_02_ePub_RK.xhtml#_idTextAnchor031)**，* *关键* *可解释性概念*中有所解释，但我们将在本章进行更详细的阐述。*
**   使用一种更可靠的基于排列的方法，称为 **PFI** ，进一步探索特征的重要性。*   使用**PDP**检查对最重要功能结果的边际影响。这样，我们就可以知道哪些特征值与预测最相关。*   通过 ICE 图获得单个特征如何影响模型预测的更精细的可视化。*

 *我们开始吧！

# 制剂

您将在此处找到该示例的代码:

[https://github . com/packt publishing/Interpretable-Machine-Learning-with-Python/blob/master/chapter 04/birth order . ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python/blob/master/Chapter04/BirthOrder.ipynb)

## 加载库

若要运行此示例，您需要安装以下库:

*   `mldatasets`加载数据集
*   `pandas`和`numpy`来操纵它
*   `sklearn` (scikit-learn)拆分数据并拟合模型
*   `matplotlib`和`pdpbox`来可视化的解释

您应该首先使用以下代码加载它们:

```py
import math
import mldatasets
import pandas as pd
import numpy as np
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler 
from sklearn.model_selection import train_test_split
from sklearn import metrics, linear_model, tree,\
        discriminant_analysis, ensemble, neural_network, inspection
import matplotlib.pyplot as plt
from pdpbox import pdp
```

现在，我们可以继续进行数据准备和理解步骤。

## 了解和准备数据

我们将数据加载到一个名为`birthorder_df`的数据帧中，如下所示:

```py
birthorder_df = mldatasets.load("personality-birthorder",\ 
                                 prepare=True)
```

`prepare=True`确保执行一些数据准备工作，例如按大多数说英语的国家进行过滤和分类编码。这种设置将为我们节省一些时间。应该有将近 26，000 条记录和 97 列。我们可以用`print(birthorder_df.shape`来验证这一点，它应该返回`(25813, 97)`，与我们预期的一致。

### 数据字典

我们不会在这里描述数据字典的每一栏，因为有太多的，主要是关于特定的个性问题。不过，如果你对这些特定的问题感到好奇，你可以在一个名为`FBPS-ValidationData-Codebook.txt`的文件中找到它们，该文件位于:

[https://www . ka ggle . com/lucasgreenwell/first born-personality-scale-responses](https://www.kaggle.com/lucasgreenwell/firstborn-personality-scale-responses)

然而，我们将提供数据字典的 76 个心理学问题、6 个人口统计学特征和 5 个技术特征的简要概述。

数据字典的心理特征(测验答案)概述如下:

*   `Q1`，`Q2`，..`Q26`:序数；26 个出生顺序研究问题的答案(基于 5 点李克特量表，从 1 =不同意到 3 =中立到 5 =同意，以及 0 =没有答案)。
*   `EXT1`、`EXT2`、…`EXT10`；`EST1`、`EST2`、…`EST10`；`AGR1`、`AGR2`、…`AGR10`；`CSN1`、`CSN2`、…`CSN10`；`OPN1`、`OPN2`、… `OPN10`:序数；IPIP“五大”问卷。它由 50 个问题组成(答案也是 5 分制的李克特量表，从 1 =不同意到 3 =中立到 5 =同意，以及 0 =没有答案)。

数据字典的人口统计特征概述如下:

*   `age`:序数；参与者的年龄(年)
*   `engnat`:二进制；英语是否是他们的母语(1 =是，2 =否)
*   `gender`:绝对的；性别(男性、女性、其他，未定义)
*   `birthn`:序数；父母所生子女总数从 1 到 10，11(其他)
*   `country`:绝对的；参与者的国家(用两个字母的代码表示)
*   `birthorder`:序数；目标出生顺序(1:第一胎，2:第二胎，3:最后一胎)

数据字典的技术特征概述如下:

*   `source`:绝对的；用户如何基于**超文本传输协议** ( **HTTP** )推荐人(1 =直接来自谷歌，2 =网站首页，3 =任何其他)进行个性测试
*   `screensize`:序数；用于测试的屏幕尺寸(2 =大于 600 **像素** ( **px** )每边，1 =小于 600 像素)
*   `introelapse`:连续；花费在个性测试登录页面上的时间(秒)
*   `testelapse`:连续；花费在个性测试主体上的时间(秒)
*   `endelapse`:连续；花费在个性测试退出页面上的时间(秒)

如果您刚刚意识到数据字典(87)中的特征加起来不等于数据集中的列总数(97)，这是因为这三个分类特征已经使用**一次性编码**进行了**分类编码**。这一过程为每个类别创建单独的特征，以便在机器学习模型中表示它们，增加表现力和准确性。这样编码也意味着你可以独立地解释它们。

### 数据准备

由于大部分数据准备是自动完成的，我们现在要做的就是训练/测试分割数据。但是首先，我们初始化`rand`，一个常量在整个练习中充当我们的`random_state`。然后，我们将`y`定义为`birthorder`列，将`X`定义为其他所有列，然后用`train_test_split`将这两个列分成训练和测试数据集，如下面的代码片段所示:

```py
rand = 9
y = birthorder_df['birthorder']
X = birthorder_df.drop(['birthorder'], axis=1).copy()
X_train, X_test, y_train, y_test = train_test_split(X, y,\  
                                  test_size=0.33, random_state=rand)
```

我们已经完成了所有的数据理解和准备步骤，所以我们现在可以继续讨论概述中提到的主题。

# 衡量功能对结果的影响

对于这个练习，我们将训练数据拟合到六个不同的模型类别:决策树、梯度提升树、随机森林、逻辑回归、多层感知器和**线性判别分析** ( **LDA** )。我们在 [*第三章*](B16383_03_ePub_RK.xhtml#_idTextAnchor051) 、*口译挑战*中了解了前五种，所以我们会花一点时间来熟悉最后一种，这里详述:

*   LDA 是一种非常通用的方法。它做了一些与线性回归关于正态性和同方差性相同的假设；但它源于降维，与**主成分分析** ( **PCA** )无监督方法密切相关。它所做的是计算不同类的平均值之间的距离，称为**类间方差**，以及每个类内的方差，称为**类内方差**。然后，它将数据投影到一个较低维度的空间，以最大化类之间的距离并最小化类内的距离。如果你有三个以上的特征，你很难想象类可分性的概念，但是说你把你所有的数据点都缩减到只有二维。然后，有一种方法可以将它们投射到这个低维空间，在这个空间中，你以这样一种方式组织你的数据点，你在类之间有足够的分离。你可以在它们之间划一条线(通过最大化类间方差)，这样做的同时使每个类的点更接近(通过最小化类内方差)。除了分类之外，LDA 还可以用于降维和可视化类别分离。

现在，我们将 scikit-learn 模型放在一个 Python 字典(`class_models`)中，以便我们可以遍历它们、训练、评估并在完全相同的字典结构中保存我们的结果，如下所示:

```py
class_models = {
  'decision_tree':{'model': tree.\
       DecisionTreeClassifier(max_depth=6, random_state=rand,\
            class_weight='balanced')},
  'gradient_boosting':{'model':ensemble.\
       GradientBoostingClassifier(n_estimators=200,\
              max_depth=4, subsample=0.5,\
              learning_rate=0.05)},
  'random_forest':{'model':ensemble.\
        RandomForestClassifier(max_depth=11, n_estimators=300,\
            max_features='sqrt', random_state=rand)},
  'logistic':{'model': linear_model.\
         LogisticRegression(multi_class='ovr', solver='lbfgs',\
          class_weight='balanced', max_iter=500)},
  'lda':{'model':discriminant_analysis.\
          LinearDiscriminantAnalysis(n_components=2)},
  'mlp':{'model':make_pipeline(StandardScaler(), neural_network.\
       MLPClassifier(hidden_layer_sizes=(11,),\
             early_stopping=True, random_state=rand,\
             validation_fraction=0.25, max_iter=500))}
 }
```

每个模型都有超参数，这些超参数已经因特定原因进行了调整。例如，LDA 在两个维度上执行维度减少(`n_components=2`)，因为有三个类，并且它不应该超过或等于类的数量，并且一个不足以捕获 96 个特征中的变化。

说到级别，这些并不是平均分布的，这就是为什么在训练中有些级别的重量级别使用了与频率成反比的`class_weight='balanced'`。平衡有助于提高**精度**和**召回**较少表示的类。

逻辑回归有五种不同的`solver='lbfgs'`。选择它是因为它高效，没有别的原因。几乎所有其余的参数都被选择来防止过拟合，例如`max_depth`、`n_estimators`、`subsample`、`learning_rate`和`max_features`。

接下来，我们迭代`class_models`字典中的每个模型。我们将训练数据`fit`到模型中，并使用`predict`对训练和测试数据集进行预测。然后我们可以将拟合的模型保存在数据集中，使用几个性能指标，如准确度、召回率、精确度、F1 分数，以及`average='weighted'`根据类别频率对指标进行加权。例如，没有一个`Recall_score`指标，而是三个(每个类一个)，所以它做的是执行加权平均。

代码如下面的代码片段所示:

```py
for model_name in class_models.keys():
 fitted_model = class_models[model_name]['model'].\
                                               fit(X_train, y_train)
 y_train_pred = fitted_model.predict(X_train)
 y_test_pred = fitted_model.predict(X_test)
 class_models[model_name]['fitted'] = fitted_model
 class_models[model_name]['preds'] = y_test_pred
 class_models[model_name]['Accuracy_train'] =\
  metrics.Accuracy_score(y_train, y_train_pred)
 class_models[model_name]['Accuracy_test'] =\
  metrics.Accuracy_score(y_test, y_test_pred)
 class_models[model_name]['Recall_train'] =\
  metrics.Recall_score(y_train, y_train_pred, average='weighted')
 class_models[model_name]['Recall_test'] =\
  metrics.Recall_score(y_test, y_test_pred, average='weighted')
 class_models[model_name]['Precision_train'] =\
  metrics.Precision_score(y_train, y_train_pred,\
average='weighted')
 class_models[model_name]['Precision_test'] =\
  metrics.Precision_score(y_test, y_test_pred, average='weighted')
 class_models[model_name]['F1_test'] =\
  metrics.f1_score(y_test, y_test_pred, average='weighted')
 class_models[model_name]['MCC_test'] =\
  metrics.matthews_corrcoef(y_test, y_test_pred)
```

一旦我们在`class_models`字典中有了所有的度量，我们就可以使用`from_dict`将这个字典转换成`DataFrame`。我们可以通过 MCC 对这个`DataFrame`进行排序，使用`sort_values`并对其余的进行颜色编码，然后使用`style.background_gradient`，代码如下:

```py
class_metrics = pd.DataFrame.\
 from_dict(class_models, 'index')[['Accuracy_train',\
                'Accuracy_test', 'Recall_train', 'Recall_test', 'Precision_train', 'Precision_test', 'F1_test',\
                'MCC_test']]
with pd.option_context('display.Precision', 3):
 html = class_metrics.sort_values(by='MCC_test', ascending=False).style. background_gradient(\
cmap='plasma', low=0.43, high=0.63,\
      subset=['Accuracy_train', 'Accuracy_test']).\
  background_gradient(cmap='viridis', low=0.63, high=0.43,\
      subset=['F1_test'])
html
```

前面的代码生成了如图*图 4.1* 所示的表格:

![Figure 4.1 – Classification model performance metrics

](img/B16383_04_01.jpg)

图 4.1–分类模型性能指标

在*图 4.1* 中，测试精度似乎并不令人印象深刻，但请注意，为了正确解释精度，我们应该看看**无信息率** ( **NIR** )，也称为 n**全误差率**。

将 NIR 放入一个具体的例子中，假设我们正在处理一个图像分类问题，我们的数据集的 85%包含狗的图像，而 15%是猫的图像。因此，狗是大多数类。如果我们在这方面有所懈怠，我们可以预测所有的图像都是狗的，并且仍然可以达到 85%的准确率。如果我们懒洋洋地预测所有的观察值都属于多数类，那么 NIR 就是我们将得到的准确度。要计算 NIR，我们所要做的就是将多数类(`y_train[y_train==1].shape[0]`)中的观察值数量除以观察值总量(`y_train.shape[0]`)，如以下代码片段所示:

```py
print('NIR: %.4f' %\
                  (y_train[y_train==1].shape[0]/y_train.shape[0]))
```

前面的代码应该输出以下内容:

```py
NIR: 0.4215
```

我们应该努力达到这个数字以上的精度，它们都是，但不是一个巨大的差距。考虑到模型是为提高预测性能而调整的，这是令人失望的，但这不是本次练习的重点。超越 NIR 是重要的，因为否则，模型不会比我们最好的“懒惰”猜测更好。否则，这意味着我们应该质疑我们模型的复杂性、选择的正则化方法和特征选择，更不用说我们数据的质量和我们假设的有效性了。然而，我们在这里试图做的是**利用模型的能力来挖掘变量之间的潜在关系**来帮助我们连接测验答案和出生顺序之间的点，如果它们可以连接的话。

无论如何，准确性不是唯一重要的衡量标准。我们也有加权的召回率、精确度和 F1 分数。它们并不特别令人印象深刻，但由于我们对假阳性比对假阴性没有偏好，所以精确度和召回率对我们来说具有同等价值，所以它们或多或少相等是好事。只有决策树之间有更高的间隔。对于其余的模型，由于 F1 分数是精确度和召回率的调和平均值，所以它是一个相似的数字，这并不奇怪。另一方面，MCC 很好地描述了我们的预测性能，因为它说我们的模型大约有 20%处于随机预测和完美预测之间。请记住，MCC 的范围在-1(如果我们的每个预测都是错的)到 1(如果它们都是对的)之间，如果它们是随机的，它就是 0。

另一件要注意的事情是，与测试相比，这些指标的训练规模更大，这告诉我们我们的模型过度拟合了多少。像`gradient_boosting`和`random_forest`一样，经常很难找到最大化测试准确性而又不过分适应的最佳点。如果我们打算生产这些模型，我们将需要密切关注这一点，但这不是本次练习的目标。我们的目标是利用这些模型作为知识发现工具。

## 基于树的模型的特征重要性

我们的三个模型最简单。对于所有基于树的模型(甚至是集合模型)，已经使用节点杂质减少的加权和计算了特征的重要性。`feature_importances_`拟合模型中的属性。我们将获取这些重要性，并将其与其他模型的特征名称一起保存在`DataFrame`中:决策树(`dt_imp_df`)、梯度增强树(`gb_imp_df`)和随机森林(`rf_imp_df`)，如下所示:

```py
dt_imp_df = pd.DataFrame({ 'name': X_train.columns,\
    'dt_imp': class_models['decision_tree']['fitted'].\
       feature_importances_})
gb_imp_df = pd.DataFrame({ 'name': X_train.columns,\
    'gb_imp': class_models['gradient_boosting']['fitted'].\
       feature_importances_})
rf_imp_df = pd.DataFrame({ 'name': X_train.columns,\
    'rf_imp': class_models['random_forest']['fitted'].\
       feature_importances_})
```

有 96 个特征，并且由于树结构的不同，所有三个模型的特征重要性不在相同的范围内。最好将特征重要性解释为一个相对的度量，将一个特征与其他特征进行比较，而不是跨不同的模型。因此，我们可以比较它们的排名，而不是比较这些度量。我们可以使用 pandas `rank`函数来计算每个模型中每个特征的重要性度量的等级，并将它们保存为一个`DataFrame`。这样做不会改变特征的顺序，因为它们是未排序的。

代码如下面的代码片段所示:

```py
dt_rank_df = pd.DataFrame({'dt_rank': dt_imp_df['dt_imp'].\
       rank(method='first', ascending=False).astype(int)})
gb_rank_df = pd.DataFrame({'gb_rank': gb_imp_df['gb_imp'].\
       rank(method='first', ascending=False).astype(int)})
rf_rank_df = pd.DataFrame({'rf_rank': rf_imp_df['rf_imp'].\
       rank(method='first', ascending=False).astype(int)})
```

现在，让我们将每个特征重要性`DataFrame`与其对应的等级`DataFrame`连接起来，并将它们全部合并到一个名为`tree_ranks_df`的数据帧中，该数据帧具有每个模型的特征重要性度量和重要性等级。我们可以平均所有的排名(`avg_rank`)，然后按此排序，这样我们就可以首先看到最重要的平均特征。

代码如下面的代码片段所示:

```py
tree_ranks_df = pd.merge(\
     pd.merge(\
      pd.concat((dt_imp_df, dt_rank_df), axis=1),\
      pd.concat((gb_imp_df, gb_rank_df), axis=1), 'left'),\
     pd.concat((rf_imp_df, rf_rank_df), axis=1), 'left')
tree_ranks_df['avg_rank'] = (tree_ranks_df['dt_rank'] +\
       tree_ranks_df['gb_rank'] +\
       tree_ranks_df['rf_rank'])/3
tree_ranks_df.sort_values(by='avg_rank')
```

前面的代码将产生如图*图 4.2* 所示的数据帧:

![Figure 4.2 – Feature importance for tree-based models

](img/B16383_04_02.jpg)

图 4.2-基于树的模型的特征重要性

通过*图 4.2* 可以看出，决策树(`dt_rank`)、梯度提升树(`dt_rank`)和随机森林(`rf_rank`)秩之间有一些相似之处，尤其是后两者。事实上，重要性的衡量标准似乎并不相同，所以我们使用了比较排名的方法。另一种方法是对重要性测量值进行最小-最大缩放，使其最低值为 0，最高值为 1，但这将揭示更多的特征之间的相对重要性距离，而不是顺序。现在，我们对订单更感兴趣。

除了特定于模型之外，基于树的模型的特征重要性方法是`county_GB`和`gender_undefined`是二元的——因此，两个唯一的值。你不得不怀疑，根据平均排名，是否`age`原因比任何问题都更重要，因为这种偏见，每个问题都比二元特征更重要。

## 特征对逻辑回归的重要性

在前两章中，我们已经讨论了特征对逻辑回归的重要性。你已经知道拟合的逻辑回归模型有系数，这些系数可能是关于哪个特征更重要的有用线索。然而，这一次有一个转折。让我们打印出适合模型的`coef_`属性的`shape`，如下所示:

```py
print(class_models['logistic']['fitted'].coef_.shape)
```

上述代码将输出以下内容:

```py
(3, 96)
```

原来有三组系数！但是为什么呢？！

因为这个模型不是一个而是三个分类器合二为一，所以有三个集合。如果你回到模型定义，你可以看到哪里写着`multi_class='ovr'`。 **OvR** 代表 **One-vs-Rest** ，它在幕后所做的是独立地预测长子、次子和次子的类别。换句话说，各有各的二进制分类问题。然后，它比较每个观察值的每个类的预测概率，可能性最高的一个就是预测的类。OvR 就是你最后如何得到三组系数，而这些系数只能告诉你预测每一类最重要的特征。

如第 2 章 *的 [*中所解释的，可解释性的关键概念*，系数是每增加一个特征单位所增加的对数几率，即一个类别是一个正匹配，所有其他特征保持不变。在这个例子中，我们有三组对应于每一类预测的系数。因此，第一组系数通过每个特征的每个额外单元的对数优势的增加告诉你参与者是第一个出生的。如果它是负的，它表示每增加一个单位](B16383_02_ePub_RK.xhtml#_idTextAnchor031)*的*的对数优势减少。*

由于我们的模型不适用于归一化数据，因此我们的所有特征都有不同的尺度，这就是为什么，为了说明这一点，我们可以将每个系数乘以其标准偏差来近似特征重要性。 [*第 3 章*](B16383_03_ePub_RK.xhtml#_idTextAnchor051) 、*解释挑战、*讨论了为什么这只是一个近似值，并且对于获得逻辑回归的特征重要性的最佳方法还没有达成共识。了解这一点后，我们可以首先计算标准偏差(`stdv`)并创建一个新的`DataFrame`、`lr_imp_df`，其中我们将每个类别的系数乘以标准偏差后放在特征的`name`旁边。

代码如下面的代码片段所示:

```py
stdv = np.std(X_train, 0)
lr_imp_df = pd.DataFrame({\
    'name': X_train.columns,\
    'first_coef_norm':
             class_models['logistic']['fitted'].coef_[0] * stdv,\
    'middle_coef_norm':
             class_models['logistic']['fitted'].coef_[1] * stdv,\
    'last_coef_norm': 
             class_models['logistic']['fitted'].coef_[2] * stdv}).\
   reset_index(drop=True)
```

为了估计每个特征对模型的影响程度，我们可以用*先验*对它们进行加权，这是每个类在数据集中表示的程度。幸运的是，LDA 的拟合模型将其保存为一个`priors_`属性。我们可以将其保存到我们自己的`class_priors`变量中，如下所示:

```py
class_priors = class_models['lda']['fitted'].priors_
print(class_priors)
```

如`class_priors`数组所示，第一个孩子占所有参与者的 42%，第二个孩子占 24%，最后一个孩子占剩余的 34%。我们可以使用这个数组来创建一个加权平均值，使用称为`coef_weighted_avg`的系数的绝对值。在下面的代码片段中，我们使用这个加权平均值的绝对值，因为我们对它是增加还是减少对数优势不感兴趣，只对它增加或减少的程度感兴趣:

```py
lr_imp_df['coef_weighted_avg'] =\
      (abs(lr_imp_df['first_coef_norm']) * class_priors[0]) +\
      (abs(lr_imp_df['middle_coef_norm']) * class_priors[1]) +\
      (abs(lr_imp_df['last_coef_norm']) * class_priors[2])
```

我们刚刚得出的加权平均值只是特征重要性的近似值，因此我们可以从最高到最低重要性对特征进行排序。接下来，我们将使用`sort_values`来执行此操作，并使用`background_gradient`对系数列进行颜色编码，以便更容易理解每列中值的差异，如下所示:

```py
lr_imp_df.\
 sort_values(by='coef_weighted_avg', ascending=False).style.\
 background_gradient(cmap='viridis', low=-0.1, high=0.1,\
 subset=['first_coef_norm', 'middle_coef_norm', 'last_coef_norm'])
```

前面的代码将产生如图*图 4.3* 所示的数据帧:

![Figure 4.3 – Feature importance for the Logistic Regression model

](img/B16383_04_03.jpg)

图 4.3–逻辑回归模型的特征重要性

在*图 4.3* 中，确切的顺序并不总是那么重要，重要的是哪些功能在顶部(非常相关)，哪些功能在底部(不相关)，以及哪些功能位于两者之间的某处(有点相关)。至于每一类的系数，我们可以通过哪些是正的或负的，以及或多或少的多少来解释它们——例如，我们知道`birthn`与长子的正匹配负相关。这种见解直觉上是有道理的。一个家庭的孩子越多，其中一个是长子的可能性就越小。最后一个孩子也是如此——只有中间孩子的几率随着孩子数量的增加而增加。随着`age`的增加，最后出生的几率会降低。这个结论也是有道理的，因为家庭曾经更大，但不清楚为什么第一个孩子的数量会增加。然而，我们需要一个不同的工具来更好地检查这一点。

我们还可以从问题 1 ( `Q1`)中的陈述看出这种一致性，问题 1 说“*我读了很多书*”，问题 13 ( `Q13`)说“*我指挥别人*”增加了参与者是长子的可能性。此外，第 20 题(`Q20`)说“*我不需要别人的赞美*”，增加了这个孩子是中间孩子的几率。您可以看出，尽管是分别拟合的，但这些类大部分是相互对立的，当然，很少有某个要素的三个类的系数都为正或都为负的情况。

这种特定于模型的特征重要性方法对于从整体上评估所有类的所有特征的重要性并不十分可靠。此外，由于模型是逻辑回归，它对数据做出了一些可能不成立的假设，例如要素之间很少或没有多重共线性，以及与*对数比*的线性关系。然而，如果这些假设或多或少是正确的，OvR 逻辑回归的优势在于类别之间的分离。您可以单独检查每个特征与每个类的关系。

## 特征对 LDA 的重要性

与 OvR 逻辑回归一样，我们也可以为 LDA 的每个特征提取三组系数。为了验证，检查`shape`，像这样:

```py
print(class_models['lda']['fitted'].coef_.shape)
```

它应该输出(`3, 96`)。区别在于这些系数的意义。它们告诉我们每个特征在类的**可分性**中的权重。系数的绝对值越高，该特征越有助于分类。另一方面，系数的绝对值越低，表明该特征对类别可分性没有贡献。毕竟，LDA 就像 PCA，但是它将特征分解为分离性而不是相关性。

为了查看这些系数，我们可以创建一个新的`DataFrame`、`lda_imp_df`，其中我们将每个类别的系数乘以标准偏差，放在特征的`name`旁边，如下所示:

```py
lda_imp_df = pd.DataFrame({\
'name': X_train.columns,\
'first_coef_norm': class_models['lda']['fitted'].coef_[0] * stdv,\
'middle_coef_norm': class_models['lda']['fitted'].coef_[1] * stdv,\
'last_coef_norm': class_models['lda']['fitted'].coef_[2] * stdv}).\
reset_index(drop=True)
```

我们现在可以像逻辑回归一样，使用`class_priors`变量创建系数绝对值的加权平均值(`coef_weighted_avg`)。我们这样做的唯一目的是能够对表进行排序，并大致了解哪些特征最重要，同时认识到这不是一门精确的科学。

代码如以下代码片段所示:

```py
lda_imp_df['coef_weighted_avg'] =\
      (abs(lda_imp_df['first_coef_norm']) * class_priors[0]) +\
      (abs(lda_imp_df['middle_coef_norm']) * class_priors[1]) +\
      (abs(lda_imp_df['last_coef_norm']) * class_priors[2])
```

我们现在可以使用加权平均值(`coef_weighted_avg`)对特征进行排序，并以与逻辑回归相同的方式对其进行颜色编码，如下所示:

```py
lda_imp_df.\
 sort_values(by='coef_weighted_avg', ascending=False).style.\
 background_gradient(cmap='viridis', low=-0.1, high=0.1,\
 subset=['first_coef_norm', 'middle_coef_norm', 'last_coef_norm'])
```

在前面代码生成的*图 4.4* 中，您可以看到，在逻辑回归分析中排在前 10 位的许多相同特征在 LDA 中也排在前 10 位。您还可以看到不同类别之间的相似模式，例如中间的孩子比其他任何人都更符合`birthn`，而其他两个类别在帮助预测他们的特征方面更平衡。

这里可以看到输出:

![Figure 4.4 – Feature importance for the LDA model

](img/B16383_04_04_merged.jpg)

图 4.4-LDA 模型的特征重要性

类似于 OvR 逻辑回归，LDA 特征重要性具有特定于模型和由 LDA 模型做出的假设的缺点。LDA 假设要素和**多元正态性**之间很少或没有多重共线性-也就是说，要素在每个类中呈正态分布。它也分享了 OvR 逻辑回归同样的主要优势，即能够观察每个特征与每个类别的关系。然而，LDA 对假设违反更加稳健，因此可以用于噪声更大的数据。话虽如此，**二次判别分析** ( **QDA** )在这种情况下更胜一筹。QDA 类似于 LDA，但是没有进行正态假设，并且使用二次决策边界而不是线性决策边界来划分类别。

## 多层感知器的特征重要性

神经网络缺乏能够毫不费力地帮助确定特征重要性的内在属性，就像其他模型类一样。即使对于这个单一隐藏层的示例，情况也会变得更加复杂，因为每一层都有两组对应的权重矩阵，如下面的代码片段所示:

```py
print(class_models['mlp']['fitted'][1].coefs_[0].shape)
print(class_models['mlp']['fitted'][1].coefs_[1].shape)
```

两个数组的形状输出如下:

```py
(96, 11)
(11, 3)
```

每个矩阵中的权重可能会产生误导，因为它们可能会被彼此放大或衰减。如果您将这两个矩阵点积在一起并转置它们，您将得到一个具有熟悉(`96, 3`)形状的矩阵，其中的单元对应于每个特征和类别组合，这是我们用于逻辑回归和 LDA 的矩阵。然而，这并不是在前向传播期间使用权重进行预测的精确方式。首先，在这些矩阵运算之间和之后有非线性激活函数，如`relu`和`softmax`。假设已经用归一化的数据进行了训练，已经有人提议取权重的绝对乘积之和以及没有绝对值的权重乘积之和。还有更精细的方案涉及加权和归一化权重，但是这些忽略了隐藏层激活函数的影响。

结论是，对于如何从神经网络的内在参数中提取特征重要性，还没有达成共识。正如我们将在本书后面了解到的，神经网络还有其他内在可解释的方面——例如，第 8 章 *中的 [*显著图，可视化卷积神经网络，*](B16383_08_ePub_RK.xhtml#_idTextAnchor162) *[*中的*和综合梯度，第 9 章](B16383_09_ePub_RK.xhtml#_idTextAnchor188)* *中的多变量预测和敏感性分析的解释方法*。*

虽然我们能够利用内在参数来获得所有其他模型的特征重要性，但是使用的方法是不一致的。因此，结果的不同不仅是因为模型的不同，也是因为方法的不同。那么，对于任何模型，计算特征重要性的可靠方法是什么呢？它被称为 PFI，我们将在接下来讨论它。

# 练习 PFI

PFI 的概念比任何特定于模型的特征重要性方法更容易解释！一旦每个特征的值被打乱，它仅仅测量预测误差的增加。PFI 的理论是基于这样一种逻辑，即如果特征与目标变量有关系，改组将破坏它并增加误差。另一方面，如果特征与目标变量没有很强的关系，预测误差不会增加很多，如果有的话。然后，如果你按照那些最容易增加误差的特征来排列特征，你就会明白哪些特征对模型最重要。

除了作为**模型不可知的**方法，PFI 还可以用于看不见的数据，比如测试数据集，这是一个巨大的优势。在这种情况下，因为它与随机森林和梯度增强树过度拟合，从内在参数导出的特征重要性能有多可靠？它告诉你，根据从训练数据中学到的东西，模型认为什么是重要的，但一旦你引入看不见的数据，它就不能告诉你什么是最重要的。

在他的书*可解释的机器学习*中，Christoph Molnar 提出了支持利用训练数据的论点，这可以告诉你更多关于对训练模型中每个特征的依赖，而不是对其对概括预测性能的单独贡献。我们对后者更感兴趣，所以这就是我们使用测试数据集的原因。

为了计算我们所有模型上的排列重要性，我们可以通过迭代它们中的每一个来再次利用我们的`class_models`字典，然后调用 scikit-learn 的`permutation_importance`函数来计算 PFI。`permutation_importance`函数的主要参数是拟合模型(`fitted_model`)和数据集的特征(`X_test`)和标签(`y_test`)。我们也将准确度定义为我们想要使用的预测误差度量或计分器(`scoring='Accuracy'`)来比较特征被置换后准确度的下降。

代码如下面的代码片段所示:

```py
for model_name in class_models.keys():
 fitted_model = class_models[model_name]['fitted']
 permutation_imp = inspection.permutation_importance(\
      fitted_model, X_test, y_test, n_jobs=-1,\
      scoring='Accuracy', n_repeats=8, random_state=rand)
 class_models[model_name]['importances_mean'] =\
      permutation_imp.importances_mean
```

PFI 不止一次地混洗特征，然后平均预测误差，这就是为什么定义它应该混洗特征的次数(`n_repeats=8`)以及`random_state`对于再现性是至关重要的。PFI 可以并行执行，利用系统的所有处理器(`n_jobs=-1`)。最后，一旦对每个模型执行了 PFI，它将保存预测误差的平均值(`importances_mean`)。

现在，我们可以计算每个模型的平均重要性，并将它们放在新的`DataFrame`、`perm_imp_df`的单独列中，与每个特征的`name`放在一起，如下面的代码片段所示:

```py
perm_imp_df = pd.DataFrame({\
  'name': X_train.columns,\
  'dt_imp': class_models['decision_tree']['importances_mean'],\
  'gb_imp': class_models['gradient_boosting']['importances_mean'],\
  'rf_imp': class_models['random_forest']['importances_mean'],\
  'log_imp': class_models['logistic']['importances_mean'],\
  'lda_imp': class_models['lda']['importances_mean'],\
  'mlp_imp': class_models['mlp']['importances_mean']}).\
reset_index(drop=True)
```

仅仅是为了按照某种方式对`perm_imp_df` `DataFrame`进行排序，让我们将所有六个模型的重要性平均到一个新列中，我们称之为`avg_imp`，如下所示:

```py
perm_imp_df['avg_imp'] = (perm_imp_df['dt_imp'] +
                  perm_imp_df['gb_imp'] + perm_imp_df['rf_imp'] + perm_imp_df['log_imp'] + perm_imp_df['lda_imp'] + perm_imp_df['mlp_imp'])/6
```

现在，我们可以`round`，按`avg_imp`排序，并将`perm_imp_df`保存到一个名为`perm_imp_sorted_df`的新数据帧中。然后，我们用颜色编码输出，就像这样:

```py
perm_imp_sorted_df = perm_imp_df.round(5).\
                     sort_values(by='avg_imp', ascending=False)
perm_imp_sorted_df.style.\
            background_gradient(cmap='viridis_r', low=0, high=0.2, subset=['dt_imp', 'gb_imp', 'rf_imp', 'log_imp', 'lda_imp', 'mlp_imp'])
```

前面的代码产生了图 4.5 中所示的数据帧:

![Figure 4.5 – Test PFI for all models

](img/B16383_04_05.jpg)

图 4.5–测试所有型号的 PFI

*图 4.5* 显示了本章中安装的所有模型的测试数据集的 PFI。它证实了模型本质上对`birthn`有很大的依赖，但也证实了它远比下一个最重要的特征更重要。事实上，`birthn`对这些模型如此重要，以至于如果我们从每个模型的测试精度中扣除预测误差的平均增长(在这种情况下，对应于精度的下降),它们将下降到无信息率以下！通过从`class_models`字典中获取`Accuracy_test`属性(该字典存储了每个模型的所有测试精度)，并在排序的重要性`DataFrame` ( `perm_imp_sorted_df`)中从第一行的`0`中扣除前六个值(`1:7`)，这很容易证明，如下面的代码片段所示:

```py
pd.DataFrame.\
    from_dict(class_models, 'index')[['Accuracy_test']] -\
    perm_imp_sorted_df.iloc[0,1:7].to_numpy().reshape((6,1))
```

从前面代码生成的*图 4.6* 中可以看出，一旦扣除`birthn`的 PFI，没有一个模型的准确率高于 NIR ( `0.4215`):

![Figure 4.6 – Test Accuracy for all models once you deduct the PFI of birthn

](img/B16383_04_06.jpg)

图 4.6-扣除出生时的 PFI 后，测试所有模型的准确性

假设没有有意义的水平`birthn`做得这么好，能从剩下的特征中学到什么。我们接下来将研究的方法，如 PDP 和 ICE 图，将有助于揭示特定特征及其与目标和彼此之间的关系。

## PFI 的缺点

PFI 在模型解释技术中并不少见，它的主要缺点是该方法不会拾取彼此相关的特征的影响。换句话说，**多重共线性**将胜过特征重要性。当您混洗一个要素时，其相关要素将保持未混洗，保持错误率相对不变，这意味着相关要素的聚类将具有比其应有的更低的重要性。有一个处理这个问题的策略，我们将在第 12 章 、*单调性约束和模型可解释性调整*中讨论。

# 解释 PDP

PDP 传达了一个特征在该特征的所有(或内插的)可能值中对预测的边际影响。这是一种全局模型解释方法，可以直观地展示某个特征的影响以及与目标的关系性质(线性、指数、单调等)。

它还可以扩展到包括两个特征，以说明它们的相互作用对模型的影响。一个特征图在 *y* 轴显示预测结果或该结果的相对变化，而 *x* 轴显示该特征的所有可能值。如果单个特征发生变化，则通过将所有观察值的特征值更改为在 *x* 轴上的值并对预测值进行平均来计算绘制线，以获得 *y* 轴坐标。

PDP 的一个变化是从 *y* 轴的所有观察值中扣除期望值，从而将边际效应集中到期望值。另一种 PDP 变体将通过直方图或地毯图显示特征的分布。由于 PDP 线是用平均值计算的，这一点很重要，因为在特征分布更稀疏的绘图区域，平均值并不可靠。

首先，让我们创建两个我们希望解释的特征名称(`feature_names`)及其各自标签(`feature_labels`)的列表，显示在 *x* 轴标签和标题中，如下所示:

```py
feature_names = ['birthn', 'Q1', 'Q13', 'age']
feature_labels = ['# of Births', 'Question #1', 'Question #13', 'Age']
```

现在，我们可以迭代每个特征名称，并使用 PDPbox 的`pdp_isolate`函数，使用拟合模型(`model`)、数据集(`dataset`)、所有特征列的名称(`model_features`)以及您想要的在 *x* 轴上的特征(`feature`)来计算具有所有 PDP 平均值(`pdp_feat_df`)的数据帧。

对于拟合模型，我们使用梯度推进树，因为没有模型最接近前四个重要特征的平均 PFI。但是，您可以对此进行更改，以查看特征与目标的平均关系如何因模型而异。你会发现有些是锯齿状的，有些是平滑的，有些是线性的，等等。

对于`dataset`，我们将使用测试数据集，原因与我们用于 PFI 的原因完全相同。需要注意的一点是，`dataset`需要整个数据集(要素和标签)，因为我们将它们分为`X_test`和`y_test`，所以我们必须使用 pandas `concat`函数将它们连接起来。一旦我们有了数据帧，我们所要做的就是绘制它，PDPbox 有一个生成 Matplotlib 图的函数，称为`pdp_plot`。它采用先前生成的数据帧(`pdp_isolate_out`)和几个可选的图形参数，详述如下:

*   `center=True`使 *y 轴相对于最高或最低值。*
*   `x_quantile=True`使 *x* 轴刻度的间距对应于分位数。PDPbox 不包括显示特征分布的直方图或地毯图，因此这是克服与稀疏或不均匀分布相关的解释挑战的好方法。
*   `ncols=3`将所有三个类放在一行中。
*   `plot_lines=True`将绘制对应于一个样本的观察线。
*   `frac_to_plot=100`告诉它绘制 100 个采样观察值。
*   `feature_name`是 *x* 轴上特征的标签。

以下代码迭代所有四个特征，生成`pdp_isolate`数据帧，然后用它绘制 PDP:

```py
for i in range(len(feature_names)):
 pdp_feat_df = pdp.pdp_isolate(\
          model=class_models['gradient_boosting']['fitted'],\
          dataset=pd.concat((X_test, y_test), axis=1),  
          model_features=X_test.columns, feature=feature_names[i])
 fig, axes = pdp.pdp_plot(\
          pdp_isolate_out=pdp_feat_df, center=True, x_quantile=True,\
          ncols=3, plot_lines=True, frac_to_plot=100, figsize=(15,6), feature_name=feature_labels[i])
```

前面的代码产生如图*图 4.7-4.10* 所示的曲线。您可以在此查看*图 4.7* :

![Figure 4.7 – PDP for birthn

](img/B16383_04_07.jpg)

图 4.7–出生 PDP

*图 4.7* 传达了我们之前注意到的逻辑回归特征重要性，但现在我们有了一个可视化的表示。第一胎(`class 0`)和最后一胎(`class 2`)的概率随着出生人数(`birthn`)的增加而持续下降。中间的孩子(`class 1`)概率走向相反的方向，从几乎 0%开始，因为不可能有中间的孩子有两个孩子！这一切都说得通。您还可以从细线与粗线(平均值)的接近程度来判断这是一个很强的特征，在所有的类预测中几乎没有变化。

*图 4.8* 可在此查看:

![Figure 4.8 – PDP for Q1

](img/B16383_04_08.jpg)

图 4.8-Q1 的 PDP

*图 4.8* 对应于`Q1`的李克特量表，“*我读过的书多得离谱*”，所以对于第一胎来说，概率在 N/A (0)和不同意(1)之间下降，但之后的爬升超过了零标记(无变化)，并且在超过中性线(3)时明显增加。`Q1`对最后出生的孩子有完全相反的效果。第二个孩子的结果更有趣，因为你可以看到采样观察值(细线)到处都是，所以对此要有所保留，但他们的平均值表明 3 岁以后的第一个孩子和 3 岁以前的最后一个孩子是混合的。换句话说，完全不同意和同意`Q1`都表明中间的孩子更有可能介于这两个极端之间。

*图 4.9* 可在此处查看:

![Figure 4.9 – PDP for Q13

](img/B16383_04_09.jpg)

图 4.9–Q13 的 PDP

在*图 4.9* 中`Q13`(*我对*周围的人发号施令)的 PDP 与`Q1`的第一个和最后一个孩子的目标有类似的关系，但在李克特量表的不一致端更明显，在另一端稍不明显。在`Q13`比`Q1`中，对中间孩子的矛盾心理要少得多，随着认同水平的提高，这个班级更不可能。

*图 4.10* 可在此处查看:

![Figure 4.10 – PDP for age

](img/B16383_04_10.jpg)

图 4.10–年龄的 PDP

*图 4.10* 涉及`age`特征的 PDP。我们可以看出，平均而言，随着`age`的增加，成为第一个孩子的概率缓慢而持续地增加。虽然我们可以解释这一点，但很难找到一个合乎逻辑的解释，因为家庭曾经是更大的，所以我们可能希望概率随着年龄的增长而降低。谢天谢地，分位数可以提供线索。请注意，16-22 岁的刻度线仅相隔 2 年，但之后这一间隔会增加到 4、6、10 岁，最后增加到 38 岁。这意味着`age`分布是右偏的，这不一定是一件坏事，但分布也可能是不均衡的，每个年龄组的阶级。

为了证明这个假设，我们先把`age`和`birthorder`放到各自的 dataframe 中(`birthorder_abbrev_df`)。然后，我们利用 pandas `cut`函数将指数设置为分位数中的相同年龄组。现在我们先保存一个按这个年龄组指数和`birthorder`分组的数列(`agegroup_birthorder_counts_s`)，和另一个刚好按指数分组的数列(`agegroup_counts_s`)。现在，您可以将总年龄组和出生顺序数除以年龄组数，得到一个带有百分比的序列(`agegroup_pct_birthorder_s`)。最后，您可以使用`unstack()`将序列转换为数据帧，并使用 pandas `plot.bar`函数将其转换为`stacked`条形图，如下面的代码片段所示:

```py
birthorder_abbrev_df = birthorder_df[['age', 'birthorder']]
birthorder_abbrev_df.set_index(pd.cut(birthorder_abbrev_df['age'],\
       [12, 16, 18, 20, 22, 26, 30, 36, 46, 88]), inplace=True)
agegroup_birthorder_counts_s = birthorder_abbrev_df.\
     groupby([birthorder_abbrev_df.index, 'birthorder']).size()
agegroup_counts_s =\ 
         birthorder_abbrev_df.groupby(\ 
birthorder_abbrev_df.index) ['birthorder'].count()
agegroup_pct_birthorder_s =\  
              agegroup_birthorder_counts_s.div(agegroup_counts_s, axis=0,level=0)
agegroup_pct_birthorder_s.unstack().plot.bar(stacked=True,\
figsize=(15,8))
```

前面的代码产生如图*图 4.11* 所示的图形:

![Figure 4.11 – Class priors per equal-sized age groups

](img/B16383_04_11.jpg)

图 4.11-每个同等年龄组的班级优先

*图 4.11* 显示了各个阶层(`birthorder`)在不同年龄组中的表现。这看起来并不多，但是从 16 岁到 46 岁，第一个出生的孩子(1 级)的比例从 38%上升到近 49%，而最后一个出生的孩子(3 级)从 38%下降到 29%。与此同时，中间的孩子(二班)只波动了 3%。所有这些都与我们对人口统计学的了解相悖，因为我们知道，在这些年龄组的 75 年间，在所代表的国家中，每个家庭的平均孩子数至少减少了两个，在这 75 年的最后 50 年中，几乎减少了一个。理论上，这意味着随着年龄的增长，第一胎或最后一胎出生的可能性会降低，而第二胎出生的可能性会增加。

一个看似合理的假设是，长子在较大年龄组中的比例过高，因为他们越来越有可能参与这些在线测验，并且该模型发现了这一偏差。不管有没有偏见，我们都应该处理相关的阶级不平衡。我们将在 [*第 11 章*](B16383_11_ePub_RK.xhtml#_idTextAnchor231) *、偏差减轻和因果推断方法*中更深入地讨论偏差，届时我们将了解如何利用人口统计数据来减少阶层失衡，以及由此导致的模型偏差。

## 互动 PDP

PDP 也可以同时应用于多个特征，这在检查两个特征的交互如何与目标变量相关时很有用。

我们也可以使用 PDPbox 生成 PDP 交互图。它的`pdp_interact`函数与`pdp_interact`非常相似，拥有所有相同的参数，除了`feature`是`features`的列表。除了选择`birthn`和`Q1`作为我们的特征，我们还有`n_jobs=-1`参数，它利用我们所有的处理器进行并行计算。`pdp_interact`将输出一个`pdp_birthn_Q1_df`数据帧。现在，我们应该用`pdp_interact_plot`来绘制它。对于`pdp_interact_plot`，您将看到与`pdp_plot`相似的参数。例如，`pdp_interact_out`类似于`pdp_isolate_out`，取上一步产生的数据帧；`feature_names`类似于`feature_name`,但它接受一系列特征标签，而不是一个标签。`plot_type='grid'`告诉它生成一个网格，这对于像`birthn`和`Q1`这样的低基数或序数特征非常有用。

代码如下面的代码片段所示:

```py
pdp_birthn_Q1_df = pdp.pdp_interact(\
               model=class_models['random_forest']['fitted'],\
               dataset=pd.concat((X_test, y_test), axis=1),\
               model_features=X_test.columns,                features=['birthn','Q1'],\
               n_jobs=-1)
fig, axes = pdp.pdp_interact_plot(\
               pdp_interact_out=pdp_birthn_Q1_df,\               plot_type='grid',\   
               x_quantile=True, ncols=2, figsize=(15,15),\
               feature_names=['# of Births','Question #1'])
```

在*图 4.12* 中，作为前面代码的结果输出，您可以通过颜色编码的网格看出，随着出生数量(`birthn`)的减少和与`Q1`的一致性的增加，第一胎(`class 0`)的平均概率增加。对于 lastborns ( `class 2`)，对于`birthn`也是一样，但是对于`Q1`正好相反。到目前为止，这些交互不应该令人惊讶，因为就好像您已经为这些功能中的每一个组合了单独的 PDP。然而，对于中间的孩子(`class 1`)`Q1`图表有点矛盾，但重要的是要注意一个特征如何抵消另一个特征的平均影响。一旦你看到它和`birthn`互动，概率大多朝一个方向移动，随着`birthn`特征增加。

*图 4.12* 可在此处查看:

![Figure 4.12 – Grid-interaction PDP for birthn and Q1

](img/B16383_04_12.jpg)

图 4.12-birth n 和 Q1 的网格互动 PDP

PDPbox 有另一种类型的 PDP 交互图，称为`contour`，这更适合高基数或连续特征，所以这次我们将使用`age`和`testelapse`(耗时测试)。除了`features`、`feature_names`和`plot_type`不同之外，输出图的代码与前一个完全相同。

代码如下面的代码片段所示:

```py
pdp_age_testelapse_df = pdp.pdp_interact(\  
    model=class_models['random_forest']['fitted'],\   
    dataset=pd.concat((X_test, y_test), axis=1),\  
    model_features=X_test.columns,\     features=['age','testelapse'],\
    n_jobs=-1)
fig, axes = pdp.pdp_interact_plot(\
   pdp_interact_out=pdp_age_testelapse_df,\
   plot_type='contour', x_quantile=True, ncols=2,\    figsize=(15,15),\
   feature_names=['Age','Time taking test (minutes)'])
```

前面的代码产生的输出如图*图 4.13* 所示:

![Figure 4.13 – Contour-interaction PDP for age and testelapse

](img/B16383_04_13.jpg)

图 4.13–年龄和测试时间的等高线交互 PDP

*图 4.13* 传达了第一胎(`class 0`)的概率随着考试时间的减少和年龄的增加而增加，所以如果你年龄更大，考试速度更快，你是第一胎的可能性更大。最晚出生的人(`class 2`)或多或少相反:如果你越年轻越慢，你成为最晚出生的人的机会就越大。中间的孩子(`class 1`)在一个方向上增加更多，随着`testelapse`的增加，变得稍微更有可能，除非`age`在 46 以上，并且概率随着年龄快速增加。

## 等离子显示器的缺点

PDP 的主要缺点是它一次只能显示两个功能，并且当它们可能相互关联时，它假定功能是独立的。为了解决独立性的问题，我们将在下一章讲述**累积局部效应** ( **ALE** )的情节。

正如我们在本节中了解到的，PDP 有助于了解平均而言特征与目标之间的关系，但如果我们希望将关系分解可视化(换句话说，每个单独的观察结果而不是平均值)，该怎么办呢？这种聚集是另一个缺点，也正是 ICE 图的用途，我们接下来将简要介绍这些。

# 解释冰图

ICE 图是这个问题的答案:*如果我的 PDP 图掩盖了我的特征-目标关系的变化，该怎么办？*事实上，当你试图理解一个特征如何与一个模型的预测联系起来时，通过平均会失去很多东西。如果您仔细观察单个特征的 PDP 图，许多特征的细线不仅远离平均粗线，而且甚至不遵循其大致方向。这些变化可以提供额外的洞察力——顺便说一下，细线本质上就是冰图的内容，只是你可以用它们做更多的事情。

ICE 图可以包含所有数据集，但是在图中包含多条线可能计算量很大，而且更重要的是，很难理解。这就是为什么建议对数据集进行采样或绘制具有透明度的线。

我们将使用这两种方法，但是让我们首先对数据集进行采样。我们首先用`np.random.seed`设置随机种子以获得再现性，然后我们将`sample_size`设置为数据集的 10%,并使用`sample_idx`随机选择将在我们的 ICE 图中表示的 10%的指数。然后，我们将采样观察值保存在新的数据帧中(`X_test_samp`)。

代码如下面的代码片段所示:

```py
np.random.seed(rand) 
sample_size = 0.1
sample_idx = np.random.choice(\
             X_test.shape[0],
math.ceil(X_test.shape[0]*sample_size),\
             replace=False)
X_test_samp = X_test.iloc[sample_idx,:]
```

我们默认使用的 Python ICE 实现使用了`predict`函数，这对于回归问题非常有用。尽管如此，对于分类来说，你最终会得到一条在另一条之上的直线，通向三个可能类别中的一个。要解决这个问题，您可以使用`predict_proba`来代替，它返回预测的概率。但是，这返回了三组预测概率，实现无法理解这一点。为了解决这个问题，我们可以创建我们的`predict`函数，每个类一个，如下所示:

```py
def predict_prob_first_born(test_df):
 return class_models['random_forest']['fitted'].\
                                         predict_proba(test_df)[:,0]
def predict_prob_middle_child(test_df):
 return class_models['random_forest']['fitted'].\
                                         predict_proba(test_df)[:,1]
def predict_prob_last_born(test_df):
 return class_models['random_forest']['fitted'].\
                                         predict_proba(test_df)[:,2]
```

通过查看三个`predict_prob`函数可以看出，我们使用`fitted`模型和测试数据集来说明 ICE。现在，我们可以使用一个`mldatasets`函数(`plot_data_vs_ice`)来计算并绘制一个冰图，其中包含用于生成冰图的数据。在 *x* 轴上，我们可以使用我们的`birthn`顶部特征。为了让这个练习更有趣，我们甚至会根据`Q1`的答案对线条进行颜色编码。

为此，让我们首先用李克特量表(`legend_key`)创建一个字典，我们将用它作为`Q1`的图例，如下所示:

```py
legend_key = {0:'N/A', 1:'Disagree', 2:'Somewhat Disagree',
              3:'Neutral', 4:'Somewhat Agree', 5:'Agree'}
```

然后，我们使用`plot_data_vs_ice`函数来生成图表。如果你很好奇，在引擎盖下，它使用`pycebox`库来绘制冰图。我们不会深入讨论如何直接利用这个库的细节，因为我们的重点是解释，但是您可以查看这里的教程:

[https://github.com/AustinRochford/PyCEbox/](https://github.com/AustinRochford/PyCEbox/ )

`plot_data_vs_ice`函数所需的前两个参数是预测函数和放入 *y* 轴的标签。该标签与使用`predict`功能预测的内容相关。它还需要用于预测的`X`数据、绘制在 *x* 轴上的特征名称(`feature_name`)及其标签(`feature_label`)。可选地，我们可以指定一个特征用于颜色编码(`color_by`)和我们对这个特征的图例(`legend_key`)。

我们将首先生成第一胎的预测概率图，如下所示:

```py
mldatasets.plot_data_vs_ice(predict_prob_first_born,\
                            'Probability of Firstborn',\
                            X=X_test_samp,\
                            feature_name='birthn',\
                            feature_label='# of Births',\
                            color_by='Q1', legend_key=legend_key)
```

前面的代码生成了图 4.14 和图 4.15 所示的图。

第一幅图可以在这里看到:

![Figure 4.14 – Probability of firstborn data points as birthn increases, color-coded for Q1 answers

](img/B16383_04_14.jpg)

图 4.14-随着出生人数的增加，第一个出生数据点的概率，用颜色表示 Q1 答案

您可以通过修改每个观察值的`birthn`来判断*图 4.15* 连接了*图 4.14* 中的点，使其与 *x* 轴中的值相匹配。此外，冰图线显示了第一个孩子的`birthn`和`birthorder`之间关系的变化示例。运行代码后可见的颜色编码丰富了解释。你可以看出，许多紫色和蓝色的线是不稳定的，甚至是非单调的，总体上概率较低，而黄色和绿色更一致，更高。似乎你越是不同意`Q1`(*我读过的书多得离谱*)中的说法，`birthn`在预测长子方面就越不靠谱。

*图 4.15* 可在此处查看:

![Figure 4.15 – Firstborn ICE plot as birthn increases, color-coded for Q1 answers

](img/B16383_04_15.jpg)

图 4.15-随着出生时间的增加，第一胎冰图，用颜色标记 Q1 答案

我们现在可以用同样的代码对`middle_child`做同样的事情，除了我们替换了`plot_data_vs_ice`函数中的前两个参数，如下所示:

```py
mldatasets.plot_data_vs_ice(predict_prob_middle_child,\
                            'Probability of Middle Child',\
                            X=X_test_samp,\                            feature_name='birthn',\
                            feature_label='# of Births',\
                            color_by='Q1', legend_key=legend_key)
```

前面的代码生成了两个图，就像我们对图 4.14 和图 4.15 所做的一样。第二个如图*图 4.16* 所示:

![Figure 4.16 – Probability of middle-child ICE plot as birthn increases, color-coded for Q1 answers

](img/B16383_04_16.jpg)

图 4.16-随着出生人数的增加，中间孩子出现 ICE 图的概率，用颜色表示 Q1 答案

在*图 4.16* 中，排行老二的孩子比老大的孩子更加一致。所有线从 2 到 3 突然增加，然后从 3 到 6 平稳增加，然后趋于平稳。颜色编码表明，他们越同意`Q1`中的说法，他们就越不可能是中间孩子，不管他们是什么年龄。

最后，让我们尝试对 lastborns 做和对其他两个类一样的事情，如下所示:

```py
mldatasets.plot_data_vs_ice(predict_prob_last_born,\
                            'Probability of Lastborn',\
                            X=X_test_samp,\                            feature_name='birthn',\
                            feature_label='# of Births',\
                            color_by='Q1', legend_key=legend_key)
```

*此处显示的图 4.17* 是前面代码的输出结果:

![Figure 4.17 – Probability of lastborn ICE plot as birthn increases, color-coded for Q1 answers

](img/B16383_04_17.jpg)

图 4.17——随着出生时间的增加，最后出生的冰的概率图，Q1 答案的颜色编码

图 4.17 中*最后一胎概率的冰图比第一胎的变化更大。然而，就像长子一样，与`Q1`越不一致的线可能越不稳定，但是，与长子不同，与`Q1`一致的概率更小，不管`birthn`有多大。*

## 冰的缺点

ICE 曲线和 PDP 一样，假设了特征的独立性，所以它们有相同的缺点。除此之外，使用 ICE 你不能与两个连续的或者高基数的特征交互。例如，我们能够对`Q1`进行颜色编码，但仅仅是因为`Q1`有六个可能的值。另一个缺点是很难确定特征和目标之间的平均关系，但这正是 PDP 图的用途。

最终，ICE plots 擅长的是在这种关系的变化中寻找线索，而不是在它的集合上。

# 任务完成

任务是确定机器学习可以从 40，000 个测验条目的数据集中发现什么。心理学研究人员想知道他们是否可以信任使用这些数据来为他们的研究提供前进的道路。他们还想知道机器学习解释是否会向他们显示哪些特征和特征值对结果影响最大。

使用 PDP，我们发现年龄和出生顺序的分布有一些差异，因为中间孩子的比例必须随着年龄的增长而增加。如果任何建模练习要在真实世界场景中工作，训练数据**必须匹配真实世界分布**。然而，一切并没有全部失去。您可以通过平衡这些分配来采取纠正措施。可能必须对数据进行重大修改，以使其在研究中更加可靠。话虽如此，由于这是一个匿名的在线测试，你可以预期说谎是司空见惯的，所以误差幅度必须相应地设定。

至于透明度问题，据国际促进和平基金会称，出生人数(`birthn`)是迄今为止最重要的特征。然而，该练习使用 PDP 成功地识别出了诸如`Q1`(*我读过数量惊人的书籍*)和`Q13`(*我在*周围指挥人)等与出生顺序一致相关的问题，并验证了它们对`testelapse`的影响。一旦随着年龄的增长，分布问题得到解决，也许这一点会变得更加明显。

# 总结

读完这一章，你应该掌握使用模型的内在参数来检查特征的重要性的缺点。我们也已经回答了这个问题:*有哪些有用的模型无关的替代方法可以根据重要性对特征进行排序，并可视化它们的预测效果？在下一章中，我们将研究更健壮的全局模型不可知方法，这些方法克服了在这一章中看到的那些方法所面临的一些挑战。*

# 数据集来源

*   *开源心理测量项目*。(2019).在线性格测试的原始数据。[https://openpsychometrics.org/_rawdata/](https://openpsychometrics.org/_rawdata/ )

# 延伸阅读

*   克莱卡，W. R. (1980 年)。*判别分析*。社会科学中的定量应用系列，第 19 期。千橡，加州:圣人出版社。
*   1994 年，中国农业大学出版社出版。*为什么有些前馈网络无法学习某些多项式*。神经计算，6，763-768。https://doi.org/10.1162/neco.1994.6.4.761
*   博格和古特曼(1997 年)。*从人工神经网络模型中提取知识*，IEEE 系统、Man 和控制论会议，佛罗里达州奥兰多。[https://doi.org/10.1109/ICSMC.1997.633051](https://doi.org/10.1109/ICSMC.1997.633051)
*   莫尔纳尔，C. (2019)。*可解释的机器学习。让黑盒模型变得可解释的指南*。https://christophm.github.io/interpretable-ml-book/*