

# 六、局部模型不可知的解释方法

在前两章中，我们专门讨论了全局解释方法。这一章将探讨局部解释方法，这些方法用来解释为什么会做出一个或一组预测。它将涵盖如何利用`KernelExplainer`以及另一种称为**局部可解释模型不可知解释** ( **LIME** )的方法进行局部解释。我们还将探索如何对表格和文本数据使用这些方法。

这些是我们将在本章中涉及的主要话题:

*   利用 SHAP 的`KernelExplainer`对 SHAP 价值观进行本土解读
*   使用石灰
*   使用 LIME 进行**自然语言处理** ( **NLP** )
*   尝试 SHAP 的自然语言处理
*   比较 SHAP 和石灰

# 技术要求

本章的示例使用了`mldatasets`、`pandas`、`numpy`、`sklearn`、`nltk`、`lightgbm`、`rulefit`、`matplotlib`、`seaborn`、`shap`和`lime`库。关于如何安装所有这些库的说明在本书的序言中。本章的代码位于以下位置:

[https://github . com/packt publishing/Interpretable-Machine-Learning-with-Python/tree/master/chapter 06](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python/tree/master/Chapter06)

# 任务

谁不爱吃巧克力？！它是全球最受欢迎的食物，大约 90%的人喜欢它，每天大约有 10 亿人吃它。一种流行的消费形式是巧克力棒。然而，即使是普遍喜爱的成分也可能被用在并不普遍吸引人的地方——所以，巧克力棒可以从高级到普通，再到完全令人不快。通常，这仅仅是由可可的质量或额外的成分决定的，有时一旦与异国风味结合起来，它就会成为一种后天习得的味道。

一家痴迷于卓越的法国巧克力制造商向你伸出了援手。他们有一个问题。他们所有的酒吧都得到了评论家的高度评价，然而评论家们的味蕾却非常特别。而他们钟爱的一些酒吧，销量却莫名其妙的一般，但非评论家在焦点小组和品鉴会上似乎都很喜欢他们，所以他们很疑惑为什么销量和他们的市场调研不吻合。他们发现了一个由知识渊博的巧克力爱好者对巧克力棒进行评级的数据集，而这些评级恰好与它们的销量相吻合。为了获得公正的意见，他们向你寻求专业知识。

至于数据集，*曼哈顿巧克力协会*的成员自 2007 年以来一直在开会，唯一的目的是品尝和评判优质巧克力，教育消费者并激励巧克力制造商生产更高质量的巧克力。从那时起，他们已经收集了超过 2200 个巧克力棒的数据，由他们的成员按照以下标准进行评级:

*   4.0 - 5.00 =优秀
*   3.5 - 3.99 =强烈推荐
*   3.0 - 3.49 =推荐值
*   2.0 - 2.99 =令人失望
*   1.0 - 1.90 =不愉快

这些等级是根据香气、外观、质地、风味、余味和总体评价等因素得出的，被评级的巧克力棒大多是深色巧克力棒，因为其目的是欣赏可可的风味。除了评级之外，*Manhattan Chocolate Society*数据集还包括许多特征，如可可豆的养殖国家、巧克力棒有多少种成分、是否含盐以及用于描述它的词语。

我们的目标是了解为什么一家巧克力制造商的巧克力棒被评为优秀，但销售却很差，而另一家销量令人印象深刻的巧克力棒被评为令人失望。

# 走近

你已经决定使用本地模型解释来解释为什么每个酒吧是这样评级的。为此，您将准备数据集，然后训练分类模型来预测巧克力条评级是否高于或等于*高度推荐的*，因为客户希望他们所有的巧克力条都低于此阈值。您将需要训练两个模型:一个用于表格数据的,另一个用于描述巧克力棒的单词的 NLP 模型。我们将分别使用**支持向量机** ( **SVMs** )和**光梯度推进机** ( **LightGBM** )来完成这些任务。如果你没有使用过这些黑盒模型，不要担心——我们将简要解释它们。一旦你训练了模型，那么有趣的部分就来了:利用两种本地的模型不可知的解释方法来理解是什么使得一个特定的巧克力棒少于*强烈推荐的*或者不是。这些方法是 SHAP 和莱姆，当它们结合在一起时，将提供更丰富的解释来传达给你的客户。然后，我们将比较这两种方法，以了解它们的优势和局限性。

# 制剂

您将在此处找到该示例的代码:

[https://github . com/packt publishing/Interpretable-Machine-Learning-with-Python/blob/master/chapter 06/chocoratings . ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python/blob/master/Chapter06/ChocoRatings.ipynb)

## 加载库

若要运行此示例，您需要安装以下库:

*   `mldatasets`加载数据集
*   `pandas`、`numpy`和`nltk`来操纵它
*   `sklearn` (scikit-learn)和`lightgbm`拆分数据并拟合模型
*   `matplotlib`、`seaborn`、`shap`和`lime`可视化解释

您应该首先加载它们，如下所示:

```
import math
import mldatasets
import pandas as pd
import numpy as np
import re
import nltk
from nltk.probability import FreqDist
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn import metrics, svm
from sklearn.feature_extraction.text import TfidfVectorizer
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import lime
import lime.lime_tabular
from lime.lime_text import LimeTextExplainer
```

您还需要确保在加载之前下载了`stopwords`和`punkt`标记器，如下所示:

```
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
```

## 了解和准备数据

我们将数据加载到一个名为`chocolateratings_df`的数据帧中，如下所示:

```
chocolateratings_df = mldatasets.load("chocolate-bar-ratings") 
```

应该有 2200 多条记录和 18 列。我们可以通过检查数据帧的内容来验证这一点，如下所示:

```
chocolateratings_df
```

在*图 6.1* 中显示的输出符合我们的预期:

![Figure 6.1 – Contents of chocolate-bar dataset

](img/B16383_06_01.jpg)

图 6.1-巧克力棒数据集的内容

### 数据字典

数据字典包括以下内容:

*   `company`:分类；巧克力棒的制造商(在 500 多种不同的巧克力棒中)
*   `company_location`:分类；制造商所在的国家(66 个不同的国家)
*   `review_date`:连续；审查律师资格的年份(从 2006 年到 2020 年)
*   `country_of_bean_origin`:分类；可可豆收获的国家(62 个不同的国家)
*   `cocoa_percent`:分类；可可占巧克力的百分之几
*   `rating`:连续；曼哈顿巧克力协会*给出的评分*(可能值:1-5)
*   `counts_of_ingredients`:连续；酒吧中配料的数量
*   `beans`:二进制；它是用可可豆做的吗？(`have_bean`或`have_not_bean`)
*   `cocoa_butter`:二进制；它是用可可脂做的吗？(`have_cocoa_butter`或`have_not_cocoa_butter`)
*   `vanilla`:二进制；它是用香草做的吗？(`have_vanilla`或`have_not_vanilla`)
*   `lecithin`:二进制；是用卵磷脂做的吗？(`have_lecithin`或`have_not_lecithin`)
*   `salt`:二进制；它是用盐做的吗？(`have_salt`或`have_not_salt`)
*   `sugar`:二进制；它是用糖做的吗？(`have_sugar`或`have_not_sugar`)
*   `sweetener_without_sugar`:二进制；是用甜味剂做的不含糖吗？(`have_sweetener_without_sugar`或`have_not_sweetener_without_sugar`)
*   `first_taste`:文字；用于描述第一种味道的单词
*   `second_taste`:文字；用于描述第二种味道的单词
*   `third_taste`:文字；用于描述第三种味道的词语
*   `fourth_taste`:文字；用于描述第四种味道的词语

现在我们已经看了一眼数据，我们可以快速地准备这些数据，然后进行建模和解释！

### 数据准备

我们应该做的第一件事是把文本特征放在一边，这样我们就可以分别处理它们。我们可以先用它们创建一个名为`tastes_df`的数据帧，然后从`chocolateratings_df`中删除它们。然后我们可以使用`head`和`tail`来看看`tastes_df`，如下面的代码片段所示:

```
tastes_df = chocolateratings_df[['first_taste', 'second_taste', 'third_taste', 'fourth_taste']]
chocolateratings_df = chocolateratings_df.\
     drop(['first_taste', 'second_taste', 'third_taste', 'fourth_taste'], axis=1)
tastes_df.head(90).tail(10)
```

前面的代码产生了图 6.2 中所示的数据帧:

![Figure 6.2 – Tastes columns have quite a few null values

](img/B16383_06_02.jpg)

图 6.2–品味列有相当多的空值

现在，让我们对分类特征进行分类编码。`company_location`和`country_of_bean_origin`的国家太多了，我们来建立一个门槛。比方说，如果任何国家/地区的数据少于 3.333%(或 74 行)，那么让我们将其归入一个`Other`类别，然后对该类别进行编码。我们可以用上一章使用的`make_dummies_with_limits`函数很容易地做到这一点，下面的代码片段再次展示了这个过程:

```
chocolateratings_df =\
       mldatasets.make_dummies_with_limits(chocolateratings_df,
          'company_location', 0.03333)
chocolateratings_df =\        
       mldatasets.make_dummies_with_limits(chocolateratings_df,
          'country_of_bean_origin', 0.03333)
```

除了`beans`之外，所有二进制特征都必须转换为 1 和 0，这始终是相同的值(`have_bean`)，使用以下代码:

```
chocolateratings_df = chocolateratings_df.\
                      drop(['beans'], axis=1)
binary_features = ['cocoa_butter', 'vanilla', 'lecithin', 'salt',
                 'sugar', 'sweetener_without_sugar']
chocolateratings_df[binary_features] =\
    chocolateratings_df[binary_features].\
      apply(lambda x: np.where(x.str.contains('not'), 0, 1))
```

现在，为了处理`tastes_df`的内容，我们应该注意到，我们最希望看到的是表示——或者至少唤起——一种味道的形容词。因此，停用词——即常见的词，如`and`、`of`和`with`——可以安全地丢弃，因此我们需要从`nltk`加载一个`english`停用词列表，如下所示:

```
stop = stopwords.words('english')
```

如果您检查`tastes_df`的内容，您会发现除了停用词之外，其他元素也会给模型添加噪声。你会发现标点符号如`&`，副词如`overly`，拼写错误如`astringnet`，甚至形容词-名词组合如`full body`。这些可以去掉，或者用一个形容词代替。我们可以使用正则表达式来做到这一点，一举多得。为此，让我们首先用替换内容创建一个字典(`trans_dict`)并将它们转换成正则表达式，如下所示:

```
trans_dict = {'?':'','&':'', 'overly intense':'intensest',
    'overly sweet':'sweetest', 'overly tart':'tartest',
    'sl. bitter':'bitterness', 'sl. burnt':'burntness',
    'sl. sweet':'sweetness', 'sl. dry':'dryness',
    'sl. chalky':'chalkiness', 'sl. Burnt':'burntness',
    'hints fruit':'fruitiness', 'hint fruit':'fruitiness',
    'high acid':'acidic', 'high acidity':'acidic',
    'moderate acidity':'acid', 'high roast':'roast',
    'astringcy':'astringent', 'astringnet':'astringent',
    'full body':'robust', 'astringency':'astringent',
    'high astringent':'acidic', 'rich cocoa':'rich',
    'mild bitter':'bitterish', 'fruit long':'fruit',
    'base cocoa':'basic', 'basic cocoa':'basic', '-like':'',
    'smomkey':'smokey', 'true':'real', '(n)':'','/':' ',
    '-':' ',' +':' ' }
trans_regex = re.\
         compile("|".join(map(re.escape, trans_dict.keys( ))))
```

下面的代码用空字符串替换所有的空值，然后将`tastes_df`中的所有列连接在一起，形成一个序列。然后，它去掉开头和结尾的空白，并将所有文本转换成小写。它有两个`apply`实例——第一个执行所有的正则表达式替换，第二个删除停用词。代码如下面的代码片段所示:

```
tastes_s = tastes_df.replace(np.nan, '', regex=True).\
        agg(' '.join, axis=1).str.strip().str.lower().\
        apply(lambda s: trans_regex.sub(lambda match:
             trans_dict[match.group(0)], s)).\
          apply(lambda s: ' '.join([word for word in s.split()          
                        if word not in (stop)]))
```

瞧！您可以验证结果是一个熊猫系列(`tastes_s`)，带有(大部分)与味道相关的形容词，如下所示:

```
print(tastes_s)
```

正如所料，这个系列与`chocolateratings_df`数据帧的长度相同，如下面的代码片段所示:

```
0          cocoa blackberry robust
1             cocoa vegetal savory
2                rich fatty bready
3              fruity melon roasty
4                    vegetal nutty
                   ...            
2221       muted roasty accessible
2222    fatty mild nuts mild fruit
2223            fatty earthy cocoa
Length: 2224, dtype: object
```

但是让我们用下面的代码来看看它的短语中有多少是独特的:

```
print(np.unique(tastes_s).shape)
```

我们可以从下面的输出中看出，重复的短语不到 50 个，因此按短语进行标记不是一个好主意:

```
(2178,)
```

这里有很多方法可以采用，比如通过二元语法(两个单词的序列)或者甚至是子词(将单词分成逻辑部分)进行标记。然而，即使顺序不太重要(因为第一个单词与第一个味道有关，等等)，我们的数据集太小，并且有太多的空值(特别是在`third taste`和`fourth taste`中)，无法从顺序中获得意义。这就是为什么将所有的“口味”连接在一起是一个好的选择，从而消除了它们之间的明显差异。

还有一点需要注意的是，我们的单词(大部分)都是形容词。我们做了一点努力来删除副词，但仍然有一些名词存在，如“水果”和“坚果”，而不是形容词，如“水果”和“坚果”。我们不能确定评判巧克力棒的巧克力鉴赏家使用“水果”而不是“果味”是否有所不同。然而，如果我们确定了这一点，我们就可以执行**词干**或**词汇化**，将所有“水果”、“果味”和“水果性”的实例转换为一致的“fru”(*词干*或“水果”(*词汇*)。我们不会关心这个问题，因为我们的许多形容词的变体在短语中并不常见。

让我们找出最常见的单词，首先用`word_tokenize`标记它们，然后用`FreqDist`统计它们的频率。然后我们可以将得到的`tastewords_fdist`字典放入一个数据帧(`tastewords_df`)。我们可以只保存那些超过 74 个实例的单词作为列表(`commontastes_l`)。代码如下面的代码片段所示:

```
tastewords_fdist = FreqDist(word for word in
                word_tokenize(tastes_s.str.cat(sep=' ')))
tastewords_df = pd.DataFrame.from_dict(tastewords_fdist,\
                orient='index').rename(columns={0:'freq'})
commontastes_l = tastewords_df[tastewords_df.freq > 74].\
                  index.to_list()
print(commontastes_l)
```

从下面的`commontastes_l`输出可以看出，最常见的单词大多不同(除了`spice`和`spicy`):

```
['cocoa', 'rich', 'fatty', 'roasty', 'nutty', 'sweet', 'sandy', 'sour', 'intense', 'mild', 'fruit', 'sticky', 'earthy', 'spice', 'molasses', 'floral', 'spicy', 'woody', 'coffee', 'berry', 'vanilla', 'creamy']
```

我们可以用这个列表来增强我们的表格数据集，就是把这些常用词变成二进制特征。换句话说，对于这些“普通口味”(`commontastes_l`)中的每一种都有一列，如果巧克力棒的“口味”包括它，则该列的值为 1，否则为 0。幸运的是，我们可以用两行代码轻松做到这一点。首先，我们用 text-tastes 系列创建一个新列(`tastes_s`)。然后，我们使用上一章中使用的`make_dummies_from_dict`函数，通过在新专栏的内容中查找每个“共同爱好”来生成虚拟特征，如下所示:

```
chocolateratings_df['tastes'] = tastes_s
chocolateratings_df =\   
        mldatasets.make_dummies_from_dict(chocolateratings_df,
                      'tastes', commontastes_l)
```

现在我们已经完成了我们的特征工程，我们可以使用`info()`来检查我们的数据框架，就像这样:

```
chocolateratings_df.info()
```

输出具有除`company`之外的所有数值非空特征。有超过 500 家公司，所以`Other`，它可能会引入对最具代表性的少数公司的偏见。所以，不如干脆把这个栏目去掉。输出如下所示:

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2224 entries, 0 to 2223
Data columns (total 46 columns):
 #   Column                      Non-Null Count  Dtype  
---  ------                      --------------  -----  
 0   company                     2224 non-null   object 
 1   review_date                 2224 non-null   int64  
 2   cocoa_percent               2224 non-null   float64
 3   rating                      2224 non-null   float64
 4   counts_of_ingredients       2224 non-null   int64  
 :  :           :     :    :
 43  tastes_berry                2224 non-null   int64  
 44  tastes_vanilla              2224 non-null   int64  
 45  tastes_creamy               2224 non-null   int64  
dtypes: float64(2), int64(30), object(1), uint8(13)
memory usage: 601.7+ KB
```

我们为建模准备数据的最后一步从初始化`rand`开始，这个常量在整个练习中充当我们的“随机状态”。然后，我们定义`y`为大于或等于 3.5 时转换为 1 的`rating`列，否则为 0。`X`就是其他的一切(不包括`company`)。然后，我们用`train_test_split`将`X`和`y`分成训练和测试数据集，如下面的代码片段所示:

```
rand = 9
y = chocolateratings_df['rating'].\
       apply(lambda x: 1 if x >= 3.5 else 0)
X = chocolateratings_df.drop(['rating','company'], axis=1).copy()
X_train, X_test, y_train, y_test = train_test_split(X, y,\
                    test_size=0.33, random_state=rand)
```

除了表格测试和训练数据集，对于我们的 NLP 模型，我们将需要与我们的`train_test_split`一致的纯文本特征数据集，以便我们可以使用相同的`y`标签。为此，我们可以通过对我们的品味系列(`tastes_s`)进行子集化来实现，使用我们的`X_train`和`X_test`集合的`index`来产生该系列的 NLP 特定版本，如下所示:

```
X_train_nlp = tastes_s[X_train.index]
X_test_nlp = tastes_s[X_test.index]
```

好吧！我们现在都准备好了。让我们开始建模和解释我们的模型吧！

# 利用 SHAP 的内核解释器对 SHAP 价值观进行本地解释

对于这个部分，以及为了的后续使用，我们将首先训练一个**支持向量分类器** ( **SVC** )模型。

### 训练 C-SVC 模型

SVM 是一系列模型类，它们在高维空间中操作以寻找最优超平面，其中它们试图用它们之间的最大间隔来分离类。支持向量是最接近决策边界(划分超平面)的点，如果移除这些点，将会改变决策边界。为了找到最佳超平面，他们使用了一个名为**铰链损失**的成本函数和一个计算成本低廉的方法在高维空间中操作，名为**核技巧**，甚至尽管超平面暗示了线性可分性，但它并不总是局限于线性核。

我们将使用的 scikit-learn 实现称为 C-SVC。SVC 使用一个名为 *C* 的 L2 正则化参数，默认情况下，使用一个名为**径向基函数** ( **RBF** )的内核，它显然是非线性的。对于 RBF， **gamma** 超参数定义了内核中每个训练示例的影响半径，但是是以反比例的方式。因此，低值会增大半径，而高值会减小半径。

SVM 家族包括分类的几个变体，甚至通过**支持向量回归** ( **SVR** )回归类。SVM 模型最显著的优点是，当与观测值相比有许多特征时，甚至当特征超过观测值时，它们往往能有效和高效地工作！它还倾向于发现数据中潜在的非线性关系，而不会过度拟合或变得不稳定。然而，SVM 不可扩展到更大的数据集，并且很难调整其超参数。

由于我们将使用由`set()`激活的`seaborn`绘图样式，对于本章的一些绘图，我们将首先保存原始的`matplotlib`设置(`rcParams`，以便我们以后可以恢复它们。关于`SVC`需要注意的一点是，它本身并不产生概率，因为它是线性代数。然而，如果是`probability=True`，scikit-learn 实现将使用交叉验证，然后将逻辑回归模型拟合到 SVC 的分数，以生成概率。我们还使用了`gamma=auto`，这意味着它被设置为 1/# features——所以，1/44。与往常一样，建议设置您的`random_state`参数以获得再现性。一旦我们将模型拟合到训练数据，我们就可以使用`evaluate_class_mdl`来评估我们的模型的预测性能，如以下代码片段所示:

```
orig_plt_params = plt.rcParams
sns.set()
svm_mdl = svm.SVC(probability=True, gamma='auto', random_state=rand)
fitted_svm_mdl = svm_mdl.fit(X_train, y_train)
y_train_svc_pred, y_test_svc_prob, y_test_svc_pred =\
      mldatasets.evaluate_class_mdl(fitted_svm_mdl, X_train,\
                         X_test, y_train, y_test)
```

前面的代码产生了图 6.3 中所示的输出:

![Figure 6.3 – Predictive performance of our SVC model

](img/B16383_06_03.jpg)

图 6.3–我们的 SVC 模型的预测性能

实现的性能(参见*图 6.3* )并不差，考虑到这是一个小的不平衡数据集，而对于机器学习模型的用户评级来说，这已经是一个具有挑战性的领域。无论如何，曲线 ( **AUC** )曲线下的**面积在抛硬币虚线上方，**马修斯相关系数** ( **MCC** )安全在 0 以上。更重要的是，精确度大大高于召回率，考虑到将一块糟糕的巧克力棒错误归类为*强烈推荐*的假设成本，这是非常好的。我们更喜欢精确而不是召回，因为我们更希望假阳性比假阴性少。**

### 使用 KernelExplainer 计算 SHAP 值

考虑到强力计算 SHAP 值的计算强度，SHAP 库采用了许多统计上有效的快捷方式。正如我们在 [*第 5 章*](B16383_05_ePub_RK.xhtml#_idTextAnchor106) *、全局模型不可知解释方法*中了解到的，这些捷径的范围从利用决策树的结构(`TreeExplainer`)到神经网络激活的差异，从基线(`DeepExplainer`)到神经网络的梯度(`GradientExplainer`)。这些快捷方式使得解释器明显不那么模型不可知，因为它们被限制在一个模型类家族中。然而，在 SHAP 有一个真正的模型不可知的解释者，叫做`KernelExplainer`。

`KernelExplainer`有两个快捷方式:它对联盟的所有特征排列的子集进行采样，并根据联盟的大小使用加权方案来计算 SHAP 值。第一种捷径是减少计算时间的推荐技术。第二个来自 LIME 的加权方案，我们将在本章接下来讨论，SHAP 的作者这样做是为了使它仍然符合 Shapley。然而，对于联盟中的“缺失”特征，它从背景训练数据集中的特征值中随机采样，这违反了`KernelExplainer`具有 Shapley 值的所有其他好处，并且是 LIME 的主要优势之一。

在我们使用`KernelExplainer`之前，重要的是要注意，对于分类模型，它会产生多个 SHAP 值的列表。你可以用一个索引来访问每个类。如果这个索引不是您期望的顺序，可能会引起混乱，因为它是按照模型提供的顺序排列的。因此，通过运行以下命令来确保模型中的类的顺序是非常重要的:

```
svm_mdl.classes_
```

输出告诉你*不强烈推荐*的索引为 0，正如你所料，而*强烈推荐*的索引为 1。我们对后者的 SHAP 值感兴趣，因为这是我们试图预测的。代码如下所示:

```
array([0, 1])
```

`KernelExplainer`接受模型的预测函数(`fitted_svm_mdl.predict_proba`)和一些背景训练数据(`X_train_summary`)。`KernelExplainer`强烈建议采用其他措施来最小化计算。其中之一是使用`nsamples=200`得出 SHAP 值。它在拟合过程中使用 L1 正则化(`l1_reg`)。我们在这里告诉它的是将其正则化到只有 20 个相关特征的程度。最后，我们可以使用一个`summary_plot`来绘制类 1 的 SHAP 值。代码如下面的代码片段所示:

```
np.random.seed(rand)
X_train_summary = shap.kmeans(X_train, 10)
shap_svm_explainer =\
          shap.KernelExplainer(fitted_svm_mdl.predict_proba,
                       X_train_summary)
shap_svm_values_test = shap_svm_explainer.shap_values(X_test,
                nsamples=200, l1_reg="num_features(20)")
shap.summary_plot(shap_svm_values_test[1], X_test, plot_type="dot")
```

前面的代码产生了如图 6.4 所示的输出。尽管本章的重点是局部模型解释，但重要的是从全局形式开始，以确保结果是直观的。如果不是，可能是出了什么问题。

这里可以看到输出:

![Figure 6.4 – Global model interpretation with SHAP using a summary plot

](img/B16383_06_04.jpg)

图 6.4-SHAP 使用汇总图的全球模型解释

在*图 6.4* 中，我们可以看出最高(红色)可可百分比(`cocoa_percent`)倾向于与*高度推荐*的可能性下降相关，但是中间值(紫色)倾向于增加它。这一发现很直观，因为最黑的巧克力比不太黑的巧克力更像是一种后天习得的味道。低值(蓝色)分散在各处，因此没有显示趋势，但这可能是因为没有多少趋势。另一方面，`review date`暗示早些年很可能是*极力推荐的*。0 的两边都有明显的红色和紫色阴影，所以很难在这里确定趋势。一个依赖图，比如那些在 [*第五章*](B16383_05_ePub_RK.xhtml#_idTextAnchor106) *中使用的，全局模型不可知的解释方法*，对于这个目的会更好。然而，二进制特征很容易看到高值和低值(1 和 0)对模型的影响。例如，我们可以看出，可可味、奶油味、浓郁味和浆果味增加了巧克力被推荐的可能性，而甜味、泥土味、酸味和脂肪味则相反。同样，如果巧克力是在美国生产的，强烈推荐的巧克力的几率也会降低。抱歉，我们。

### 使用决策图对一组预测进行局部解释

对于局部解释，你不必一次想象一个点——相反，你可以一次解释几个点。关键是提供一些上下文来充分比较这些点，不能太多以至于你无法区分它们。通常，您会发现异常值或仅符合特定标准的异常值。在本练习中，我们将只选择那些由您的客户生产的条形，如下所示:

```
sample_test_idx = X_test.index.\
               get_indexer_for([5,6,7,18,19,21,24,25,27])
```

Shapley 的一个优点是它的可加性，这很容易证明。如果将所有 SHAP 值添加到用于计算它们的期望值中，就会得到一个预测值。当然，这是一个分类问题，所以预测是一个概率；所以，要得到一个布尔数组，我们必须检查概率是否大于 0.5。我们可以通过运行以下代码来检查这个布尔数组是否与我们的模型的测试数据集预测(`y_test_svc_pred`)相匹配:

```
expected_value = shap_svm_explainer.expected_value[1]
y_test_shap_pred =\
            (shap_svm_values_test[1].sum(1) + expected_value) > 0.5
print(np.array_equal(y_test_shap_pred, y_test_svc_pred))
```

这是应该的，事实也的确如此！您可以在此处看到确认信息:

```
True
```

SHAP 的决策情节有一个突出的特点，我们可以用它来突出假阴性(`FN`)。现在，让我们弄清楚哪些样本观察值是`FN`，如下所示:

```
FN = (~y_test_shap_pred[sample_test_idx]) &
    (y_test.iloc[sample_test_idx] == 1).to_numpy()
```

我们现在可以快速将我们的绘图样式重置回默认的`matplotlib`样式，并绘制一个`decision_plot`。它采用了`expected_value`、SHAP 值以及我们希望绘制的那些项目的实际值。或者，我们可以用虚线提供一个我们想要突出显示的项目的布尔数组—在本例中是假阴性(`FN`)，如下面的代码片段所示:

```
sns.reset_orig()
plt.rcParams.update(orig_plt_params)
shap.decision_plot(expected_value,\
            shap_svm_values_test[1][sample_test_idx],\
              X_test.iloc[sample_test_idx], highlight=FN)
```

在*图 6.5* 中生成的图对每个观察值都有一条颜色编码线。每条线的颜色代表的不是任何特征的值，而是模型输出。因为我们在`KernelExplainer`中使用了`predict_proba`，所以这是一个概率，否则它会显示 SHAP 值，并且当它们撞击顶部 *x* 轴时的值是预测值。这些要素根据重要性进行排序，但仅在绘制的观察值中进行排序，您可以看出这些线根据每个要素水平增加和减少。它们变化的程度和方向取决于特征对结果的贡献。灰色线表示类的期望值，类似于线性模型中的截距。事实上，类似地，所有行都从这个值开始，使得从下到上阅读绘图最好。

您可以在这里查看的输出:

![Figure 6.5 – Local model interpretation with SHAP for a sample of predictions, highlighting false negatives

](img/B16383_06_05.jpg)

图 6.5-预测样本的 SHAP 本地模型解释，突出了假阴性

你可以看出在*图 6.5* 中有三个假阴性，因为它们有虚线。使用该图，我们可以很容易地看到哪些特征使他们最向左转向，因为这是使他们做出负面预测的原因。例如，我们知道最左边的假阴性在期望值线的右边，直到`lecithin`，然后继续减少，直到`company_location_France`，并且`review_date`增加了*高度推荐*的可能性，但是这还不够。您可以看出`county_of_bean_origin_Other`降低了两个错误分类的可能性。这个决定可能是不公平的，因为这个国家可能是 50 多个没有自己特色的国家之一。很有可能，这些国家的咖啡豆有很大的差异。

决策图也可以隔离单个观察值。当它这样做时，它在虚线旁边打印每个特征的值。让我们为同一家公司的决策图绘制一个图(正真观察#696)，如下所示:

```
shap.decision_plot(expected_value, shap_svm_values_test[1][696], X_test.iloc[696], highlight=0)
```

*这里的图 6.6* 是由前面的代码输出的:

![Figure 6.6 – Local model interpretation with SHAP for a single true positive in the sample of predictions

](img/B16383_06_06.jpg)

图 6.6–预测样本中单个真阳性的 SHAP 局部模型解释

在*图 6.6* 中，您可以看到`lecithin`和`counts_of_ingredients`将高度推荐的*可能性降低到可能危及它的程度。幸运的是，以上所有的特征都明显向右倾斜，因为`company_location_France=1`、`cocoa_percent=70`和`tastes_berry=1`都是受欢迎的。*

### 使用力图一次对单个预测进行局部解释

你的客户，巧克力制造商，有两种巧克力棒想要你比较。5 号杠是*优秀*而 24 号杠是*令人失望*。它们都在您的测试数据集中。比较它们的一种方法是将它们的值并排放在一个数据帧中，以了解它们到底有什么不同。我们将等级、实际标签`y`和`y_pred`预测标签连接到这些观察值，如下所示:

```
eval_idxs = (X_test.index==5) | (X_test.index==24)
X_test_eval = X_test[eval_idxs]
eval_compare_df = pd.concat([\
     chocolateratings_df.iloc[X_test[eval_idxs].index].rating,\
     pd.DataFrame({'y':y_test[eval_idxs]}, index=[5,24]),\
     pd.DataFrame({'y_pred':y_test_svc_pred[eval_idxs]},\
     index=[24,5]),\
     X_test_eval], axis=1).transpose()
eval_compare_df
```

前面的代码产生了图 6.7 中*所示的数据帧。有了这个数据框架，您可以确认它们没有被错误分类，因为`y=y_pred`。错误的分类可能会使模型解释不可靠，无法理解为什么人们更喜欢一块巧克力。然后，您可以检查这些特征以找出差异—例如，您可以看出`review_date`相差 2 年。此外，*优秀的*棒的豆子来自委内瑞拉，而*令人失望的*豆子来自另一个不太受欢迎的国家。*优秀的*有浆果味，而*令人失望的*有土味。*

你可以在这里看到观察结果:

![Figure 6.7 – Observations #5 and #24 side by side, with feature differences highlighted in yellow

](img/B16383_06_07.jpg)

图 6.7–并排观察#5 和#24，特征差异用黄色突出显示

force plot 可以告诉我们一个完整的故事，告诉我们模型决策中的权衡因素(大概还有评审人员)，并为我们提供消费者可能更喜欢什么的线索。绘制`force_plot`需要您感兴趣的类别的期望值(`expected_value`)、您感兴趣的观察的 SHAP 值以及该观察的实际值。我们将从观察#5 开始，如下面的代码片段所示:

```
shap.force_plot(expected_value,\     
            shap_svm_values_test[1][X_test.index==5],\
            X_test[X_test.index==5], matplotlib=True)
```

前面的代码产生了如图*图 6.8* 所示的图。这个力图描绘了`review_date`、`cocoa_percent`和`tastes_berry`在预测中的权重，而唯一似乎在相反方向上加权的特征是`counts_of_ingredients`。

这里可以看到输出:

![Figure 6.8 – Force plot for observation #5 (Outstanding)

](img/B16383_06_08.jpg)

图 6.8–观察#5 的力图(未完成)

让我们将其与观察结果#24 的力图进行比较，如下所示:

```
shap.force_plot(expected_value,\  
          shap_svm_values_test[1][X_test.index==24],\
          X_test[X_test.index==24], matplotlib=True)
```

前面的代码产生了如图*图 6.9* 所示的图。我们可以很容易地看出`tastes_earthy`和`country_of_bean_origin_Other`被我们的模型认为是高度负面的属性。这一结果可以用“浆果”和“泥土”的巧克力味道的差异来解释。尽管我们有所发现，但咖啡豆的原产地还需要进一步调查。毕竟，有可能实际的原产地与糟糕的评级没有关联。

这里可以看到输出:

![Figure 6.9 – Force plot for observation #24 (Disappointing)

](img/B16383_06_09.jpg)

图 6.9–观察#24 的力图(令人失望)

在这个部分，我们介绍了的`KernelExplainer`，它使用了一些从 LIME 学到的技巧。但是什么是石灰呢？我们接下来会发现这一点！

# 使用石灰

到目前为止，我们已经讨论过的模型不可知的解释方法试图协调模型的输入和输出的总和。为了让这些方法更好地理解`X`如何以及为什么会变成`y_pred`，它们首先需要一些数据。然后，他们用这些数据进行模拟，将数据的变化推进并评估模型的结果。有时，他们甚至利用一个全球代理来连接这些点。通过使用他们在这个过程中学到的东西，他们产生了重要性、分数、规则或值，这些值量化了一个特征在全球层面上的影响、交互或决策。对于许多方法，如 SHAP，这些也可以在当地观察到。然而，即使可以在当地观察到，全球量化的东西也不一定适用于当地。出于这个原因，应该有另一种方法来量化仅用于局部解释的特征的局部影响——比如 LIME！

### 什么是石灰？

**LIME** 训练本地代理解释单个预测。为此，它首先会问你想要解读哪个*数据点*。您还向它提供了您的黑盒模型和一个样本数据集。然后，它使用模型对数据集的*扰动*版本进行预测，创建一个方案，通过该方案，它进行采样，并且如果*更接近*您选择的数据点，则点的权重会更高。您的点周围的区域称为邻域。然后，使用该邻域中的采样点和黑盒预测，它训练加权的*内在可解释代理模型*。最后，解释了代理模型。

这里有很多关键词要解包，所以让我们定义它们，如下所示:

*   **选择的数据点** : LIME 调用你想要解释一个*实例*的数据点、行或观察值。只是这个概念的另一种说法。
*   **扰动** : LIME 通过扰动从分类特征的训练数据集分布和连续特征的正态分布中提取的每个特征来模拟新样本。
*   **加权方案** : LIME 使用指数平滑内核来定义邻域半径，并确定如何对最远的点和最近的点进行加权。
*   **Closer** : LIME 对表格和图像数据使用欧氏距离，对文本使用余弦相似度。这在高维特征空间中很难想象，但您可以计算任意维数的点之间的距离，并找到哪些点最接近感兴趣的点。
*   **本质上可解释的替代模型** : LIME 使用带有加权岭正则化的稀疏线性模型。然而，它可以使用任何内在可解释的模型，只要数据点可以被加权。这背后的想法是双重的。它需要一个能够产生可靠的内在参数的模型，比如告诉它每个特征对预测有多大影响的系数。它还需要更多地考虑最接近所选点的数据点，因为这些数据点更相关。

与**k-最近邻** ( **k-NN** )非常相似，LIME 背后的直觉是邻域中的点具有共同性，因为您可以期望彼此靠近的点具有相似的(如果不是相同的)标签。分类器有决策边界，所以当接近的点被一除时，这可能是一个非常天真的假设。

与最近邻族中的另一个模型类**半径最近邻**相似，LIME 会将沿半径的距离考虑在内，并相应地对点进行加权，尽管它是以指数方式进行的。但是，LIME 不是一个模型类，而是一个解释方法，所以相似之处仅限于此。它不在相邻模型之间“投票”预测，而是适合加权代理稀疏线性模型，因为它假设每个复杂模型在局部都是线性的，并且因为它不是模型类，代理模型做出的预测无关紧要。事实上，代理模型甚至不需要像手套一样拟合数据，因为你需要的只是系数。当然，话虽如此，但最好是拟合得很好，以便在解释中有更高的保真度。

LIME适用于表格、图像和文本数据，通常具有较高的局部保真度，这意味着它可以在局部水平上很好地近似模型预测。然而，这取决于正确定义的邻域，这源于选择正确的核宽度和保持真实的局部线性假设。

### 使用 LimeTabularExplainer 一次对单个预测进行局部解释

为了解释单个预测，首先通过向提供一个 NumPy 2D 数组(`X_test.values`)中的样本数据集、一个包含特征名称(`X_test.columns`)的列表、一个包含分类特征索引(只有前三个特征不是分类的)的列表以及类名来实例化一个`LimeTabularExplainer`。即使只需要样本数据集，也建议您为要素和类提供名称，以便解释有意义。对于表格数据，告诉 LIME 哪些要素是分类的(`categorical_features`)非常重要，因为它对分类要素的处理不同于对连续要素的处理，不指定这一点可能会导致不合适的局部替代。另一个可以极大影响本地代理的参数是`kernel_width`。这定义了邻域的直径，从而回答了什么被认为是局部的问题。它有一个默认值，这个值可能会也可能不会产生对您的实例有意义的解释。您可以逐个实例地调整这个参数，以优化您的解释。代码可以在下面的代码片段中看到:

```
lime_svm_explainer =\  
  lime.lime_tabular.LimeTabularExplainer(X_test.values,\
         feature_names=X_test.columns,\        
         categorical_features=list(range(3,44)),\
         class_names=['Not Highly Recomm.', 'Highly Recomm.'])
```

有了实例化的解释器，您现在可以使用`explain_instance`来使本地代理模型适合观察#5。我们还将使用我们模型的分类器函数(`predict_proba`)，并将我们的特征数量限制为八个(`num_features=8`)。我们可以将返回的“解释”立即用`show_in_notebook`可视化。在的同时，`predict_proba`参数确保它也包括一个图，根据本地代理模型显示哪个类是最可能的。代码如下面的代码片段所示:

```
lime_svm_explainer.\
  explain_instance(X_test[X_test.index==5].values[0],\
             fitted_svm_mdl.predict_proba,\
               num_features=8). show_in_notebook(predict_proba=True)
```

前置代码提供如图*图 6.10* 所示的输出。根据当地代理人的说法，小于或等于 70 的`cocoa_percent`值是一个有利的属性，浆果味道也是如此。在这个模型中，缺少酸味、甜味和糖蜜味道也是有利的。然而，缺乏丰富的、奶油的和可可的味道会产生相反的效果，但不足以将天平推向不太推荐的方向。

这里可以看到输出:

![Figure 6.10 – LIME tabular explanation for observation #5 (Outstanding)

](img/B16383_06_10.jpg)

图 6.10–观察结果#5 的石灰表解释(未完成)

通过对产生*图 6.10* 的代码做一个小的调整，我们可以产生相同的图，但是对于观察#24，如下所示:

```
lime_svm_explainer.\
  explain_instance(X_test[X_test.index==24].values[0],\
              fitted_svm_mdl.predict_proba,\
              num_features=8).\
  show_in_notebook(predict_proba=True)
```

这里，在*图 6.11* 中，我们可以清楚地看到为什么本地代理认为观察# 24*不是高度推荐的*:

![Figure 6.11 – LIME tabular explanation for observation #24 (Disappointing)

](img/B16383_06_11.jpg)

图 6.11–观察结果#24 的石灰表解释(令人失望)

一旦你比较#24 ( *图 6.11* )和#5 ( *图 6.10* )的解释，问题就很明显了。一个单一的特征`tastes_berry`是区分这两种解释的原因。当然，我们已经把它限制在八大特征中，所以可能还有更多。然而，你会期望前八个特征包括最重要的特征。

根据 SHAP 的说法，知道`tastes_earthy=1`是全球解释#24 巧克力棒令人失望的本质，但这似乎是违反直觉的。发生了什么事？结果是观察值#5 和#24 相对相似，因此在相同的邻域中。这附近也有许多浆果口味的巧克力棒，很少有泥土口味的。然而，没有足够多的土的认为它是一个显著的特征，所以它将*高度推荐*和*不高度推荐*之间的差异归因于其他似乎更经常区分的特征，至少在局部上。原因有两个:局部邻域可能太小，线性模型由于简单，处于*偏差-方差权衡*的偏差端。这种偏见只会因为这样一个事实而加剧，即一些特征，如`tastes_berry`可能比`tastes_earthy`出现得相对更频繁。我们可以使用一种方法来解决这个问题，我们将在下一节讨论这个问题。

# 使用石灰进行 NLP

在本章的开头，我们留出了训练和测试数据集，其中包含 NLP 的所有“tastes”列的清理内容。我们可以看一下 NLP 的测试数据集，如下所示:

```
print(X_test_nlp)
```

这将输出以下内容:

```
1194                 roasty nutty rich
77      roasty oddly sweet marshmallow
121              balanced cherry choco
411                sweet floral yogurt
1259           creamy burnt nuts woody
                     ...              
327          sweet mild molasses bland
1832          intense fruity mild sour
464              roasty sour milk note
2013           nutty fruit sour floral
1190           rich roasty nutty smoke
Length: 734, dtype: object
```

没有机器学习模型可以将数据作为文本摄取，所以我们需要将其转换为数字格式——换句话说，将其矢量化。我们可以使用许多技术来做到这一点。在我们的例子中，我们对单词在每个短语中的位置和语义都不感兴趣。然而，我们感兴趣的是它们的相对发生率——毕竟，这是我们在上一节讨论的问题。

出于这些原因，`TfidfVectorizer`来自 scikit-learn。然而，当您必须进行 TD-IDF 评分时，这些评分将符合训练数据集，因为这样一来，转换后的训练和测试数据集对每个术语的评分都是一致的。看看下面的代码片段:

```
vectorizer = TfidfVectorizer(lowercase=False)
X_train_nlp_fit = vectorizer.fit_transform(X_train_nlp)
X_test_nlp_fit = vectorizer.transform(X_test_nlp)
```

为了了解 TF-IDF 的得分情况，我们可以将所有的特征名称放在数据帧的一个列中，并将它们在一次观察中各自的得分放在另一个列中。注意由于矢量器产生了一个`scipy`稀疏矩阵，我们必须用`todense()`将其转换成一个 NumPy 矩阵，然后用`asarray()`将其转换成一个 NumPy 数组。我们可以根据 TD-IDF 分数对该数据帧进行降序排序。代码如下面的代码片段所示:

```
pd.DataFrame({'taste':vectorizer.get_feature_names(),\
          'tf-idf': np. asarray(X_test_nlp_fit[X_test_nlp.index==5]. todense())[0]}).\
 sort_values(by='tf-idf', ascending=False)
```

前面的代码产生了图 6.12 中所示的输出:

![Figure 6.12 – The TF-IDF scores for words present in observation #5

](img/B16383_06_12.jpg)

图 6.12–观察#5 中出现的单词的 TF-IDF 得分

而且从*图 6.12* 中可以看出，TD-IDF 得分是介于 0 和 1 之间的归一化值，语料库中最常见的那些具有较低的值。有趣的是，我们意识到表格数据集中的观察值#5 有`berry=1`,因为有`berry`,不管它是否匹配整个单词。这不是问题,因为树莓是一种浆果,树莓不是我们的常见口味之一，它有自己的二元列。

既然我们已经对 NLP 数据集进行了矢量化，我们就可以继续建模了。

### 训练 LightGBM 模型

`X_train_nlp_fit`稀疏矩阵！这很好地总结了我们在这个练习中使用 LightGBM 的原因。

为了训练light GBM 模型，我们首先通过设置最大树深度(`max_depth`)、学习速率(`learning_rate`)、要拟合的提升树的数量(`n_estimators`)、`objective`(二进制分类)以及最后但同样重要的用于再现性的`random_state`来初始化模型。通过`fit`，我们使用我们的矢量化 NLP 训练数据集(`X_train_nlp_fit`)和用于 SVM 模型的相同标签(`y_train`)来训练模型。一旦训练完毕，我们就可以使用我们在 SVM 时使用的`evaluate_class_mdl`进行评估。代码如下面的代码片段所示:

```
lgb_mdl = lgb.LGBMClassifier(max_depth=13, learning_rate=0.05,\
   n_estimators=100, objective='binary', random_state=rand)
fitted_lgb_mdl = lgb_mdl.fit(X_train_nlp_fit, y_train) 
y_train_lgb_pred, y_test_lgb_prob, y_test_lgb_pred =\
 mldatasets.evaluate_class_mdl(fitted_lgb_mdl, X_train_nlp_fit, X_test_nlp_fit, y_train, y_test)
```

前面的代码生成了*图 6.13* ，如下所示:

![Figure 6.13 – Predictive performance of our LightGBM model

](img/B16383_06_13.jpg)

图 6.13–我们的 LightGBM 模型的预测性能

LightGBM 实现的性能(见*图 6.13* )略低于 SVM ( *图 6.3* )，但仍然相当不错，安全地在抛硬币线以上。SVM 关于这个模型更注重精确度而不是召回率的评论也适用于此。

### 使用 LimeTextExplainer 一次对单个预测进行局部解释

为了用 LIME 解释任何黑盒模型预测，您需要为您的模型指定一个分类器函数，如`predict_proba`，它将使用该函数对您实例附近的扰动数据进行预测，然后用它训练一个线性模型。实例必须是数字形式，换句话说，是矢量化的。但是，如果您可以提供任意文本，然后它可以动态地对其进行矢量化，那就更容易了。这正是管道能为你做的。使用 scikit-learn 的`make_pipeline`函数，您可以定义一系列转换数据的估计器，然后定义一个适合它的估计器。在这种情况下，我们只需要`vectorizer`来转换我们的数据，然后是我们的 LightGBM 模型(`lgb_mdl`)，它接受转换后的数据，如下面的代码片段所示:

```
lgb_pipeline = make_pipeline(vectorizer, lgb_mdl)
```

初始化一个`LimeTextExplainer`非常简单。所有参数都是可选的，但是建议为您的类指定名称。就像`LimeTabularExplainer`一样，`kernel_width`可选参数可能很关键，因为它定义了邻域的大小，并且有一个默认值，它可能不是最佳的，但可以根据具体情况进行调整。代码如下所示:

```
lime_lgb_explainer = LimeTextExplainer(class_names=['Not Highly Recomm.', 'Highly Recomm.'])
```

用`LimeTextExplainer`解释一个实例类似于用`LimeTabularExplainer`来解释。不同之处在于，我们使用管道(`lgb_pipeline`)，我们提供的数据(第一个参数)是文本，因为管道可以为我们转换它。代码如下面的代码片段所示:

```
lime_lgb_explainer.\
    explain_instance(X_test_nlp[X_test_nlp.index==5].values[0],\
              lgb_pipeline.predict_proba, num_features=4).\
   show_in_notebook(text=True)
```

根据 LIME text 解释者(参见*图 6.14* )的说法，LightGBM 模型预测*强烈推荐*进行观察#5，因为**焦糖**这个词。至少根据当地街坊的说法，**树莓**不是一个因素。

这里可以看到输出:

![Figure 6.14 – LIME text explanation for observation #5 (Outstanding)

](img/B16383_06_14.jpg)

图 6.14–观察结果#5 的石灰文本说明(未完成)

现在，让我们对比一下第 5 条观察结果和第 24 条观察结果的解释，就像我们以前做过的那样。我们可以使用相同的代码，但只需用 24 代替 5，如下所示:

```
lime_lgb_explainer.\
    explain_instance(X_test_nlp[X_test_nlp.index==24].values[0], \
               lgb_pipeline.predict_proba, num_features=4).
   show_in_notebook(text=True)
```

根据*图 6.15* ，你可以知道观察#24，描述为品尝起来像**烧焦**木头木头**泥土**巧克力是*不高度推荐*，因为有**泥土**和**烧焦**的字样。

这里可以看到输出:

![Figure 6.15 – LIME tabular explanation for observation #24 (Disappointing)

](img/B16383_06_15.jpg)

图 6.15–观察#24 的石灰表解释(令人失望)

考虑到我们使用的管道可以对任意文本进行矢量化，让我们享受一下其中的乐趣吧！我们将首先尝试由我们怀疑我们的模型喜欢的形容词组成的短语，然后尝试由不喜欢的形容词组成的短语，最后尝试使用我们的模型不应该熟悉的单词，如下所示:

```
lime_lgb_explainer.explain_instance('creamy rich complex fruity', \
               lgb_pipeline.predict_proba, num_features=4).
   show_in_notebook(text=True)
lime_lgb_explainer.explain_instance('sour bitter roasty molasses',
               lgb_pipeline.predict_proba, num_features=4).
   show_in_notebook(text=True)
lime_lgb_explainer.explain_instance('nasty disgusting gross stuff', \
   lgb_pipeline.predict_proba, num_features=4).
   show_in_notebook(text=True)
```

在*图 6.16* 中，对**奶油** **丰富** **复杂** **果味**和**酸味** **苦味** **烘烤** **糖蜜**的解释非常准确，因为模型知道这些词要么非常有利，要么非常不利。这些词在当地也很常见。

您可以在这里看到输出:

![Figure 6.16 – Arbitrary phrases not in the training or test dataset can be effortlessly explained with LIME, as long as words are in the corpus

](img/B16383_06_16.jpg)

图 6.16-只要单词在语料库中，不在训练或测试数据集中的任意短语可以毫不费力地用 LIME 解释

然而，你可能会错误地认为*的预测不是强烈推荐*为**龌龊** **恶心** **恶心** **玩意儿**与文字有关。LightGBM 模型之前没有见过这些词，所以预测更多的是*不太推荐*是多数类，这是一个很好的猜测，这个短语的稀疏矩阵全是零。因此，LIME 很可能在其邻域中发现很少的远点(如果有的话),因此 LIME 的本地代理模型的零系数反映了这一点。

# 尝试将 SHAP 用于自然语言处理

大多数 SHAP 的解释者将会使用表格数据。`DeepExplainer`可以做文本但仅限于深度学习模型，而且，正如我们将在 [*第八章*](B16383_08_ePub_RK.xhtml#_idTextAnchor162) *中所涵盖的，可视化卷积神经网络*，其中三个做图像，包括`KernelExplainer`。事实上，SHAP 的`KernelExplainer`被设计成一个通用的真正模型不可知的方法，但是它并没有被提升为 NLP 的一个选项。很容易理解为什么:它很慢，并且 NLP 模型往往非常复杂，需要启动数百个(如果不是数千个)功能。在这种情况下，词序不是一个因素，您有几百个特征，但是前 100 个出现在您的大多数观察中，`KernelExplainer`可能会起作用。

除了克服缓慢，你还需要克服几个技术障碍。其中之一是`KernelExplainer`与管道兼容，但它期望返回一组预测。但是 LightGBM 返回两套，每类一套:*不强烈推荐*和*强烈推荐*。为了克服这个问题，我们可以创建一个包含`predict_proba`函数的`lambda`函数(`predict_fn`)，该函数只返回那些对*强烈推荐的*的预测。下面的代码片段说明了这一点:

```
predict_fn = lambda X: lgb_mdl.predict_proba(X)[:,1]
```

第二个技术障碍是 SHAP 与 SciPy 的稀疏矩阵不兼容，对于我们的解释者来说，我们需要这种格式的样本矢量化测试数据。为了解决这个问题，可以将 SciPy 稀疏矩阵格式的数据转换为 NumPy 矩阵，然后转换为 pandas 数据帧(`X_test_nlp_samp_df`)。为了克服任何缓慢，我们可以使用与上次相同的`kmeans`技巧。除了为克服障碍而进行的调整之外，以下代码与 SHAP 用 SVM 模型执行的代码完全相同:

```
X_test_nlp_samp_df = pd.DataFrame(shap.\
                               sample(X_test_nlp_fit, 50).todense())
shap_lgb_explainer =\  
    shap.KernelExplainer(predict_fn,\
                         shap.kmeans(X_train_nlp_fit.todense(), 10))
shap_lgb_values_test =\ 
   shap_lgb_explainer.shap_values(X_test_nlp_samp_df,\
                                  l1_reg="num_features(20)")
shap.summary_plot(shap_lgb_values_test, X_test_nlp_samp_df,\
      plot_type="dot", feature_names=vectorizer.get_feature_names())
```

通过使用 SHAP 在*图 6.17* 中的总结图，你可以看出在全球范围内`KernelExplainer`这个词，有一个权衡需要考虑。

您可以在这里查看输出:

![Figure 6.17 – SHAP summary plot for the LightGBM NLP model

](img/B16383_06_17.jpg)

图 6.17-light GBM NLP 模型的 SHAP 汇总图

现在我们已经在全球范围内验证了我们的 SHAP 值，我们可以使用它们在力图中进行局部解释。不像石灰，我们不能使用任意的数据。对于 SHAP，我们仅限于那些我们之前已经生成 SHAP 值的数据点。例如，让我们从测试数据集样本中取出第 18 个观察值，如下所示:

```
print(shap.sample(X_test_nlp, 50).to_list()[18])
```

前面的代码输出这个短语:

```
woody earthy medicinal
```

重要的是要注意哪些单词出现在第 18 次观察中，因为`X_test_nlp_samp_df`数据帧包含矢量化表示。此数据帧中的第 18 个观察值的行用于生成力图，以及此观察值的 SHAP 值和类的预期值，如以下代码片段所示:

```
shap.force_plot(shap_lgb_explainer.expected_value,\ 
                shap_lgb_values_test[18,:],\     
                X_test_nlp_samp_df.iloc[18,:],\                 
                feature_names=vectorizer.get_feature_names())
```

*图 6.18* 为**木质**土质 药用的原力图。如你所知，**泥土**和**木质**在预测中对*强烈推荐*的权重很大。**药用**这个词没有出现在原力剧情中取而代之的是缺乏**奶油**和**可可**作为负面因素。正如您所想象的那样，*药用*不是一个经常用来描述巧克力棒的词，所以在包含它的采样数据集中只有一个观察结果。因此，它对可能的联盟的平均边际贡献将大大减少。

您可以在这里查看输出:

![Figure 6.18 – SHAP force plot for the 18th observation of the sampled test dataset

](img/B16383_06_18.jpg)

图 6.18–采样测试数据集第 18 次观察的 SHAP 力图

让我们试试另一个，如下:

```
print(shap.sample(X_test_nlp, 50).to_list()[9])
```

第九个观察是下面的短语:

```
intense spicy floral
```

除了用`9`替换`18`之外，为该观察生成`force_plot`与之前相同。如果您运行这段代码，您将生成如图 6.19*所示的输出:*

![Figure 6.19 – SHAP force plot for the 9th observation of the sampled test dataset

](img/B16383_06_19.jpg)

图 6.19–采样测试数据集第 9 次观察的 SHAP 力图

正如你在*图 6.19* 中所能体会到的，短语中的所有词语都在原力剧情中出现:**花香**和**辣**向*推强烈推荐*，以及**强烈**向*不强烈推荐*。那么，现在你知道了如何用 SHAP 进行表格和 NLP 解释，它与 LIME 相比如何？

# 比较 SHAP 和石灰

正如你现在已经注意到的，SHAP 和莱姆都有局限性，但它们也有长处。SHAP 是基于博弈论和近似沙普利值，所以它的 SHAP 值是有意义的。这些有很好的属性，如可加性、效率和可替代性，使其一致，但违反了虚拟属性。它总是累加起来，并且不需要调整参数来完成。然而，它更适合全球解释，而且它最不依赖模型的解释器之一`KernelExplainer`非常慢。`KernelExplainer`还通过使用随机值来处理缺失值，这可能会对不太可能的观察值施加过多的权重。

LIME 速度很快，与模型无关，并且适用于所有类型的数据。然而，它不是基于严格和一致的原则，而是有一种邻居都相似的直觉。正因为如此，它可能需要复杂的参数调整来最佳地定义邻域大小，即使这样，它也只适合于局部解释。

# 任务完成

任务是理解为什么你客户的一个酒吧是*优秀的*，而另一个是*令人失望的*。您的方法采用了机器学习模型的解释来得出以下结论:

*   根据表格模型中的 SHAP，杰出的巧克力棒之所以被评为优秀，是因为它的浆果味道和 70%的可可含量。另一方面，令人失望的巧克力棒的负面评价主要是因为它的泥土气息和咖啡豆的原产国(`Other`)。审查日期的作用较小，但似乎在那个时期(2013-15 年)审查的巧克力棒处于优势。
*   酸橙证实了`cocoa_percent<=70`是一种令人向往的特征，除了**浆果**、**奶油**、**可可**和**富含**都是令人喜欢的口味，而**甜**、**酸**和**糖蜜**则是令人讨厌的。
*   使用表格模型的两种方法之间的共同点是，尽管有许多与味道无关的属性，味道特征是其中最突出的。因此，只有通过 NLP 模型来解释用于描述每个巧克力棒的单词才是合适的。
*   *突出的*条用短语`LIMETextExplainer`表示，**焦糖**为正，**油状**为负。另外两个词是中性的。另一方面，*令人失望的*棒以**焦木土巧克力**为代表，其中**焦木**和**土巧克力**不利，另外两个有利。
*   表格和 NLP 解释中的口味不一致是由于较少代表的口味的存在，包括**树莓**，它不像**浆果**那样常见。
*   根据 SHAP 对 NLP 模型的全局解释，**奶油**，**浓郁**，**可可**，**水果**，**麻辣**，**坚果**，**浆果**对模型朝向预测*强烈推荐*。反之，**甜**、**酸**、**土**、**哈米**、**桑迪**、**胖子**则相反。

曼哈顿巧克力协会的成员认为巧克力棒的特征和口味不太吸引人，有了这些概念，客户可以改变他们的巧克力棒配方以吸引更广泛的观众——也就是说，如果这个群体代表他们的目标观众的假设是正确的。

可以这么说，很明显，像**土气的**和**焦了的**这样的词并不是与巧克力棒联系在一起的好词，而**焦糖色的**才是。所以，不用机器学习我们也能得出这个结论！但是首先，没有数据支持的结论是一种观点，其次，背景就是一切。此外，不能总是依靠人类来客观地将一个点放在它的上下文中——尤其是考虑到它是成千上万个记录中的一个！

此外，局部模型解释*不仅仅是对一个预测*的解释，因为它与模型如何做出所有预测有关，更重要的是，它如何对相似点做出预测——换句话说，在局部邻域中！在下一章中，我们将通过观察我们可以在当地找到的共性(*锚*)和不一致性(*反事实*)来扩展在当地附近意味着什么。

# 总结

看完这一章，你应该知道如何使用 SHAP 的`KernelExplainer`，以及它的决定和武力情节进行局部解读。您还应该知道如何使用 LIME 的实例解释器对表格和文本数据进行同样的操作。最后，你应该了解 SHAP 的`KernelExplainer`和莱姆的优缺点。在下一章中，我们将学习如何为一个模型的决策创建更多人类可理解的解释，例如“*如果满足 X 条件，那么 Y 就是结果*”。

# 数据集来源

*   布雷林斯基，布雷迪(2020)。曼哈顿巧克力协会。[http://flavorsofcacao.com/mcs_index.html](http://flavorsofcacao.com/mcs_index.html)

# 延伸阅读

*   J. C .普拉特(1999 年)。*支持向量机的概率输出以及与正则化似然方法的比较*。大间距分类器的进展，麻省理工学院出版社。[https://www . cs . Colorado . edu/~ mozer/Teaching/syl labi/6622/papers/Platt 1999 . pdf](https://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf )
*   Lundberg，s .和 Lee，S. (2017 年)。解释模型预测的统一方法。神经信息处理系统进展，30。[https://arxiv.org/abs/1705.07874](https://arxiv.org/abs/1705.07874)(https://github.com/slundberg/shap[SHAP](https://github.com/slundberg/shap)的文件)
*   里贝罗，M. T .，辛格，s .和 Guestrin，C. (2016 年)。“我为什么要相信你？”:解释任何分类器的预测。第 22 届 ACM SIGKDD 知识发现和数据挖掘国际会议论文集。http://arxiv.org/abs/1602.04938
*   柯国光，孟，陈，王，陈，马，叶，陈，刘(2017)。 *LightGBM:一种高效的梯度推进决策树*。神经信息处理系统进展第 30 卷，第 3149-3157 页。[https://papers . nips . cc/paper/6907-light GBM-a-高效梯度推进决策树](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)