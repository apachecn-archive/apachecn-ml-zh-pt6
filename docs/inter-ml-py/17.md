

# 十四、机器学习可解释性的下一步是什么？

在过去的十三章中，我们探索了**机器学习** ( **ML** )可解释性的领域。正如序言中所说，这是一个广泛的研究领域，其中大部分还没有离开实验室，还没有被广泛使用，本书并不打算涵盖所有的内容。相反，目标是充分深入地展示各种可解释性工具，作为初学者的起点，甚至补充更高级读者的知识。本章将总结我们在 ML 可解释性方法的生态系统中所学到的东西，然后推测接下来会发生什么！

这些是我们将在本章中涉及的主要话题:

*   理解 ML 可解释性的现状
*   对 ML 可解释性未来的思考

# 了解 ML 可解释性的现状

首先，我们将提供一些关于本书如何与 ML 可解释性的主要目标相关的背景，以及从业者如何开始应用这些方法来实现那些广泛的目标。然后，我们将讨论当前的研究增长领域。

## 将所有东西绑在一起！

如 [*第一章*](B16383_01_ePub_RK.xhtml#_idTextAnchor015) 、*所讨论的解释、可解释性和可说明性；为什么这些都很重要？*在谈论 ML 可解释性时，有三个主题:**公平、问责和透明** ( **FAT** )，其中每一个都呈现了一系列的关注点(参见*图 14.1* )。我想我们都同意这些都是模型的理想属性！事实上，这些问题都为改进人工智能系统提供了机会。这些改进从利用模型解释方法来评估模型、确认或质疑假设以及发现问题开始。

你的目标是什么将取决于你在 ML 工作流程中所处的阶段。如果模型已经在生产中，目标可能是用一整套度量标准来评估它，但是如果模型仍然在早期开发中，目标可能是发现度量标准不能发现的更深层次的问题。也许你也只是在使用黑盒模型进行知识发现，就像我们在*第 4 章*、*第 5 章*和*第 6 章*中所做的那样；换句话说，利用模型从数据中学习，而不打算将其投入生产。如果是这种情况，您可能会确认或质疑您对数据以及模型的假设。

在任何情况下，这些目标没有一个是相互排斥的，您可能应该总是寻找问题和争论假设，即使模型看起来表现良好！

而且不管目的和首要关注点是什么，建议你使用多种解读方法，不仅因为没有一种技术是完美的，还因为所有的问题和目的都是相互关联的。换句话说，没有一致性就没有公正，没有透明度就没有可靠性。事实上，你可以自下而上地阅读*图 14.1* ，就好像它是一个金字塔，因为透明度是基础，其次是第二层的问责制，最终，公平是顶部的樱桃。因此，即使目标是评估模型的公平性，也应该对模型的健壮性进行压力测试。应该理解所有特征的重要性和相互作用。否则，如果预测不可靠、不透明，那也没用:

![Figure 14.1 – ML interpretation methods

](img/B16383_14_01.jpg)

图 14.1–ML 解释方法

*图 14.1* 中涵盖了许多解释方法，但这些并不是所有可用的解释方法。它们代表了最流行的方法，背后有维护良好的开源库。在本书中，我们已经触及了其中的大部分，尽管其中一些只是简略地提及。那些没有被讨论的用*斜体*表示，那些被讨论的旁边有相关的章节号。人们一直关注**黑盒监督学习模型**的**模型不可知**方法。然而，在这个领域之外，还有许多其他的解释方法，比如那些在强化学习、生成模型中发现的方法，或者许多严格用于线性回归的统计方法。即使在监督学习黑盒模型领域，也有数百种特定于应用的模型解释方法，用于从化学图形 CNN 到变压器网络的应用。

也就是说，本书中讨论的许多方法都可以针对各种各样的应用进行定制。集成梯度可用于解释音频分类器、水文预报模型和 NLP 情感分类器。敏感性分析可用于金融建模和传染病风险模型。因果推理方法可以用来改善用户体验和药物试验。

*改进*是这里的关键词，因为解释方法也有反面！

在本书中，另一面被称为*为可解释性而调整，*这意味着创造脂肪问题的解决方案。这些解决方案可以在图 14.2 中看到:

![Figure 14.2 – Toolset to treat FAT issues

](img/B16383_14_02.jpg)

图 14.2–处理肥胖问题的工具集

我观察了五种可解释性解决方案的方法:

*   **减轻偏差**:考虑偏差而采取的任何纠正措施。请注意，这种偏差是指数据中的采样、排除、偏见和测量偏差，以及 ML 工作流程中引入的任何其他偏差。
*   **放置护栏**:任何确保模型不与领域知识相矛盾和没有信心预测的解决方案。
*   **提高可靠性**:提高预测可信度和一致性的任何修正，不包括那些通过降低复杂性来提高可信度和一致性的修正。
*   **降低复杂度**:任何引入稀疏度的手段。作为副作用，这通常通过更好地概括来增强可靠性。
*   **确保隐私**:保护私有数据和模型架构不受第三方侵犯的任何努力。我们没有在本书中讨论这种方法。

这些方法还可以应用于三个领域:

*   **数据(“预处理”)**:修改训练数据
*   **模型(“进行中”)**:通过修改模型、其参数或训练程序
*   **预测(“后处理”)**:通过介入模型的推断

第四个领域会影响其他三个领域。即数据和算法治理。这包括规定某种方法或框架的法规和标准。这是一个缺失的栏目，因为很少有行业和司法管辖区有法律规定应该采用什么方法和途径来遵守 FAT。例如，治理可以强加一个标准来解释算法决策、数据来源或健壮性认证阈值。我们将在下一节进一步讨论这一点。

你可以在*图 14.2* 中看出，许多方法会重复用于脂肪。**特征选择和工程**、**单调约束**和**正则化**对这三者都有好处，但并不总是被同一种方法利用。**数据扩充**还可以增强公平性和问责制的可靠性。与*图 14.1* 一样，书中没有涉及斜体的项目，其中两个主题非常突出:**对抗性鲁棒性**和**隐私保护**是引人入胜的主题，值得单独出书。

## 当前趋势

人工智能采用的最大阻碍之一是缺乏可解释性，这是 50-90%的人工智能项目从未启动的部分原因，另一个原因是由于不遵守 FAT 而发生的道德越轨。在这方面，**可解释机器学习** ( **iML** )有能力在整体上领先 ML，因为它可以用*图 14.1* 和*图 14.2* 中的相应方法帮助实现这两个目标。

令人欣慰的是，我们正在见证对 iML 的兴趣和产量的增加，主要是在**可解释的人工智能**(**XAI**)——见*图 14.3* 。在科学界，iML 仍然是最流行的术语，但 XAI 在公共场合占据主导地位:

XAI 与 iML——使用哪一个？

我的观点是:尽管他们被理解为同义词，而且 iML 更多地被认为是一个学术术语，但 ML 从业者，甚至是那些在行业中的从业者，都应该对使用 XAI 这个术语保持警惕。话语可以有巨大的暗示力。*可解释的*假定完全理解，但是*可解释的*留下了犯错的空间，当谈论模型时总是应该有，而且是非常复杂的黑箱模型。此外，人工智能抓住了公众的想象力，被视为灵丹妙药，或者被诋毁为危险的。不管怎样，随着术语*可解释*的出现，它让那些认为它是万灵药的人更加狂妄自大，或许也平息了那些认为它很危险的人的一些担忧。XAI 作为一个营销术语可能是有目的的。然而，对于那些建立模型的人来说，单词*可解释*的暗示力量会让我们对自己的解释过于自信。话虽如此，这只是一种看法。

![Figure 14.3 – Publication and search trends for iML and XAI

](img/B16383_14_03.jpg)

图 14.3–iML 和 XAI 的出版和搜索趋势

这意味着，就像 ML 开始被标准化、规范、整合并整合到一大堆其他学科中一样，口译也将很快获得一席之地。

ML 正在取代所有行业的软件。随着越来越多的自动化，越来越多的模型被部署到云中。而且会随着**物的人工智能** ( **AIoT** )越来越差。部署传统上不属于 ML 从业者的领域。这就是为什么 ML 越来越依赖**机器学习操作** ( **MLOps** )。自动化的步伐意味着需要更多的工具来构建、测试、部署和监控这些模型。同时，需要工具、方法和度量标准的标准化。这正在缓慢但肯定地发生。从 2017 年开始，我们有了**开放神经网络交换** ( **ONNX** )，这是一个开放的互操作标准。在撰写本文的时候，**国际标准化组织** ( **ISO** )有二十多个人工智能标准正在编写中(其中一个已经发布)，其中几个涉及到可解释性。自然地，由于 ML 模型类、方法、库、服务提供者和实践的合并，一些东西会因为通用而标准化。随着时间的推移，每个领域中的一个或几个将成为胜利者。最后，鉴于 ML 在算法决策中的巨大作用，它们受到监管只是时间问题。只有一些金融市场监管交易算法，比如美国的**证券交易委员会** ( **SEC** )和英国的**金融行为监管局** ( **FCA** )。除此之外，只有数据隐私和出处法规被广泛执行，例如美国的 HIPAA 和巴西的 LGPD。欧盟的 GDPR 在算法决策的“解释权”上更进一步，但是预期的范围和方法仍然不清楚。

ML 的可解释性发展很快，但是落后于 ML。一些解释工具已经集成到云生态系统中，从 SageMaker 到 DataRobot。它们尚未完全自动化、标准化、整合和监管，但毫无疑问这将会发生。

# 推测 ML 可解释性的未来

我已经习惯于听到这个时期的比喻是“人工智能的狂野西部”，或者更糟，是“人工智能淘金热”！它使人联想到未开发和未驯服的领域被急切地征服，或者更糟，被文明化的景象。然而，在 19 世纪，美国西部地区与地球上的其他地区没有太大的不同，并且已经有几千年的美洲土著人居住，所以这个比喻不太适用。我们用人工授精所能达到的准确和自信的预测会吓到我们的祖先，对我们人类来说这不是一个“自然”的立场。这更像是飞行，而不是探索未知的土地。

文章*迈向机器学习的喷气时代*(链接在本章末尾的*延伸阅读*部分)提出了一个更恰当的比喻，即人工智能就像航空的黎明。这是新的和令人兴奋的，人们仍然惊叹于我们从下面能做什么(见*图 14.4* )！

然而，它还必须实现其潜力。在谷仓时代的几十年后，航空成熟进入了安全、可靠和高效的喷气式飞机时代，即商用航空时代。就航空业而言,的承诺是，它可以在不到一天的时间内可靠地将货物和人员运送到世界的另一端。在人工智能的例子中，承诺是它可以做出公平、负责和透明的决定——也许不是对任何决定，但至少是那些它被设计来做出的决定，除非它是人工通用智能的例子:

![Figure 14.4 – Barnstorming during the 1920s (United States Library of Congress's Prints and Photographs Division)

](img/B16383_14_04.jpg)

图 14.4-20 世纪 20 年代的游说活动(美国国会图书馆印刷品和照片部)

那我们怎么去呢？以下是我预计在追求 ML 的喷气式飞机时代的过程中会出现的一些想法。

## ML 的新愿景

由于我们打算让人工智能走得比以前更远，未来的人工智能从业者必须更加意识到天空的危险。说到天空，我指的是预测和规范分析的新领域。风险数不胜数，涉及各种偏见和假设、已知和潜在数据的问题，以及我们模型的数学属性和局限性。很容易被以为自己是软件的 ML 模型欺骗。尽管如此，在这个类比中，软件在本质上是完全确定的——它牢牢地固定在地面上，而不是在空中盘旋！

为了使民航变得安全，它需要一种新的思维方式——一种新的文化。二战中的战斗机飞行员尽管能力出众，却不得不接受再训练，以便在民用航空领域工作。这不是同一个任务，因为当你知道你在船上运载乘客，风险很高，一切都变了。伦理人工智能，推而广之，iML，最终需要这种意识，即模型直接或间接将乘客“带上飞机”。而且这些模型并不像它们看起来的那么强大。一个坚固的模型必须能够可靠地经受住几乎任何条件的反复考验，就像今天的飞机一样。为此，我们需要使用更多的工具，这些工具以解释方法的形式出现。

## 多学科方法

符合 FAT 原则的模型需要与许多学科进行更紧密的整合。这意味着人工智能伦理学家、律师、社会学家、心理学家、以人为中心的设计师和无数其他职业的更重要的参与。他们将与人工智能技术专家和软件工程师一起，帮助将最佳实践编入标准和法规。

## 充分的标准化

新的标准不仅是代码、度量标准和方法论需要的，语言也需要。数据背后的语言大多来源于统计学、数学、计算机科学和计量经济学，这导致了很多混乱。

## 执行法规

很可能要求所有生产型号满足以下规格:

*   被证明是稳健和公平的
*   能够用跟踪命令解释一个预测背后的推理，并且在某些情况下，需要用预测来传递推理
*   可以放弃他们没有信心的预测
*   所有预测的置信水平
*   拥有包含训练数据来源(即使匿名)和作者身份的元数据，并且在需要时，将法规遵从性证书和元数据绑定到公共分类账(可能是区块链)
*   像网站一样拥有安全证书，以确保一定程度的信任
*   过期，过期后停止工作，直到用新数据对它们进行重新训练
*   当模型诊断失败时自动离线，只有通过时才重新在线
*   拥有**连续训练/连续训练** ( **CT/CI** )管道，帮助重新训练模型并定期执行模型诊断，以避免任何模型停机
*   当它们灾难性地失败并造成公共损失时，由认证的人工智能审计员进行诊断

新法规可能会创造新的职业，如人工智能审计师和模型诊断工程师。但他们也将支持 MLOps 工程师和 ML 自动化工具。

## 具有内置解释的无缝机器学习自动化

在未来，我们不会编程一个 ML 管道；这将主要是一个拖放的事情，提供各种指标的仪表板。它将演变成大部分自动化。自动化不应该令人惊讶，因为一些现有的库执行自动化的特征选择模型训练。一些增强可解释性的过程可以自动完成，但是大多数需要人的判断。然而，解释应该贯穿整个过程，就像大多数自己飞行的飞机都有提醒飞行员注意问题的仪器；价值在于告知 ML 从业者每一步的潜在问题和改进。它找到单调约束的推荐特征了吗？它是否发现了一些可能需要调整的失衡？它是否在数据中发现了可能需要纠正的异常？向从业者展示做出明智决定需要看到什么，并让他们做出决定。

## 与 MLOps 工程师更紧密的集成

只需点击一个按钮，即可训练、验证和部署经认证的强大模型，这需要的不仅仅是云基础设施，还需要工具、配置和经过 MLOps 训练的人员的协调，以定期监控它们并执行维护。

就像航空花了几十年才成为最安全的交通方式一样，人工智能也需要几十年才能成为最安全的决策方式。将我们带到那里需要一个地球村，但这将是一个令人兴奋的旅程！记住，预测未来的最好方法是创造未来。

# 延伸阅读

*   奥尼尔，C. (2017)。摧毁数学的武器。企鹅图书。
*   Talwalkar，A. (2018 年 4 月 25 日)。走向机器学习的喷射时代。奥赖利。[https://www . oreilly . com/content/迈向机器学习时代/](https://www.oreilly.com/content/toward-the-jet-age-of-machine-learning/)