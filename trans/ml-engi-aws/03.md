<title>Chapter 2: Deep Learning AMIs</title>

# 2

# 深度学习 AMIs

在 [*第一章*](B18638_01.xhtml#_idTextAnchor017) 、*AWS 上的 ML 工程介绍*的*必要前提*部分，我们大概花了一个小时左右的时间来设置我们的 Cloud9 环境。在我们能够处理实际的**机器学习** ( **ML** )需求之前，我们必须花一些时间安装几个包，以及一些依赖项。最重要的是，我们必须确保我们对某些包使用了正确的版本，以避免遇到各种各样的问题。如果您认为这很容易出错并且很乏味，那么想象一下，您被分配了为一组数据科学家准备 20 ML 环境的任务！让我重复一遍……*二十*！重复做同样的事情会花费我们大约 15 到 20 个小时。在使用你准备的 ML 环境一周后，数据科学家然后要求你也在这些环境中安装深度学习框架 **TensorFlow** 、 **PyTorch** 和 **MXNet** ，因为他们将使用这些 ML 框架测试不同的深度学习模型。此时，你可能已经在问自己，“*有更好的方法吗？*”。好消息是，有多种方法可以更有效地处理这些类型的需求。一个可能的解决方案是利用**亚马逊机器映像** ( **AMIs** )，特别是 AWS **深度学习 AMIs** ( **DLAMIs** )到显著加快准备 ML 环境的过程。当启动新实例时，这些 ami 将充当包含相关软件和环境配置的预配置模板。

在 **DLAMIs** 存在之前，ML 工程师必须花费数小时在 EC2 实例中安装和配置深度学习框架，然后才能在 AWS 云中运行 ML 工作负载。从零开始手动准备这些 ML 环境的过程是乏味的，并且容易出错。一旦 DLAMIs 可用，数据科学家和 ML 工程师就能够使用他们首选的深度学习框架直接运行他们的 ML 实验。

在本章中，我们将看到使用特定于框架的深度学习 AMI 来设置 GPU 实例是多么方便。然后我们将在这个环境中使用 **TensorFlow** 和 **Keras** 训练一个深度学习模型。一旦训练步骤完成，我们将使用测试数据集评估模型。之后，我们将执行清理步骤并终止 EC2 实例。在本章末尾，我们还将简短讨论 AWS 定价如何适用于 EC2 实例。这将帮助您掌握管理在这些实例中运行 ML 工作负载的总成本所需的知识。

也就是说，我们将在本章中讨论以下主题:

*   深度学习人工智能入门
*   使用深度学习 AMI 启动 EC2 实例
*   下载样本数据集
*   训练一个 ML 模型
*   加载和评估模型
*   清理
*   了解 AWS 定价如何适用于 EC2 实例

本章中的实践解决方案将帮助您将任何现有的 **TensorFlow** 、 **PyTorch** 和 **MXNet** 脚本和模型迁移到 AWS cloud。除了前面提到的成本讨论之外，我们还将讨论一些安全指南和最佳实践，以帮助我们确保我们设置的环境具有良好的初始安全配置。考虑到这些，我们开始吧！

# 技术要求

在我们开始之前，我们必须有一个网络浏览器(最好是 Chrome 或 Firefox)和一个 AWS 帐户来使用本章中的动手解决方案。确保您可以访问您在第 1 章 、*AWS 上的 ML 工程简介*中使用的 AWS 帐户。

Jupyter 笔记本、源代码和其他用于每章的文件都可以在本书的 GitHub 资源库中找到:[https://GitHub . com/packt publishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS)。

# 深度学习人工智能入门

在我们谈论 ami 之前，我们必须对 ami 有一个很好的概念。我们可以把 AMI 想象成一个有机体的“DNA”。使用这个类比，有机体将对应并映射到一个或多个 EC2 实例:

![Figure 2.1 – Launching EC2 instances using Deep Learning AMIs

](img/B18638_02_001.jpg)

图 2.1–使用深度学习 AMIs 启动 EC2 实例

如果我们使用相同的 AMI 启动两个 EC2 实例(类似于图 2.1*所示)，那么在实例启动时，两个实例将具有相同的安装包、框架、工具和操作系统。当然，并非一切都需要相同，因为这些实例可能有不同的实例类型、不同的安全组和其他可配置的属性。*

AMIs 允许工程师在一致的环境中轻松启动 EC2 实例，而不必花费数小时安装不同的包和工具。除了安装步骤之外，这些 EC2 实例需要进行配置和优化，然后才能用于特定的工作负载。DLAMIs 等预建的 AMIs 已经预装了 **TensorFlow** 、 **PyTorch** 、 **MXNet** 等热门深度学习框架。这意味着数据科学家、开发人员和 ML 工程师可以继续进行 ML 实验和部署，而不必担心安装和设置过程。

如果我们必须准备安装了这些深度学习框架的 20 ML 环境，我很确定我们不会花 20 个小时或更多时间来完成。如果我们使用 DLAMIs，大概 2 到 3 个小时就足够完成工作了。你不相信我？ *在下一节中，我们将这么做！*当然，我们将只准备一个 ML 环境，而不是 20 个。在学习本章中的动手解决方案时，您会注意到在设置和配置运行 ML 实验所需的先决条件时，速度有了显著的提高。

注意

值得注意的是，我们可以选择在现有的非盟特派团基础上构建，并准备我们自己的定制非盟特派团。然后，我们可以在启动新的 EC2 实例时使用这些定制的 ami。

# 使用深度学习 AMI 启动 EC2 实例

从 DLAMI 启动 EC2 实例非常简单。一旦我们有了使用哪个 DLAMI 的想法，剩下的步骤将集中在配置和启动 EC2 实例上。这里很酷的一点是，我们并不局限于从现有映像启动单个实例。在配置阶段，在从 AMI 启动一个实例之前，重要的是要注意我们可以为要启动的实例数量指定所需的值(例如，`20`)。这意味着我们不是启动一个实例，而是同时启动 20 个实例。

![Figure 2.2 – Steps to launch an EC2 instance using a DLAMI

](img/B18638_02_002.jpg)

图 2.2–使用 DLAMI 启动 EC2 实例的步骤

我们将把这一部分分成四个部分。如上图所示，我们将首先在`p3.2xlarge`中定位特定于框架的深度学习 AMI，作为实例类型。然后，我们将配置实例要使用的安全设置，包括网络安全设置。最后，我们将启动该实例，并使用 **EC2 Instance Connect** 从浏览器连接到它。

## 定位特定于框架的 DLAMI

在寻找 AMI 时，我们首先应该查看的是 **AWS AMI 目录**。在 AMI 目录中，我们应该可以找到各种类型的 AMI。这些 DLAMIs 可以分为多框架 DLAMIs 或特定于框架的 DLAMIs。*有什么区别？*多框架 AMI 包括单个 AMI 中的多个框架，如 **TensorFlow** 、 **PyTorch** 或 **MXNet** 。这使得开发人员、ML 工程师和数据科学家可以轻松地试验和探索几种框架。另一方面，特定于框架的 DLAMIs 更适合生产环境，并且只支持单个框架。在本章中，我们将使用特定于框架(TensorFlow)的深度学习 AMI。

在下一组步骤中，我们将导航到 AMI 目录，并使用特定于框架的(TensorFlow)深度学习 AMI 来启动一个实例:

1.  导航到 AWS 管理控制台，然后在搜索栏中键入`ec2`。从结果列表中选择 **EC2** :

![Figure 2.3 – Navigating to the EC2 console

](img/B18638_02_003.jpg)

图 2.3–导航至 EC2 控制台

我们应该会看到匹配结果的列表，如 **EC2** 、 **EC2 图像生成器**和 **AWS 计算优化器**，类似于图 2.2 中所示的内容。从这个列表中，我们将选择第一个，这将把我们重定向到 EC2 控制台。

1.  在侧边栏中，定位并点击**图片**下的 **AMI 目录**，导航至 **EC2** > **AMI 目录**页面。
2.  接下来，在 **AMI 目录**页面的搜索栏中输入`deep learning ami`。确保您按下**输入**来搜索与搜索查询相关的相关 ami:

![Figure 2.4 – Searching for the framework-specific Deep Learning AMI

](img/B18638_02_004.jpg)

图 2.4–搜索特定于框架的深度学习 AMI

如前面的截图所示，我们应该在 **Quickstart AMIs** 下有几个匹配结果。在 **AWS Marketplace AMIs** 和 **Community AMIs** 下应该也有匹配的结果。快速启动 AMI 包括常用于关键工作负载的 AMI，如**亚马逊 Linux 2** AMI、 **Ubuntu Server 20.04 LTS** AMI、**深度学习 AMI** (亚马逊 Linux 2) AMI 等等。AWS Marketplace AMIs 包括几个由 AWS 创建的 ami，以及由可信的第三方来源创建的 ami。这些应该包括 AMI，如 **OpenVPN 访问服务器** AMI、 **Kali Linux** AMI 和 **Splunk Enterprise** AMI。所有公开可用的 ami 都可以在**社区 ami**下找到。

1.  向下滚动**quick start AMI**列表，找到特定框架的深度学习 AMI，如下截图所示:

![Figure 2.5 – Locating the TensorFlow DLAMI

](img/B18638_02_005.jpg)

图 2.5–定位 TensorFlow DLAMI

这里，我们为 **Amazon Linux 2** 选择特定于框架的(TensorFlow)深度学习 AMI，因为我们将在本章稍后使用 TensorFlow 训练一个 ML 模型。通过阅读 AMI 的名称和描述来验证选择。然后，点击**选择**按钮。

1.  在上一步中单击了**选择**按钮后，向上滚动到页面的顶部，然后单击**用 AMI 启动实例**按钮，如下面的截图所示:

![Figure 2.6 – Launch Instance with AMI

](img/B18638_02_006.jpg)

图 2.6–使用 AMI 启动实例

正如我们所看到的，带有 AMI 按钮的**启动实例就在带有 AMI** 按钮的**创建模板旁边。**

重要说明

使用 **AWS 深度学习 AMIs** 没有额外费用。这意味着我们只需要考虑与创建的基础设施资源相关的成本。但是，其他 ami 的使用可能不是免费的。例如，其他公司创建的 ami(来自**AWS market place ami**下的列表)可能会按使用小时收取额外费用。也就是说，检查使用这些 ami 启动的基础设施资源上的任何额外费用是很重要的。

点击**用 AMI** 启动实例按钮应该会重定向到**启动实例**页面，如下面的截图所示:

![Figure 2.7 – The Launch an instance page

](img/B18638_02_007.jpg)

图 2.7–启动实例页面

由于 AWS 会定期更新在控制台中启动和管理资源的体验，因此在执行下一组步骤时，您可能会看到一些差异。然而，无论您在本部分工作时控制台看起来是什么样的，期望的最终配置将是相同的。

1.  在**名称**字段的`MLE-CH02-DLAMI`下。

在设置了 **Name** 字段的值之后，下一步是为我们的 EC2 实例选择所需的实例类型。在我们继续选择所需的实例类型之前，我们必须快速讨论一下哪些实例可用，哪些类型的实例适合大规模 ML 工作负载。

## 选择实例类型

在进行深度学习实验时，数据科学家和 ML 工程师一般更喜欢 GPU 实例而不是 CPU 实例。**图形处理单元**(**GPU**)有助于显著加快深度学习实验的速度，因为 GPU 可以用于同时处理多个并行计算。由于 GPU 实例通常比 CPU 实例更昂贵，数据科学家和 ML 工程师在处理 ML 需求时使用两种类型的组合。例如，ML 从业者可能会限制 GPU 实例的使用，仅用于训练深度学习模型。这意味着 CPU 实例将用于部署训练模型的推理端点。这在大多数情况下是足够的，一旦考虑到成本，这将被认为是一个非常实际的举措。

![Figure 2.8 – CPU instances versus GPU instances

](img/B18638_02_008.jpg)

图 2.8–CPU 实例与 GPU 实例的对比

也就是说，我们需要确定哪些实例属于 GPU 实例组，哪些实例属于 CPU 实例组。上图展示了一些 GPU 实例的例子，包括`p3.2xlarge`、`dl1.24xlarge`、`g3.4xlarge`、`p2.8xlarge`和`g4ad.8xlarge`。还有其他 GPU 实例类型的例子不在此列表中，但是您应该能够通过检查实例系列来识别它们。例如，我们确信`p3.8xlarge`是一个 GPU 实例类型，因为它与`p3.2xlarge`实例类型属于同一个家族。

既然我们对 CPU 和 GPU 实例有了更好的了解，让我们继续从实例类型的选项列表中定位和选择`p3.2xlarge`:

1.  继续我们在*定位特定于框架的 DLAMI* 部分停止的地方，让我们定位并点击**实例类型**窗格下的**比较实例类型**链接。这会将您重定向到**比较实例类型**页面，如下面的屏幕截图所示:

![Figure 2.9 – Compare instance types

](img/B18638_02_009.jpg)

图 2.9–比较实例类型

在这里，我们可以看到不同的实例类型，以及它们相应的规格和每小时的成本。

1.  单击搜索字段(带有**过滤器实例类型**占位符文本)。这将打开一个下拉选项列表，如下面的屏幕截图所示:

![Figure 2.10 – Using the Filter instance types search field

](img/B18638_02_010.jpg)

图 2.10–使用过滤器实例类型搜索字段

从选项列表中找到并选择**GPU**。这将打开**为 GPU 添加过滤器**窗口。

1.  在旁边的文本字段中的`0`。之后点击**确认**按钮。

注意

我们应用的过滤器应该将结果集限制在 GPU 实例中。我们应该会找到几个加速计算实例家族，比如 *P3* 、 *P2* 、 *G5* 、 *G4dn* 、 *G3* 等等。

1.  接下来，让我们单击**首选项**按钮，如下图所示:

![Figure 2.11 – Opening the Preferences window

](img/B18638_02_011.jpg)

图 2.11–打开首选项窗口

这将打开**偏好设置**窗口。在**属性列**下，确保**GPU**单选按钮处于打开状态，如下图所示:

![Figure 2.12 – Displaying the GPUs attribute column

](img/B18638_02_012.jpg)

图 2.12–显示 GPU 属性列

之后点击**确认**按钮。这将更新表格列表显示，并显示列表中每种实例类型的 GPU 数量，如下所示:

![Figure 2.13 – GPUs of each instance type

](img/B18638_02_013.jpg)

图 2.13–每种实例类型的 GPU

在这里，我们应该看到一种模式，即在同一个实例族中，随着实例类型变得“更大”, GPU 的数量通常会增加。

1.  定位并选择对应于 **p3.2xlarge** 实例类型的行。记下可用 GPU 的数量，以及 **p3.2xlarge** 实例类型的每小时成本(按需 Linux 定价)。
2.  之后点击**选择实例类型**按钮(位于屏幕右下方)。

这将关闭**比较实例类型**窗口，并返回到**启动实例**页面。

## 确保默认安全配置

当启动 EC2 实例时，我们需要管理安全配置，这将影响如何访问实例。这包括配置以下内容:

*   **密钥对**:包含用于安全访问实例的凭证的文件(例如，使用 SSH)
*   **虚拟私有云** ( **VPC** ):一个逻辑上隔离的虚拟网络，规定了如何访问资源以及资源如何相互通信
*   **安全组**:虚拟的防火墙，使用基于配置的协议和端口过滤流量的规则来控制进出 EC2 实例的流量

也就是说，在启动 EC2 实例之前，让我们继续完成剩余的配置参数:

1.  继续我们在*选择实例类型*部分离开的地方，让我们继续创建一个新的密钥对。在**密钥对(登录)**下，定位并点击**创建新的密钥对**。
2.  在`dlami-key`中为`RSA`
3.  `.pem`
4.  点击本地机器上的`.pem`文件。注意，对于本章的动手解决方案，我们不需要这个`.pem`文件，因为我们稍后将使用 **EC2 实例连接**(通过浏览器)来访问实例。

重要说明

不要共享下载的密钥文件，因为这是用来通过 SSH 访问实例的。对于生产环境，也可以考虑在正确配置的 VPC 中隐藏非公共实例。当谈到保护我们的 ML 环境时，有很多要讨论的。我们将在 [*第 9 章*](B18638_09.xhtml#_idTextAnchor187) 、*安全、治理和遵从性策略*中详细讨论安全性。

1.  在`vpc-xxxxxxxx (default)`下
2.  `Enable`
3.  `Create security group`
4.  在**网络设置**的**入站安全组规则**下，指定一组安全组规则，类似于下面截图中的配置:

![Figure 2.14 – Inbound security groups rules

](img/B18638_02_014.jpg)

图 2.14–入站安全组规则

如您所见，我们将使用以下规则配置新的安全组:

*   `SSH`；`TCP`；`22`；`Anywhere`|`0.0.0.0/0`；`SSH`–允许任何“计算机”，比如您的本地机器，通过端口 22 上的**安全外壳** (SSH)协议连接到 EC2 实例
*   `Custom TCP`；`TCP`；`6006`；`Anywhere`|`0.0.0.0/0`；`Tensorboard`–允许任何“计算机”，如您的本地机器，访问 EC2 实例的端口 6006(它可能正在运行一个应用程序，如 **TensorBoard** )
*   `Custom TCP`；`TCP`；`8888`；`Anywhere`|`0.0.0.0/0`；`Jupyter`–允许任何“计算机”如您的本地机器访问 EC2 实例的端口 8888(它可能正在运行一个应用程序，如 **Jupyter Notebook** app)

一旦使用**安全组名称-必填**和**描述-必填**以及相关的一组**入站安全组规则**配置了新的安全组，您就可以继续下一步。

注意

请注意，一旦我们需要为生产使用准备设置，就需要进一步检查和保护该配置。首先，`0.0.0.0/0`)因为这种配置允许任何计算机或服务器通过开放端口访问我们的实例。也就是说，我们只能有限地访问本地机器的 IP 地址。同时，我们拥有的配置应该可以完成任务，因为一旦我们完成本章，我们将立即删除实例。

1.  找到并点击**配置存储**下的**添加新卷**按钮:

![Figure 2.15 – Configuring the storage settings

](img/B18638_02_015.jpg)

图 2.15–配置存储设置

在 **1x** 和**钩**之间的文本字段中指定`35`。类似于我们在前面的截图。

我们还可以配置和调整更多选项(在**高级细节**下)，但我们将保持默认值不变。

## 启动实例并使用 EC2 实例连接进行连接

有不同的方法可以连接到 EC2 实例。在前面，我们将实例配置为可以使用一个密钥文件通过 SSH 访问它(例如，从本地机器的终端)。另一个可能的选择是使用 **EC2 实例连接**通过浏览器访问实例。我们还可以使用**会话管理器**通过 SSH 访问实例。在本节中，我们将使用 EC2 实例连接来访问我们的实例。

继续我们在*确保默认安全配置*部分停止的，让我们继续启动 EC2 实例并从浏览器访问它:

1.  配置存储设置后，找到并单击**摘要**窗格(位于屏幕右侧)下的**启动实例**按钮。请确保在实例启动后的一小时内终止该实例，因为这些类型的实例的每小时速率相对于其他类型的实例要高一些。您可以查看本章的*清理*部分了解更多详情。

注意

确保`1`中指定的值。从技术上讲，通过将这个值设置为`20`，我们可以一次启动 20 个实例。然而，我们不想这样做，因为这将是非常昂贵和浪费。现在，让我们坚持使用`1`，因为这对于处理本章中的深度学习实验来说应该绰绰有余。

1.  您应该看到一个成功通知，以及正在启动的资源的实例 ID，类似于下面的屏幕截图所示:

![Figure 2.16 – Launch success notification

](img/B18638_02_016.jpg)

图 2.16–启动成功通知

单击包含实例 ID ( `i-xxxxxxxxxxxxxxxxx`)的链接，如前面的屏幕截图中突出显示的那样，导航到出现在实例列表中的`MLE-CH02-DLAMI`。

注意

等待一两分钟，然后再进行下一步。如果您在启动实例时遇到一个`InsufficientInstanceCapacity`错误，请随意使用一个不同的`p3`实例。要进一步解决这一问题，您也可以参考[https://AWS . Amazon . com/premium support/knowledge-center/ec2-individual-capacity-errors/](https://aws.amazon.com/premiumsupport/knowledge-center/ec2-insufficient-capacity-errors/)了解更多信息。

1.  通过切换下图中突出显示的复选框来选择实例。之后点击**连接**按钮:

![Figure 2.17 – Connecting to the instance directly

](img/B18638_02_017.jpg)

图 2.17–直接连接到实例

在这里，我们可以看到有一个使用浏览器直接连接到实例的选项。

1.  在`AA.BB.CC.DD`中)到本地机器上的文本编辑器中。请注意，您将获得不同的公共 IP 地址值。我们将在本章稍后访问`root`时使用该 IP 地址值，然后点击**连接**:

![Figure 2.18 – EC2 Instance Connect

](img/B18638_02_018.jpg)

图 2.18–EC2 实例连接

这应该会打开一个新的标签页，允许我们直接从浏览器运行终端命令。如果您收到一条**连接到实例时出现问题**的错误消息，请等待大约 2 到 3 分钟，然后刷新页面或单击**重试**按钮:

![Figure 2.19 – EC2 Instance Connect terminal

](img/B18638_02_019.jpg)

图 2.19–EC2 实例连接终端

我们可以看到，`TensorFlow 2.9.1`和其他实用程序库安装在`/usr/local/bin/python3.9`中。请注意，您可能会得到不同的 TensorFlow 和 Python 版本，这取决于您用来启动实例的 DLAMI 版本的。

那不是很容易吗？此时，我们现在应该能够使用 TensorFlow 执行深度学习实验，而不必在 EC2 实例中安装额外的工具和库。

注意

注意，使用**启动模板**可以进一步加速从 AMI 启动实例的过程，启动模板已经指定了实例配置信息，比如 AMI ID、实例类型、密钥对和安全组。我们不会在本书中介绍启动模板的用法，因此可以随时查看以下链接了解更多详细信息:[https://docs . AWS . Amazon . com/AWS C2/latest/user guide/ec2-Launch-Templates . XHTML](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-templates.xhtml)。

# 下载样本数据集

在本章的后续部分，我们将使用一个非常简单的合成数据集，其中只包含两列—*x*和 *y* 。这里， *x* 可以表示一个物体在 *X* 轴上的相对位置，而 *y* 可以表示同一物体在 *Y* 轴上的位置。下面的屏幕截图显示了数据的一个示例:

![Figure 2.20 – Sample dataset

](img/B18638_02_020.jpg)

图 2.20–样本数据集

ML 是关于寻找模式的。有了这个数据集，我们将构建一个模型，在本章后面给出 *x* 的值的情况下，尝试预测 *y* 的值。一旦我们能够用这样一个简单的例子建立模型，处理包含两列以上的更真实的数据集就容易多了，类似于我们在第 1 章 、*AWS 上的 ML 工程简介*中处理的 [*。*](B18638_01.xhtml#_idTextAnchor017)

注意

在这本书里，我们不会仅仅局限于表格数据和简单的数据集。在 [*第六章*](B18638_06.xhtml#_idTextAnchor132) ， *SageMaker 训练和调试解决方案*中，例如，我们将使用带标签的图像数据，并使用 **Amazon SageMaker** 的几个功能和特性构建两个图像分类模型。在 [*第 7 章*](B18638_07.xhtml#_idTextAnchor151) ， *SageMaker 部署解决方案*中，我们将使用文本数据并使用各种部署选项部署一个**自然语言处理** ( **NLP** )模型。

也就是说，让我们从*启动实例并使用 EC2 实例连接*部分中停止的地方继续，并继续下载我们将在本章中用于训练深度学习模型的数据集:

1.  在`data`目录中:

    ```
    mkdir -p data
    ```

2.  使用`wget`命令:

    ```
    wget https://bit.ly/3h1KBx2 -O data/training_data.csv
    ```

    ```
    wget https://bit.ly/3gXYM6v -O data/validation_data.csv
    ```

    ```
    wget https://bit.ly/35aKWem -O data/test_data.csv
    ```

    下载训练、验证和测试数据集
3.  可选地，我们可以使用`yum`包管理工具:

    ```
    yum install tree
    ```

    安装`tree`实用程序

如果这是您第一次遇到`tree`命令，它用于以树状结构列出目录和文件。

注意

也可以从 EC2 实例创建一个定制 AMI。如果我们要从我们现在使用的 EC2 实例创建一个定制 AMI，我们将能够从新的定制 AMI 启动新的 EC2 实例，并且已经安装了以下内容:(1)来自 DLAMI 的已安装框架、库和工具，以及(2)我们在定制 AMI 创建之前安装的`tree`实用程序。

1.  使用`tree`命令查看当前设置的目录和当前目录下的文件:

    ```
    tree
    ```

这将产生一个树状结构，类似于下面的屏幕截图所示:

![Figure 2.21 – Results after using the tree command

](img/B18638_02_021.jpg)

图 2.21–使用 tree 命令后的结果

在这里，我们可以看到我们已经使用前面的`wget`命令成功下载了 CSV 文件。

1.  现在，让我们验证并检查我们下载的一个 CSV 文件的内容。使用`head`命令查看`training_data.csv`文件的前几行:

    ```
    head data/training_data.csv
    ```

这将为我们提供多行 *(x，y)对*，类似于下面的截图所示:

![Figure 2.22 – The first few rows of the training_data.csv file

](img/B18638_02_022.jpg)

图 2.22–training _ data . CSV 文件的前几行

您也可以使用`head`命令检查`validation_data.csv`和`test_data.csv`的内容。

注意

值得注意的是，本例中的第一列是 *y* 列。一些 ML 实践者遵循一个约定，其中第一列被用作目标列(包含我们想要使用数据集的其他列预测的值的列)。当使用某些算法，如 **XGBoost** 和**线性学习器**的 **SageMaker** 内置算法时，第一列被假定为目标列。如果您使用自己的定制脚本来加载数据，那么您可以遵循您喜欢的任何约定，因为您可以自由选择如何从文件中加载和解释数据。

你可能已经注意到，到目前为止，在这本书里，我们一直在使用干净的和预处理过的数据集。在真实的 ML 项目中，您将处理带有各种问题的原始数据，比如缺失值和重复行。在 [*第 5 章*](B18638_05.xhtml#_idTextAnchor105) 、*实用数据处理和分析*中，我们将使用“更脏”版本的*预订*数据集，并使用各种 AWS 服务和功能，如 **AWS Glue DataBrew** 和**Amazon SageMaker Data Wrangler**来分析、清理和处理数据。然而，在本章中，我们将使用“干净”的数据集，因为我们需要专注于使用 **TensorFlow** 和 **Keras** 训练深度学习模型。也就是说，让我们继续生成一个接受 *x* 作为输入并返回预测的 *y* 值作为输出的模型。

# 训练一个 ML 模型

在 [*第 1 章*](B18638_01.xhtml#_idTextAnchor017) 、*AWS 上的 ML 工程简介*中，我们训练了一个二元分类器模型，旨在使用可用信息预测酒店预订是否会被取消。在本章中，我们将使用从*下载样本数据集*得到的(有意简化的)数据集，并训练一个回归模型，该模型将在给定 *x* 的值的情况下预测 *y* (连续变量)的值。我们将不再依赖现成的 AutoML 工具和服务，而是使用自定义脚本:

![Figure 2.23 – Model life cycle

](img/B18638_02_023.jpg)

图 2.23–模型生命周期

编写自定义培训脚本时，我们通常遵循类似于上图所示的顺序。我们从定义和编译模型开始。之后，我们加载数据并使用它来训练和评估模型。最后，我们将模型序列化并保存到一个文件中。

注意

模型保存后会发生什么？模型文件可以在推理端点中使用和加载，推理端点是一个 web 服务器，在给定一组输入值(例如，input *x* 值)的情况下，它使用经过训练的 ML 模型来执行预测(例如，预测的 *y* 值)。在本章的*加载和评估模型*部分，我们将使用`tf.keras.models`中的`load_model()`函数将生成的模型文件加载到 Jupyter 笔记本中。然后，我们将使用`predict()`方法，使用提供的测试数据集执行样本预测。

在本章中，我们将使用一个脚本文件，该文件使用 **TensorFlow** 和 **Keras** 来构建一个**神经网络**模型——一组相互连接的节点，可以学习输入和输出之间的复杂模式。由于我们将在本书中使用神经网络和深度学习概念，我们必须对以下概念有一个基本的了解:

*   **神经元**:这些是神经网络的构建模块，接受并处理输入值以产生输出值。*如何计算输出值？*通过神经元的每个输入值乘以相关的**权重**值，然后加上一个数值(也称为**偏差**)。称为**激活函数**的非线性函数随后被应用于结果值，这将产生输出。这个非线性函数帮助神经网络学习输入值和输出值之间的复杂模式。我们可以在下图中看到神经元的表示:

![Figure 2.24 – A representation of a neuron

](img/B18638_02_024.jpg)

图 2.24–一个神经元的代表

在这里，我们可以看到，我们可以使用一个公式来计算 *y* 的值，该公式包括 *x* 输入值、相应的权重值、偏差和激活函数。也就是说，我们可以将神经元视为“数学函数”,将神经网络视为“一堆数学函数”,试图通过不断更新权重和偏差值来映射输入值和输出值。

*   **层**:层是由位于神经网络中特定位置或深度的一组神经元组成的；

![Figure 2.25 – An input layer, output layer, and multiple hidden layers

](img/B18638_02_025.jpg)

图 2.25–一个输入层、一个输出层和多个隐藏层

在这里，我们可以看到神经网络的不同层次。**输入层**是接收输入值的层，而**输出层**是产生输出值的层。在输入层和输出层之间是被称为**隐藏层**的处理层，其处理和转换来自输入层的数据到输出层。(超过一两个隐层的神经网络一般称为**深度神经网络**。)

*   **正向传播**:这个是指信息从输入层到隐藏层，再到输出层的正向流动，产生输出值。
*   **成本函数**:该函数用于计算预测计算值与实际值的偏差。假设训练神经网络的目标是生成尽可能接近实际值的预测值，我们应该使用优化算法(如**梯度下降**)来寻找成本函数(代表模型的误差)的最小值。
*   **反向传播**:这是根据预测值和实际值之间的差异调整神经网络中的权重的过程(包括计算**梯度**或对每层的权重进行小的更新):

![Figure 2.26 – Backpropagation

](img/B18638_02_026.jpg)

图 2.26–反向传播

这里，我们可以看到反向传播涉及将计算的误差从输出层反向传播到输入层(并相应地更新权重)。

*   **学习率**:这影响在训练神经网络时用于调整与损失梯度相关的网络中的权重的数量。
*   **历元**:这是一个训练迭代，涉及使用整个训练数据集的一个正向和一个反向传播。在每次训练迭代之后，更新神经网络的权重，并且期望神经网络在将该组输入值映射到该组输出值方面表现得更好。

注意

我们不会在本书中深入探讨深度学习和神经网络的细节。如果你有兴趣了解更多关于这些话题的知识，网上有几本书可供选择:[https://www.amazon.com/Neural-Network/s?k=Neural+Network](https://www.amazon.com/Neural-Network/s?k=Neural+Network)。

现在我们对什么是神经网络有了更好的了解，我们可以继续训练一个神经网络模型。在接下来的一组步骤中，我们将使用自定义脚本，通过上一节下载的数据来训练深度学习模型:

1.  首先，让我们使用`mkdir`命令:

    ```
    mkdir -p logs
    ```

    创建一个名为`logs`的目录
2.  接下来，使用`wget`命令下载`train.py`文件:

    ```
    wget https://bit.ly/33D0iYC -O train.py
    ```

3.  使用`tree`命令快速检查文件和目录结构:

    ```
    tree
    ```

这将产生一个树状结构，类似于下面的屏幕截图所示:

![Figure 2.27 – Results after using the tree command

](img/B18638_02_027.jpg)

图 2.27–使用 tree 命令后的结果

请注意，数据和日志目录与`train.py`文件处于同一级别。

1.  在运行`train.py`文件之前，执行以下命令:

    ```
    for a in /sys/bus/pci/devices/*; do echo 0 | sudo tee -a $a/numa_node; done
    ```

这将有助于我们在本章稍后列出 GPU 设备时，避免出现**从 SysFS 成功读取 NUMA 节点具有负值(-1)** 警告消息。

1.  在运行下载的`train.py`脚本之前，让我们在单独的浏览器选项卡中打开[https://github . com/packt publishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter 02/train . py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter02/train.py)来检查其内容:

![Figure 2.28 – The train.py file

](img/B18638_02_028.jpg)

图 2.28–train . py 文件

在前面的截图中，我们可以看到我们的`train.py`脚本执行了以下操作:

*   (`prepare_model()`功能
*   (`load_data()`功能
*   ( **3** )准备**张量板**回调对象
*   (`fit()`方法并传递`callback`参数值
*   (`save()`法

注意

值得注意的是，我们的`train.py`脚本中的`prepare_model()`函数执行*定义模型*和*编译模型*步骤。该函数中定义的神经网络是一个具有五层的样本序列模型。要了解更多信息，请访问[https://github . com/packt publishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter 02/train . py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter02/train.py#L24)查看`prepare_model()`函数的实现。

1.  让我们通过在 EC2 实例连接终端中运行以下命令来开始训练步骤:

    ```
    python3.9 train.py
    ```

这应该产生一组日志，类似于下面的截图所示:

![Figure 2.29 – train.py script logs

](img/B18638_02_029.jpg)

图 2.29–train . py 脚本日志

请注意，完成培训步骤可能需要大约 5 分钟。一旦`train.py`脚本执行完毕，您可以使用`tree`命令检查在`logs`和`model`目录中生成的新文件。

注意

这里发生了什么事？这里，我们在`train.py`中定义的模型的`fit()`方法是用设置为`500`的历元(迭代)数来训练模型。对于每次迭代，我们都在更新神经网络的权重，以最小化实际值和预测值之间的“误差”(例如，使用交叉验证数据)。

1.  接下来，运行下面的命令来运行`tensorBoard`应用程序，它可以帮助可视化和调试 ML 实验:

    ```
    tensorboard --logdir=logs --bind_all
    ```

2.  打开一个新的浏览器标签，打开`http://<IP ADDRESS>:6006`。在*使用深度学习 AMI 启动 EC2 实例*一节中，用我们复制到文本编辑器的公共 IP 地址替换`<IP ADDRESS>`:

![Figure 2.30 – TensorBoard

](img/B18638_02_030.jpg)

图 2.30–张量板

这将加载一个 web 应用程序，类似于前面的屏幕截图。我们不会深入探究我们能用 TensorBoard 做什么，所以请随意查看[https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)以获取更多信息。

注意

我们如何解读这些图表？如图*图 2.30* 所示，培训和验证损失一般会随着时间的推移而减少。在第一个图表(顶部)中，*X*-轴对应于纪元编号，而*Y*-轴显示训练和验证损失。应该注意的是，在这个图表中，训练和验证“学习曲线”是重叠的，并且随着时期或迭代次数的增加，两者都继续减少到某一点。应该注意的是，这些类型的图表有助于诊断 ML 模型的性能，这将有助于避免诸如**过度拟合**(其中已训练的模型在训练数据上表现良好，但在看不见的数据上表现不佳)和**欠拟合**(其中已训练的模型在训练数据集和看不见的数据上表现不佳)的问题。我们不会详细讨论这一点，所以可以随意查看其他可用的 ML 和深度学习资源。

1.  导航回到 **EC2 实例内容**终端，用 *Ctrl* + *C* 停止正在运行的 **TensorBoard** 应用进程。

此时，我们应该在`model`目录中有一个经过训练的模型的工件。在下一节中，我们将在 Jupyter 笔记本环境中加载和评估这个模型。

# 加载和评估模型

在上一节中，我们使用终端训练了我们的深度学习模型。当进行 ML 实验时，通常使用基于网络的交互环境更方便，例如 **Jupyter 笔记本**。从技术上讲，我们可以在终端中运行所有后续的代码块，但是为了方便起见，我们将使用 Jupyter 笔记本。

在下一组步骤中，我们将从命令行启动 Jupyter 笔记本。然后，我们将运行几个代码块来加载和评估我们在上一节中训练的 ML 模型。让我们开始吧:

1.  继续我们在*训练一个 ML 模型*部分中停止的地方，让我们在 **EC2 实例连接**终端中运行下面的命令:

    ```
    jupyter notebook --allow-root --port 8888 --ip 0.0.0.0
    ```

这将启动 Jupyter 笔记本，并通过`8888`端口访问它:

![Figure 2.31 – Jupyter Notebook token

](img/B18638_02_031.jpg)

图 2.31–Jupyter 笔记本令牌

确保从运行`jupyter notebook`命令后生成的日志中复制生成的随机令牌。请参考前面的屏幕截图，了解如何获取生成的令牌。

1.  打开一个新的浏览器标签，打开`http://<IP ADDRESS>:8888`。在*使用深度学习 AMI 启动 EC2 实例*一节中，将`<IP ADDRESS>`替换为我们复制到文本编辑器中的公共 IP 地址:

![Figure 2.32 – Accessing the Jupyter Notebook

](img/B18638_02_032.jpg)

图 2.32–访问 Jupyter 笔记本

在这里，我们可以看到我们被要求输入密码或令牌，然后才能使用 **Jupyter 笔记本**。只需输入从上一步生成的日志中获得的令牌。

重要说明

请注意，该设置还不能用于生产环境。有关如何保护 Jupyter 笔记本服务器的更多信息，请查看 https://Jupyter-Notebook . readthedocs . io/en/stable/Security . XHTML。我们还将在 [*第 9 章*](B18638_09.xhtml#_idTextAnchor187) 、*安全、治理和合规性策略*中讨论一些提高此设置安全性的策略。

1.  通过点击**新建**并从下拉选项列表中选择 **Python 3 (ipykernel)** 来创建一个新的笔记本，类似于下面的截图所示:

![Figure 2.33 – Creating a new Jupyter Notebook

](img/B18638_02_033.jpg)

图 2.33–创建新的 Jupyter 笔记本

这将打开一个空白的笔记本，我们可以在那里运行我们的 Python 代码。

1.  导入`tensorflow`然后使用`list_physical_devices()`列出我们实例中可见的 GPU:

    ```
    import tensorflow as tf
    ```

    ```
    tf.config.list_physical_devices('GPU')
    ```

这将返回一个带有单个`PhysicalDevice`对象的列表，类似于`[PhysicalDevice(name='/physical_device:GPU:0',device_type='GPU')]`。

注意

由于我们使用的是`p3.2xlarge`实例，前面的代码块返回了一个可见的 GPU 设备。如果我们启动了一个`p3.16xlarge`实例，我们应该得到 8 个可见的 GPU 设备。请注意，我们可以通过并行技术，如**数据并行**(在每个 GPU 中使用相同的模型，但使用不同的数据集块进行训练)和**模型并行**(模型被分成与 GPU 数量相等的几个部分)，同时利用多个 GPU 设备来显著减少训练时间。当然，ML 实验脚本需要修改以利用多个 GPU。有关如何在 TensorFlow 中使用 GPU 的更多信息，请随时查看以下链接了解更多细节:[https://www.tensorflow.org/guide/gpu](https://www.tensorflow.org/guide/gpu)。

1.  使用`tf.keras.models.load_model()`加载模型。使用`model.summary()`:

    ```
    model = tf.keras.models.load_model('model')
    ```

    :

    ```
    model.summary()
    ```

    检查模型

这应该会产生一个模型摘要，如下面的屏幕截图所示:

![Figure 2.34 – Model summary

](img/B18638_02_034.jpg)

图 2.34–模型总结

该模型总结应反映我们在*培训 ML 模型*部分准备和培训的模型的属性。

重要说明

确保使用`load_model()`函数(以及其他类似的函数)只加载来自可信来源的 ML 模型。攻击者可以很容易地准备一个模型(带有恶意负载)，当加载时，攻击者可以访问运行 ML 脚本的服务器(例如，通过**反向外壳**)。关于这个主题的更多信息，你可以查看作者关于如何黑客攻击和保护 ML 环境和系统的演讲:[https://speaker deck . com/ARV slat/pycon-APAC-2022-hacking-and-securing-machine-learning-environments-and-systems？slide=21](https://speakerdeck.com/arvslat/pycon-apac-2022-hacking-and-securing-machine-learning-environments-and-systems?slide=21) 。

1.  定义`load_data()`函数，返回指定文件位置的 CSV 文件的值:

    ```
    import numpy as np
    ```

    ```
    def load_data(training_data_location):
    ```

    ```
        fo = open(training_data_location, "rb")
    ```

    ```
        result = np.loadtxt(fo, delimiter=",")
    ```

    ```
        y = result[:, 0]
    ```

    ```
        x = result[:, 1]
    ```

    ```
        return (x, y)
    ```

2.  现在，让我们测试加载的模型是否可以执行作为一组输入值给出的预测。使用`load_data()`加载测试数据，并使用`model.predict()` :

    ```
    x, y = load_data("data/test_data.csv")
    ```

    ```
    predictions = model.predict(x[0:5])
    ```

    ```
    predictions
    ```

    执行一些样本预测

这应该会产生一个浮点值数组，类似于下面的屏幕截图:

![Figure 2.35 – Prediction results

](img/B18638_02_035.jpg)

图 2.35–预测结果

这里，我们有预测的 *y* 目标值的数组，对应于五个输入 *x* 值中的每一个。请注意，这些预测的 *y* 值与从`test_data.csv`文件加载的实际 *y* 值不同。

1.  使用`model.evaluate()` :

    ```
    results = model.evaluate(x, y, batch_size=128)
    ```

    ```
    results
    ```

    评估加载的模型

这应该给我们一个类似于或接近于`2.705784797668457`的值。如果您想知道这个数字的含义，这是对应于*预测值与实际值的距离*的数值:

![Figure 2.36 – How model evaluation works

](img/B18638_02_036.jpg)

图 2.36-模型评估如何工作

在这里，我们可以看到一个模型评估如何处理回归问题的例子。首先，诸如**均方根误差** ( **RMSE** )、**均方误差** ( **MSE** )和**平均绝对误差** ( **MAE** )的评估度量在计算单个评估度量值之前，计算 *y* 的实际值和预测值之间的差。这意味着，与 RMSE 值较高的模型相比，RMSE 值较低的模型通常会犯较少的错误。

此时，您可能会决定利用前面的代码块以及 Python web 框架(如 **Flask** 、 **Pyramid** 或 **Django** )来构建一个定制的后端 API。但是，您可能想要先检查其他内置解决方案，例如**tensor flow Serving**(tensor flow 模型的 ML 模型服务系统)，它是为生产环境设计的。

如果你仔细想想，我们已经在最后几节*中完成了一个完整的 ML 实验，而不需要安装任何额外的库、包或框架*(除了可选的`tree`实用程序)。这样，你就知道了**深度学习人工智能**是多么有用和强大了！同样，如果我们必须设置 20 个或更多这样的 ML 环境，我们可能需要不到 2 个小时来设置和准备好一切。

# 清理

既然我们已经完成了一个端到端的 ML 实验，是时候执行清理步骤来帮助我们管理成本了:

1.  关闭包含 **EC2 实例连接**终端会话的浏览器选项卡。
2.  导航到我们使用深度学习 AMI 启动的实例的 **EC2 实例**页面。点击**实例状态**打开下拉选项列表，然后点击**终止实例**:

![Figure 2.37 – Terminating the instance

](img/B18638_02_037.jpg)

图 2.37–终止实例

正如我们所见，还有其他选项可用，如**停止实例**和**重启实例**。如果您还不想删除该实例，您可能希望停止该实例，并在以后的日期和时间启动它。请注意，停止的实例会产生成本，因为 EC2 实例停止时不会删除连接的 EBS 卷。也就是说，如果 EBS 卷中没有存储任何关键文件，最好终止实例并删除任何附加的 EBS 卷。

1.  在**终止实例？**窗口，点击**终止**。这应该会删除 EC2 实例，以及附加了卷的。

当不再需要管理和降低成本时，应关闭、终止或删除未使用的资源。由于我们的 ML 和 ML 工程要求需要更多的资源，我们将不得不利用几种成本优化策略来管理成本。我们将在下一节讨论其中的一些策略。

# 了解 AWS 定价如何适用于 EC2 实例

在我们结束本章之前，我们必须对 AWS 定价在处理 EC2 实例时如何工作有一个好的了解。我们还需要了解架构和设置如何影响在云中运行 ML 工作负载的总成本。

假设我们最初在俄勒冈地区有一个全天 24 小时运行的单个`p2.xlarge`实例。在这个实例中，数据科学团队定期运行一个脚本，该脚本使用首选的 ML 框架来训练深度学习模型。这个训练脚本一般每周运行两次，每次 3 小时左右。鉴于新数据可用性的不可预测的时间表，很难知道训练脚本将在何时运行以产生新的模型。然后，生成的 ML 模型被立即部署到 web API 服务器，该服务器充当同一个实例中的推理端点。*根据这些信息，安装费用是多少？*

![Figure 2.38 – Approximate cost of running a p2.xlarge instance per month

](img/B18638_02_038.jpg)

图 2.38–每月运行一个 p2.xlarge 实例的大概成本

在这里，我们可以看到该设置的总成本大约至少为每月*648 美元*。我们是如何得到这个号码的？我们首先寻找在俄勒冈州地区运行一个`p2.xlarge`实例每小时的按需成本(使用下面的链接作为参考:【https://aws.amazon.com/ec2/pricing/on-demand/】的)。在撰写本文时，在俄勒冈州(`us-west-2`)地区，一个`p2.xlarge`实例的每小时按需成本将是*$ 0.90/小时*。由于我们将一整月 24/7 运行这个实例，我们将不得不计算*估计的每月总小时数*。假设我们每个月大约有 30 天，那么我们一个月大约有 720 个小时，也就是 T4。

请注意，我们也可以使用 *730.001 小时*作为每月总小时数的更准确值。然而，我们现在将坚持 720 小时，以稍微简化事情。下一步是将运行 EC2 实例每小时的*成本* ( `$0.90 per hour`)和每月的*总小时数* ( `720 hours per month`)相乘。这将给我们一个月内运行 EC2 实例的总成本(`$0.90 x 720 = $648`)。

注意

为了简化本节的计算，我们将只考虑使用 EC2 实例的每小时成本。在现实生活中，我们需要考虑与使用其他资源相关的成本，如 EBS 卷、VPC 资源(NAT 网关)等。为了获得更准确的估计，请务必使用 AWS 价格计算器:https://calculator.aws/.

过了一会儿，数据科学团队决定在同一个实例中训练另一个模型，在这个实例中我们已经运行了一个训练脚本和一个 web 服务器(推理端点)。由于担心在同时运行两个训练脚本时可能会遇到性能问题和瓶颈，团队请求将`p2.xlarge`实例升级到`p2.8xlarge`实例。根据这些信息，新的设置会花费多少钱？

![Figure 2.39 – Approximate cost of running a p2.8xlarge instance per month

](img/B18638_02_039.jpg)

图 2.39–每月运行一个 2.39 大型实例的大概成本

在这里，我们可以看到这种设置的总成本大约至少为每月*5，184 美元*。我们是如何得到这个数字的？我们必须遵循与前面的示例类似的步骤，并寻找运行一个`p2.8xlarge`实例每小时的按需成本。在这里，我们可以看到运行一个`p2.8xlarge`实例的成本( *$7.20 每小时*)是运行一个`p2.xlarge`实例的成本( *$0.90 每小时*)的八倍。也就是说，我们预计总成本将是之前初始设置的八倍。将运行 p2.8xlarge 实例每小时的*成本* ( `$7.20 per hour`)和每月的*总小时数* ( `720 hours per month`)相乘后，我们应该得到单月运行`p2.8xlarge`实例的总成本(`$7.20 x 720 = $5,184`)。

## 使用多个较小的实例来降低运行 ML 工作负载的总体成本

在这一点上，您可能想知道是否有更好的方法来进行设置，以便在运行相同的 ML 工作负载集时显著降低成本。好消息是，有多种方法可以改善我们目前的状况，并将成本从每月*5184 美元*降低到更低的水平，例如每月*86.40 美元*！请注意，与运行原始设置的成本相比，这也要小得多( *$648 每月*)。我们如何做到这一点？

我们需要做的第一件事是利用多个“较小的”实例，而不是单个`p2.8xlarge`实例。一种可能的设置是为每个训练脚本使用一个`p2.xlarge`实例( *$0.90 每小时*)。因为我们正在使用两个训练脚本，所以我们总共有两个`p2.xlarge`实例。除此之外，我们将使用一个`m6i.large`实例( *$0.096 每小时*)来托管部署模型的推理端点。由于训练脚本只在有新数据可用时运行(大约每周两次)，我们可以让`p2.xlarge`实例只在需要运行训练脚本时运行。这意味着，如果我们每月大约有 *720 个小时*，那么与其中一个训练脚本相关联的`p2.xlarge`实例应该每月总共只运行大约 *24 个小时*(大部分时间实例是关闭的)。

注意

*我们怎么得到这个数字*？由于训练脚本预计每周运行两次，每次大约 3 小时，那么公式将是`[3 hours per run] x [2 times per week] x [4 weeks]`，这将产生 24 小时的值。这意味着，如果每个`p2.xlarge`实例在一个月内总共只运行大约 24 小时，那么每个`p2.xlarge`实例每月将花费大约*21.60 美元*。

即使这些`p2.xlarge`实例大部分时间是关闭的，我们的 ML 推理端点仍然会在它的专用`m6i.large`实例中全天候运行。运行`m6i.large`实例一整月的成本大约是每月*69.12 美元*(使用`[$0.096 per hour] x [720 hours per month]`公式):

![Figure 2.40 – Using multiple smaller instances to reduce the overall cost

](img/B18638_02_040.jpg)

图 2.40–使用多个较小的实例来降低总成本

也就是说，我们应该能够将总成本降低到大约每月*112.32 美元*，类似于上图所示。我们是如何得到这个号码的？我们简单地添加了一个月内运行每个实例的预期成本:`$21.60 + $21.60 + $69.12 = $112.32`。

## 使用 spot 实例降低运行培训作业的成本

值得注意的是，我们可以通过利用用于运行训练脚本的`p2.xlarge`实例来进一步降低成本。使用 spot 实例，我们可以通过利用 AWS 中可用的备用计算能力，将使用特定 EC2 实例类型的成本降低大约 60%到 90%。这意味着在运行`p2.xlarge`实例时，我们可能只支付*每小时 0.36 美元*，而不是支付*每小时 0.90 美元*，假设我们使用 spot 实例将节省大约 60%。*使用 spot 实例时有什么问题？*使用 spot 实例时，在这些实例中运行的应用程序有可能被中断！这意味着我们应该只运行在意外中断后可以恢复的任务(比如 ML 培训任务)。

注意

我们是怎么得到这个号码的？60%的节省相当于每小时的按需成本(*$ 0.90/小时*)乘以 0.40。这将给我们`[$0.90 per hour] x [0.40] = [$0.36 per hour]`。

由于使用 spot 实例时可能会出现中断，因此不建议您将它们用于 web 服务器(推理端点)正在运行的 24/7 `m6i.large`实例:

![Figure 2.41 – Using spot instances to reduce the cost of running training jobs

](img/B18638_02_041.jpg)

图 2.41–使用 spot 实例降低运行培训工作的成本

一旦我们为`p2.xlarge`实例使用了 spot 实例，我们就能够将总成本降低到大约每月*86.40 美元*，类似于我们在前面的图表中所得到的。同样，这个最终值不包括其他成本，以稍微简化计算。然而，正如您所看到的，这个值远远小于运行一个`p2.8xlarge`实例的成本(每月*5184 美元*)。

是不是很神奇？！我们只是稍微改变了一下架构，我们能够将成本从*每月 5184 美元*降低到*每月 86.40 美元*！请注意，还有其他方法可以优化在云中运行 ML 工作负载的总体成本(例如，利用**计算节省计划**)。你在这一节学到的应该足够了，因为我们将在本书接下来的几章中继续这些类型的讨论。

# 总结

在本章中，我们能够使用一个**深度学习 AMI** 启动一个 EC2 实例。这使得我们可以立即拥有一个可以执行 ML 实验的环境，而不用担心安装和设置步骤。然后，我们继续使用 **TensorFlow** 来训练和评估我们的深度学习模型，以解决一个回归问题。我们简要讨论了 AWS 定价如何适用于 EC2 实例，从而结束了本章。

在下一章，我们将关注 **AWS 深度学习容器**如何帮助显著加快 ML 实验和部署过程。

# 延伸阅读

对于深度学习人工智能，我们只是触及了表面。除了具有预安装框架的便利性，DLAMIs 还使 ML 工程师能够轻松利用其他优化解决方案，如 **AWS 推理**、 **AWS 神经元**、**分布式训练**和**弹性织物适配器**。有关更多信息，请随时查看以下资源:

*   *什么是 AWS 深度学习 AMI？*([https://docs . AWS . Amazon . com/dlami/latest/dev guide/what-is-dlami . XHTML](https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.xhtml))
*   *AWS 定价如何工作*([https://docs . AWS . Amazon . com/whites/latest/How-AWS-Pricing-Works/How-AWS-Pricing-Works . pdf](https://docs.aws.amazon.com/whitepapers/latest/how-aws-pricing-works/how-aws-pricing-works.pdf))
*   *弹力织物适配器*([https://docs . AWS . Amazon . com/dlami/latest/dev guide/tutorial-EFA . XHTML](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-efa.xhtml))