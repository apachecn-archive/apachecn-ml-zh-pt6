

# 5

# 语用数据处理与分析

在训练**机器学习** ( **ML** )模型时，数据在使用之前需要先进行分析、转换和处理。在过去，数据科学家和 ML 实践者必须使用各种库、框架和工具(如 **pandas** 和 **PySpark** )从头开始编写定制代码，以执行所需的分析和处理工作。这些专业人员编写的定制代码通常需要调整，因为在用于模型训练之前，必须对数据进行测试，以测试数据处理脚本中编程步骤的不同变体。这占用了 ML 从业者很大一部分时间，而且由于这是一个手动过程，通常也容易出错。

处理和分析数据的一种更实用的方法是在加载、清理、分析和转换来自不同数据源的原始数据时使用无代码或低代码工具。使用这些类型的工具将大大加快这个过程，因为我们不需要担心从头开始编写数据处理脚本。在本章中，我们将使用 **AWS Glue DataBrew** 和**Amazon SageMaker Data Wrangler**来加载、分析和处理一个样本数据集。在清理、处理和转换数据之后，我们将在一个 **AWS CloudShell** 环境中下载和检查结果。

也就是说，我们将涵盖以下主题:

*   数据处理和分析入门
*   准备必要的先决条件
*   使用 AWS Glue DataBrew 自动准备和分析数据
*   使用 Amazon SageMaker Data Wrangler 准备 ML 数据

在使用本章中的动手解决方案时，您会注意到在使用 **AWS Glue DataBrew** 和**Amazon SageMaker Data Wrangler**时有一些相似之处，当然，您也会注意到一些不同之处。在我们开始使用和比较这些服务之前，让我们先简短地讨论一下数据处理和分析。

# 技术要求

开始之前，我们必须准备好以下内容:

*   网络浏览器(最好是 Chrome 或 Firefox)
*   访问本书前四章中使用的 AWS 帐户

Jupyter 笔记本、源代码和其他用于每章的文件都可以在这个资源库中获得:[https://github . com/packt publishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS)。

重要说明

确保退出并不使用在第四章 、*AWS 上的无服务器数据管理*中创建的 IAM 用户。在本章中，您应该使用 root 帐户或拥有一组权限的新 IAM 用户来创建和管理 **AWS Glue DataBrew** 、**亚马逊 S3** 、 **AWS CloudShell** 和**亚马逊 SageMaker** 资源。运行本书中的示例时，建议使用具有有限权限的 IAM 用户，而不是 root 帐户。我们将在 [*第 9 章*](B18638_09.xhtml#_idTextAnchor187) 、*安全、治理和合规性策略*中进一步详细讨论这一点以及其他安全最佳实践。

# 数据处理和分析入门

在的前一章中，我们利用了数据仓库和数据湖来存储、管理和查询我们的数据。存储在这些数据源中的数据通常必须经过一系列类似于图 5.1 中*所示的数据处理和数据转换步骤，才能用作 ML 实验的训练数据集；*

![Figure 5.1 – Data processing and analysis

](img/B18638_05_001.jpg)

图 5.1-数据处理和分析

在*图 5.1* 中，我们可以看到这些数据处理步骤可能涉及合并不同的数据集，以及使用各种选项和技术清理、转换、分析和转换数据。在实践中，数据科学家和 ML 工程师通常会花费大量时间清理数据，并为 ML 实验做好准备。一些专业人士可能习惯于编写和运行定制的 Python 或 R 脚本来执行这项工作。然而，在处理这些类型的需求时，使用无代码或低代码解决方案可能更实用，如 AWS Glue DataBrew 和 Amazon SageMaker Data Wrangler。首先，这些解决方案使用起来更方便，因为我们不需要担心管理基础设施，也不需要从头开始编写数据处理脚本。我们还将使用易于使用的视觉界面，这将有助于大大加快工作。监控和安全管理也更容易，因为它们与其他 AWS 服务集成在一起，例如:

*   **AWS 身份和访问管理**(**IAM**)—用于控制和限制对 AWS 服务和资源的访问
*   **亚马逊虚拟私有云**(**VPC**)——用于定义和配置逻辑隔离的网络，规定如何访问资源以及每个资源如何与网络中的其他资源通信
*   **Amazon cloud watch**——用于监控性能和管理所用资源的日志
*   **AWS cloud trail**——用于监控和审计账户活动

注意

有关如何使用这些服务来保护和管理 AWS 帐户中的资源的更多信息，请随时查看 [*第 9 章*](B18638_09.xhtml#_idTextAnchor187) 、*安全、治理和合规性策略*。

重要的是注意到 AWS 中还有其他选项可以帮助我们处理和分析数据。其中包括以下内容:

*   **Amazon Elastic MapReduce**(**EMR**)和**EMR server less**——适用于使用 Apache Spark、Apache Hive 和 Presto 等各种开源工具的大规模分布式数据处理工作负载
*   **亚马逊 kine sis**——用于处理和分析实时流数据
*   **Amazon quick sight**——支持高级分析和自助式商业智能
*   **AWS 数据管道**——使用有助于定制管道资源的调度、依赖性跟踪和错误处理的特性，在各种服务(例如**亚马逊 S3** 、**亚马逊关系数据库服务**和**亚马逊 DynamoDB** )之间处理和移动数据
*   **SageMaker 处理**–在使用 SageMaker 的 AWS 托管基础设施上运行定制数据处理和分析脚本(包括偏差指标和特性重要性计算)

请注意，这不是一个详尽的列表，还有更多服务和功能可以用于这些类型的需求。使用这些服务有什么好处？当处理相对较小的数据集时，在我们的本地机器上执行数据分析和转换可能会奏效。然而，一旦我们需要处理更大的数据集，我们可能需要使用一组更专用的资源，具有更强的计算能力，以及允许我们专注于我们需要做的工作的功能。

注意

我们将在 [*第 9 章*](B18638_09.xhtml#_idTextAnchor187) 、*安全、治理和合规策略*中更详细地讨论偏差检测和特性重要性。

在本章中，我们将重点介绍 AWS Glue DataBrew 和 Amazon SageMaker Data Wrangler，并且我们将展示一些在处理和分析我们的数据时如何使用它们的示例。我们将从“脏”数据集(包含几行无效值)开始，并对该数据集执行以下类型的转换、分析和操作:

*   运行分析数据集的数据分析作业
*   筛选出包含无效值的行
*   从现有列创建新列
*   应用转换后导出结果

一旦包含处理结果的文件上传到输出位置，我们将通过下载文件并检查是否应用了转换来验证结果。

# 准备必要的先决条件

在此部分，我们将确保在继续本章的动手解决方案之前，以下先决条件已准备就绪:

*   要分析和处理的拼花文件
*   将上传拼花文件的 S3 存储桶

## 下载拼花文件

在这一章中，我们将使用与前几章相似的`bookings`数据集。但是，这次源数据存储在一个 Parquet 文件中，我们已经修改了一些行，因此数据集将包含脏数据。也就是说，让我们将`synthetic.bookings.dirty.parquet`文件下载到本地机器上。

可以在这里找到:[https://github . com/packt publishing/Machine-Learning-Engineering-on-AWS/raw/main/chapter 05/synthetic . bookings . dirty . parquet](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/raw/main/chapter05/synthetic.bookings.dirty.parquet)。

注意

请注意，使用 Parquet 格式存储数据比使用 CSV 格式存储数据更可取。一旦需要处理更大的数据集，所生成的拼花文件和 CSV 文件的文件大小差异就变得很明显。例如，一个 1 GB 的 CSV 文件最终可能只有 300 MB(甚至更少)作为一个拼花文件！有关这个话题的更多信息，请随时查看以下链接:[https://parquet.apache.org/docs/](https://parquet.apache.org/docs/)。

在继续之前，确保将`synthetic.bookings.dirty.parquet`文件下载到您的本地机器。

## 准备 S3 桶

您可以为本章中的动手解决方案创建一个新的 S3 存储桶，也可以重用在之前章节中创建的现有存储桶。在使用 AWS Glue DataBrew 和 Amazon SageMaker Data Wrangler 运行数据处理和转换步骤后，这个 S3 存储桶将用于存储`synthetic.bookings.dirty.parquet`源文件和输出目标结果。

一旦这两个先决条件都准备好了，我们就可以继续使用 AWS Glue DataBrew 来分析和处理我们的数据集。

# 使用 AWS Glue DataBrew 自动进行数据准备和分析

AWS Glue DataBrew 是一个无代码数据准备服务，旨在帮助数据科学家和 ML 工程师清理、准备和转换数据。类似于我们在 [*第四章*](B18638_04.xhtml#_idTextAnchor079) 、*AWS*上的无服务器数据管理中使用的服务，Glue DataBrew 也是*无服务器*。这意味着，当使用该服务执行数据准备、转换和分析时，我们将无需担心基础设施管理。

![Figure 5.2 – The core concepts in AWS Glue DataBrew

](img/B18638_05_002.jpg)

图 5.2–AWS Glue data brew 的核心概念

在*图 5.2* 中，我们可以看到使用 AWS Glue DataBrew 时涉及到不同的概念和资源。在使用服务之前，我们需要很好地了解这些是什么。以下是对所用概念和术语的快速概述:

*   **数据集**–数据存储在现有数据源中(例如**亚马逊 S3** 、**亚马逊红移**或**亚马逊 RDS** )或从本地机器上传到S3 桶。
*   **配方**–要在数据集上执行的一组数据转换或数据准备步骤。
*   **作业**–运行某些指令来分析或转换数据集的过程。用于评估数据集的作业称为档案作业**。另一方面，用于运行一组指令来清理、归一化和转换数据的作业被称为配方作业。我们可以使用一个名为**数据沿袭**的视图来跟踪数据集经历的转换步骤，以及作业中配置的起点和终点。**
***   **数据剖析**–在对数据集运行剖析作业后生成的报告。*   **项目**–一个管理的数据收集、转换步骤和工作。**

 **既然我们已经对概念和术语有了一个好的概念，让我们继续创建一个新的数据集。

## 创建新的数据集

在本章的*准备必要的先决条件*部分，我们下载了一个拼花文件到我们的本地机器。在下一组步骤中，我们将创建一个新的数据集，将这个拼花文件从本地机器上传到一个现有的亚马逊 S3 存储桶:

1.  使用 **AWS 管理控制台**的搜索栏导航至 **AWS Glue DataBrew** 控制台。

重要说明

本章假设我们在使用服务管理和创建不同类型的资源时使用了`us-west-2`区域。您可以使用不同的区域，但如果某些资源需要转移到所选的区域，请确保执行任何必要的调整。

1.  点击*图 5.3* 中高亮显示的侧边栏图标，进入**数据集**页面:

![Figure 5.3 – Navigating to the DATASETS page

](img/B18638_05_003.jpg)

图 5.3–导航至数据集页面

1.  点击**连接到新数据集**。
2.  点击**文件上传**，如图*图 5.4* 中高亮显示的:

![Figure 5.4 – Locating the File upload option

](img/B18638_05_004.jpg)

图 5.4–定位文件上传选项

请注意，加载数据和连接到数据集有不同的方式。我们可以使用 **AWS Glue 数据目录**连接并加载存储在 Amazon Redshift、Amazon RDS 和 AWS Glue 中的数据。

注意

请随意查看[https://docs . AWS . Amazon . com/data brew/latest/DG/supported-data-connection-sources . XHTML](https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml)了解更多信息。

1.  指定`bookings`作为**数据集名称**字段的值(在**新数据集细节**下)。
2.  从本地机器的`synthetic.bookings.dirty.parquet`文件下。
3.  接下来，定位，点击**下的**浏览 S3** 按钮，输入 S3 目的地**。选择您在第 4 章 、*AWS 上的无服务器数据管理*的*准备必要先决条件*部分创建的 S3 存储桶。

注意

请注意，您本地机器中的`synthetic.bookings.dirty.parquet`文件将被上传到在这一步中选择的 S3 存储桶。在学习本章中的动手解决方案时，您可以创建和使用不同的 S3 铲斗。您可以使用 AWS 管理控制台或通过 AWS CloudShell 使用 AWS CLI 随意创建新的 S3 存储桶。

1.  在**附加配置**下，确保**选定文件类型**字段设置为**拼花**。
2.  点击**创建数据集**按钮(位于页面右下方)。
3.  此时，`bookings`数据集已经创建，应该出现在数据集列表中，如图*图 5.5* 所示:

![Figure 5.5 – Navigating to the Datasets preview page

](img/B18638_05_005.jpg)

图 5.5–导航到数据集预览页面

有了，让我们点击*图 5.5* 中突出显示的`bookings`数据集名称。这将重定向到**数据集预览**页面，如图*图 5.6* 所示:

![Figure 5.6 – The dataset preview

](img/B18638_05_006.jpg)

图 5.6–数据集预览

通过点击**数据集预览**窗格右上角的相应按钮，随意检查**模式**、**文本**和**树**视图。

既然我们已经成功上传了 Parquet 文件并创建了一个新的数据集，那么让我们继续创建并运行一个概要文件作业来分析数据。

## 创建和运行概要文件作业

在执行任何数据清理和数据转换步骤之前，最好先分析数据，并查看数据集中每一列的属性和统计信息。我们可以使用 AWS Glue DataBrew 的功能为我们自动生成不同的分析报告，而不是手动完成。我们可以通过运行配置文件作业来自动生成这些报告。

在下一组步骤中，我们将创建并运行一个配置文件作业，以生成我们上传的数据集的数据配置文件:

1.  首先，点击**数据档案概述**选项卡，导航至**数据档案概述**页面。
2.  接下来，点击**运行数据配置文件**按钮。这将重定向到**创建工单**页面。
3.  在**创建作业**页面，向下滚动找到**作业输出**设置部分，然后点击**浏览**按钮设置 **S3 位置**字段值。
4.  在`synthetic.bookings.dirty.parquet`文件是在较早的一步上传的。
5.  在`mle`下为**新 IAM 角色后缀**的值。
6.  点击**创建并运行任务**按钮。

注意

完成此步骤可能需要 3 到 5 分钟。请随意喝杯咖啡或茶！请注意，在等待结果出现时，您可能会看到一个正在进行的 **1 作业**加载消息。

1.  配置文件作业完成后，向下滚动并查看结果:

![Figure 5.7 – An overview of the data profile

](img/B18638_05_007.jpg)

图 5.7–数据概要的概述

您应该会看到一组类似于*图 5.7* 中所示的结果。请随意检查由配置文件作业生成的以下报告:

*   **汇总**–显示总行数、总列数、缺失单元格和重复行
*   **相关性**–显示相关性矩阵(显示每个变量是如何相关的)
*   **比较值分布**–显示跨列分布的比较视图
*   **列摘要**–显示每个列的摘要统计数据

或者，您可以导航到**列统计数据**选项卡，并在该选项卡中查看报告。

如您所见，我们只需点击几下鼠标，就可以生成一个可用于分析数据集的数据配置文件。在继续本章的下一部分之前，请随意查看由配置文件作业生成的不同报告和统计信息。

## 创建项目和配置配方

现在是我们创建和使用 AWS Glue DataBrew 项目的时候了。创建项目包括使用数据集和配方来执行所需的数据处理和转换工作。因为我们还没有配方，所以当我们创建和配置项目时，将会创建一个新的配方。在本节中，我们将配置一个具有以下功能的配方:

*   过滤掉包含无效`children`列值的行
*   基于现有列(`booking_changes`)的值创建新列(`has_booking_changes`)

在下一组步骤中，我们将创建一个项目，并使用交互式用户界面来配置清理和转换数据的方法:

1.  在页面的右上角，找到并点击**使用该数据集创建项目**按钮。这会将您重定向到**创建项目**页面。
2.  作为**项目名称**字段的值。
3.  向下滚动并找到**权限**下的**角色名称**下拉字段。选择在前面的步骤中创建的现有 IAM 角色。
4.  之后点击**创建项目**按钮:

![Figure 5.8 – Waiting for the project to be ready

](img/B18638_05_008.jpg)

图 5.8–等待项目准备就绪

在点击**创建项目**按钮后，你应该被重定向到类似于*图 5.8* 所示的页面。在创建一个项目之后，我们应该能够使用一个高度交互的工作空间，在这里我们可以测试和应用各种数据转换。

注意

完成此步骤可能需要 2 到 3 分钟

1.  一旦项目会议准备就绪，我们将对数据进行快速检查，找出数据中的任何错误条目和问题(以便我们可以过滤掉它们)。在显示数据网格视图的左窗格中，找到并滚动(向左或向右)到**子**列，类似于*图 5.9* 所示:

![Figure 5.9 – Filtering out the rows with invalid cell values

](img/B18638_05_009.jpg)

图 5.9–过滤掉具有无效单元格值的行

我们应该看到在`adults`和`babies`和`-1`之间的`children`列，并且在`children`列下有值为`-1`的单元格。一旦查看了`children`栏下的不同值，点击**过滤器**按钮，如图*图 5.9* 中突出显示的。

注意

请注意，我们特意在本章使用的 Parquet 文件的`children`列下添加了一定数量的`-1`值。鉴于`children`列的值不可能小于`0`，我们将在下一组步骤中过滤掉这些行。

1.  点击**过滤器**按钮后，会出现一个下拉菜单。根据条件从**下的选项列表中定位选择大于等于**的**。这将更新页面右侧的窗格，并显示**过滤值**操作的配置选项列表。**
2.  在带有占位符背景文本**的字段中的`0`选项列表的`children`列中，输入一个过滤值**。
3.  点击**预览修改**。这将更新左侧窗格，并提供数据集的网格视图:

![Figure 5.10 – Previewing the results

](img/B18638_05_010.jpg)

图 5.10–预览结果

我们应该看到`children`列下值为`-1`的行已被过滤掉，类似于*图 5.10* 所示。

1.  接下来，点击**应用**按钮。
2.  让我们继续添加一个创建新列的步骤(从一个现有的列开始)。定位并点击**添加步骤**按钮，如图*图 5.11* 所示:

![Figure 5.11 – Adding a step

](img/B18638_05_011.jpg)

图 5.11–添加步骤

**添加步骤**按钮应与**清除所有**连杆位于同一行。

1.  用搜索栏中的`create`打开下拉栏。从结果列表中选择**基于条件**选项:

![Figure 5.12 – Locating the Based on conditions option

](img/B18638_05_012.jpg)

图 5.12–定位基于条件选项

如果您正在寻找搜索字段，只需参考*图 5.12* 中高亮显示的框(在顶部)。

1.  在`booking_changes`
2.  `Greater than`
3.  `0`
4.  `True or False`
5.  `has_booking_changes`
6.  点击`has_booking_changes`:

![Figure 5.13 – Previewing the changes

](img/B18638_05_013.jpg)

图 5.13–预览变更

正如我们在*图 5.13* 中看到的，如果`booking_changes`列的值大于`0`，则这个新列的值为`true`，否则为`false`。

1.  在点击**应用**按钮之前，查看预览结果。
2.  此时，我们的配方中应该有两个应用步骤。点击**发布**，如图*图 5.14* 所示:

![Figure 5.14 – Locating and clicking the Publish button

](img/B18638_05_014.jpg)

图 5.14–定位并点击发布按钮

这将打开**发布配方**窗口。

1.  在弹出的**发布配方**窗口中，点击**发布**。注意，我们可以在发布当前配方时指定一个可选的版本描述。

现在我们已经发布了一个配方，我们可以继续创建一个配方作业，该作业将执行配方中配置的不同步骤。

注意

在发布配方之后，我们仍然需要在应用更改之前运行一个配方作业(导致生成一个具有应用的数据转换的新文件)。

## 创建和运行配方作业

正如我们在图 5.15 中看到的，配方作业需要配置一个源和一个目的地。作业读取存储在源中的数据，执行相关配方中配置的转换步骤，并将处理后的文件存储在目标中。

![Figure 5.15 – A job needs to be configured with a source and a destination

](img/B18638_05_015.jpg)

图 5.15–作业需要配置一个源和一个目的地

值得注意的是，源数据不会被修改，因为配方作业仅以只读方式连接。配方作业处理完所有步骤后，作业结果将存储在一个或多个已配置的输出目的地中。

在下一组步骤中，我们将使用在上一节中发布的配方创建并运行配方作业:

1.  使用左侧边栏导航至**食谱**页面。
2.  选择名为`bookings-project-recipe`的行(这将切换复选框并高亮显示整行)。
3.  点击**用该配方创建作业**按钮。这将把您重定向到**创建工作**页面。
4.  在`bookings-clean-and-add-column`上
5.  `Project`
6.  `bookings-project`。
7.  **作业输出设置**
    *   **S3 地点**:点击**浏览**。找到并选择本章前面步骤中使用的同一个 S3 铲斗。
8.  **权限**:
    *   **角色名称**:选择在前面步骤中创建的 IAM 角色。

注意

我们不局限于在 S3 存储作业输出结果。我们还可以将输出结果存储在 **Amazon Redshift** 、 **Amazon RDS** 表中，等等。如需了解更多信息，请随时查看以下链接:[https://docs . AWS . Amazon . com/data brew/latest/DG/supported-data-connection-sources . XHTML](https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml)。

1.  查看指定的配置，然后点击**创建并运行作业**按钮。如果您不小心点击了**创建任务**按钮(在**创建并运行任务**按钮旁边)，那么在任务创建完成后，您可以点击**运行任务**按钮。

注意

等待 3 到 5 分钟，让此步骤完成。在等待的时候，请随意喝杯咖啡或茶！

那不是很容易吗？创建、配置和运行配方作业非常简单。请注意，我们可以配置这个配方作业，并通过关联一个时间表来自动运行作业。关于这个话题的更多信息，可以查看以下链接:[https://docs . AWS . Amazon . com/data brew/latest/DG/jobs . recipe . XHTML](https://docs.aws.amazon.com/databrew/latest/dg/jobs.recipe.xhtml#jobs.scheduling)。

## 验证结果

现在，让我们继续检查 AWS CloudShell 中的 recipe 作业输出结果，这是一个免费的基于浏览器的 Shell，我们可以使用终端来管理我们的 AWS 资源。在下一组步骤中，我们将把 recipe 作业输出结果下载到 CloudShell 环境中，并检查预期的更改是否反映在下载的文件中:

1.  一旦作业的**最后一次作业运行状态**变为**成功**，点击**输出**栏下的 **1 输出**链接。这将打开**工作输出位置**窗口。点击**目的地**列下的 S3 链接，在新标签中打开 S3 页面。
2.  使用`bookings-clean-and-add-column`。确保按下 *ENTER* 键过滤对象列表。导航到`bookings-clean-and-add-column`并以`part00000`结束。
3.  选择 CSV 文件(应切换复选框)，然后点击**复制 S3 URI** 按钮。
4.  通过点击图标导航到 **AWS CloudShell** ，如*图 5.16* 中突出显示的:

![Figure 5.16 – Navigating to CloudShell

](img/B18638_05_016.jpg)

图 5.16–导航到 CloudShell

我们可以在 AWS 管理控制台的右上角找到这个按钮。您还可以使用搜索栏导航到 CloudShell 控制台。

1.  当看到**欢迎使用 AWS CloudShell** 窗口时，点击**关闭**按钮。等待环境运行(大约 1 到 2 分钟)后再继续。
2.  在 CloudShell 环境中运行以下命令(在`<PASTE COPIED S3 URL>`之后，使用在前面步骤中复制到剪贴板的内容:

    ```
    TARGET=<PASTE COPIED S3 URL>
    ```

    ```
    aws s3 cp $TARGET bookings.csv
    ```

这应该会将输出 CSV 文件从 S3 下载到 CloudShell 环境中。

1.  使用`head`命令检查`bookings.csv`文件的前几行:

    ```
    head bookings.csv
    ```

这将返回包含 CSV 文件头的第一行，以及数据集的前几条记录:

![Figure 5.17 – Verifying the job results

](img/B18638_05_017.jpg)

图 5.17–验证工作结果

在*图 5.17* 中，我们可以看到处理后的数据集现在包括了包含`true`或`false`值的`has_booking_changes`列。您可以进一步检查 CSV 文件，并验证在`children`列下没有更多的`-1`值。我们将把这个留给你作为练习。

现在我们已经使用 AWS Glue DataBrew 分析和处理了我们的数据，我们可以继续使用 Amazon SageMaker Data Wrangler 来执行类似的操作。

重要说明

完成本章中的动手解决方案后，不要忘记删除所有 Glue DataBrew 资源(如配方作业、配置文件作业、配方、项目和数据集)。

# 使用 Amazon SageMaker Data Wrangler 准备 ML 数据

Amazon SageMaker 有很多功能和特性来帮助数据科学家和 ML 工程师满足不同的 ML 需求。SageMaker 专注于加速数据准备和数据分析的能力之一是 SageMaker 数据牧马人:

![Figure 5.18 – The primary functionalities available in SageMaker Data Wrangler

](img/B18638_05_018.jpg)

图 5.18–sage maker Data Wrangler 的主要功能

在*图 5.18* 中，我们可以看到使用 SageMaker Data Wrangler 时我们可以对数据做些什么:

1.  首先，我们可以从各种数据源导入数据，比如亚马逊 S3、亚马逊雅典娜和亚马逊红移。
2.  接下来，我们可以创建一个数据流，并使用各种数据格式和数据转换选项来转换数据。只需点击几下鼠标，我们还可以使用内置和自定义选项来分析和可视化数据。
3.  最后，我们可以通过导出数据处理管道中配置的一个或多个转换来自动化数据准备工作流。

SageMaker Data Wrangler 集成到 SageMaker Studio 中，使我们能够使用这一功能来处理我们的数据并自动化我们的数据处理工作流，而无需离开开发环境。我们不必使用各种工具、库和框架(如 pandas 和 PySpark)从头开始编写所有代码，我们只需使用 SageMaker Data Wrangler 来帮助我们使用一个接口准备自定义数据流，并在几分钟内自动生成可重用的代码！

重要说明

确保退出并且*而不是*使用在 [*第 4 章*](B18638_04.xhtml#_idTextAnchor079) 、*AWS*上的无服务器数据管理中创建的 IAM 用户。您应该使用 root 帐户或拥有一组权限的新 IAM 用户来创建和管理 AWS Glue DataBrew、亚马逊 S3、AWS CloudShell 和亚马逊 SageMaker 资源。运行本书中的示例时，建议使用具有有限权限的 IAM 用户，而不是 root 帐户。我们将在 [*第 9 章*](B18638_09.xhtml#_idTextAnchor187) 、*安全、治理和遵从性策略*中详细讨论这一点以及其他安全最佳实践。

## 访问数据管理器

我们需要打开 SageMaker Studio 来访问 SageMaker Data Wrangler。

注意

在继续之前，确保您已经完成了 [*第 1 章*](B18638_01.xhtml#_idTextAnchor017) 、*AWS 上的 ML 工程介绍*的*sage maker 和 SageMaker Studio* 部分中的实践解决方案。你也可以更新 SageMaker Studio 和 Studio 应用程序(如果你用的是旧版本的话)。关于这个话题的更多信息，可以查看以下链接:[https://docs . AWS . Amazon . com/sagemaker/latest/DG/studio-tasks-update-studio . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update-studio.xhtml)。请注意，本节中的步骤假设我们使用的是 JupyterLab 3.0。如果您使用的是不同的版本，在布局和用户体验方面可能会有一些不同。

在下一组步骤中，我们将启动 SageMaker Studio，并从**文件**菜单中访问 Data Wrangler:

1.  在 AWS 管理控制台的搜索栏中导航到`sagemaker studio`，并从**功能**下的结果列表中选择 **SageMaker Studio** 。

重要说明

本章假设我们在使用服务管理和创建不同类型的资源时使用了`us-west-2`区域。您可以使用不同的区域，但请确保在需要将某些资源转移到所选区域时进行必要的调整。

1.  接下来，点击侧边栏中 **SageMaker 域**下的 **Studio** 。
2.  点击**启动 app** ，如图*图 5.19* 所示。从下拉选项列表中选择**工作室**:

![Figure 5.19 – Opening SageMaker Studio

](img/B18638_05_019.jpg)

图 5.19–打开 SageMaker Studio

这将把你重定向到 **SageMaker 工作室**。等待几秒钟，让接口加载。

1.  打开正在配置的`ml.m5.4xlarge`实例，以运行 Data Wrangler。准备就绪后，您将看到**导入数据**页面。

至此，让我们继续在下一节中导入我们的数据。

重要说明

一旦您完成了本章中的动手解决方案，需要立即关闭用于运行 Data Wrangler 的`ml.m5.4xlarge`实例，以避免任何额外费用。单击并找到左侧边栏上的圆形图标，以显示正在运行的实例、应用程序、内核会话和终端会话的列表。当您使用完 SageMaker Studio 时，确保关闭**运行实例**下的所有运行实例。

## 导入数据

导入数据用于 Data Wrangler 时，有几个选项。我们可以从各种来源导入和加载数据，包括亚马逊 S3、亚马逊雅典娜、亚马逊红移、数据砖(JDBC)和雪花。

在接下来的一组步骤中，我们将着重于导入存储在 Parquet 文件中的数据，该文件上传到我们帐户的 S3 存储桶中:

1.  在**导入数据**页面(在**导入**选项卡下)，点击**亚马逊 S3** 。
2.  上传到你 AWS 账户的一个 S3 桶里的`synthetic.bookings.dirty.parquet`文件。

重要说明

如果您跳过了本章的*使用 AWS Glue DataBrew* 自动准备和分析数据一节，您需要将本章的*准备必要的先决条件*一节中下载的拼花文件上传到一个新的或现有的亚马逊 S3 桶中。

1.  如果您看到类似于*图 5.20* 中所示的**预览错误**通知，您可以通过打开**文件类型**下拉菜单并从选项列表中选择**拼花**来移除它。

![Figure 5.20 – Setting the file type to Parquet

](img/B18638_05_020.jpg)

图 5.20–将文件类型设置为拼花

从**文件类型**下拉菜单中选择**拼花**选项后,**预览错误**消息应该消失。

如果您使用的是 JupyterLab 版本 3.0 的，那么已经预先选择了**拼花**选项。

1.  如果您使用的是 JupyterLab 版，可能会预先选择 **csv** 选项，而不是**拼花**选项。也就是说，不管版本如何，我们都应该将**文件类型**下拉列表值设置为**拼花**。点击页面右上角的**导入**按钮。这将使您返回到**数据流**页面。

请注意，我们并不局限于从亚马逊 S3 进口。我们还可以从 Amazon Athena、Amazon Redshift 和其他数据源导入数据。您可以查看[https://docs . AWS . Amazon . com/sagemaker/latest/DG/data-wrangler-import . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-import.xhtml)了解更多信息。

## 转换数据

在处理和转换我们的数据时，SageMaker Data Wrangler 中有许多内置选项。在这一章中，我们将展示一个如何使用自定义 PySpark 脚本来转换数据的快速演示。

注意

有关众多可用数据转换的更多信息，请随时查看以下链接:[https://docs . AWS . Amazon . com/sage maker/latest/DG/data-wrangler-transform . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml)。

在下一组步骤中，我们将添加并配置一个自定义 PySpark 转换来清理和处理我们的数据:

1.  如果您可以看到**数据类型转换:synthetic . bookings . dirty . parquet**页面，请通过单击页面左上角的 **<数据流**按钮，导航回**数据流**页面。在下一步快速查看数据流的当前配置后，我们将很快返回到该页面。
2.  在**数据流**页面，点击 **+** 按钮，如图*图 5.21* 中高亮显示。从选项列表中选择**添加变换**:

![Figure 5.21 – Adding a transform

](img/B18638_05_021.jpg)

图 5.21–添加转换

在本章中，我们将只使用一个数据集。然而，需要注意的是，我们可以使用 **Join** 选项处理并合并两个数据集，如图 5.21 中的*所示。*

1.  在页面左侧的**所有步骤**窗格中，点击**添加步骤**按钮。这将显示用于转换数据集的选项列表。

注意

如果您使用的是 **JupyterLab 1.0** ，您应该会看到左侧窗格标记为**转换**而不是**所有步骤**。

1.  从可用选项列表中选择**自定义变换**。
2.  在**自定义转换**中，在代码编辑器中输入以下代码:

    ```
    df = df.filter(df.children >= 0)
    ```

    ```
    expression = df.booking_changes > 0
    ```

    ```
    df = df.withColumn('has_booking_changes', expression)
    ```

这个代码块所做的是选择并保留所有列`children`的值为`0`或更高的行，并创建一个新列`has_booking_changes,`，如果`booking_changes`列中的值大于`0`或`false`，则该列的值为`true`。

注意

如果您使用的是 **JupyterLab 1.0** ，您应该会看到左侧窗格中标有 **CUSTOM PYSPARK** 而不是 **CUSTOM TRANSFORM** 。

1.  单击`has_booking_changes`列，类似于我们在*图 5.22* 中看到的:

![Figure 5.22 – Previewing the changes

](img/B18638_05_022.jpg)

图 5.22–预览变更

你应该在`total_of_special_requests`列(预览中最左边的一列)旁边找到`has_booking_changes`列。

1.  一旦您完成了对数据的预览，您可以在点击**添加**按钮之前提供一个可选的**名称**值。
2.  在上一步点击**添加**按钮后，定位并点击页面右上方的 **<数据流**链接(或**返回数据流**链接)。

注意

需要注意的是，这些步骤还没有执行，因为我们只是定义了将在后面的步骤中运行的内容。

请注意，我们在这里只是触及了 SageMaker Data Wrangler 的皮毛。这里是其他可用变换的几个例子:

*   平衡数据(例如，随机过采样、随机欠采样和 SMOTE)
*   对分类数据进行编码(例如，一键编码、相似性编码)
*   处理缺失的时间序列数据
*   从时间序列数据中提取特征

要获得更完整的转换列表，请随意查看以下链接:[https://docs . AWS . Amazon . com/sagemaker/latest/DG/data-wrangler-transform . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml)。

注意

如果您有兴趣更深入地了解如何使用各种技术(如随机过采样、随机欠采样和 SMOTE)来平衡数据，请随意查看下面的博客文章:[https://AWS . Amazon . com/blogs/machine-learning/balance-your-data-for-machine-learning-with-Amazon-sage maker-data-wrangler/](https://aws.amazon.com/blogs/machine-learning/balance-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/)。

## 分析数据

至关重要的是，我们要分析将在后续步骤中用于训练 ML 模型的数据。我们需要很好地了解可能会无意中影响使用该数据训练的 ML 模型的行为和性能的属性。分析数据集的方法有很多种，SageMaker Data Wrangler 的优势在于它允许我们从一系列预建的分析选项和可视化中进行选择，包括以下列表中的选项:

*   **直方图**–能否使用来显示数据分布的“形状”
*   **散点图**–能否使用显示两个数值变量之间的关系(使用点代表数据集中的每个数据点)
*   **表格汇总**–可用于显示数据集的汇总统计数据(例如，每列的记录数或最小值和最大值)
*   **特征重要性分数**(使用快速模型)–用于分析每个特征在预测目标变量时的影响
*   **目标泄漏分析**–能否使用来检测数据集中与我们想要预测的列密切相关的列
*   **时间序列数据**的异常检测–可用于检测时间序列数据中的异常值
*   **偏差报告**—能否用于检测数据集中的潜在偏差(通过计算不同的偏差指标)

注意

请注意，这不是一个详尽的列表，当您在本节的实践部分工作时，您可能会看到其他选项。

在下一组步骤中，我们将创建分析并生成偏差报告:

1.  点击 **+** 按钮，从选项列表中选择**添加分析**，类似于*图 5.23* :

![Figure 5.23 – Adding an analysis

](img/B18638_05_023.jpg)

图 5.23–添加分析

您应该看到位于页面左侧的**创建分析**窗格。

1.  在`Bias report`中指定以下配置选项
2.  `Sample analysis`
3.  `is_cancelled`
4.  `1`
5.  `babies`
6.  `Threshold`
7.  `1`
8.  向下滚动到页面底部。找到并点击**检查偏差**按钮(在**保存**按钮旁边)。
9.  向上滚动并找到偏差报告，类似于*图 5.24* 所示:

![Figure 5.24 – The bias report

](img/B18638_05_024.jpg)

图 5.24–偏差报告

在这里，我们可以看到`0.92`。这意味着数据集非常不平衡，与弱势群体(`is_cancelled = 0`)相比，优势群体(`is_cancelled = 1`)的比例更高。

注意

我们将在 [*第 9 章*](B18638_09.xhtml#_idTextAnchor187) 、*安全、治理和合规性策略*中更深入地了解如何计算和解释偏差指标的细节。

1.  向下滚动并点击**保存**按钮(在**检查偏差**按钮旁边)
2.  定位并点击 **<数据流**链接(或**返回数据流**链接)返回**数据流**页面。

除了偏倚报告，我们还可以生成数据可视化，如直方图和散点图，以帮助我们分析数据。我们甚至可以使用提供的数据集生成一个快速模型，并生成一个特性重要性报告(当预测一个目标变量时，用分数显示每个特性的影响)。如需了解更多信息，请随时查看以下链接:[https://docs . AWS . Amazon . com/sage maker/latest/DG/data-wrangler-analyses . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.xhtml)。

## 导出数据流

一切准备就绪后，让我们继续导出我们在前面章节中准备的数据流。执行导出操作时有不同的选项。这包括将数据导出到亚马逊 S3 存储桶。我们还可以使用包含相关代码块的 Jupyter 笔记本，选择将一个或多个步骤从数据流导出到 **SageMaker 管道**。同样，我们也可以选择将我们准备好的特征导出到 **SageMaker 特征库**。还有一个将数据流步骤直接导出到 Python 代码的选项。

注意

一旦数据流步骤被导出并转换为代码，就可以运行生成的代码和 Jupyter 笔记本来执行数据流中配置的不同步骤。最后，如果需要，有经验的 ML 实践者可以选择修改生成的笔记本和代码。

在下一组步骤中，我们将执行导出操作并生成一个 Jupyter 笔记本，该笔记本将利用 **SageMaker 处理**作业来处理数据并将结果保存到 S3 存储桶:

1.  点击第三个框后的 **+** 按钮 **Python (PySpark)** (或使用您在之前步骤中指定的自定义名称)，如图*图 5.25* 中高亮显示，然后打开**导出到**下的选项列表:

![Figure 5.25 – Exporting the step

](img/B18638_05_025.jpg)

图 5.25–导出步骤

这个应该给我们一个选项列表，包括以下内容:

*   **亚马逊 S3(通过 Jupyter 笔记本)**
*   **SageMaker 管道(通过 Jupyter 笔记本)**
*   **Python 代码**
*   **SageMaker 特色店(通过 Jupyter 笔记本)**

注意

如果您正在使用 JupyterLab 1.0，您需要首先通过点击**数据流**选项卡旁边的**导出**选项卡导航到**导出数据流**页面。之后，您需要点击第三个框(在**自定义 PySpark** 下)，然后点击**导出步骤**按钮(这将打开下拉选项列表)。

1.  从选项列表中选择**亚马逊 S3(通过 Jupyter 笔记本)**。这将生成并打开**保存到 S3 的 SageMaker 加工作业** Jupyter 笔记本。注意，在这一点上，配置的数据转换还没有被应用，我们需要运行生成的笔记本文件中的单元来应用转换。
2.  找到并单击第一个可运行单元。使用**运行所选细胞并推进**按钮运行，如图*图 5.26* 中突出显示的:

![Figure 5.26 – Running the first cell

](img/B18638_05_026.jpg)

图 5.26–运行第一个单元

正如我们在*图 5.26* 中所看到的，我们可以在**输入和输出**下找到第一个可运行的单元。您可能会看到一个“**注意:内核仍在启动。请在内核启动后再次执行该单元。**"等待内核启动时的消息。

注意

等待内核启动。在配置 ML 实例以运行 Jupyter 笔记本单元时，此步骤可能需要大约 3 到 5 分钟。一旦您完成了本章中的动手解决方案，需要立即关闭用于运行 Jupyter 笔记本单元的 ML 实例，以避免任何额外费用。单击并找到左侧边栏上的圆形图标，以显示正在运行的实例、应用程序、内核会话和终端会话的列表。当您使用完 SageMaker Studio 时，确保关闭**运行实例**下的所有运行实例。

1.  一旦内核准备就绪，点击包含第一个代码块的单元格`run_optional_steps`下的变量被设置为`False`。

注意

如果您想知道什么是 SageMaker 处理作业，它是一个利用 AWS 的托管基础设施来运行脚本的作业。该脚本被编码为执行由用户(或脚本的创建者)定义的一组操作。你可以查看[https://docs . AWS . Amazon . com/sage maker/latest/DG/processing-job . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.xhtml)了解更多关于这个话题的信息。

用 SageMaker 处理作业 Jupyter 笔记本运行**保存到 S3 中的所有单元格可能需要 10 到 20 分钟。等待时，让我们快速检查笔记本中的不同部分:**

*   **输入和输出**–在这里我们指定流输出的输入和输出配置
*   **运行处理作业**——在这里我们配置并运行一个 SageMaker 处理作业
*   **(可选)接下来的步骤**–我们可以选择将处理后的数据加载到 pandas 中进行进一步检查，并使用 SageMaker 训练一个模型

注意

如果您遇到类似于存储在`data_sources`列表中的`ProcessingInput`对象的`input_name`值的错误消息(列表中应该只有一个`ProcessingInput`对象)。如果遇到其他意外错误，请根据需要对 Python 代码进行故障排除。

1.  一旦`SystemExit`在**(可选)下一步**中上升，定位并滚动到**工作状态& S3 输出位置**下的单元格，并将*图 5.27* 中高亮显示的 S3 路径复制到本地机器上的文本编辑器(例如 VS 代码):

![Figure 5.27 – Copying the S3 path where the job results are stored

](img/B18638_05_027.jpg)

图 5.27–复制存储作业结果的 S3 路径

在继续之前，您应该找到紧接在`SystemExit`之后的 S3 路径。

既然我们已经运行完了生成的 Jupyter 笔记本中的单元，您可能会问，*首先生成 Jupyter 笔记本的目的是什么*？为什么不直接运行数据流步骤，而不必生成脚本或笔记本呢？答案很简单:这些生成的 Jupyter 笔记本是用来作为初始模板的，可以根据需要进行的工作需求进行定制。

注意

等等！数据集的处理版本在哪里？在下一节中，我们将快速关闭 SageMaker Studio 自动启动的实例来管理成本。关闭资源后，我们将在本章末尾的*验证结果*部分下载并检查保存在 S3 的输出 CSV 文件。

## 关闭资源

需要注意的是，每当我们使用和访问 SageMaker Data Wrangler 时，SageMaker Studio 会自动启动一个`ml.m5.4xlarge`实例(在撰写本文时)。除此之外，在 Jupyter 笔记本中运行一个或多个单元时，还会提供另一个 ML 实例。如果我们要使用类似于图 5.28 中的 AWS 深度学习容器在 Jupyter 笔记本上创建并运行 ML 实验，那么也可以提供一个`ml.g4dn.xlarge`实例。这些实例和资源需要手动关闭和删除，因为它们即使在不活动期间也不会自动关闭:

![Figure 5.28 – A high-level view of how SageMaker Studio operates

](img/B18638_05_028.jpg)

图 5.28-sage maker Studio 如何运作的高级视图

关闭这些资源至关重要，因为我们不想为这些资源未被使用的时间付费。在下一组步骤中，我们将在 SageMaker Studio 中找到并关闭正在运行的实例:

1.  点击侧边栏中的圆形图标，如*图 5.29* 所示:

![Figure 5.29 – Turning off the running instances

](img/B18638_05_029.jpg)

图 5.29–关闭正在运行的实例

单击圆圈图标应该会打开并显示 SageMaker Studio 中正在运行的实例、应用程序和终端。

1.  点击*图 5.29* 中高亮显示的**关闭**按钮，关闭**运行实例**下的所有运行实例。点击**关闭**按钮将打开一个弹出窗口，确认实例关闭操作。点击**关闭所有**按钮继续。

接下来，您可能希望安装和使用一个 JupyterLab 扩展，它可以在不活动期间自动关闭某些资源，类似于 **SageMaker Studio 自动关闭扩展**。你可以在这里找到扩展:[https://github . com/AWS-samples/sage maker-studio-auto-shut down-extension](https://github.com/aws-samples/sagemaker-studio-auto-shutdown-extension)。

重要说明

即使安装了扩展，仍然建议在使用 SageMaker Studio 后手动检查并关闭资源。确保定期检查和清理资源。

## 验证结果

此时，数据集的处理过的版本应该存储在您复制到本地机器的文本编辑器中的目标 S3 路径中。在下一组步骤中，我们将把它下载到 AWS CloudShell 环境中，并检查预期的更改是否反映在下载的文件中:

1.  在 SageMaker Studio 中，打开**文件**菜单，从选项列表中选择**注销**。这将把你重定向回 **SageMaker 域名**页面。
2.  点击*图 5.30* 中高亮显示的图标，导航至 **CloudShell** :

![Figure 5.30 – Navigating to CloudShell

](img/B18638_05_030.jpg)

图 5.30–导航到 CloudShell

我们可以在 AWS 管理控制台的右上角找到这个按钮。您还可以使用搜索栏导航到 CloudShell 控制台。

1.  一旦终端准备就绪，通过运行以下命令(在 **$** )将 CloudShell 环境中的所有文件移动到`/tmp`目录中:

    ```
    mv * /tmp 2>/dev/null
    ```

2.  使用`aws s3 cp`命令将生成的存储在 S3 的 CSV 文件复制到 CloudShell 环境中。确保用从用 SageMaker 处理作业笔记本**保存到 S3 复制的 S3 URL 替换`<PASTE S3 URL>`到本地机器上的文本编辑器:

    ```
    S3_PATH=<PASTE S3 URL>
    ```

    ```
    aws s3 cp $S3_PATH/ . --recursive
    ``` 
3.  使用下面的命令递归地列出文件和目录:

    ```
    ls -R
    ```

您应该会看到一个 CSV 文件存储在`<UUID>/default`中。

1.  最后，使用`head`命令检查 CSV 文件:

    ```
    head */default/*.csv
    ```

这将给出 CSV 文件的前几行，类似于图 5.31 中的内容:

![Figure 5.31 – Verifying the changes

](img/B18638_05_031.jpg)

图 5.31–验证更改

在这里，我们可以看到数据集有了新的`has_booking_changes`列，其中包含了`true`和`false`值。您可以进一步检查 CSV 文件，并验证在`children`列下不再有`-1`值。我们将把这个留给您作为练习(即验证 CSV 文件的`children`列下不再有`-1`值)。

现在我们已经使用 Amazon SageMaker Data Wrangler 和 AWS Glue DataBrew 处理和分析了一个样本数据集，您可能想知道何时使用其中一个工具。以下是决定时的一些一般性建议:

*   如果您计划使用 PySpark 进行自定义转换，就像我们在本章中所做的那样，那么您可能希望使用 Amazon SageMaker Data Wrangler。
*   如果 SageMaker Data Wrangler 不支持源、连接或文件类型格式(例如，Microsoft Excel 工作簿格式或`.xlsx`文件)，那么您可能需要使用 AWS Glue Data Brew。
*   如果你想导出数据处理工作流程，自动生成一个 Jupyter 笔记本，那么你可能要用亚马逊 SageMaker 数据牧马人。
*   如果该工具的主要用户只有很少的编码经验，并且更喜欢处理和分析数据，而不需要阅读、定制或编写一行代码，那么可以使用 AWS Glue Data Brew 来代替 Amazon SageMaker Data Wrangler。

当然，这些只是您可以使用的一些指导原则，但是使用哪种工具的决定最终将取决于需要完成的工作的上下文，以及需要做出决定时工具的局限性。特性和限制会随着时间的推移而变化，因此在决定时，请确保尽可能多的从不同角度进行审查。

# 总结

在用于训练 ML 模型之前，需要对数据进行清理、分析和准备。由于处理这些类型的需求需要时间和精力，因此在分析和处理我们的数据时，建议使用无代码或低代码解决方案，如 AWS Glue DataBrew 和 Amazon SageMaker Data Wrangler。在本章中，我们能够使用这两个服务来分析和处理我们的样本数据集。从一个样本“脏”数据集开始，我们执行了各种转换和操作，其中包括(1)分析和分析数据，(2)过滤掉包含无效数据的行，(3)从现有列创建新列，(4)将结果导出到输出位置，以及(5)验证转换是否已应用到输出文件。

在下一章中，我们将仔细研究 Amazon SageMaker，我们将更深入地研究如何在执行机器学习实验时使用这种托管服务。

# 延伸阅读

有关本章主题的更多信息，请随时查阅以下资源:

*   *AWS Glue DataBrew 产品和服务集成*([https://docs . AWS . Amazon . com/data brew/latest/DG/data brew-integrations . XHTML](https://docs.aws.amazon.com/databrew/latest/dg/databrew-integrations.xhtml))
*   *AWS 中的安全胶水 data brew*([https://docs . AWS . Amazon . com/data brew/latest/DG/Security . XHTML](https://docs.aws.amazon.com/databrew/latest/dg/security.xhtml))
*   *创建并使用数据牧马人流*([https://docs . AWS . Amazon . com/sagemaker/latest/DG/Data-Wrangler-Data-Flow . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-data-flow.xhtml))
*   *Data Wrangler–Transform*([https://docs . AWS . Amazon . com/sagemaker/latest/DG/Data-Wrangler-Transform . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml))
*   *数据牧马人-故障排除*([https://docs . AWS . Amazon . com/sagemaker/latest/DG/Data-Wrangler-trouble-shooting . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-trouble-shooting.xhtml))**