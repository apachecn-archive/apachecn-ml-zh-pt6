

# 4

# AWS 上的无服务器数据管理

企业通常利用在数据库中收集和存储用户信息以及交易数据的系统。一个很好的例子是一家电子商务初创公司，它有一个 web 应用程序，客户可以在其中创建一个帐户，并使用他们的信用卡进行在线购物。存储在几个生产数据库中的用户资料、交易数据和购买历史可以用来构建一个**产品推荐引擎**，这个引擎可以帮助推荐客户可能也想购买的产品。然而，在分析这些存储的数据并使用来训练**机器学习** ( **ML** )模型之前，必须将其合并并加入到**集中式数据存储**中，以便可以使用各种工具和服务对其进行转换和处理。对于这些类型的用例，经常使用几个选项，但是我们将在本章中关注其中的两个—**数据仓库**和**数据湖**。

当涉及到**数据存储**和**数据管理**时，数据仓库和数据湖扮演着至关重要的角色。当生成报告时，没有数据仓库或数据湖的公司可能会直接在运行的应用程序的生产数据库中执行查询。不建议使用这种方法，因为它可能会导致连接到数据库的应用程序的服务降级甚至意外停机。这将不可避免地影响销售数字，因为客户将无法使用电子商务应用程序在线购买产品。数据仓库和数据湖帮助我们处理和分析大量数据，这些数据可能来自连接到正在运行的应用程序的多个较小的数据库。如果您有建立数据仓库或数据湖的经验，那么您可能知道管理这类环境的总体成本、稳定性和性能需要技能、经验和耐心。无服务器服务已经开始帮助我们满足这些需求，这是一件好事。

在本章中，我们将关注数据管理，并使用各种*无服务器*服务来管理和查询我们的数据。我们将首先准备一些先决条件，包括一个新的 IAM 用户、一个 VPC 和一个存储样本数据集的 S3 存储桶。一旦先决条件准备就绪，我们将使用**红移无服务器**来设置和配置一个无服务器数据仓库。之后，我们将使用 **AWS 湖形成**、 **AWS 胶水**、**亚马逊雅典娜**来准备一个无服务器的数据湖。

在本章中，我们将讨论以下主题:

*   无服务器数据管理入门
*   准备必要的先决条件
*   使用 Amazon Redshift 无服务器运行大规模分析
*   建立湖泊形态
*   使用亚马逊雅典娜在亚马逊 S3 查询数据

此时，您可能想知道这些服务是什么，以及这些服务是如何使用的。在继续之前，让我们先快速讨论一下无服务器数据管理是如何工作的！

# 技术要求

开始之前，我们必须准备好以下内容:

*   网络浏览器(最好是 Chrome 或 Firefox)
*   访问本书前几章中使用的 AWS 帐户

每一章的 Jupyter 笔记本、源代码和其他文件都可以在本书的 GitHub 资源库中找到:[https://GitHub . com/packt publishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS)。

# 无服务器数据管理入门

几年前，开发人员、数据科学家和 ML 工程师不得不花费数小时甚至数天来建立数据管理和数据工程所需的基础设施。如果需要分析存储在 S3 的大型数据集，数据科学家和 ML 工程师团队会执行以下步骤:

1.  启动并配置 EC2 实例集群。
2.  将数据从 S3 拷贝到连接到 EC2 实例的卷。
3.  使用安装在 EC2 实例中的一个或多个应用程序对数据执行查询。

这种方法的一个已知挑战是供应的资源可能最终未被充分利用。如果数据查询操作的时间表是不可预测的，那么管理正常运行时间、成本和设置的计算规范也是很棘手的。除此之外，系统管理员和 DevOps 工程师需要花时间管理集群中安装的应用程序的安全性、稳定性、性能和配置。

如今，在这些类型的场景和用例中使用**无服务器**和托管服务要实用得多。如下图所示，我们将有更多时间专注于我们需要做的事情，因为在使用无服务器服务时，我们不再需要担心服务器和基础设施管理:

![Figure 4.1 – Serverless versus not serverless

](img/B18638_04_001.jpg)

图 4.1–无服务器与非无服务器

我们所说的*实际工作*是什么意思？以下是数据分析师、数据科学家和数据工程师需要在服务器管理之外开展工作的快速列表:

*   生成图表和报告。
*   分析趋势和模式。
*   检测并解决数据完整性问题。
*   将数据存储与商业智能工具集成。
*   向管理层提出建议。
*   使用该数据来训练 ML 模型。

注意

当使用无服务器服务时，我们也只为我们使用的服务付费。这意味着我们不会为计算资源不运行的空闲时间付费。如果我们同时设置了试运行环境和生产环境，我们可以确信试运行环境的成本只是生产环境设置的一小部分，因为生产环境中的资源会有更高的预期利用率。

在处理以下*无服务器*数据管理和数据处理需求时，我们可以利用不同的 AWS 服务:

*   **无服务器数据仓库**:亚马逊红移无服务器
*   **无服务器数据湖** : AWS Lake Formation、AWS Glue 和 Amazon Athena
*   **无服务器流处理**:亚马逊 Kinesis、AWS Lambda、DynamoDB
*   **无服务器分布式数据处理**:亚马逊 EMR 无服务器

请注意，这只是冰山一角，我们可以使用更多的无服务器服务来满足我们的需求。在这一章中，我们将着重于建立和查询一个**无服务器数据仓库**和一个**无服务器数据湖**。在我们进行这些之前，首先，让我们准备好先决条件。

注意

此时，您可能想知道何时使用数据湖，何时使用数据仓库。当被查询和处理的数据是预先定义的关系数据时，最好使用数据仓库。存储在数据仓库中的数据质量也应该很高。也就是说，数据仓库被用作数据的“真实来源”,通常用于涉及批量报告和商业智能的用例。另一方面，当被查询和处理的数据涉及来自不同数据源的关系和非关系数据时，最好使用数据湖。存储在数据湖中的数据可能包括原始数据和干净数据。除此之外，数据存储在数据湖中，在数据捕获期间，您不必担心数据结构和模式。最后，数据湖可以用于涉及 ML、预测性分析和**探索性数据分析** ( **EDA** )的用例。由于数据湖和数据仓库服务于不同的目的，一些组织利用这两个选项来满足他们的数据管理需求。

# 准备必要的先决条件

在本节中，我们将确保在本章中继续设置我们的数据仓库和数据湖之前，准备好以下先决条件:

*   本地机器上的文本编辑器(例如，VS 代码)
*   拥有创建和管理我们将在本章中使用的资源的权限的 IAM 用户
*   我们将在 VPC 发布 Redshift 无服务器终端
*   一个新的 S3 桶，我们的数据将使用 AWS CloudShell 上传

在本章中，我们将在`us-west-2`区域创建和管理我们的资源。在继续下一步之前，请确保您已经设置了正确的区域。

## 在本地机器上打开文本编辑器

确保您的本地机器上有一个打开的文本编辑器(例如，， **VS 代码**)。我们将把一些字符串值复制到文本编辑器中，供本章稍后使用。以下是我们将在本章后面部分复制的值:

*   IAM 登录链接、用户名和密码(*准备必要的先决条件* > *创建 IAM 用户*)
*   VPC ID ( *准备必要的先决条件* > *创造一个新的 VPC*
*   当前设置为默认角色的已创建 IAM 角色的名称(*使用 Amazon 红移无服务器大规模运行分析* > *设置红移无服务器端点*)
*   AWS 帐户 ID ( *通过亚马逊红移无服务器* > *向 S3 卸载数据*)大规模运行分析

如果没有安装 VS 代码，你可以使用**文本编辑**、**记事本**、**记事本++** 或 **GEdit** ，这取决于你在本地机器上安装的。

## 创建 IAM 用户

重要的是注意，如果我们直接使用 root 帐户，我们在**红移无服务器**中运行查询时可能会遇到问题。也就是说，我们将在这一部分创建一个 IAM 用户。该 IAM 用户将被配置为具有执行本章中所有动手解决方案所需的适当权限集。

注意

确保在创建新的 IAM 用户时使用 root 帐户。

按照以下步骤从 IAM 控制台创建 IAM 用户:

1.  使用搜索栏导航到 IAM 控制台，如以下屏幕截图所示:

![Figure 4.2 – Navigating to the IAM console

](img/B18638_04_002.jpg)

图 4.2–导航到 IAM 控制台

在搜索栏中键入`iam`后，我们必须从搜索结果列表中选择 **IAM** 服务。

1.  在工具条中找到**访问管理**，然后点击**用户**导航到**用户**列表页面。
2.  在屏幕的右上角，找到并点击**添加用户**按钮。
3.  在**设置用户详细信息**页面上，使用类似于下面截图的配置添加一个新用户:

![Figure 4.3 – Creating a new IAM user

](img/B18638_04_003.jpg)

图 4.3–创建新的 IAM 用户

这里，我们将`mle-ch4-user`设置为**用户名**字段的值。在**选择 AWS 访问类型**下，我们确保在**选择 AWS 凭证类型**下勾选了**密码-AWS 管理控制台访问**复选框。对于**控制台密码**，我们选择**自动生成密码**。对于**要求密码重置**，我们取消勾选**用户必须在下次登录时创建新密码**。

注意

更安全的配置需要在 IAM 用户帐户首次登录时重置密码。但是，我们将在本章中跳过这一步，以减少总的步骤数。

1.  点击**下一步:权限**按钮。
2.  在**设置权限**页面，选择**直接附加现有策略**。
3.  使用搜索过滤器找到并选中以下托管策略的复选框:
    *   **亚马逊 3 全接入**
    *   **AmazonRedshiftFullAccess**
    *   **amazonvpcfullacess**
    *   **awscloudshellfullacess**
    *   **awsglueconsolefullacess**
    *   **亚马逊全接入**
    *   **IAMFullAccess**

在下面的截图中可以看到一个的例子:

![Figure 4.4 – Attaching existing policies directly

](img/B18638_04_004.jpg)

图 4.4-直接附加现有策略

这些是由 AWS 准备和管理的策略，目的是让 AWS 帐户用户能够方便、轻松地管理 IAM 权限。

重要说明

请注意，我们在本章中的权限配置可以进一步改进。在生产级帐户中管理 IAM 权限时，请确保您实践了最小权限**原则**。这意味着 IAM 身份应该只具有执行其任务的最小权限集，这包括在使用服务时授予对特定资源的特定操作的粒度访问权限。有关更多信息，请随时查看第 9 章 、*安全、治理和合规性策略*。

1.  选择受管策略后，单击**下一步:标签**按钮。
2.  在**添加标签(可选)**页面，点击**下一步:审核**。
3.  在**审核**页面，点击**创建用户**按钮。
4.  您应该会看到一个成功通知，以及新用户的登录链接和凭证。将登录链接(例如，`https://<account>.signin.aws.amazon.com/console`)、用户名和密码复制到本地机器上的文本编辑器中(例如，Visual Studio 代码)。之后单击关闭按钮。

重要说明

不要与任何人共享登录链接、用户名和密码。如果我们在创建时为 IAM 用户配置了权限，拥有这些凭据的 IAM 用户可以轻松接管整个帐户。

1.  点击成功通知中的`mle-ch4-user`，导航至用户详细信息页面。
2.  **添加内联策略**，如以下截图所示:

![Figure 4.5 – Adding an inline policy

](img/B18638_04_005.jpg)

图 4.5–添加内嵌策略

除了直接附加的受管策略之外，我们还将附加一个内联策略。我们将在下一步自定义使用内联策略配置的权限。

注意

有关托管策略和内联策略的更多信息，请查看[https://docs . AWS . Amazon . com/IAM/latest/user guide/access _ policies _ managed-vs-inline . XHTML](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.xhtml)。

1.  在**创建策略**页面，导航到 **JSON** 选项卡，如下所示:

![Figure 4.6 – Creating a policy using the JSON editor

](img/B18638_04_006.jpg)

图 4.6–使用 JSON 编辑器创建策略

在前面截图中突出显示的策略编辑器中，指定以下 JSON 配置:

```
{

    "Version": "2012-10-17",

    "Statement": [

        {

            "Effect": "Allow",

            "Action": "redshift-serverless:*",

            "Resource": "*"

        },

        {

            "Effect": "Allow",

            "Action": "sqlworkbench:*",

            "Resource": "*"

        },

        {

            "Effect": "Allow",

            "Action": "lakeformation:*",

            "Resource": "*"

        }

    ]

}
```

这个策略赋予我们的 IAM 用户权限来创建和管理**红移无服务器**、**湖泊形成**和 **SQL 工作台**(一个 SQL 查询工具)资源。如果没有这个额外的内联策略，我们在本章后面使用红移无服务器时会遇到问题。

注意

你可以在官方的 GitHub 资源库中找到这个内联策略的副本:[https://GitHub . com/packt publishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter 04/inline-policy](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter04/inline-policy)。

1.  接下来，点击**查看政策**。
2.  在`custom-inline-policy`上作为**名称**字段的值。然后，点击**创建策略**。

此时，`mle-ch4-user` IAM 用户应该附加了八个策略:七个 AWS 管理的策略和一个内联策略。在本章结束之前，该 IAM 用户应该拥有足够的权限来执行所有操作。

接下来，我们将使用我们复制到本地计算机的文本编辑器中的凭据登录，并测试我们是否可以成功登录:

1.  通过执行以下操作，退出 AWS 管理控制台会话:
    1.  点击屏幕右上角的你的名字
    2.  点击**登出**按钮
2.  导航到登录链接(格式类似于`https://<account>.signin.aws.amazon.com/console`)。确保您用 AWS 帐户的帐户 ID 或别名替换了`<account>`:

![Figure 4.7 – Sign in as IAM user

](img/B18638_04_007.jpg)

图 4.7–以 IAM 用户身份登录

这应该会将您重定向到以 IAM 用户身份登录的**页面，类似于前面截图中的内容。输入**账号 ID** 、 **IAM 用户名**和**密码**值，然后点击**签到**按钮。**

那不是很容易吗？现在我们已经成功创建了我们的 IAM 用户，我们可以创建一个新的 VPC 了。这个 VPC 将在我们创建红移无服务器端点时使用。

## 创建新 VPC

**亚马逊虚拟私有云** ( **VPC** )使我们能够为我们的资源创建和配置隔离的虚拟网络。在本节中，我们将从头开始创建一个新的 VPC，即使当前区域中已经有一个。这允许我们的 Redshift 无服务器实例在其自己的隔离网络中启动，这允许网络与其他现有的 VPC 分开配置和保护。

创建和配置 VPC 有多种方法。更快的方法之一是使用 **VPC 向导**，它让我们在几分钟内建立一个新的 VPC。

重要说明

在继续之前，请确保您以`mle-ch4-user` IAM 用户身份登录。

按照以下步骤使用 **VPC 向导**创建一个新的 VPC:

1.  使用菜单栏中的区域下拉菜单导航到选择的区域。在本章中，我们将假设我们将在`us-west-2`区域创建和管理我们的资源。
2.  通过执行以下操作导航到 VPC 控制台:
    1.  在 AWS 管理控制台的搜索栏中输入`VPC`
    2.  在搜索结果列表下选择 **VPC** 服务
3.  接下来，点击**启动 VPC 向导/创建 VPC** 按钮。这将重定向到**创建 VPC** 向导，如下面的屏幕截图所示:

![Figure 4.8 – The Create VPC wizard

](img/B18638_04_008.jpg)

图 4.8–创建 VPC 向导

在这里，我们可以看到，使用 VPC 向导，只需点击几下，就可以创建和配置相关的 VPC 资源。

注意

您可能希望进一步定制和保护这个 VPC 设置，但这超出了本章的范围。有关更多信息，请查看[https://docs . AWS . Amazon . com/VPC/latest/user guide/VPC-security-best-practices . XHTML](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml)。

1.  在 VPC 向导中，保留除以下内容之外的所有内容不变:
    *   `mle-ch4-vpc`
    *   **可用区域数量** : **3**
    *   **公共子网数量** : **3**
    *   **私有子网数量** : **0**
    *   **NAT 网关($)** : **无**
2.  一旦你完成了 VPC 的配置，点击页面底部的**创建 VPC** 按钮。

注意

创建 VPC 可能需要大约 1 到 2 分钟的时间。

1.  点击**查看 VPC** 。
2.  将 VPC ID(例如，`vpc-abcdefghijklmnop`)复制到本地机器上的编辑器中(例如，Visual Studio 代码)。

既然已经创建了所需的 VPC 资源，我们可以继续处理最后一组先决条件。

## 将数据集上传到 S3

在 [*第一章*](B18638_01.xhtml#_idTextAnchor017) ，*AWS 上的 ML 工程介绍*中，我们使用一个 **AWS Cloud9** 环境上传一个样本数据集到**亚马逊 S3** 。在本章中，我们将使用 AWS CloudShell 上传和下载数据到 S3。如果这是您第一次听说 AWS CloudShell，它是一个基于浏览器的 Shell，我们可以在其中运行不同的命令来管理我们的资源。有了 CloudShell，我们可以使用 AWS CLI 运行命令，而不必担心基础设施管理。

重要说明

在继续之前，请确保您使用的是创建 VPC 资源的同一区域。本章假设我们正在使用`us-west-2`区域。同时，请确保您以`mle-ch4-user` IAM 用户的身份登录。

按照以下步骤使用 CloudShell 和 AWS CLI 将我们的样本数据集上传到 S3:

1.  点击下面截图中突出显示的按钮，导航到 **CloudShell** :

![Figure 4.9 – Launching CloudShell

](img/B18638_04_009.jpg)

图 4.9–启动 CloudShell

您可以在 AWS 管理控制台的右上角找到此按钮。您还可以使用搜索栏导航到 CloudShell 控制台。

1.  如果看到**欢迎使用 AWS CloudShell** 弹出窗口，点击**关闭**按钮。

注意

`[cloudshell-user@ip-XX-XX-XX-XX ~]$`可能需要一两分钟。

1.  在终端控制台运行下面的单行`wget`命令(在 **$** 符号之后)下载一个包含 10 万条订票记录的 CSV 文件:

    ```
    wget https://bit.ly/3L6FsRg -O synthetic.bookings.100000.csv
    ```

2.  接下来，使用`head`命令:

    ```
    head synthetic.bookings.100000.csv
    ```

    检查下载的文件

这将产生 CSV 文件的前几行，类似于下面的屏幕截图:

![Figure 4.10 – Results after using the head command

](img/B18638_04_010.jpg)

图 4.10–使用 head 命令后的结果

正如我们所见，`head`命令显示了`synthetic.bookings.100000.csv`文件的前 10 行。这里，我们在第一行中有包含 CSV 文件的所有列名的标题。

请注意，该数据集类似于我们在第 1 章 、*AWS 上的 ML 工程简介*中使用的 [*酒店预订数据集。唯一的主要区别是我们将在本章中使用的 CSV 文件包含 100，000 条记录，因为我们想测试从数据仓库和数据湖中查询数据的速度。*](B18638_01.xhtml#_idTextAnchor017)

1.  使用`aws s3 mb`命令创建一个新的 S3 铲斗。确保将`<INSERT BUCKET NAME>`替换为全球唯一的存储桶名称——所有其他 AWS 用户以前从未使用过的 S3 存储桶名称:

    ```
    BUCKET_NAME=<INSERT BUCKET NAME>
    ```

    ```
    aws s3 mb s3://$BUCKET_NAME
    ```

有关 S3 存储桶命名规则的更多信息，请查看[https://docs . AWS . Amazon . com/Amazon S3/latest/user guide/bucket naming grules . XHTML](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.xhtml)。

重要说明

确保记住在此步骤中创建的铲斗的名称。我们将在本章的不同解决方案和示例中使用这个 S3 桶。

1.  将您创建的 S3 存储桶的名称复制到本地机器上的文本编辑器中。
2.  使用`aws s3 cp`命令上传`synthetic.bookings.100000.csv`文件:

    ```
    FILE=synthetic.bookings.100000.csv
    ```

    ```
    aws s3 cp $FILE s3://$BUCKET_NAME/input/$FILE
    ```

现在所有的先决条件都准备好了，我们可以使用红移无服务器 T21 来加载和查询我们的数据。

# 使用 Amazon Redshift 无服务器运行大规模分析

数据仓库在数据管理、数据分析和数据工程中起着至关重要的作用。数据工程师和 ML 工程师花时间建立数据仓库，以从事涉及**批量报告**和**商业智能**的项目。

![Figure 4.11 – Data warehouse

](img/B18638_04_011.jpg)

图 4.11-数据仓库

如上图所示，数据仓库包含来自不同关系数据源(如 PostgreSQL 和 MySQL 数据库)的组合数据。当查询数据以满足报告和商业智能需求时，它通常作为唯一的真实来源。在 ML 实验中，数据仓库可以作为干净数据的来源，我们可以从中提取用于构建和训练 ML 模型的数据集。

注意

在生成报告时，企业和初创企业可能会直接在运行 web 应用程序所使用的生产数据库上执行查询。值得注意的是，这些查询可能会导致连接到数据库的 web 应用程序意外停机(因为数据库可能会“忙于”处理额外的查询)。为了避免这种情况，建议将应用程序数据库中的数据连接并加载到一个中央数据仓库，在那里可以安全地运行查询。这意味着我们可以生成自动化报告，并对数据副本执行读取查询，而无需担心任何意外停机。

如果你需要在 AWS 上建立一个数据仓库，Amazon Redshift 是可用的主要选项之一。随着**亚马逊红移无服务器**的发布，数据工程师和 ML 工程师不再需要担心基础设施管理。与非无服务器的同类产品和替代产品相比，当数据仓库空闲且未被使用时，是不收费的。

## 设置红移无服务器端点

入门和设置红移无服务器很容易。我们需要做的就是导航到红移控制台，并创建一个新的红移无服务器端点。当创建一个新的红移无服务器端点时，我们需要担心的是 VPC 和 IAM 用户，这是我们在本章的*准备基本先决条件*一节中准备的。

重要说明

在继续之前，请确保您使用的是创建 S3 时段和 VPC 资源的同一区域。本章假设我们使用的是`us-west-2`区域。同时，请确保您以`mle-ch4-user` IAM 用户的身份登录。

按照以下步骤设置我们的 Redshift 无服务器端点:

1.  导航到 AWS 管理控制台搜索栏中的`redshift`，然后从结果列表中选择**红移**服务。
2.  接下来，点击**尝试亚马逊红移无服务器**按钮。
3.  在`Customize settings`上
4.  `dev`
5.  **管理员用户凭证** > **自定义管理员用户凭证** : *【未选中】*
6.  在**权限**下，打开**管理 IAM 角色**下拉菜单，然后从选项列表中选择**创建 IAM 角色**。
7.  在**创建默认 IAM 角色**弹出窗口中，选择**任意 S3 桶**。

重要说明

请注意，一旦我们需要配置我们的设置以供生产使用，就需要进一步保护这个配置。理想情况下，红移被配置为只访问有限的一组 S3 桶。

1.  点击**创建 IAM 角色为默认**按钮。

注意

您应该会看到类似于**的通知消息:IAM 角色 AmazonRedshift-commandaccessrole-xxxxxxxxxxxxxxxxxx 已成功创建并设置为默认值。**点击**后创建 IAM 角色为默认**按钮。确保将当前设置为默认角色的已创建 IAM 角色的名称复制到本地计算机上的文本编辑器中。

1.  接下来，对**网络和安全**使用以下配置设置:
    *   **虚拟专用云(VPC)** :通过选择适当的 VPC ID，使用您在本章中创建的 VPC。
    *   **VPC 安全组**:使用默认的 VPC 安全组。
    *   **子网**:勾选下拉菜单中可用选项列表中的所有子网。
2.  点击**保存配置**按钮。

注意

完成此步骤可能需要 3 到 5 分钟。在等待安装完成时，您应该会看到一个弹出窗口。与此同时，你可以喝杯咖啡或茶！

1.  一旦设置完成，点击**继续**按钮关闭弹出窗口。

那不是很容易吗？在这一点上，你可能会担心与我们现在的**红移无服务器**设置相关的成本。这里很酷的一点是，当我们的无服务器数据仓库空闲时，计算能力是免费的。请注意，根据存储的数据，我们仍将收取存储费用。完成本章中的动手解决方案后，请确保删除此设置并执行相关的 AWS 资源清理步骤，以避免任何意外费用。

注意

有关 Redshift 无服务器计费的更多信息，请随时查看[https://docs . Amazon AWS . cn/en _ us/Redshift/latest/mgmt/server less-billing . XHTML](https://docs.amazonaws.cn/en_us/redshift/latest/mgmt/serverless-billing.xhtml)。

## 打开红移查询编辑器 v2

有不同的方式来访问我们已配置和准备好的 Redshift 无服务器端点。更方便的方法之一是使用**红移查询编辑器**，我们可以使用 web 浏览器访问它。

重要说明

在继续之前，请确保您使用的是创建 S3 时段和 VPC 资源的同一区域。本章假设我们使用的是`us-west-2`区域。同时，请确保您以`mle-ch4-user` IAM 用户的身份登录。

让我们打开红移查询编辑器，看看我们可以做些什么:

1.  在**无服务器仪表板**区域，点击**查询数据**，如下图所示:

![Figure 4.12 – Locating the Query data button in the Serverless dashboard

](img/B18638_04_012.jpg)

图 4.12–在无服务器仪表板中定位查询数据按钮

在这里，我们可以看到**查询数据**按钮位于**无服务器仪表板**页面的右上角附近。点击**查询数据**按钮将打开**红移查询编辑器 v2** 服务(在新的标签页中)，如以下截图所示:

![Figure 4.13 – Redshift query editor v2

](img/B18638_04_013.jpg)

图 4.13–红移查询编辑器 v2

使用红移查询编辑器很简单。我们可以使用左侧边栏中的选项来管理我们的资源，并且可以在右侧的编辑器中运行 SQL 查询。

注意

如果您在单击**查询数据**按钮后无法打开红移查询编辑器 v2，请确保您的浏览器不会阻止打开新窗口或弹出窗口。

1.  点击**无服务器**连接资源的箭头符号，如下图所示:

![Figure 4.14 – Connecting to the Serverless resource

](img/B18638_04_014.jpg)

图 4.14–连接到无服务器资源

当红移查询编辑器连接到红移无服务器端点时，您应该会看到一个**连接到无服务器**通知。

一旦建立了连接，我们就可以继续创建表了。

## 创建表格

在亚马逊红移中有不同的方法来创建表。按照以下步骤，使用 CSV 文件作为表模式的参考来创建表:

1.  从官方的 GitHub 库下载`synthetic.bookings.10.csv`文件到你的本地机器。可以在这里访问包含 10 个样本行的 CSV 文件:[https://raw . githubusercontent . com/packt publishing/Machine-Learning-Engineering-on-AWS/main/chapter 04/synthetic . bookings . 10 . CSV](https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter04/synthetic.bookings.10.csv)。
2.  在**红移查询编辑器 v2** 中，点击 **+创建**下拉菜单，然后从选项列表中选择**表**。
3.  在**创建表**弹出窗口中，将**模式**下拉值设置为**公共**并将**表**字段值设置为**预订**。

注意

模式用于管理和分组数据库对象和表。默认情况下，新创建的数据库将拥有`PUBLIC`模式。在本章中，我们不会创建一个新的模式，而只是使用默认的`PUBLIC`模式。

1.  点击本地机器上的`synthetic.bookings.10.csv`文件:

![Figure 4.15 – Load from CSV

](img/B18638_04_015.jpg)

图 4.15–从 CSV 加载

在这里，我们可以看到 **Load from CSV** 选项使用存储在所选 CSV 文件中的记录来推断将用于配置和创建新表的列名、数据类型和编码。

1.  点击**创建表格**。您应该会看到一个通知，说明**预订表已成功创建**。

请注意，用作创建表的参考的 CSV 文件在理想情况下应该是更大的完整数据集的子集。在我们的例子中，我们使用了一个包含 10 条记录的 CSV 文件，这些记录来自包含 100，000 条记录的原始 CSV 文件。

## 从 S3 加载数据

现在我们的表已经准备好了，我们可以将存储在 S3 的数据加载到我们的表中。按照以下步骤使用**红移查询编辑器 v2** 从 S3 加载数据:

1.  点击**加载数据**按钮(在 **+创建**下拉菜单旁边)。应该会出现一个类似如下的弹出窗口:

![Figure 4.16 – Loading data from S3

](img/B18638_04_016.jpg)

图 4.16–从 S3 加载数据

在这里，我们可以看到用于从 S3 加载数据的不同配置选项。

1.  打开 **S3 URI** 下的 **S3 文件位置**下拉菜单，从可用选项列表中选择 **us-west-2** 。

注意

该设置假设我们在执行本章中的动手解决方案时使用了`us-west-2`区域。如果 S3 文件位于其他地区，请随意更改。

1.  接下来，点击我们在本章中创建的 S3 桶的**输入**文件夹中的`synthetic.bookings.100000.csv`文件:

![Figure 4.17 – Choose archive in S3

](img/B18638_04_017.jpg)

图 4.17–选择 S3 的存档

选择`synthetic.bookings.100000.csv`文件后，点击**选择**按钮。

1.  打开**选择 IAM 角色**下拉菜单，选择与您在*设置红移无服务器端点*部分复制到本地机器上的文本编辑器中的 IAM 角色同名的 IAM 角色。

注意

如果您无法将 IAM 角色复制到本地机器的文本编辑器中，您可以打开一个新的选项卡并导航到`default` **名称空间配置**页面(在 AWS 管理控制台中)。您应该在**安全和加密**选项卡中找到 IAM 角色(在**角色类型**下标记为**默认**)。

1.  在**高级设置**下，点击**数据转换参数**。确保**忽略标题行**的复选框*被选中*。点击**完成**。
2.  点击**选择一个模式**，然后从下拉选项列表中选择 **public** 。接下来，点击**选择一张桌子**，然后从下拉选项列表中选择**预订**:

![Figure 4.18 – Loading data from the S3 bucket

](img/B18638_04_018.jpg)

图 4.18–从 S3 存储桶加载数据

此时，您应该有一组类似于前面屏幕截图所示的配置参数。

1.  点击**加载数据**按钮。这将关闭**加载数据**窗口，并自动运行加载数据操作。

注意

您可以在查询编辑器中运行`SELECT * FROM sys_load_error_detail;` SQL 语句来解决您可能遇到的任何问题或错误。

完成最后一步可能需要大约 1 到 2 分钟。如果在运行加载数据操作后没有遇到任何问题，您可以继续查询数据库！

## 查询数据库

既然我们已经成功地将 CSV 文件从 S3 存储桶加载到我们的 Redshift 无服务器表中，那么让我们把重点放在使用 SQL 语句执行查询上:

1.  点击 **+** 按钮(位于第一页签左侧)，然后选择**笔记本**，如下图:

![Figure 4.19 – Creating a new SQL Notebook

](img/B18638_04_019.jpg)

图 4.19–创建新的 SQL 笔记本

**SQL 笔记本**帮助组织和记录使用红移查询编辑器运行的多个 SQL 查询的结果。

1.  运行以下 SQL 语句:

    ```
    SELECT * FROM dev.public.bookings;
    ```

确保点击**运行**按钮，如下图所示:

![Figure 4.20 – Running the SQL query

](img/B18638_04_020.jpg)

图 4.20–运行 SQL 查询

这应该会返回一组结果，类似于下面的截图:

![Figure 4.21 – The Add SQL button

](img/B18638_04_021.jpg)

图 4.21–添加 SQL 按钮

这里，在运行查询后，我们应该最多只能获得 100 条记录，因为**限制 100 条**复选框已从*切换到*上。

1.  之后点击**添加 SQL** 按钮，在当前结果集下创建一个新的 SQL 单元格。
2.  接下来，在新的 SQL 单元格中运行下面的 SQL 语句:

    ```
    SELECT COUNT(*) FROM dev.public.bookings WHERE is_cancelled = 0;
    ```

运行查询后，我们应该得到结果`66987`。

重要说明

您可以运行`SELECT * FROM sys_load_error_detail;` SQL 语句来排查和调试任何问题。

1.  让我们尝试查看至少有一次取消的客人取消的预订。也就是说，让我们在一个新的 SQL 单元格中运行以下 SQL 语句:

    ```
    SELECT * FROM dev.public.bookings WHERE is_cancelled = 1 AND previous_cancellations > 0;
    ```

2.  让我们回顾一下等待名单上的天数超过 50 天的客人取消的预订:

    ```
    SELECT * FROM dev.public.bookings WHERE is_cancelled = 1 AND days_in_waiting_list > 50;
    ```

3.  注意，我们还可以使用类似于下面的查询来检查**数据完整性问题**:

    ```
    SELECT booking_changes, has_booking_changes, * 
    ```

    ```
    FROM dev.public.bookings 
    ```

    ```
    WHERE 
    ```

    ```
    (booking_changes=0 AND has_booking_changes='True') 
    ```

    ```
    OR 
    ```

    ```
    (booking_changes>0 AND has_booking_changes='False');
    ```

使用这个查询，我们应该能够列出`booking_changes`列值与`has_booking_changes`列值不匹配的记录。

1.  同样，我们可以使用下面的查询找到其他与数据完整性有关的记录:

    ```
    SELECT total_of_special_requests, has_special_requests, * 
    ```

    ```
    FROM dev.public.bookings 
    ```

    ```
    WHERE 
    ```

    ```
    (total_of_special_requests=0 AND has_special_requests='True') 
    ```

    ```
    OR 
    ```

    ```
    (total_of_special_requests>0 AND has_special_requests='False');
    ```

通过这个查询，我们应该能够列出`total_of_special_requests`列值与`has_special_requests`列值不匹配的记录。

注意

在使用数据训练 ML 模型之前，应该解决这些类型的数据完整性问题。

1.  我们还可以创建包含预计算结果集的物化视图，这有助于加速重复查询:

    ```
    CREATE MATERIALIZED VIEW data_integrity_issues AS
    ```

    ```
    SELECT * 
    ```

    ```
    FROM dev.public.bookings 
    ```

    ```
    WHERE
    ```

    ```
    (booking_changes=0 AND has_booking_changes='True') 
    ```

    ```
    OR 
    ```

    ```
    (booking_changes>0 AND has_booking_changes='False')
    ```

    ```
    OR
    ```

    ```
    (total_of_special_requests=0 AND has_special_requests='True') 
    ```

    ```
    OR 
    ```

    ```
    (total_of_special_requests>0 AND has_special_requests='False');
    ```

2.  最后，我们可以使用下面的查询来查询物化视图中预先计算的数据:

    ```
    SELECT booking_changes, has_booking_changes, total_of_special_requests, has_special_requests FROM data_integrity_issues;
    ```

这将为我们提供一个记录列表，其中包含`total_of_special_requests`列值与`has_special_requests`列值不匹配的记录，以及`booking_changes`列值与`has_booking_changes`列值不匹配的记录。

注意

有关这个主题的更多信息，请随时查看[https://docs . AWS . Amazon . com/redshift/latest/DG/materialized-view-overview . XHTML](https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.xhtml)。

随意运行其他 SQL 查询来研究存储在 *bookings* 表中的数据。

## 向 S3 卸载数据

最后，我们将把存储在 *bookings* 表中的数据复制并卸载到亚马逊 S3。这里，我们将配置并使用`UNLOAD`命令来并行执行该操作，分割数据，并将其存储在 S3 的几个文件中。

注意

一旦数据被卸载到亚马逊 S3，我们就可以使用可以直接从 S3 加载数据的服务、工具和库对这些数据执行其他操作。在我们的例子中，我们将在下一节中使用卸载的数据文件，*设置湖形成*，并且使用 **AWS 胶水**沿着和 **Amazon Athena** 来处理数据文件。

按照以下步骤将存储在我们的 Redshift 无服务器表中的数据卸载到 S3 存储桶中:

1.  打开屏幕右上角的菜单(`mle-ch4-user@<ACCOUNT ALIAS>`)。点击下面截图中突出显示的方框，复制账户 ID。将复制的帐户 ID 值保存到本地计算机上的文本编辑器中:

![Figure 4.22 – Copying the account ID

](img/B18638_04_022.jpg)

图 4.22–复制帐户 ID

请注意，在复制到本地机器上的文本编辑器时，帐户 ID 应该没有破折号。

1.  在**红移查询编辑器 v2** 中，点击**添加 SQL** 按钮，然后在新的 SQL 单元格中运行以下 SQL 语句:

    ```
    UNLOAD ('SELECT * FROM dev.public.bookings;') 
    ```

    ```
    TO 's3://<INSERT BUCKET NAME>/unloaded/'
    ```

    ```
    IAM_ROLE 'arn:aws:iam::<ACCOUNT ID>:role/service-role/<ROLE NAME>'
    ```

    ```
    FORMAT AS CSV DELIMITER ',' 
    ```

    ```
    PARALLEL ON
    ```

    ```
    HEADER;
    ```

请确保替换以下值:

*   `<INSERT BUCKET NAME>`使用我们在*上传数据集到 S3* 部分创建的存储桶的名称
*   `<ACCOUNT ID>`用 AWS 账户的账户 ID
*   `<ROLE NAME>`使用您在*设置红移无服务器端点*部分复制到本地机器上的文本编辑器中的 IAM 角色名称

由于在运行`UNLOAD`命令时指定了`PARALLEL ON`，该`UNLOAD`操作将分割*预订*表中存储的数据，并将这些数据并行存储在多个文件中。

1.  点击下图中突出显示的按钮，导航至 **AWS CloudShell** :

![Figure 4.23 – Launching CloudShell

](img/B18638_04_023.jpg)

图 4.23–启动 CloudShell

我们可以在 AWS 管理控制台的右上角找到这个按钮。您还可以使用搜索栏导航到 CloudShell 控制台。

1.  运行以下命令来列出我们的 S3 bucket 的`unloaded`文件夹中的文件。确保将`<INSERT BUCKET NAME>`替换为我们在*上传数据集到 S3* 部分创建的存储桶的名称:

    ```
    BUCKET_NAME=<INSERT BUCKET NAME>
    ```

    ```
    aws s3 ls s3://$BUCKET_NAME/unloaded/
    ```

2.  使用`tmp`命令:

    ```
    mv * /tmp
    ```

    将当前工作目录下的所有文件移动到`/tmp`目录下
3.  使用`aws s3 cp`命令下载存储在 S3 存储桶

    ```
    aws s3 cp s3://$BUCKET_NAME/unloaded/ . --recursive
    ```

    的`unloaded`文件夹中的文件的副本
4.  使用`ls`命令检查已下载文件的文件名:

    ```
    ls
    ```

这将产生一个文件名列表，类似于下面的截图:

![Figure 4.24 – Results after using the ls command

](img/B18638_04_024.jpg)

图 4.24–使用 ls 命令后的结果

在这里，我们可以看到在*将数据卸载到 S3* 部分中执行的`UNLOAD`操作将*预订*表的副本划分并存储在几个文件中。

1.  使用`head`命令检查每个下载文件的前几行:

    ```
    head *
    ```

这将产生如下所示的输出:

![Figure 4.25 – Results after using the head command

](img/B18638_04_025.jpg)

图 4.25–使用 head 命令后的结果

在这里，我们可以看到每个输出文件都有一个标题，标题中有每一列的对应名称。

现在，我们已经完成了从 Redshift *bookings* 表中卸载数据到我们的 S3 桶中，我们将继续使用 AWS Lake Formation 设置我们的数据湖！

注意

在 Amazon Redshift 和 Amazon Redshift Serverless 中，我们可以使用更多功能。这包括性能调优技术(显著提高缓慢查询的速度)、**红移 ML** (我们可以使用 SQL 语句训练和使用 ML 模型进行推理的)，以及**红移频谱**(我们可以使用它直接从存储在 S3 桶中的文件中查询数据)。这些话题超出了本书的范围，所以你可以去 https://docs.aws.amazon.com/redshift/index.xhtml 了解更多信息。

# 建立湖泊编队

现在，是时候进一步了解如何在 AWS 上建立我们的无服务器数据湖了！在开始之前，让我们定义什么是数据湖，数据湖中存储的是什么类型的数据。**数据湖**是一个集中式数据存储，包含来自不同数据源的各种结构化、半结构化和非结构化数据。如下图所示，数据可以存储在数据湖中，而不必担心结构和格式。在数据湖中存储数据时，我们可以使用各种文件类型，比如 JSON、CSV 和 Apache Parquet。除此之外，数据湖可能包括原始数据和经过处理的(干净的)数据:

![Figure 4.26 – Getting started with data lakes

](img/B18638_04_026.jpg)

图 4.26–数据湖入门

ML 工程师和数据科学家可以使用数据湖作为构建和训练 ML 模型的数据源。由于存储在数据湖中的数据可能是原始数据和干净数据的混合，因此在它可以用于 ML 需求之前，需要额外的数据处理、数据清理和数据转换步骤。

如果您计划在 AWS 中建立和管理数据湖，那么 **AWS Lake Formation** 是一个不错的选择！AWS Lake Formation 是一项服务，它使用 AWS 上的各种服务来帮助建立和保护数据湖，例如**亚马逊 S3** 、 **AWS Glue** 和**亚马逊雅典娜**。由于我们在 AWS Lake Formation 中使用了*无服务器*服务，因此在建立数据湖时，我们不必担心管理任何服务器。

## 创建数据库

类似于数据库如何在红移和其他服务中工作，如**关系数据库服务** ( **RDS** )， **AWS 湖形成**数据库可以包含一个或多个表。但是，在创建表之前，我们需要创建一个新的数据库。

重要说明

在继续之前，请确保您使用的是创建 S3 时段和 VPC 资源的同一区域。本章假设我们使用的是`us-west-2`区域。同时，请确保您以`mle-ch4-user` IAM 用户的身份登录。

按照以下步骤在 AWS Lake Formation 中创建新数据库:

1.  通过在 AWS 管理控制台的搜索框中键入`lake formation`，然后从结果列表中选择 **AWS 湖形成**，导航到 AWS 湖形成控制台。
2.  在**欢迎来到湖泊编队**弹出窗口中，确保**添加我自己**复选框被*选中*。点击**开始**按钮。
3.  在侧边栏中，找到并点击**数据目录**下的**数据库**。
4.  点击位于**数据库**页面右上角的**创建数据库**按钮。
5.  作为**名称**字段的值。保持其他一切不变，然后单击**创建数据库**按钮:

![Figure 4.27 – Creating a Lake Formation database

](img/B18638_04_027.jpg)

图 4.27–创建湖泊形成数据库

您应该会看到一个成功通知，说明您的数据库已经成功创建。您可以忽略前面截图中显示的**未知错误**消息通知。

注意

**未知错误**消息很可能是由于当前 IAM 用户执行操作所允许的有限权限造成的。

现在我们已经创建了我们的湖泊形成数据库，让我们继续使用 AWS Glue Crawler 创建一个表。

## 使用 AWS Glue Crawler 创建表格

**AWS Glue** 是一个无服务器**提取-转换-加载** ( **ETL** )服务，为数据集成提供不同的相关组件和能力。在本章中，我们将使用 **AWS 胶水**的组件之一—**AWS 胶水爬行器**:

![Figure 4.28 – How an AWS Glue Crawler works

](img/B18638_04_028.jpg)

图 4.28–AWS Glue Crawler 如何工作

如上图所示， **AWS Glue Crawler** 处理存储在目标数据存储中的文件，然后根据所处理文件的结构和内容推断出一个模式。该模式用于在 **AWS 粘合数据目录**中创建一个表或一组表。当直接在 S3 查询数据时，这些表可以被亚马逊雅典娜**这样的服务使用。**

记住这些，让我们继续创建 AWS Glue Crawler:

1.  点击侧边栏中的**表格**，导航至**表格**列表页面。
2.  接下来，单击**使用爬虫创建表**按钮(位于页面的左上角)。这将在新标签中打开 **AWS 胶水控制台**。
3.  点击**爬虫** ( **遗留**)，如下图所示:

![Figure 4.29 – Navigating to the Crawlers page

](img/B18638_04_029.jpg)

图 4.29-导航到爬网程序页面

如我们所见，我们可以使用屏幕左侧的侧边栏导航到**爬虫**页面。

1.  点击**添加爬虫**按钮创建一个新的爬虫。
2.  将`mle-ch4-crawler`作为**爬虫名称上**字段的值。然后，点击下一个的**。**
3.  在**添加爬虫** > **指定爬虫来源类型**页面，选择**数据存储**为**爬虫来源类型**。在**重复抓取 S3 数据存储**下，选择**抓取所有文件夹**。然后，点击下一个的**。**
4.  在**添加爬虫** > **添加数据存储**页面，点击文件夹图标，为**包含路径**字段设置 S3 路径位置。这应该打开**选择 S3 路径**弹出窗口，如下图所示:

![Figure 4.30 – Choose S3 path

](img/B18638_04_030.jpg)

图 4.30-选择 S3 路径

找到并切换我们在本章的*准备基本先决条件*部分创建的 S3 存储桶中的`unloaded`文件夹的复选框。之后点击**选择**按钮。

重要说明

如果你跳过了本章的*红移无服务器入门*部分，你可以在 S3 桶中创建一个空的`unloaded`文件夹，然后将`synthetic.bookings.100000.csv`文件上传到`unloaded`文件夹。您可以使用 AWS 管理控制台或通过 AWS CLI 使用 S3 存储桶的`input`文件夹来手动完成此操作。

1.  设置`100`。
2.  在继续操作之前，请确保您在**添加数据存储**页面上设置的配置与我们在以下截图中的相似:

![Figure 4.31 – Adding a data store

](img/B18638_04_031.jpg)

图 4.31–添加数据存储

查看完数据存储配置后，点击**下一步**。

1.  在**添加爬虫** > **添加另一个数据存储**页面，选择**否**选项。之后点击**下一个**按钮。
2.  在`ch4-iam`上作为`AWSGlueServiceRole-ch4-iam`下的输入字段值。之后点击**下一个**。
3.  在**添加爬虫** > **为该爬虫**创建一个调度页面上，从**频率**下的下拉选项列表中选择**按需运行**。之后点击**下一个**按钮。
4.  在的**添加爬虫** > **配置爬虫的输出**页面，从**数据库**下的下拉选项列表中选择我们已经创建的数据库(例如 **mle-ch4-db** )。之后点击**下一个**按钮。
5.  点击**完成**使用指定的配置参数创建 AWS 胶合爬虫。
6.  让我们通过导航到**爬虫**页面(新界面/不是**遗留**页面)来运行爬虫，选择爬虫，然后单击**运行**按钮:

![Figure 4.32 – Running the crawler

](img/B18638_04_032.jpg)

图 4.32–导航到爬网程序页面

注意

完成此步骤可能需要大约 1 到 3 分钟。

1.  导航回到**湖形成**控制台(使用搜索框)。
2.  导航到由 AWS Glue crawler 生成的`unloaded`表:

![Figure 4.33 – Refreshing the Tables list page

](img/B18638_04_033.jpg)

图 4.33–刷新表格列表页面

单击刷新按钮后，您应该在表列表中看到`unloaded`表。

1.  点击**卸载**链接，导航至**表格详情**页面:

![Figure 4.34 – Table details and Schema of the unloaded table

](img/B18638_04_034.jpg)

图 4.34–卸载表的表详细信息和模式

如前面的截图所示，我们应该看到**表细节**和**模式**信息。

1.  打开**动作**下拉菜单，从选项列表中选择**查看数据**。这将打开**预览数据**弹出窗口，通知我们将被带到 Athena 控制台。点击**确定**按钮继续。

*那不是很容易吗？*请注意我们只是触及了能用 **AWS 胶水**做的事情的表面。更多信息，请查看[https://docs . AWS . Amazon . com/glue/latest/DG/what-is-glue . XHTML](https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.xhtml)。

# 使用亚马逊雅典娜查询亚马逊 S3 的数据

**Amazon Athena** 是一个无服务器查询服务，允许我们使用 SQL 语句从存储在 S3 的文件中查询数据。有了 Amazon Athena，我们不必担心基础设施管理，它会自动扩展以处理我们的查询:

![Figure 4.35 – How Amazon Athena works

](img/B18638_04_035.jpg)

图 4.35-亚马逊雅典娜如何工作

如果您要自己设置，您可能需要使用应用程序来设置 EC2 实例集群，比如 **Presto** 。除此之外，您还需要自己管理这个 EC2 集群设置的总体成本、安全性、性能和稳定性。

## 设置查询结果位置

如果**在你运行第一个查询之前，你需要在亚马逊 S3 设置一个查询结果位置**通知出现在**编辑器**页面上，这意味着你必须在亚马逊 Athena **设置**页面上进行快速配置更改，这样 Athena 就可以在每次有查询时将查询结果存储在指定的 S3 桶位置。这些查询结果随后显示在 Athena 控制台的 UI 中。

按照以下步骤设置查询结果位置，我们的 Amazon Athena 查询将存储在该位置:

1.  如果您在运行第一个查询之前看到了**，您需要在亚马逊 S3** 通知中设置查询结果位置，点击**查看设置**导航到**设置**页面。否则，您可以点击**设置**选项卡，如下图所示:

f

![Figure 4.36 – Navigating to the Settings tab

](img/B18638_04_036.jpg)

图 4.36–导航至设置选项卡

1.  点击位于**查询结果和加密设置**窗格右上角的**管理**:

![Figure 4.37 – Managing the query result and encryption settings

](img/B18638_04_037.jpg)

图 4.37–管理查询结果和加密设置

1.  在**管理设置**中的**查询结果位置和加密**下，点击**浏览 S3** ，定位到您在本章创建的 S3 桶。打开单选按钮并点击**选择**按钮。
2.  点击**保存**按钮。

既然我们已经为 Amazon Athena 完成了查询结果位置的配置，我们可以开始运行我们的查询了。

## 使用 Athena 运行 SQL 查询

一切准备就绪后，我们可以开始使用 SQL 语句查询存储在 S3 的数据。在本节中，我们将检查我们的数据，并运行一些查询来检查现有的数据完整性问题。

按照以下步骤查询存储在 S3 时段中的数据:

1.  点击**编辑器**选项卡，导航回**编辑器**页面。
2.  在**编辑器的**选项卡中，运行以下查询:

    ```
    SELECT * FROM "AwsDataCatalog"."mle-ch4-db"."unloaded" limit 10;
    ```

确保点击**运行**按钮，如下图所示:

![Figure 4.38 – Running the SQL query

](img/B18638_04_038.jpg)

图 4.38–运行 SQL 查询

这将返回一组类似于下面截图的结果。请注意，Amazon Athena 可能会在每次运行相同的查询时返回不同的结果集。您可以在查询中添加一个`ORDER BY`子句，以确保使用相同查询时返回的结果的一致性:

![Figure 4.39 – Athena query results

](img/B18638_04_039.jpg)

图 4.39–Athena 查询结果

在这里，我们可以看到我们的查询在不到半秒的时间内被处理。如果我们在没有`LIMIT`子句的情况下运行相同的查询，运行时间可能会超过一秒钟。

注意

**性能调优**不在本书的讨论范围之内，但是您可以随意查看[https://AWS . Amazon . com/blogs/big-data/top-10-Performance-tuning-tips-for-Amazon-Athena/](https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/)以获得关于这个主题的更多信息。

1.  运行以下查询来计算未取消的预订数:

    ```
    SELECT COUNT(*) FROM "AwsDataCatalog"."mle-ch4-db"."unloaded" WHERE is_cancelled=0;
    ```

这应该会给我们一个结果`66987`，这应该与我们执行类似的红移无服务器查询时得到的结果相同(在*使用 Amazon 红移无服务器大规模运行分析*一节中)。

1.  接下来，让我们列出至少有一次先前取消的客人取消的预订:

    ```
    SELECT * 
    ```

    ```
    FROM "AwsDataCatalog"."mle-ch4-db"."unloaded" 
    ```

    ```
    WHERE is_cancelled=1 AND previous_cancellations > 0 
    ```

    ```
    LIMIT 100;
    ```

2.  我们还来回顾一下等待名单上天数超过 50 天的客人取消的预订:

    ```
    SELECT * 
    ```

    ```
    FROM "AwsDataCatalog"."mle-ch4-db"."unloaded" 
    ```

    ```
    WHERE is_cancelled=1 AND days_in_waiting_list > 50 
    ```

    ```
    LIMIT 100;
    ```

3.  注意，我们还可以使用类似于下面的查询来检查**数据完整性问题**:

    ```
    SELECT booking_changes, has_booking_changes, * 
    ```

    ```
    FROM "AwsDataCatalog"."mle-ch4-db"."unloaded" 
    ```

    ```
    WHERE 
    ```

    ```
    (booking_changes=0 AND has_booking_changes=true) 
    ```

    ```
    OR 
    ```

    ```
    (booking_changes>0 AND has_booking_changes=false)
    ```

    ```
    LIMIT 100;
    ```

使用这个查询，我们应该能够列出`booking_changes`列值与`has_booking_changes`列值不匹配的记录。

1.  类似地，我们可以使用以下查询找到其他具有数据完整性问题的记录:

    ```
    SELECT total_of_special_requests, has_special_requests, * 
    ```

    ```
    FROM "AwsDataCatalog"."mle-ch4-db"."unloaded" 
    ```

    ```
    WHERE 
    ```

    ```
    (total_of_special_requests=0 AND has_special_requests=true) 
    ```

    ```
    OR 
    ```

    ```
    (total_of_special_requests>0 AND has_special_requests=false)
    ```

    ```
    LIMIT 100;
    ```

使用这个查询，我们应该能够列出`total_of_special_requests`列值与`has_special_requests`列值不匹配的记录。

1.  我们还可以创建一个可以被以后查询引用的视图:

    ```
    CREATE OR REPLACE VIEW data_integrity_issues AS
    ```

    ```
    SELECT * 
    ```

    ```
    FROM "AwsDataCatalog"."mle-ch4-db"."unloaded" 
    ```

    ```
    WHERE
    ```

    ```
    (booking_changes=0 AND has_booking_changes=true) 
    ```

    ```
    OR 
    ```

    ```
    (booking_changes>0 AND has_booking_changes=false)
    ```

    ```
    OR
    ```

    ```
    (total_of_special_requests=0 AND has_special_requests=true) 
    ```

    ```
    OR 
    ```

    ```
    (total_of_special_requests>0 AND has_special_requests=false);
    ```

请注意，视图不包含任何数据——视图中定义的查询在每次视图被另一个查询引用时运行。

1.  也就是说，让我们运行一个示例查询，它引用我们在上一步中准备的视图:

    ```
    SELECT booking_changes, has_booking_changes, 
    ```

    ```
    total_of_special_requests, has_special_requests 
    ```

    ```
    FROM data_integrity_issues 
    ```

    ```
    LIMIT 100;
    ```

这将为我们提供一个记录列表，其中包含`total_of_special_requests`列值与`has_special_requests`列值不匹配的记录，以及`booking_changes`列值列值与`has_booking_changes`列值不匹配的记录。

注意

如果您想知道我们是否可以使用**boto 3**(Python 的 AWS SDK)以编程方式查询我们在 S3 的数据，那么答案是*是的*。我们甚至可以使用 Amazon Athena 和 Amazon SageMaker 直接在 SQL 语句中使用部署的 ML 模型生成预测。有关这个主题的更多信息，请查看《亚马逊 SageMaker Cookbook 的*机器学习一书的 [*第 4 章*](B18638_04.xhtml#_idTextAnchor079) 、*AWS 上的无服务器数据管理*。您还可以在这里找到如何使用 Python 和 boto3 运行 Athena 和 Athena ML 查询的快速示例:[https://bit.ly/36AiPpR](https://bit.ly/36AiPpR)。*

*那不是很容易吗？在 AWS 上建立一个**无服务器数据湖**很容易，只要我们使用正确的工具和服务。在继续下一章之前，请确保您已经查看了并删除了您在本章中创建的所有资源。*

# 总结

在本章中，我们能够更深入地了解几个帮助组织实现无服务器数据管理的 AWS 服务。使用**无服务器**服务时，我们不再需要担心基础设施管理，这有助于我们专注于我们需要做的事情。

我们能够利用**亚马逊红移无服务器**来准备无服务器数据仓库。我们还能够使用 **AWS Lake Formation** 、 **AWS Glue** 和 **Amazon Athena** 从无服务器数据湖中创建和查询数据。有了这些*无服务器*服务，我们能够在几分钟内加载和查询数据。

# 延伸阅读

有关本章涵盖的主题的更多信息，请随时查阅以下资源:

*   *你的 VPC 的安全最佳实践*([https://docs . AWS . Amazon . com/VPC/latest/user guide/VPC-Security-best-practices . XHTML](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml))
*   *介绍亚马逊红移无服务器*([https://AWS . Amazon . com/blogs/AWS/Introducing-Amazon-Redshift-server less-run-analytics-at-any-scale-without-have-to-manage-infra structure/](https://aws.amazon.com/blogs/aws/introducing-amazon-redshift-serverless-run-analytics-at-any-scale-without-having-to-manage-infrastructure/))
*   *安在 AWS 湖编队*([https://docs . AWS . Amazon . com/Lake-Formation/latest/DG/Security . XHTML](https://docs.aws.amazon.com/lake-formation/latest/dg/security.xhtml))