

# 7

# SageMaker 部署解决方案

在训练完我们的**机器学习** ( **ML** )模型之后，我们可以继续将它部署到 web API。然后，其他应用程序(例如，移动应用程序)可以调用该 API 来执行“预测”或推断。例如，我们在 [*第 1 章*](B18638_01.xhtml#_idTextAnchor017) 、*AWS 上的 ML 工程介绍*中训练的 ML 模型，可以部署到 web API，然后在给定一组输入的情况下，用于预测客户取消预订或不取消预订的可能性。将 ML 模型部署到 web API 允许不同的应用程序和系统访问 ML 模型。

几年前，ML 从业者不得不花时间构建一个定制的后端 API 来从零开始托管和部署模型。如果您有这个需求，您可能会使用 Python 框架，如 **Flask** 、 **Pyramid** 或 **Django** 来部署 ML 模型。构建一个自定义 API 作为推理端点可能需要一周左右的时间，因为大多数应用程序逻辑需要从头开始编写。如果我们要为 API 设置 **A/B 测试**、**自动缩放**或**模型监控**，那么除了最初花费的时间之外，我们可能还需要花费几周时间来设置基础 API。ML 工程师和软件开发人员通常低估了构建和维护 ML 推理端点所需的工作量。需求随着时间的推移而发展，随着需求和解决方案的堆积，定制的应用程序代码变得更加难以管理。此时，你可能会问:“有没有更好更快的方法来做这件事？”。好消息是，如果我们使用 **SageMaker** 来部署我们的模型，我们可以在“不到一天”的时间内完成所有的工作！SageMaker 已经自动化了大部分工作，我们需要做的只是指定正确的配置参数，而不是从头开始构建一切。如果需要，SageMaker 允许我们定制某些组件，我们可以轻松地用我们自己的定制实现替换一些默认的自动化解决方案。

使用 SageMaker 时的一个误解是，ML 模型需要先在 SageMaker 中接受训练，然后才能部署到 **SageMaker 托管服务**中。值得注意的是“这不是真的”,因为该服务是为支持不同的场景而设计和构建的，包括立即部署预先训练好的模型。这意味着，如果我们有一个在 SageMaker 之外训练的预训练模型，那么我们*可以*继续部署它，而不必再次经历训练步骤。在这一章中，您将发现在执行模型部署时使用 **SageMaker Python SDK** 是多么容易。在短短几行代码中，我们将向您展示如何将我们预先训练的模型部署到各种推理端点类型中——实时、**无服务器**和**异步推理端点**。在本章的后面，我们还将讨论何时最好使用这些推理端点类型。同时，我们将讨论在 SageMaker 中执行模型部署时的不同策略和最佳实践。

也就是说，我们将涵盖以下主题:

*   SageMaker 中的模型部署入门
*   准备预训练的模型工件
*   准备 SageMaker 脚本模式先决条件
*   将预训练模型部署到实时推理端点
*   将预训练模型部署到无服务器推理端点
*   将预训练模型部署到异步推理端点
*   清理
*   部署策略和最佳实践

我们将快速讨论部署 ML 模型时的其他选择和选项。在你完成了本章的动手解决方案后，你将会更有信心在 SageMaker 中部署不同类型的 ML 模型。一旦您对使用 SageMaker Python SDK 达到了一定的熟悉和掌握程度，您应该能够在短短几个小时，甚至几分钟内建立一个 ML 推断端点！

# 技术要求

开始之前，请务必准备好以下内容:

*   网络浏览器(最好是 Chrome 或 Firefox)
*   访问本书第一章中使用的 AWS 帐户和 **SageMaker Studio** 域

Jupyter 笔记本、源代码和其他用于每章的文件都可以在这个资源库中获得:[https://github . com/packt publishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS)。

重要说明

运行本书中的示例时，建议使用具有有限权限的 IAM 用户，而不是 root 帐户。我们将在第 9 章 、*安全、治理和法规遵从性策略*中详细讨论这一点以及其他安全最佳实践。如果您刚刚开始使用 AWS，您可以同时继续使用 root 帐户。

# sage maker 中的模型部署入门

在 [*第六章*](B18638_06.xhtml#_idTextAnchor132) ， *SageMaker 训练和调试解决方案*中，我们使用 **SageMaker Python SDK** 训练和部署了一个图像分类模型。在那一章中，我们在着手解决问题时使用了内置算法。当使用内置算法时，我们只需准备好训练数据集，并指定一些配置参数，就万事俱备了！请注意，如果我们想要使用我们最喜欢的 ML 框架(如 TensorFlow 和 PyTorch)训练一个定制模型，那么我们可以准备我们的定制脚本，并使用**脚本模式**使它们在 SageMaker 中工作。这给了我们更多的灵活性，因为我们可以通过一个定制的脚本来调整 SageMaker 与我们的模型的接口，这个脚本允许我们在训练我们的模型时使用不同的库和框架。如果我们希望训练脚本运行的环境具有最高级别的灵活性，那么我们可以选择使用我们自己的定制容器映像。SageMaker 有自己的一套在训练模型时使用的预构建容器图像。然而，如果需要的话，我们可以决定构建和使用我们自己的。

![Figure 7.1 – The different options when training and deployment models

](img/B18638_07_001.jpg)

图 7.1–训练和部署模型时的不同选项

正如我们在*图 7.1* 中看到的，在 SageMaker 中训练 ML 模型时可用的选项在使用 SageMaker 托管服务部署模型时也可用。在这里，我们用任意的标签(例如， **T1** 或 **T2** )来标记每个方法或选项，以帮助我们更详细地讨论这些选项。当在 SageMaker 中执行模型部署时，我们可以选择使用内置算法的容器( **D1** )来部署模型。我们还可以选择使用**脚本模式** ( **D2** )部署我们的深度学习模型。有了这个选项，我们需要准备将在预建的**深度学习容器**中运行的定制脚本。我们还可以选择为将要部署 ML 模型的环境提供和使用我们自己的定制容器映像( **D3** )。

重要说明

选择使用哪种选项组合通常取决于执行 ML 实验和部署时所需的定制级别(以定制脚本和容器映像的形式)。在开始使用 SageMaker 时，建议使用 SageMaker 内置算法，以便在训练模型( **T1** )和部署模型( **D1** )时更好地了解事情是如何工作的。如果我们需要在带有 SageMaker 的 AWS 托管基础设施之上使用 TensorFlow、PyTorch 或 MXNet 等框架，我们将需要准备一套定制脚本，以便在训练( **T2** )和部署( **D2** )期间使用。最后，当我们需要更大的灵活性时，我们可以准备定制的容器映像，并在训练模型( **T3** )和部署模型( **D3** )时使用它们。

值得注意的是，在训练和部署模型时，我们可以组合和使用不同的选项。例如，我们可以使用脚本模式( **T2** )训练一个 ML 模型，并在模型部署期间使用一个定制的容器映像( **D3** )。另一个例子包括在 SageMaker ( **T4** )之外训练模型，并在模型部署( **D1** )期间为内置算法使用预先构建的推理容器映像。

现在，让我们讨论一下模型部署如何使用sage maker 托管服务:

![Figure 7.2 – Deploying a model using the SageMaker hosting services

](img/B18638_07_002.jpg)

图 7.2–使用 SageMaker 托管服务部署模型

在*图 7.2* 中，我们有一个使用 SageMaker 托管服务的模型部署工作方式的高级图表。假设在训练步骤之后，`model.tar.gz`文件(包含 ML 模型工件和输出文件)已经被上传到 S3 存储桶，那么`model.tar.gz`文件从 S3 存储桶被下载到 ML 计算实例中，该实例用作 ML 推理端点的专用服务器。在这个 ML compute 实例中，存储在`model.tar.gz`文件中的模型工件被加载到一个包含推理代码的运行容器中，该容器可以加载模型，并使用它对传入的请求进行推理。如前所述，推理代码和用于推理的容器映像既可以由 AWS(内置或预建)提供，也可以由 ML 工程师使用 SageMaker(自定义)提供。

让我们展示几个示例代码块来帮助我们解释这些概念。我们的第一个示例涉及使用内置的**主成分分析** ( **PCA** )算法来训练和部署模型，该算法可用于诸如降维和数据压缩等用例:

```
from sagemaker import PCA

# [1] TRAINING

estimator = PCA(

    role=role,

    instance_count=1,

    instance_type='ml.c4.xlarge',

    num_components=2,

    sagemaker_session=session

)

estimator.fit(...)

# [2] DEPLOYMENT

predictor = estimator.deploy(

    initial_instance_count=1,

    instance_type='ml.t2.medium'

)
```

在这里，SageMaker 在训练和部署 PCA 模型时使用了一个预构建的容器映像。这个容器映像是由 AWS 团队准备的，因此我们在使用内置算法时不必担心自己实现它。请注意，我们也可以跳过训练步骤，继续进行SageMaker 中的部署步骤，只要我们有一个预先训练好的模型，并且该模型与内置算法可用的预先构建的容器兼容。

现在，让我们快速看一下在 SageMaker 中部署模型时如何使用定制脚本的例子:

```
from sagemaker.pytorch.model import PyTorchModel

# [1] HERE, WE DON'T SHOW THE TRAINING STEP

model_data = estimator.model_data

# [2] DEPLOYMENT

model = PyTorchModel(

    model_data=model_data, 

    role=role, 

    source_dir="scripts",

    entry_point='inference.py', 

    framework_version='1.6.0',

    py_version="py3"

)

predictor = model.deploy(

    instance_type='ml.m5.xlarge', 

    initial_instance_count=1

)
```

在这个例子中，SageMaker 利用预先构建的深度学习容器映像来部署 PyTorch 模型。正如在 [*第三章*](B18638_03.xhtml#_idTextAnchor060) 、*深度学习容器*中所讨论的，相关的包和依赖项已经安装在这些容器镜像中。在部署步骤中，容器运行在`PyTorchModel`对象初始化期间提供的自定义`inference.py`脚本中指定的自定义代码。定制的代码将加载模型并在处理发送到 SageMaker 推理端点的请求时使用它。

注意

在提供的例子中，我们初始化了一个`PyTorchModel`对象，并使用了`deploy()`方法将模型部署到一个实时推理端点。在推断端点内部，使用 PyTorch 推断容器映像的容器将运行装载模型并使用它进行推断的推断代码。注意，对于其他库和框架，我们也有相应的`Model`类，比如`TensorFlowModel`、`SKLearnModel`和`MXNetModel`。一旦调用了`deploy()`方法，适当的推理容器(带有相关的安装包和依赖项)将在推理端点中使用。

如果我们想要指定和使用我们自己的自定义容器图像，我们可以使用下面的代码块:

```
from sagemaker.model import Model

# [1] HERE, WE DON'T SHOW THE TRAINING STEP

model_data = estimator.model_data

# [2] DEPLOYMENT

image_uri = "<INSERT ECR URI OF CUSTOM CONTAINER IMAGE>"

model = Model(

    image_uri=image_uri, 

    model_data=model_data,

    role=role,

    sagemaker_session=session

)

predictor = model.deploy(

    initial_instance_count=1, 

    instance_type='ml.m5.xlarge'

)
```

在这个例子中，SageMaker 利用了存储在`image_uri`变量中指定位置的中的自定义容器图像。这里，假设我们已经准备并测试了定制容器映像，并且在执行模型部署步骤之前，我们已经将该容器映像推送到一个**Amazon Elastic Container Registry**存储库中。

注意

在准备定制脚本和定制容器映像时需要一点试错(类似于我们在 [*第 3 章*](B18638_03.xhtml#_idTextAnchor060) 、*深度学习容器*中如何准备和测试我们的定制容器映像)。如果您正在使用一个笔记本实例，那么您可以使用 SageMaker **本地模式**，这为我们提供了一种在托管 ML 实例中运行定制脚本和定制容器映像之前，在本地环境中测试它们的方法。

本节中显示的代码示例假设我们将在实时推理端点中部署 ML 模型。然而，在 SageMaker 中部署 ML 模型时，有不同的选项可供选择:

*   第一个选项涉及在一个**实时推理端点**中部署和托管我们的模型。
*   第二个选项涉及在使用 SageMaker Python SDK 在**无服务器推理端点**中部署我们的模型时，对配置进行一点调整。
*   第三个选项是在一个**异步推理端点**中托管我们的模型。

我们将在本章的实践部分讨论这些选项，并且我们还将讨论每个选项的相关使用案例和场景。

注意

值得注意的是，不需要设置推理端点，也可以用模型执行推理。这涉及到使用**批量转换**，其中一个模型被加载并用于处理多个输入有效载荷值并一次性执行预测。要查看批量转换的工作示例，请随意查看以下链接:[https://bit.ly/3A9wrVy](https://bit.ly/3A9wrVy)。

现在我们对 SageMaker 模型部署如何工作有了更好的了解，让我们继续本章的实践部分。在下一节中，我们将准备包含 ML 模型工件的`model.tar.gz`文件，我们将在本章的模型部署解决方案中使用它。

# 准备预训练的模型工件

在 [*第六章*](B18638_06.xhtml#_idTextAnchor132) 、 *SageMaker 训练和调试解决方案*中，我们创建了一个名为`CH06`的新文件夹，以及一个使用所创建文件夹中的`Data Science`图像的新笔记本。在本节中，我们将创建一个新文件夹(名为`CH07`)，并在创建的文件夹中创建一个新笔记本。代替`Data Science`图像，我们将使用`PyTorch 1.10 Python 3.8 CPU Optimized`图像作为笔记本中使用的图像，因为我们将下载预训练`transformers`库的模型工件。一旦笔记本准备好，我们将使用拥抱脸`transformers`库下载一个预先训练好的模型，可以用于情感分析。最后，我们将模型工件压缩到一个`model.tar.gz`文件中，并上传到一个 S3 桶中。

注意

在继续之前，确保您已经完成了第 1 章 、*AWS 上的 ML 工程介绍*的*sage maker 和 SageMaker Studio* 部分中的实际操作解决方案。需要注意的是，本章的动手操作部分并不是我们在 [*第 6 章*](B18638_06.xhtml#_idTextAnchor132) 、 *SageMaker 训练和调试解决方案*中完成的内容的延续。只要我们有 SageMaker 工作室成立，我们应该很好去。

在下一组步骤中，我们将准备包含模型工件的`model.tar.gz`文件，然后将它上传到 S3 存储桶:

1.  导航到`sagemaker studio`进入 AWS 管理控制台的搜索栏，然后从**功能**下的结果列表中选择 **SageMaker Studio** 。我们点击侧边栏中 **SageMaker Domain** 下的 **Studio** ，然后从 **Launch app** 下拉菜单(在 **Users** 窗格上)下的下拉选项列表中选择 **Studio** 。等待一两分钟，等待 SageMaker Studio 界面加载。

重要说明

本章假设我们在使用服务管理和创建不同类型的资源时使用了`us-west-2`区域。您可以使用不同的区域，但请确保在需要将某些资源转移到所选区域时进行必要的调整。

1.  在`CH07`中的空白处点击右键。最后，双击侧边栏中的文件夹名称，导航到`CH07`目录。
2.  点击`PyTorch 1.10 Python 3.8 CPU Optimized`创建一个新的笔记本
3.  `Python 3`
4.  `No script`
5.  之后点击**选择**按钮。

注意

等待内核启动。在配置 ML 实例以运行 Jupyter 笔记本单元时，此步骤可能需要大约 3 到 5 分钟。

1.  将笔记本从`Untitled.ipynb`重命名为`01 - Prepare model.tar.gz file.ipynb`。
2.  现在我们的笔记本已经准备好了，我们可以继续生成预训练的模型工件，并将它们存储在`model.tar.gz`文件中。在 Jupyter 笔记本的第一个单元格中，让我们运行下面的代码，它将安装拥抱脸`transformers`库:

    ```
    !pip3 install transformers==4.4.2
    ```

3.  同样使用`pip`安装`ipywidgets`:

    ```
    !pip3 install ipywidgets --quiet
    ```

4.  接下来，让我们运行下面的代码块来重启内核:

    ```
    import IPython
    ```

    ```
    kernel = IPython.Application.instance().kernel
    ```

    ```
    kernel.do_shutdown(True)
    ```

这将产生类似于`{'status': 'ok', 'restart': True}`的输出值，并相应地重启内核，以确保我们在使用刚刚安装的包时不会遇到问题。

1.  让我们使用`transformers`库下载一个预先训练好的模型。我们将下载一个模型，该模型可用于情感分析，并对一个陈述是正面的还是负面的进行分类。运行下面的代码块，将预先训练好的`distilbert`模型的工件下载到当前目录:

    ```
    from transformers import AutoModelForSequenceClassification as AMSC
    ```

    ```
    pretrained = "distilbert-base-uncased-finetuned-sst-2-english"
    ```

    ```
    model = AMSC.from_pretrained(pretrained)
    ```

    ```
    model.save_pretrained(save_directory=".")
    ```

这将在与`.ipynb`笔记本文件相同的目录下生成两个文件:

*   `config.json`
*   `pytorch_model.bin`

注意

这应该如何工作？例如，如果我们有一个“`I love reading the book MLE on AWS!`”语句，训练好的模型应该将其归类为*正*语句。如果我们有一个“`This is the worst spaghetti I've had`”语句，那么经过训练的模型应该将其归类为一个*否定*语句。

1.  使用下面的代码块准备`model.tar.gz`(压缩存档)文件，该文件包含在前面的步骤中生成的模型工件文件:

    ```
    import tarfile
    ```

    ```
    tar = tarfile.open("model.tar.gz", "w:gz")
    ```

    ```
    tar.add("pytorch_model.bin")
    ```

    ```
    tar.add("config.json")
    ```

    ```
    tar.close()
    ```

2.  使用`rm`命令，通过删除前面步骤生成的模型工件来清理模型文件:

    ```
    %%bash
    ```

    ```
    rm pytorch_model.bin
    ```

    ```
    rm config.json
    ```

3.  指定 S3 铲斗的名称和前缀。在运行下面的代码块之前，确保用一个唯一的 S3 桶名替换`<INSERT S3 BUCKET NAME HERE>`的值:

    ```
    s3_bucket = "<INSERT S3 BUCKET NAME HERE>"
    ```

    ```
    prefix = "chapter07"
    ```

确保为尚不存在的 S3 时段指定一个时段名称。如果您想要重用在前面章节中创建的一个 bucket，您可以这样做，但是要确保在安装和配置 SageMaker Studio 的同一个区域中使用 S3 bucket。

1.  使用`aws s3 mb`命令:

    ```
    !aws s3 mb s3://{s3_bucket}
    ```

    创建一个新的 S3 桶

如果您计划重用在前面章节中创建的现有 S3 存储桶之一，可以跳过这一步。

1.  准备上传模型文件的 S3 路径:

    ```
    model_data = "s3://{}/{}/model/model.tar.gz".format(
    ```

    ```
        s3_bucket, prefix
    ```

    ```
    )
    ```

注意，此时，`model.tar.gz`文件还不存在于指定的 S3 路径中。在这里，我们简单地准备了 S3 位置(字符串),在那里`model.tar.gz`文件将被上传。

1.  现在，让我们使用`aws s3 cp`命令将`model.tar.gz`文件复制并上传到 S3 bucket:

    ```
    !aws s3 cp model.tar.gz {model_data}
    ```

2.  使用`%store`魔法存储`model_data`、`s3_bucket`和`prefix` :

    ```
    %store model_data
    ```

    ```
    %store s3_bucket
    ```

    ```
    %store prefix
    ```

    的变量值

这应该允许我们在本章的一个或多个后续章节中使用这些变量值，类似于*图 7.3* 中的内容:

![Figure 7.3 – The %store magic

](img/B18638_07_003.jpg)

图 7.3–商店魔力百分比

确保不要重启内核，否则我们会丢失使用`%store`魔法保存的变量值。

# 准备 SageMaker 脚本模式先决条件

在这一章中，我们将准备一个自定义脚本来使用预先训练好的模型进行预测。在之前，我们可以继续使用 **SageMaker Python SDK** 将我们的预训练模型部署到推理端点，我们需要确保所有脚本模式先决条件都已准备好。

![Figure 7.4 – The desired file and folder structure

](img/B18638_07_004.jpg)

图 7.4–所需的文件和文件夹结构

在*图 7.4* 中，我们可以看到我们需要准备三个先决条件:

*   `inference.py`
*   `requirements.txt`
*   `setup.py`

我们将把这些先决条件存储在`scripts`目录中。我们将在本章的后续章节中详细讨论这些先决条件。事不宜迟，让我们从准备`inference.py`脚本文件开始吧！

## 准备推论. py 文件

在本节中，我们将准备一个定制的 Python 脚本，SageMaker 将在处理推理请求时使用它。在这里，我们可以影响如何反序列化输入请求，如何加载定制模型，如何执行预测步骤，以及如何序列化输出预测并将其作为响应返回。要做到这一切，我们需要在脚本文件中覆盖以下推理处理函数:`model_fn()`、`input_fn()`、`predict_fn()`和`output_fn()`。我们稍后将讨论这些函数是如何工作的。

在下一组步骤中，我们将准备我们的自定义 Python 脚本，并覆盖推理处理器函数的默认实现:

1.  右键单击**文件浏览器**侧边栏窗格中的空白区域，打开类似于*图 7.5* 所示的上下文菜单:

![Figure 7.5 – Creating a new folder inside the CH07 directory

](img/B18638_07_005.jpg)

图 7.5–在 CH07 目录中创建新文件夹

从上下文菜单的可用选项列表中选择**新文件夹**，如图*图 7.5* 中突出显示的。注意，我们也可以按下 **+** 按钮旁边的信封按钮(带加号)来创建一个新文件夹。

1.  将新文件夹命名为`scripts`。
2.  接下来，双击`scripts`文件夹导航到该目录。
3.  点击**文件**菜单，并从**新建**子菜单下的选项列表中选择**文本文件**，创建一个新的文本文件。
4.  右击`inference.py`上的。
5.  点击`inference.py`脚本:

![Figure 7.6 – Getting ready to add code to the inference.py file in the Editor pane

](img/B18638_07_006.jpg)

图 7.6–准备向编辑器窗格中的 inference.py 文件添加代码

我们将把随后的代码块添加到`inference.py`文件中。确保每个代码块后面都有一个额外的空行。

1.  在`inference.py`文件中:

    ```
    import json
    ```

    ```
    from transformers import AutoModelForSequenceClassification as AMSC
    ```

    ```
    from transformers import Trainer
    ```

    ```
    from transformers import TrainingArguments
    ```

    ```
    from torch.nn import functional as F
    ```

    ```
    from transformers import AutoTokenizer
    ```

    ```
    from time import sleep
    ```

2.  指定标记符:

    ```
    TOKENIZER = "distilbert-base-uncased-finetuned-sst-2-english"
    ```

这里，我们为模型指定适当的记号化器，我们将在后面的步骤中使用它来执行预测。

注意

什么是记号赋予器？一个`"I am hungry"`，然后记号赋予器会把它拆分成记号`"I"`、`"am"`和`"`、`hungry"`。请注意，这是一个简化的示例，除了几句话可以解释的内容之外，还有更多需要标记化的内容。更多详情，可以随意查看以下链接:[https://hugging face . co/docs/transformers/main _ classes/tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer)。

1.  定义`model_fn()`功能:

    ```
    def model_fn(model_dir):
    ```

    ```
        model = AMSC.from_pretrained(model_dir)
    ```

    ```
        return model
    ```

这里，我们定义了一个模型函数，它返回一个用于执行预测和处理推理请求的模型对象。由于我们计划加载并使用一个预先训练好的模型，我们使用了来自`transformers`库的`AutoModelForSequenceClassification`的`from_pretrained()`方法来加载指定模型目录中的模型工件。然后`from_pretrained()`方法返回一个可以在预测步骤中使用的模型对象。

1.  现在，我们来定义一下`humanize_prediction()`的功能:

    ```
    def humanize_prediction(output):
    ```

    ```
        class_a, class_b = F.softmax(
    ```

    ```
            output[0][0], 
    ```

    ```
            dim = 0
    ```

    ```
        ).tolist()
    ```

    ```
        prediction = "-"
    ```

    ```
        if class_a > class_b:
    ```

    ```
            prediction = "NEGATIVE"
    ```

    ```
        else:
    ```

    ```
            prediction = "POSITIVE"
    ```

    ```
        return prediction
    ```

`humanize_prediction()`函数简单地在预测步骤中处理输入有效载荷后接受模型产生的原始输出。它向调用函数返回一个`"POSITIVE"`或`"NEGATIVE"`预测。我们将在下一步定义这个*调用函数*。

1.  接下来，让我们使用下面的代码块来定义`predict_fn()`:

    ```
    def predict_fn(input_data, model):
    ```

    ```
        # sleep(30)
    ```

    ```
        sentence = input_data['text']
    ```

    ```
        tokenizer = AutoTokenizer.from_pretrained(
    ```

    ```
            TOKENIZER
    ```

    ```
        )
    ```

    ```
        batch = tokenizer(
    ```

    ```
            [sentence],
    ```

    ```
            padding=True,
    ```

    ```
            truncation=True,
    ```

    ```
            max_length=512,
    ```

    ```
            return_tensors="pt"
    ```

    ```
        )
    ```

    ```
        output = model(**batch)
    ```

    ```
        prediction = humanize_prediction(output)
    ```

    ```
        return prediction
    ```

`predict_fn()`函数接受反串行化的输入请求数据和加载的模型作为输入。然后，它使用这两个参数值来产生预测。怎么会？由于加载的模型作为第二个参数可用，我们简单地使用它来执行预测。这个预测步骤的输入有效负载是反序列化的请求数据，它可以作为`predict_fn()`函数的第一个参数。在输出返回之前，我们利用`humanize_prediction()`函数将原始输出转换为`"POSITIVE"`或`"NEGATIVE"`。

注意

为什么我们有一个包含`sleep(30)`的评论行？稍后在*将预训练模型部署到异步推理端点部分*中，我们将使用人工的 30 秒延迟来模拟具有相对较长处理时间的推理端点。现在，我们将保留这一行的注释，我们将在这一部分的后面撤销它。

1.  让我们也定义一下`input_fn()`函数，它用于将序列化的输入请求数据转换成反序列化的形式。这种反串行化的形式将在稍后阶段用于预测:

    ```
    def input_fn(serialized_input_data, 
    ```

    ```
                 content_type='application/json'):
    ```

    ```
        if content_type == 'application/json':
    ```

    ```
            input_data = json.loads(serialized_input_data)
    ```

    ```
            return input_data
    ```

    ```
        else:
    ```

    ```
            raise Exception('Unsupported Content Type')
    ```

在`input_fn()`函数中，我们还通过为不支持的内容类型引发`Exception`来确保指定的内容类型在我们定义的支持内容类型列表中。

1.  最后，我们来定义一下`output_fn()`:

    ```
    def output_fn(prediction_output, 
    ```

    ```
                  accept='application/json'):
    ```

    ```
        if accept == 'application/json':
    ```

    ```
            return json.dumps(prediction_output), accept
    ```

    ```
        raise Exception('Unsupported Content Type')
    ```

`output_fn()`的作用是将预测结果序列化为指定的内容类型。这里，我们还通过为不支持的内容类型引发`Exception`来确保指定的内容类型在我们定义的支持内容类型列表中。

注意

我们可以将*序列化*和*反序列化*视为将数据从一种形式转换为另一种形式的数据转换步骤。例如，输入请求数据可以作为有效的 JSON *字符串*传递给推理端点。该输入请求数据通过`input_fn()`函数，该函数将其转换为 *JSON* 或*字典*。这个反序列化的值然后作为有效载荷传递给`predict_fn()`函数。此后，`predict_fn()`函数返回一个预测作为结果。然后使用`output_fn()`函数将这个结果转换成指定的内容类型。

1.  通过按下 *CTRL* + *S* 保存更改。

注意

如果你用的是 Mac，用 *CMD* + *S* 代替。或者，你可以点击**文件**菜单下选项列表下的**保存 Python 文件**。

此时，您可能想知道所有这些是如何组合在一起的。为了帮助我们理解推理处理函数如何与数据交互以及如何相互交互，让我们快速查看一下*图 7.7* 中所示的图表:

![Figure 7.7 – The inference handler functions

](img/B18638_07_007.jpg)

图 7.7-推理处理器功能

在*图 7.7* 中，我们可以看到`model_fn()`函数用于加载 ML 模型对象。一旦有请求进来，这个模型对象将被`predict_fn()`函数用来执行预测。当请求进来时，`input_fn()`函数处理序列化的请求数据，并将其转换成反序列化的形式。这个反序列化的请求数据然后被传递给`predict_fn()`函数，该函数然后使用加载的 ML 模型来执行预测，使用请求数据作为有效载荷。然后，`predict_fn()`函数返回输出预测，由`output_fn()`函数序列化。

注意

关于这个话题的更多信息，可以随意查看以下链接:[https://sagemaker . readthe docs . io/en/stable/frameworks/py torch/using _ py torch . XHTML](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.xhtml#serve-a-pytorch-model)。

现在我们已经准备好了推理脚本，让我们继续准备下一部分的`requirements.txt`文件！

## 准备 requirements.txt 文件

由于`transformers`包没有包含在 SageMaker PyTorch Docker 容器中，我们需要通过一个`requirements.txt`文件来包含它，SageMaker 使用这个文件在运行时安装额外的包。如果这是您第一次处理一个`requirements.txt`文件，它只是一个文本文件，包含一个要使用`pip install`命令安装的软件包列表。如果您的`requirements.txt`文件包含一行(例如，`transformers==4.4.2`)，那么这将在安装步骤中映射到一个`pip install transformers==4.4.2`。如果`requirements.txt`文件包含多行，那么将使用`pip install`命令安装列出的每个包。

注意

我们可以选择使用`==` (equal)将*列出的包和依赖项固定到一个特定的版本。或者，我们也可以使用`<`(小于)、`>`(大于)和其他变体来管理要安装的包的版本号的上限值和下限值。*

在下一组步骤中，我们将在`scripts`目录中创建并准备`requirements.txt`文件:

1.  点击**文件**菜单，并从**新建**子菜单下的选项列表中选择**文本文件**，创建一个新的文本文件:

![Figure 7.8 – Creating a new text file inside the scripts directory

](img/B18638_07_008.jpg)

图 7.8–在脚本目录中创建新的文本文件

在*图 7.8* 中，我们可以看到我们在`scripts`目录(`scripts`目录)。

1.  重命名文件`requirements.txt`
2.  在`requirements.txt`文件中进行如下操作:

    ```
    transformers==4.4.2
    ```

3.  确保通过按下 *CTRL* + *S* 保存更改。

注意

如果您使用的是 Mac，请使用 *CMD* + *S* 来代替。或者，你可以点击**文件**菜单下选项列表下的**保存 Python 文件**。

那不是很容易吗？现在让我们继续最后一个先决条件——`setup.py`文件。

## 准备 setup.py 文件

除了`requirements.txt`文件，我们还将准备一个`setup.py`文件，它将包含一些附加信息和元数据。

注意

我们不会深究`requirements.txt`和`setup.py`文件用法的区别。请随意查看以下链接了解更多信息:[https://docs.python.org/3/distutils/setupscript.xhtml](https://docs.python.org/3/distutils/setupscript.xhtml)。

在下一组步骤中，我们将在`scripts`目录中创建并准备`setup.py`文件:

1.  使用与上一节相同的步骤，创建一个新的文本文件，并将其重命名为`setup.py`。确保该文件与`inference.py`和`requirements.txt`文件在同一个目录下(`scripts`)。
2.  更新`setup.py`文件的内容，以包含以下代码块:

    ```
    from setuptools import setup, find_packages
    ```

    ```
    setup(name='distillbert',
    ```

    ```
          version='1.0',
    ```

    ```
          description='distillbert',
    ```

    ```
          packages=find_packages(
    ```

    ```
              exclude=('tests', 'docs')
    ```

    ```
         ))
    ```

设置脚本简单地利用`setup()`函数来描述模块分布。这里，我们在调用`setup()`函数时指定了`name`、`version`和`description`等元数据。

1.  最后，确保通过按下 *CTRL* + *S* 保存更改。

注意

如果您使用的是 Mac，请改用 *CMD* + *S* 。或者，您可以点击**文件**菜单下选项列表中的**保存 Python 文件**。

至此，我们已经准备好了运行本章所有后续章节所需的所有先决条件。至此，让我们在下一节继续将我们的预训练模型部署到实时推理端点！

# 将预训练的模型部署到实时推理端点

在这一节中，我们将使用 SageMaker Python SDK 将一个预先训练好的模型部署到一个实时推理端点。从名称本身，我们可以看出实时推理端点可以处理输入负载并实时执行预测。如果您之前已经构建了一个 API 端点(例如，它可以处理 GET 和 POST 请求)，那么我们可以将推理端点视为接受输入请求并作为响应的一部分返回预测的 API 端点。预测是如何做出的？推理端点只是将模型加载到内存中，并使用它来处理输入负载。这将产生一个作为响应返回的输出。例如，如果我们在实时推理端点中部署了预先训练的情感分析 ML 模型，那么它将根据请求中提供的输入字符串有效负载返回响应`"POSITIVE"`或`"` `NEGATIVE"`。

注意

假设我们的推理端点通过 POST 请求接收到语句`"I love reading the book MLE on AWS!"`。然后，推理端点将处理请求输入数据，并使用 ML 模型进行推理。ML 模型推断步骤的结果(例如，代表一个`"POSITIVE"`结果的数值)将作为响应的一部分返回。

![Figure 7.9 – The desired file and folder structure

](img/B18638_07_009.jpg)

图 7.9–所需的文件和文件夹结构

为了让它工作，我们只需要确保在使用 SageMaker Python SDK 准备实时推理端点之前，包括推理脚本文件(例如`inference.py`)和`requirements.txt`文件在内的先决条件文件已经准备好。在继续本节中的动手解决方案之前，确保检查并查看图 7.9 中的文件夹结构。

在下一组步骤中，我们将使用 SageMaker Python SDK 将我们的预训练模型部署到实时推理端点:

1.  并使用`Data Science`映像创建一个新的笔记本。重命名笔记本`02 - Deploying a real-time inference endpoint.ipynb`。

注意

新笔记本应该在`01 - Prepare model.tar.gz file.ipynb`旁边，类似于*图 7.9* 所示。

1.  让我们在新笔记本的第一个单元格中运行下面的代码块:

    ```
    %store -r model_data
    ```

    ```
    %store -r s3_bucket
    ```

    ```
    %store -r prefix
    ```

这里，我们使用`%store`魔法来加载`model_data`、`s3_bucket`和`prefix`的变量值。

1.  接下来，让我们准备好供 SageMaker 使用的 IAM 执行角色:

    ```
    from sagemaker import get_execution_role 
    ```

    ```
    role = get_execution_role()
    ```

2.  初始化`PyTorchModel`对象:

    ```
    from sagemaker.pytorch.model import PyTorchModel
    ```

    T12

    ```
        model_data=model_data, 
    ```

    T14

    ```
        source_dir="scripts",
    ```

    T16

    ```
        framework_version='1.6.0',
    ```

    T18

    ```
    )
    ```

让我们来看看图 7.10 来帮助我们直观地了解前面的代码块中发生了什么:

![Figure 7.10 – Deploying a real-time inference endpoint

](img/B18638_07_010.jpg)

图 7.10–部署实时推理端点

在*图 7.10* 中，我们可以看到我们在初始化步骤中通过传递几个配置参数初始化了一个`Model`对象:(1)模型数据，(2)框架版本，(3)到`inference.py`脚本文件的路径。我们还可以设置其他参数，但我们会将事情简化一点，集中在这三个参数上。为了让 SageMaker 知道如何使用预先训练好的模型进行推理，`inference.py`脚本文件应该包含定制逻辑，它加载 ML 模型并使用它来执行预测。

注意

值得注意的是，我们并不局限于命名推理脚本文件`inference.py`。我们可以使用不同的命名约定，只要我们指定正确的`entry_point`值。

如果我们在部署 ML 模型时使用 SageMaker 的脚本模式，就会出现这种情况。请注意，还有其他可用的选项，例如使用自定义容器图像，而不是传递脚本，我们将传递我们提前准备好的容器图像。当部署使用 SageMaker 的**内置算法**训练的 ML 模型时，我们可以立即着手部署这些模型，而无需任何定制脚本或容器映像，因为 SageMaker 已经提供了部署所需的所有先决条件。

1.  使用`deploy()`方法将模型部署到实时推理端点:

    ```
    %%time
    ```

    ```
    from sagemaker.serializers import JSONSerializer
    ```

    ```
    from sagemaker.deserializers import JSONDeserializer
    ```

    ```
    predictor = model.deploy(
    ```

    ```
        instance_type='ml.m5.xlarge', 
    ```

    ```
        initial_instance_count=1,
    ```

    ```
        serializer=JSONSerializer(),
    ```

    ```
        deserializer=JSONDeserializer()
    ```

    ```
    )
    ```

完成此步骤大约需要 3 到 8 分钟。

注意

当使用`deploy()`方法通过 SageMaker Python SDK 部署 ML 模型时，我们可以指定实例类型。为模型选择正确的实例类型是很重要的，并且在成本和性能之间找到最佳平衡不是一个简单的过程。有许多实例类型和大小可供选择，ML 工程师在托管服务的 SageMaker 中部署模型时，最终可能会有一个次优的设置。好消息是 SageMaker 有一个名为 **SageMaker 推理推荐器**的功能，它可以帮助您决定使用哪个实例类型。更多信息可以查看以下链接:[https://docs . AWS . Amazon . com/sage maker/latest/DG/inference-recommender . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml)。

1.  现在我们的实时推理端点正在运行，让我们使用`predict()`方法执行一个样本预测:

    ```
    payload = {
    ```

    ```
        "text": "I love reading the book MLE on AWS!"
    ```

    ```
    }
    ```

    ```
    predictor.predict(payload)
    ```

这将产生一个输出值`'POSITIVE'`。

1.  我们再来测试一个负面场景:

    ```
    payload = {
    ```

    ```
        "text": "This is the worst spaghetti I've had"
    ```

    ```
    }
    ```

    ```
    predictor.predict(payload)
    ```

这将产生一个`'NEGATIVE'`的输出值。在下一步删除端点之前，请随意测试不同的值。

1.  最后，让我们使用`delete_endpoint()`方法删除推理端点:

    ```
    predictor.delete_endpoint()
    ```

这将有助于我们避免任何未使用的推理端点的意外费用。

那不是很容易吗？使用 SageMaker Python SDK 将预训练模型部署到实时推理端点(在具有指定实例类型的 ML 实例中)是如此简单！许多工程工作已经为我们自动化了，我们需要做的就是调用`Model`对象的`deploy()`方法。

# 将预训练模型部署到无服务器推理端点

在本书的前几章中，我们已经使用了几种无服务器服务来管理和降低成本。如果您想知道在 SageMaker 中部署 ML 模型时是否有无服务器选项，那么答案将是肯定的。当您处理间歇性和不可预测的流量时，使用无服务器推理端点来托管 ML 模型可能是一个更具成本效益的选择。假设我们可以容忍**冷启动**(在一段时间的不活动后，请求需要更长的时间来处理)，并且我们每天只希望有几个请求——然后，我们可以利用无服务器推理端点来代替实时选项。当我们可以最大化推理端点时，实时推理端点是最好的。如果您希望您的端点大部分时间都得到利用，那么实时选项可能会达到目的。

![Figure 7.11 – The desired file and folder structure

](img/B18638_07_011.jpg)

图 7.11–所需的文件和文件夹结构

使用 SageMaker Python SDK 将预训练的 ML 模型部署到无服务器推理端点类似于为实时推理端点所做的。唯一的主要区别如下:

*   `ServerlessInferenceConfig`对象的初始化
*   当调用`Model`对象的`deploy()`方法时，将该对象作为参数传递

在下一组步骤中，我们将使用 SageMaker Python SDK 将我们预先训练的模型部署到一个无服务器推理端点:

1.  并使用`Data Science`图像创建一个新的笔记本。重命名笔记本`03 - Deploying a serverless inference endpoint.ipynb`。

注意

新笔记本应该在`01 - Prepare model.tar.gz file.ipynb`旁边，类似于*图 7.11* 所示。

1.  在新笔记本的第一个单元格中，让我们运行下面的代码块来加载`model_data`、`s3_bucket`和`prefix`的变量值:

    ```
    %store -r model_data
    ```

    、

    ```
    %store -r s3_bucket
    ```

    、

    ```
    %store -r prefix
    ```

如果您在运行这段代码时遇到错误，请确保您已经完成了本章*准备预训练模型工件*一节中指定的步骤。

1.  准备 SageMaker 使用的 IAM 执行角色:

    ```
    from sagemaker import get_execution_role 
    ```

    ```
    role = get_execution_role()
    ```

2.  初始化并配置`ServerlessInferenceConfig`对象:

    ```
    from sagemaker.serverless import ServerlessInferenceConfig
    ```

    ```
    serverless_config = ServerlessInferenceConfig(
    ```

    ```
      memory_size_in_mb=4096,
    ```

    ```
      max_concurrency=5,
    ```

    ```
    )
    ```

3.  初始化`PyTorchModel`对象，使用`deploy()`方法将模型部署到一个无服务器的推理端点:

    ```
    from sagemaker.pytorch.model import PyTorchModel
    ```

    ```
    from sagemaker.serializers import JSONSerializer
    ```

    ```
    from sagemaker.deserializers import JSONDeserializer
    ```

    ```
    model = PyTorchModel(
    ```

    ```
        model_data=model_data, 
    ```

    ```
        role=role, 
    ```

    ```
        source_dir="scripts",
    ```

    ```
        entry_point='inference.py', 
    ```

    ```
        framework_version='1.6.0',
    ```

    ```
        py_version="py3"
    ```

    ```
    )
    ```

    ```
    predictor = model.deploy(
    ```

    ```
        instance_type='ml.m5.xlarge', 
    ```

    ```
        initial_instance_count=1,
    ```

    ```
        serializer=JSONSerializer(),
    ```

    ```
        deserializer=JSONDeserializer(),
    ```

    ```
        serverless_inference_config=serverless_config
    ```

    ```
    )
    ```

注意

完成模型部署大约需要 3 到 8 分钟。

1.  现在我们的实时推理端点正在运行，让我们使用`predict()`方法执行一个样本预测:

    ```
    payload = {
    ```

    ```
        "text": "I love reading the book MLE on AWS!"
    ```

    ```
    }
    ```

    ```
    predictor.predict(payload)
    ```

这将产生输出值`'POSITIVE'`。

1.  让我们也测试一个负面的场景:

    ```
    payload = {
    ```

    ```
        "text": "This is the worst spaghetti I've had"
    ```

    ```
    }
    ```

    ```
    predictor.predict(payload)
    ```

这将产生一个输出值`'NEGATIVE'`。在下一步删除端点之前，请随意测试不同的值。

1.  最后，让我们使用`delete_endpoint()`方法删除推理端点:

    ```
    predictor.delete_endpoint()
    ```

这将有助于我们避免任何未使用的推理端点的意外费用。

正如您所看到的，除了`ServerlessInferenceConfig`对象的初始化和用法之外，一切都几乎相同。使用无服务器端点时，SageMaker 为我们管理计算资源，并自动执行以下操作:

*   自动分配与我们在初始化`ServerlessInferenceConfig`时指定的`memory_size_in_mb`参数值成比例的计算资源
*   使用配置的最大并发值来管理可以同时发生多少个并发调用
*   如果没有请求，资源会自动缩减到零

一旦你看到更多关于如何使用 SageMaker Python SDK 的例子，你就会开始意识到这个 SDK 的设计和实现有多好。

# 将预训练的模型部署到异步推理端点

除了实时和无服务器推理端点，SageMaker 在部署模型时还提供了第三种选择—**异步推理端点**。为什么叫异步？首先，不是期望结果立即可用，而是请求被排队，并且结果异步可用*。这适用于涉及以下一项或多项的 ML 要求:*

 **   大型输入负载(高达 1 GB)
*   预测处理持续时间长(长达 15 分钟)

异步推理端点的一个很好的用例是用于检测大型视频文件中的对象的 ML 模型(可能需要 60 多秒才能完成)。在这种情况下，一个推论可能需要几分钟而不是几秒钟。

*我们如何使用异步推理端点？*为了调用异步推理端点，我们执行以下操作:

1.  请求负载被上传到亚马逊 S3 存储桶。
2.  当调用`AsyncPredictor`对象的`predict_async()`方法(映射或表示 ML 推理端点)时，S3 路径或位置(存储请求有效负载的位置)被用作参数值。
3.  在调用端点时，异步推断端点将请求排队等待处理(一旦端点可以处理)。
4.  处理完请求后，输出推理结果被存储并上传到输出 S3 位置。
5.  发送 SNS 通知(例如，成功或错误通知)(如果已设置)。

在这一节中，我们将把 NLP 模型部署到一个异步推理端点。为了模拟延迟，我们将在推理脚本中调用`sleep()`函数，以便预测步骤比通常花费更长的时间。一旦我们可以让这个相对简单的设置工作起来，处理更复杂的需求，比如视频文件的对象检测，肯定会更容易。

![Figure 7.12 – The file and folder structure

](img/B18638_07_012.jpg)

图 7.12–文件和文件夹结构

为了让这个设置工作，我们将需要准备一个包含的文件，一个类似于*图 7.12* 所示的输入有效载荷(例如，*数据*或`input.json`)。一旦输入文件准备好了，我们将把它上传到一个亚马逊 S3 桶，然后继续把我们预先训练好的 ML 模型部署到一个异步推理端点。

记住这一点，让我们继续创建输入 JSON 文件！

## 创建输入 JSON 文件

在下一组步骤中，我们将创建一个包含输入 JSON 值的样本文件，该文件将在下一节调用异步推理端点时使用:

1.  右键单击**文件浏览器**侧边栏窗格中的空白区域，打开类似于*图 7.13* 所示的上下文菜单:

![Figure 7.13 – Creating a new folder

](img/B18638_07_013.jpg)

图 7.13–创建新文件夹

在执行该步骤之前，确保在**文件浏览器**中的`CH07`目录中。

1.  重命名文件夹`data`。
2.  双击**文件浏览器**侧边栏窗格中的`data`文件夹，导航至该目录。
3.  点击**文件**菜单，并从**新建**子菜单下的选项列表中选择**文本文件**，创建一个新的文本文件:

![Figure 7.14 – Creating a new text file

](img/B18638_07_014.jpg)

图 7.14–创建新的文本文件

当创建一个新的文本文件时，确保你在`data`目录中，类似于*图 7.14* 。

1.  重命名文件`input.json`，如图*图 7.15* :

![Figure 7.15 – Renaming the text file

](img/B18638_07_015.jpg)

图 7.15–重命名文本文件

要重命名`untitled.txt`文件，右击`input.json`中的文件，替换默认的名称值。

1.  在具有以下 JSON 值的`input.json`文件中:

    ```
    {"text": "I love reading the book MLE on AWS!"}
    ```

2.  确保通过按下 *CTRL* + *S* 保存您的更改。

注意

如果你用的是 Mac，用 *CMD* + *S* 代替。或者，您可以点击**文件**菜单下选项列表下的**保存 Python 文件**。

同样，输入文件仅在我们计划将 ML 模型部署到异步推理端点时才需要。有了这些准备，我们现在可以进行下一组步骤。

## 向推理脚本添加人工延迟

在使用 SageMaker Python SDK 将我们的预训练模型部署到异步推理端点之前，我们将向预测步骤添加一个人工延迟。这将帮助我们模拟需要一点时间才能完成的推断或预测请求。

注意

当对异步推理端点进行故障排除时，您可以选择先测试一个在几秒钟内执行预测的 ML 模型。这将有助于您立即知道是否有问题，因为输出预计将在几秒钟内上传到 S3 输出路径(而不是几分钟)。也就是说，如果您在让设置工作时遇到问题，您可能需要暂时消除人为延迟。

在下一组步骤中，我们将更新`inference.py`脚本，在执行预测时添加 30 秒的延迟:

1.  继续上一节我们离开的地方，让我们在**文件浏览器**中导航到`CH07`目录:

![Figure 7.16 – Navigating to the CH07 directory

](img/B18638_07_016.jpg)

图 7.16–导航至 CH07 目录

在这里，我们单击`CH07`链接，如图 7.16 中的*所示。*

1.  双击`scripts`文件夹，如图*图 7.17* 所示，导航到目录:

![Figure 7.17 – Navigating to the scripts directory

](img/B18638_07_017.jpg)

图 7.17–导航到脚本目录

在继续下一步之前，确保您已经完成了*准备 SageMaker 脚本模式先决条件*一节中的实际操作步骤。`scripts`目录应该包含三个文件:

*   `inference.py`
*   `requirements.txt`
*   `setup.py`

1.  双击打开`inference.py`文件，如图*图 7.18* 中高亮显示。找到`predict_fn()`函数，取消包含`sleep(30)`的代码行的注释:

![](img/B18638_07_018.jpg)

图 7.18-更新推论. py 文件

要取消代码行的注释，只需删除`sleep(30)`前的散列和空格(`#` ，类似于我们在*图 7.18* 中看到的。

1.  确保通过按下 *CTRL* + *S* 保存更改。

注意

如果您使用的是 Mac，请改用 *CMD* + *S* 。或者，你可以点击**文件**菜单下选项列表下的**保存 Python 文件**。

既然我们已经添加了一个人工的 30 秒延迟，让我们继续使用 SageMaker Python SDK 来部署我们的异步推理端点。

## 部署和测试异步推理端点

使用 SageMaker Python SDK 将预训练的 ML 模型部署到异步推理端点类似于对实时和无服务器推理端点的部署。唯一的主要区别是(1)初始化`AsyncInferenceConfig`对象，以及(2)在调用`Model`对象的`deploy()`方法时将该对象作为参数传递。

在下一组步骤中，我们将使用 SageMaker Python SDK 将我们的预训练模型部署到异步推理端点:

1.  继续我们在*中停止的向推理脚本部分*添加人工延迟，让我们导航到`Data Science`图像中的`CH07`目录。重命名笔记本`04 - Deploying an asynchronous inference endpoint.ipynb`。

注意

新笔记本应该在`01 - Prepare model.tar.gz file.ipynb`旁边。

1.  在新笔记本的第一个单元格中，让我们运行下面的代码块来加载`model_data`、`s3_bucket`和`prefix`的变量值:

    ```
    %store -r model_data
    ```

    、

    ```
    %store -r s3_bucket
    ```

    、

    ```
    %store -r prefix
    ```

如果您在运行这段代码时遇到错误，请确保您已经完成了本章*准备预训练模型工件*一节中指定的步骤。

1.  准备好上传推理输入文件的路径:

    ```
    input_data = "s3://{}/{}/data/input.json".format(
    ```

    ```
        s3_bucket,
    ```

    ```
        prefix
    ```

    ```
    )
    ```

2.  使用`aws s3 cp`命令:

    ```
    !aws s3 cp data/input.json {input_data}
    ```

    将`input.json`文件上传到 S3 存储桶
3.  准备 IAM 执行角色供 SageMaker

    ```
    from sagemaker import get_execution_role 
    ```

    ```
    role = get_execution_role()
    ```

    使用
4.  初始化`AsyncInferenceConfig`对象:

    ```
    from sagemaker.async_inference import AsyncInferenceConfig
    ```

    ```
    output_path = f"s3://{s3_bucket}/{prefix}/output"
    ```

    ```
    async_config = AsyncInferenceConfig(
    ```

    ```
        output_path=output_path
    ```

    ```
    )
    ```

在初始化`AsyncInferenceConfig`对象时，我们为将保存结果的`output_path`参数指定值。

1.  接下来，让我们初始化`PyTorchModel`对象:

    ```
    from sagemaker.pytorch.model import PyTorchModel
    ```

    ```
    model = PyTorchModel(
    ```

    ```
        model_data=model_data, 
    ```

    ```
        role=role, 
    ```

    ```
        source_dir="scripts",
    ```

    ```
        entry_point='inference.py', 
    ```

    ```
        framework_version='1.6.0',
    ```

    ```
        py_version="py3"
    ```

    ```
    )
    ```

这里我们指定参数的配置值，如`model_data`、`role`、`source_dir`、`entry_point`、`framework_version`、`py_version`。

1.  使用`deploy()`方法将模型部署到异步推理端点:

    ```
    %%time
    ```

    ```
    from sagemaker.serializers import JSONSerializer
    ```

    ```
    from sagemaker.deserializers import JSONDeserializer
    ```

    ```
    predictor = model.deploy(
    ```

    ```
        instance_type='ml.m5.xlarge', 
    ```

    ```
        initial_instance_count=1,
    ```

    ```
        serializer=JSONSerializer(),
    ```

    ```
        deserializer=JSONDeserializer(),
    ```

    ```
        async_inference_config=async_config
    ```

    ```
    )
    ```

这里，我们将在上一步中启动的`AsyncInferenceConfig`对象指定为`async_inference_config`的参数值。

![Figure 7.19 – Deploying an asynchronous inference endpoint

](img/B18638_07_019.jpg)

图 7.19–部署异步推理端点

在*图 7.19* 中，我们可以看到`deploy()`方法接受参数值为 SageMaker 配置异步推理端点，而不是实时推理端点。

注意

完成模型部署大约需要 3 到 8 分钟。

1.  一旦推断端点准备就绪，让我们使用`predict_async()`方法来执行预测:

    ```
    response = predictor.predict_async(
    ```

    ```
        input_path=input_data
    ```

    ```
    )
    ```

这应该使用存储在 S3 的`input.json`文件中的数据调用异步推理端点。

![Figure 7.20 – How the predict_async() method works

](img/B18638_07_020.jpg)

图 7.20–predict _ async()方法如何工作

在*图 7.20* 中，我们可以看到异步推理端点的输入负载来自 S3 桶。然后，在端点处理请求之后，输出被保存到 S3。如果您的输入负载很小(例如，小于 *1 MB* )，这可能没有任何意义。然而，如果输入负载涉及更大的文件，比如视频文件，那么将它上传到 S3 并利用异步推断端点进行预测会更有意义。

1.  使用`sleep()`函数等待 40 秒，然后调用`response`对象的`get_result()`函数:

    ```
    from time import sleep
    ```

    ```
    sleep(40)
    ```

    ```
    response.get_result()
    ```

这将产生一个输出值`'POSITIVE'`。

注意

为什么要等 40 秒？由于我们在预测步骤中人为添加了 30 秒的延迟，因此在输出文件在指定的 S3 位置可用之前，我们必须至少等待 30 秒。

1.  将 S3 路径字符串值存储在`output_path`变量中:

    ```
    output_path = response.output_path
    ```

2.  使用`aws s3 cp`命令将输出文件的副本下载到 Studio 笔记本实例:

    ```
    !aws s3 cp {output_path} sample.out
    ```

3.  现在我们已经下载了输出文件，让我们使用`cat`命令来检查它的内容:

    ```
    !cat sample.out
    ```

这应该给我们一个输出值`'POSITIVE'`，类似于我们在前面的步骤中使用`get_result()`方法后得到的结果。

1.  让我们通过使用`rm`命令:

    ```
    !rm sample.out
    ```

    删除输出文件的副本来做一个快速清理
2.  最后，让我们使用`delete_endpoint()`方法删除推理端点:

    ```
    predictor.delete_endpoint()
    ```

这将有助于我们避免任何未使用的推理端点的意外费用。

值得注意的是，在生产设置中，最好更新架构使其更受事件驱动，并且在初始化`AsyncInferenceConfig`对象时，必须使用适当的值字典更新`notification_config`参数值。更多信息，可以随意查看以下链接:[https://sagemaker . readthe docs . io/en/stable/overview . XHTML # sagemaker-asynchronous-inference](https://sagemaker.readthedocs.io/en/stable/overview.xhtml#sagemaker-asynchronous-inference)。

注意

SNS 是什么？SNS 是一个完全托管的消息服务，它允许架构是事件驱动的。来自一个源(*发布者*)的消息可以散开并被发送到各种接收者(*订阅者*)。如果我们要配置 SageMaker 异步推断端点来将通知消息推送到 SNS，那么最好也注册并设置一个订阅者，在预测步骤完成后等待成功(或错误)通知消息。一旦结果可用，该订户就继续执行预定义的操作。

# 清理

既然我们已经完成了本章的动手解决方案的工作，是时候清理并关闭我们不再使用的任何资源了。在下一组步骤中，我们将在 SageMaker Studio 中找到并关闭任何剩余的运行实例:

1.  点击侧边栏中的**运行实例和内核**图标，如图*图 7.21* 所示:

![Figure 7.21 – Turning off the running instance

](img/B18638_07_021.jpg)

图 7.21–关闭正在运行的实例

点击**运行实例和内核**图标将打开并显示 SageMaker Studio 中的运行实例、应用和终端。

1.  点击*图 7.21* 中高亮显示的**关闭**按钮，关闭**运行实例**下的所有运行实例。点击**关闭**按钮将打开一个弹出窗口，确认实例关闭操作。点击**关闭所有**按钮继续。
2.  确保检查并删除 **SageMaker 资源**下所有正在运行的推理端点(如果有):

![Figure 7.22 – Checking the list of running inference endpoints

](img/B18638_07_022.jpg)

图 7.22–检查正在运行的推理端点列表

要检查是否有正在运行的推理端点，点击*图 7.22* 中高亮显示的 **SageMaker 资源**图标，然后从下拉菜单的选项列表中选择**端点**。

需要注意的是，这个清理操作需要在使用 SageMaker Studio 之后执行。即使在不活动期间，SageMaker 也不会自动关闭这些资源。

注意

如果您正在寻找在 SageMaker 中运行 ML 工作负载时降低成本的其他方法，您可以查看如何利用其他特性和功能，例如 **SageMaker 节约计划**(这有助于降低成本，以换取 1 年或 3 年期的一致使用承诺)**sage maker Neo**(这有助于优化 ML 模型以进行部署，加快推理并降低成本)，以及 **SageMaker 推理推荐器**(这有助于您选择最佳实例我们不会在本书中进一步详细讨论这些，所以请随意查看下面的链接，了解有关这些主题的更多信息:[https://docs . AWS . Amazon . com/sage maker/latest/DG/inference-cost-optimization . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-cost-optimization.xhtml)。

# 部署策略和最佳实践

在本节中，我们将讨论使用 SageMaker 托管服务时的相关部署策略和最佳实践。让我们从讨论调用现有 SageMaker 推理端点的不同方式开始。到目前为止，我们使用的解决方案包括使用 SageMaker Python SDK 来调用现有端点:

```
from sagemaker.predictor import Predictor

from sagemaker.serializers import JSONSerializer

from sagemaker.deserializers import JSONDeserializer

endpoint_name = "<INSERT NAME OF EXISTING ENDPOINT>"

predictor = Predictor(endpoint_name=endpoint_name)

predictor.serializer = JSONSerializer() 

predictor.deserializer = JSONDeserializer()

payload = {

^  "text": "I love reading the book MLE on AWS!"

}

predictor.predict(payload)
```

这里，我们初始化一个`Predictor`对象，并在初始化步骤中将它指向一个现有的推理端点。然后我们使用这个`Predictor`对象的`predict()`方法来调用推理端点。

注意，我们还可以使用 **boto3** 库调用同一个端点，类似于下面的代码块所示:

```
import boto3 

import json

endpoint_name = "<INSERT NAME OF EXISTING ENDPOINT>"

runtime = boto3.Session().client('sagemaker-runtime')

payload = {

    "text": "I love reading the book MLE on AWS!"

}

response = sagemaker_client.invoke_endpoint(

    EndpointName=endpoint_name, 

    ContentType='application/json', 

    Body=json.dumps(payload)

)

json.loads(response['Body'].read().decode('utf-8'))
```

这里，当使用现有的 ML 推断端点执行预测和推断时，我们使用`invoke_endpoint()`方法。如您所见，即使没有安装 SageMaker Python SDK，我们也应该能够使用`InvokeEndpoint` API 从一个`POST`请求中调用一个现有的 ML 推理端点。

注意

如果您的后端应用程序代码使用了 Python 之外的语言(例如，Ruby、Java 或 JavaScript)，那么您需要做的就是查找该语言的现有 SDK 以及要调用的相应函数或方法。要了解更多信息，您可以查看以下包含不同工具的链接，以及每种语言可用的 SDK:[https://aws.amazon.com/tools/](https://aws.amazon.com/tools/)。

如果您想准备一个 HTTP API 来调用现有的 SageMaker 推理端点并与之交互，那么有几种可能的解决方案。以下是可能解决方案的快速列表:

*   *选项 1* : *亚马逊 API 网关 HTTP API + AWS Lambda 函数+ boto3 + SageMaker ML 推理端点*—`boto3`库调用sage maker ML 推理端点。
*   *选项 2* : *AWS Lambda 函数+ boto3 + SageMaker ML 推理端点(Lambda 函数 URL)*–直接从 Lambda 函数 URL(是触发 Lambda 函数的专用端点)调用 AWS Lambda 函数。AWS Lambda 函数然后使用`boto3`库来调用 SageMaker ML 推理端点。
*   *选项 3* : *亚马逊 API 网关 HTTP API + SageMaker ML 推理端点(API 网关映射模板)*–亚马逊 API 网关 HTTP API 接收HTTP 请求，直接使用 **API 网关映射模板**(不使用 Lambda 函数)调用 SageMaker ML 推理端点。
*   *选项 4* : *在 EC2 实例+ boto3 + SageMaker ML 推断端点*内使用 web 框架(例如 Flask 或 Django)定制基于容器的 web 应用程序——web 应用程序(在`boto3`库中的容器内运行以调用 SageMaker ML 推断端点。
*   *选项 5* : *在弹性容器服务(ECS) + boto3 + SageMaker ML 推断端点*内使用 web 框架(例如 Flask 或 Django)定制基于容器的 web 应用程序——web 应用程序(在容器内运行，使用`boto3`库调用 SageMaker ML 推断端点。
*   *选项 6* : *自定义基于容器的 web 应用程序，使用 web 框架(例如，Flask 或 Django)和弹性 Kubernetes 服务(EKS) + boto3 + SageMaker ML 推理端点*——web 应用程序(运行在`boto3`库内，调用 SageMaker ML 推理端点。
*   *选项 7*:*AWS app sync(graph QL API)+AWS Lambda 函数+ boto3 + SageMaker ML 推理端点*—`boto3`库调用 SageMaker ML 推理端点。

注意，这不是一个详尽的列表，肯定还有其他方法来设置 HTTP API 调用现有的 SageMaker 推断端点。当然，也有一些场景，我们希望从另一个 AWS 服务资源中直接调用现有的推理端点。这意味着我们不再需要准备一个单独的 HTTP API 作为两个服务之间的中间人。

重要的是要注意我们也可以直接从 **Amazon Aurora** 、 **Amazon Athena** 、 **Amazon Quicksight** 或 **Amazon Redshift** 调用 SageMaker 推断端点。在 [*第四章*](B18638_04.xhtml#_idTextAnchor079) ，*AWS 上的无服务器数据管理*中，我们使用红移和 Athena 来查询我们的数据。除了使用这些服务已经可用的数据库查询之外，我们可以使用类似于以下代码块中的语法直接执行 ML 推断(Athena 的示例查询):

```
USING EXTERNAL FUNCTION function_name(value INT)

RETURNS DOUBLE

SAGEMAKER '<INSERT EXISTING ENDPOINT NAME>'

SELECT label, value, function_name(value) AS alias

FROM athena_db.athena_table
```

在这里，我们定义并使用一个定制函数，该函数在使用 Amazon Athena 时调用现有的 SageMaker 推理端点进行预测。要了解更多信息，请随时查看以下资源和链接:

*   **亚马逊雅典娜** + **亚马逊 sage maker**:[https://docs . AWS . Amazon . com/Athena/latest/ug/query-ml model . XHTML](https://docs.aws.amazon.com/athena/latest/ug/querying-mlmodel.xhtml)。
*   **亚马逊红移** + **亚马逊 sage maker**:[https://docs . AWS . Amazon . com/Redshift/latest/DG/machine _ learning . XHTML](https://docs.aws.amazon.com/redshift/latest/dg/machine_learning.xhtml)。
*   **亚马逊 Aurora** + **亚马逊 SageMaker**:[https://docs . AWS . Amazon . com/Amazon rds/latest/Aurora user guide/Aurora-ml . XHTML](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ml.xhtml)。
*   **亚马逊 QuickSight** + **亚马逊 SageMaker**:[https://docs . AWS . Amazon . com/quick sight/latest/user/SageMaker-integration . XHTML](https://docs.aws.amazon.com/quicksight/latest/user/sagemaker-integration.xhtml)。

如果我们想在 SageMaker 托管服务之外部署一个模型，我们也可以这么做。例如，我们可以使用 SageMaker 来训练我们的模型，然后从包含训练过程中生成的模型工件文件的 S3 桶中下载`model.tar.gz`文件。生成的模型工件文件可以部署在 SageMaker 之外，类似于我们在 [*第二章*](B18638_02.xhtml#_idTextAnchor041) 、*深度学习 AMIs* 、 [*第三章*、*深度学习容器*中部署和调用模型的方式。此时，您可能会问自己:为什么使用 SageMaker 托管服务部署 ML 模型？如果您要在 SageMaker 托管服务中部署 ML 模型，以下是您可以轻松执行和设置的快速列表:](B18638_03.xhtml#_idTextAnchor060)

*   设置用于托管ML 模型的基础设施资源(ML 实例)的自动缩放(**自动缩放**)。当流量或工作负载增加时，自动缩放会自动添加新的 ML 实例，而当流量或工作负载减少时，自动缩放会减少所提供的 ML 实例的数量。
*   使用 SageMaker 的**多模型端点** ( **MME** )和**多容器端点** ( **MCE** )支持，在单个推理端点部署多个 ML 模型。还可以在单个端点后建立一个**串行推理管道**，它涉及一系列用于处理 ML 推理请求的容器(例如，预处理、预测和后处理)。
*   通过将流量分配给单个推理端点下的多个变量，设置 ML 模型的 **A/B 测试**。
*   使用 SageMaker Python SDK，只需几行代码即可设置自动化模型监控并监控(1)数据质量、(2)模型质量、(3)偏差漂移和(4)要素属性漂移。我们将在第 8 章 、*模型监控和管理解决方案*中深入探讨模型监控。
*   在部署模型时使用**弹性推理**将推理加速添加到 SageMaker 推理端点，以提高吞吐量并减少延迟。
*   在更新部署模型时执行蓝/绿部署时，使用各种流量转换模式。如果我们想一次性将所有流量从旧设置转移到新设置，我们可以使用**一次性**流量转移模式。如果我们想分两步将流量从旧设置转移到新设置，我们可以使用**金丝雀**流量转移模式。这涉及在第一次移位中仅移位一部分流量，而在第二次移位中移位剩余的流量。最后，我们可以使用**线性**流量转移模式，以预定数量的步骤将流量从旧设置迭代转移到新设置。
*   设置 **CloudWatch** 警报和 SageMaker 自动回滚配置，以自动化部署回滚过程。

如果我们要使用 SageMaker 进行模型部署，所有这些都相对容易设置。当使用这些特性和功能时，我们需要担心的只是配置步骤，因为大部分工作已经被 SageMaker 自动化了。

到目前为止，我们已经讨论了在云中部署 ML 模型时的不同选项和解决方案。在结束本节之前，让我们快速地讨论一下移动设备和智能摄像机等**边缘设备**上的 ML 模型部署。这种方法有几个优点，包括实时预测延迟、隐私保护和与网络连接相关的成本降低。当然，当在边缘设备上运行和管理 ML 模型时，由于计算和内存等相关资源限制，存在挑战。这些挑战可以通过 **SageMaker Edge Manager** 来解决，这是一种在边缘设备上优化、运行、监控和更新 ML 模型时，使使用其他几个服务、功能和特性(如 **SageMaker Neo** 、 **IoT Greengrass** 和 **SageMaker Model Monitor** )的能力。我们不会更深入地探究细节，所以请随意查看 https://docs.aws.amazon.com/sagemaker/latest/dg/edge.xhtml 来获取更多关于这个话题的信息。

# 总结

在本章中，我们讨论并关注了使用 SageMaker 的几个部署选项和解决方案。我们将预先训练的模型部署到三种不同类型的推理端点中——( 1)实时推理端点，(2)无服务器推理端点，以及(3)异步推理端点。我们还讨论了每种方法的不同之处，以及在部署 ML 模型时每种方法的最佳使用时机。在本章的末尾，我们讨论了一些部署策略，以及使用 SageMaker 进行模型部署的最佳实践。

在下一章中，我们将深入探讨 **SageMaker 模型注册表**和 **SageMaker 模型监控器**，这是 SageMaker 的功能，可以帮助我们管理和监控生产中的模型。

# 延伸阅读

有关本章主题的更多信息，请随时查阅以下资源:

*   *抱脸蒸馏模型*([https://Hugging Face . co/docs/transformers/model _ doc/蒸馏模型](https://huggingface.co/docs/transformers/model_doc/distilbert))
*   *sage maker——部署模型进行推理*(https://docs . AWS . Amazon . com/sage maker/latest/DG/deploy-model . XHTML)
*   *sage maker–推理推荐器*([https://docs . AWS . Amazon . com/sage maker/latest/DG/Inference-Recommender . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml))
*   *SageMaker-Deployment guardrails*([https://docs . AWS . Amazon . com/SageMaker/latest/DG/Deployment-guardrails . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails.xhtml))*