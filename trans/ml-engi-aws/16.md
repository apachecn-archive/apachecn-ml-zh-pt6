

# 11

# 机器学习管道与 SageMaker 管道

在 [*第十章*](B18638_10.xhtml#_idTextAnchor215) ，*亚马逊 EKS* 上带有 Kubeflow 的机器学习管道中，我们使用了 **Kubeflow** ， **Kubernetes** 和**亚马逊 EKS** 来构建和运行一个端到端的**机器学习** ( **ML** )管道。在这里，我们能够在运行的 Kubernetes 集群中自动化 ML 过程中的几个步骤。如果你想知道我们是否也可以使用 SageMaker 的不同特性和功能来构建 ML 管道，那么简单的回答就是*是的*！

在本章中，我们将使用 **SageMaker Pipelines** 来构建和运行自动化的 ML 工作流。除此之外，我们将演示如何在管道执行期间利用 **AWS Lambda** 函数将训练好的模型部署到新的(或现有的)ML 推理端点。

也就是说，在本章中，我们将讨论以下主题:

*   深入 SageMaker 管道
*   准备必要的先决条件
*   使用 SageMaker 管道运行我们的第一条管道
*   为部署创建 Lambda 函数
*   测试我们的 ML 推理端点
*   完成端到端的 ML 管道
*   清理
*   推荐的策略和最佳实践

完成本章的动手解决方案后，我们应该掌握了使用 **Amazon SageMaker** 的不同功能在 AWS 上构建更复杂的 ML 管道和工作流所需的技能！

# 技术要求

开始之前，我们必须准备好以下内容:

*   网络浏览器(最好是 Chrome 或 Firefox)
*   访问 AWS 帐户和本书前面章节中使用的 **SageMaker Studio** 域
*   本地机器上的一个文本编辑器(例如， **VS Code** )，我们将使用它来存储和复制字符串值，供本章后面使用

Jupyter 笔记本、源代码和其他用于每章的文件可以在[https://github . com/packt publishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS)的资源库中获得。

重要说明

建议您在运行本书中的示例时，使用具有有限权限的 IAM 用户，而不是 root 帐户。如果您刚刚开始使用 AWS，可以同时使用 root 帐户。

# 深入 SageMaker 管道

通常，数据科学团队从手动执行 ML 实验和部署开始。一旦他们需要标准化工作流程，并启用**自动化模型再训练**来定期更新已部署的模型，这些团队就会开始考虑使用 ML 管道来自动化他们的部分工作。在 [*第六章*](B18638_06.xhtml#_idTextAnchor132) 、 *SageMaker 训练和调试解决方案*中，我们学习了如何使用 **SageMaker Python SDK** 训练一个 ML 模型。通常，使用 SageMaker Python SDK 训练 ML 模型需要运行几行代码，类似于下面的代码块:

```
estimator = Estimator(...) 

estimator.set_hyperparameters(...)

estimator.fit(...)
```

*如果我们想准备一个自动化的 ML 管道，并把它作为其中一个步骤，会怎么样？*你会惊讶地发现，我们需要做的只是添加几行代码，将这转换成一个可以包含在管道中的步骤！使用类似于下面代码块中的`TrainingStep`对象将其转换成一个步骤:

```
step_train = TrainingStep(

    name="TrainModel",

    estimator=estimator,

    inputs=...

)
```

*哇！是不是很神奇？*这将意味着使用 **SageMaker Python SDK** 来手动训练和部署 ML 模型的现有笔记本可以很容易地转换成使用 SageMaker 管道，只需要几行额外的代码！其他步骤呢？我们也有以下的课程:

*   `ProcessingStep`–这是使用 **SageMaker 处理**处理数据的。
*   `TuningStep`–用于使用 SageMaker 的**自动模型调整**功能创建超参数调整作业。
*   `ModelStep`–这是，用于创建 SageMaker 模型并将其注册到 **SageMaker 模型注册表**。
*   `TransformStep`–这是使用 SageMaker 的**批量转换**功能在数据集上运行推理的。
*   `ConditionStep`–这是对流水线步骤执行的条件分支支持。
*   `CallbackStep`–这是用于合并 SageMaker 管道中不直接可用或不支持的自定义步骤。
*   `LambdaStep`–这是用于运行 **AWS Lambda** 功能的。

注意

请注意，这并不是详尽的步骤列表，因为还有其他步骤可用于更具体的用例。你可以在[https://docs . AWS . Amazon . com/SageMaker/latest/DG/build-and-manage-Steps . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.xhtml)找到 **SageMaker 管道步骤**的完整列表。

在 [*第 4 章*](B18638_04.xhtml#_idTextAnchor079) 、*AWS 上的无服务器数据管理*中，我们在红移集群和 Athena 表中存储和查询我们的数据。如果我们需要直接从这些数据源中查询数据，我们可以使用`ProcessingStep`对象，稍后会将它添加到管道中。处理作业完成后，它会将输出文件存储在 S3 中，然后训练作业或自动模型调整作业可以拾取并处理这些文件。如果我们需要将它转换成一个步骤，我们可以创建一个相应的`TrainingStep`对象(如果我们将运行一个训练任务)或者一个`TuningStep`对象(如果我们将运行一个自动模型调整任务)，然后将它们添加到管道中。*训练(或调整)工作完成后会发生什么？*我们可以选择将生成的模型存储在`ModelStep`对象中，该对象稍后也将被添加到管道中。让我们参考*图 11.1* 来帮助我们在准备好管道的不同步骤后直观地了解这一切是如何工作的:

![Figure 11.1 – Using SageMaker Pipelines

](img/B18638_11_001.jpg)

图 11.1–使用 SageMaker 管道

在*图 11.1* 中，我们可以看到`Pipeline`对象，它映射到 ML 管道定义:

```
pipeline = Pipeline(

    name=...,

    parameters=...,

    steps=[

        ..., 

        step_train,

        ...

    ],

)

# create (or update) the ML pipeline

pipeline.upsert(...)
```

然后，要运行管道，我们需要做的就是调用`start()`方法:

```
execution = pipeline.start()
```

一旦流水线启动，我们将不得不等待所有步骤执行完毕(一次一个步骤),或者等待流水线在其中一个步骤出错时停止。为了调试和排除运行管道的故障，我们可以很容易地导航到 **SageMaker Studio** 的 **SageMaker Resources** 窗格，并定位相应的管道资源。我们应该会看到一个与管道执行相对应的图表，类似于*图 11.2* 中的图表。

![Figure 11.2 – Pipeline execution

](img/B18638_11_002.jpg)

图 11.2–管道执行

在这里，我们可以看到管道中的所有步骤都已经成功完成，我们训练的模型也已经注册到 SageMaker 模型注册中心。如果我们希望再次运行管道(例如，使用不同的输入数据集)，我们可以简单地触发另一个管道执行，并传递一个指向新输入数据集存储位置的不同管道参数值。*很酷吧？*除此之外，我们还可以通过单击我们希望检查的步骤的相应圆角矩形，然后查看输入参数、输出值、ML 度量值、用于训练模型的超参数以及步骤执行期间生成的日志，来更深入地了解每个步骤中正在发生的事情。这使我们能够了解管道执行过程中发生的情况，并在管道执行过程中遇到错误时解决问题。

到目前为止，我们一直在讨论一个相对简单的管道，包括三个或四个顺序执行的步骤。此外， **SageMaker Pipelines** 允许我们构建更复杂的 ML 管道，利用类似于*图 11.3* 中的条件步骤:

![](img/B18638_11_003.jpg)

图 11.3–带条件步骤的 ML 管道

这里，使用`ConditionStep`，管道检查 ML 推理端点是否已经存在(给定端点名称)，并根据端点的存在执行以下步骤之一:

*   *将模型部署到一个新的端点*——使用`LambdaStep`，它映射到一个 **AWS Lambda** 函数，如果端点尚不存在，该函数将 ML 模型部署到一个新的 ML 推理端点
*   *将模型部署到现有的端点*——使用`LambdaStep`，它映射到 **AWS Lambda** 函数，如果端点已经存在，该函数将 ML 模型部署到现有的 ML 推理端点(使用**零停机部署**

*酷吧？* *更酷的是，这是我们将在本章构建的管道！起初，构建一个 ML 管道可能看起来令人生畏。然而，只要我们迭代地构建和测试管道，并使用正确的工具集，我们应该能够提出自动化手动过程所需的 ML 管道。*

现在我们对 SageMaker Pipelines 的工作原理有了更好的理解，让我们继续本章的实践部分。

注意

在这一点上，你可能想知道为什么我们应该使用 **SageMaker 管道**而不是 **Kubeflow** 和 **Kubernetes** 。SageMaker Pipelines 和 Kubeflow 之间的一个主要区别是，用于在 SageMaker 中训练 ML 模型的实例会在训练步骤完成后自动删除。这有助于降低总成本，因为这些训练实例仅在模型需要训练时运行。另一方面，Kubeflow 所需的基础设施需要在任何训练步骤开始之前启动并运行。请注意，这只是其中的一个区别，在为工作选择“正确”的工具时，还有其他需要考虑的事情。当然，在有些情况下，数据科学团队会选择 Kubeflow，因为成员们已经习惯使用 Kubernetes(或者他们已经在运行生产 Kubernetes 工作负载)。为了帮助您和您的团队正确地评估这些工具，我建议，首先，您尝试使用这两个选项来构建示例 ML 管道。

# 准备必要的先决条件

在本节中，我们将确保以下先决条件已准备就绪:

*   SageMaker Studio 域执行角色附带了`AWSLambda_FullAccess` AWS 托管权限策略——这将允许 Lambda 函数在完成本章的端到端 ML 管道部分的*中没有问题地运行。*
*   IAM 角色(`pipeline-lambda-role`)–这将用于运行本章*创建用于部署的 Lambda 函数*部分中的 Lambda 函数。
*   `processing.py`文件——这将被 **SageMaker 处理**作业用来处理输入数据，并将其分成训练集、验证集和测试集。
*   `bookings.all.csv`文件——这将用作 ML 管道的输入数据集。

重要说明

在这一章中，我们将在`us-west-2`区域创建和管理我们的资源。在继续下一步之前，请确保您已经设置了正确的区域。

准备这些必要的先决条件是至关重要的，以确保我们在本章准备和运行 ML 管道时不会遇到意外的阻塞。也就是说，让我们继续准备下一组步骤中的先决条件:

1.  让我们首先导航到 AWS 管理控制台搜索栏中的`sagemaker studio`，将鼠标悬停在**亚马逊 SageMaker** 的搜索结果框上，然后点击**顶级功能**下的 **SageMaker Studio** 链接。
2.  在 SageMaker Studio **控制面板**页面上，找到**域**框附带的**执行角色**部分(如图*图 11.4* 中高亮显示的部分):

![Figure 11.4 – Copying the Execution role name

](img/B18638_11_004.jpg)

图 11.4–复制执行角色名称

找到以下值，并将其复制到本地计算机上的文本编辑器中:

*   `arn:aws:iam::<ACCOUNT ID>:role/service-role/`)。执行角色名称可能遵循类似于图 11.4 中*的`AmazonSageMaker-ExecutionRole-<DATETIME>`格式。确保在复制执行角色名称时排除了`arn:aws:iam::<ACCOUNT ID>:role/service-role/`。*
*   `arn:aws:iam::<ACCOUNT ID>:role/service-role/`)。执行角色 ARN 应该遵循`arn:aws:iam::<ACCOUNT ID>:role/service-role/AmazonSageMaker-ExecutionRole-<DATETIME>`格式。

注意

在本章的*为部署创建 Lambda 函数*一节中测试 Lambda 函数时，我们将使用执行角色 ARN。

1.  导航到 AWS 管理控制台的搜索栏中的`iam`，将鼠标悬停在 **IAM** 的搜索结果框上，然后点击**顶级功能**下的**角色**链接。
2.  在**角色**页面上，通过在搜索框中键入执行角色名称(复制到您本地机器上的文本编辑器中)来搜索并定位执行角色(如*图 11.5* 中突出显示的):

![Figure 11.5 – Navigating to the specific role page

](img/B18638_11_005.jpg)

图 11.5–导航到特定角色页面

这将过滤结果并显示一个类似于*图 11.5* 中的单行。单击**角色名称**列下的链接，导航到可以修改角色权限的页面。

1.  找到**权限策略**表(在**权限**选项卡内)，然后点击**添加权限**打开选项下拉菜单。从可用选项列表中选择**附加策略**。这应该会将我们重定向到页面，在这里我们可以看到**当前权限策略**部分，并在**其他权限策略**下附加其他策略。
2.  使用搜索栏找到`AWSLambda_FullAccess` AWS 托管权限策略(在`AWSLambda_FullAccess`策略下)。之后，点击**附加策略**按钮。

注意

单击**附加策略**按钮后，您应该会看到以下成功通知消息，**策略已成功附加到角色**。

1.  现在，让我们创建 IAM 角色，我们将在稍后创建 Lambda 函数时使用它。导航到**角色**页面(使用侧栏)，然后点击**创建角色**按钮。
2.  在**选择可信实体**页面上(*步骤 1* ，执行以下步骤:
    *   在**可信实体类型**下，从可用选项列表中选择 **AWS 服务**。
    *   在**用例**下，选择**常用用例**下的**λ**。
    *   之后，点击**下一个**按钮。
3.  在`AmazonSageMakerFullAccess`政策中。
4.  搜索并选择`AWSLambdaExecute`策略。
5.  打开两个策略的单选按钮后，点击下一个**按钮**。
6.  上`pipeline-lambda-role`下**角色名**。
7.  向下滚动到页面底部，然后点击**创建角色**按钮。

注意

点击**创建角色**按钮后，您应该会看到以下成功通知消息:**角色管道-lambda-角色创建**。

1.  导航回到 AWS 管理控制台的搜索栏中的`sagemaker studio`，然后点击**顶级功能**下的 **SageMaker Studio** 链接(悬停在**亚马逊 SageMaker** 的搜索结果框上之后)。
2.  点击**启动应用**，然后从下拉选项列表中选择**工作室**。

注意

这将把你重定向到 **SageMaker 工作室**。等待几秒钟，让接口加载。

1.  现在，让我们继续创建`CH11`文件夹，在这里我们将存储与本章中的ML 管道相关的文件。右键单击**文件浏览器**侧栏窗格中的空白区域，打开类似于*图 11.6* 所示的上下文菜单:

![Figure 11.6 – Creating a new folder

](img/B18638_11_006.jpg)

图 11.6–创建新文件夹

选择`CH11`。之后，双击侧边栏中相应的文件夹名称，导航到`CH11`目录。

1.  通过点击`CH11`目录中的`.ipynb`文件创建一个新的笔记本，我们可以在这里运行 Python 代码。
2.  在【Sagemaker 图像下的选项)
3.  `Python 3`
4.  `No script`
5.  之后，点击**选择**按钮。

注意

等待内核启动。在配置 ML 实例以运行 Jupyter 笔记本单元时，此步骤可能需要大约 3-5 分钟。确保在完成本章中的所有动手解决方案后(或者如果您不使用它)，停止此实例。欲了解更多信息，请随时查看本章末尾的*清理*部分。

1.  右键单击选项卡名称，然后选择`Machine Learning Pipelines with SageMaker Pipelines.ipynb`。
2.  在`Machine Learning Pipelines with SageMaker Pipelines.ipynb`笔记本的第一个单元格中，运行以下命令:

    ```
    !wget -O processing.py https://bit.ly/3QiGDQO
    ```

这将下载一个`processing.py`文件，该文件执行以下操作:

*   加载`dataset.all.csv`文件并将数据存储在数据帧中
*   执行**训练-测试分割**，该分割会将数据帧分成三个数据帧(包含训练、验证和测试集)
*   确保在保存输出 CSV 文件之前已经创建了输出目录
*   将包含定型集、验证集和测试集的数据帧保存到输出目录中相应的 CSV 文件中

注意

请随意检查下载的`processing.py`文件的内容。此外，您可以在[https://github . com/packt publishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter 11/processing . py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/processing.py)找到`processing.py`脚本文件的副本。

1.  接下来，让我们使用`mkdir`命令创建一个`tmp`目录，如果它还不存在的话:

    ```
    !mkdir -p tmp
    ```

2.  之后，使用`wget`命令:

    ```
    !wget -O tmp/bookings.all.csv https://bit.ly/3BUcMK4
    ```

    下载`bookings.all.csv`文件

在这里，我们下载一个干净(er)版本的合成`bookings.all.csv`文件，类似于我们在 [*第 1 章*](B18638_01.xhtml#_idTextAnchor017) 、*AWS 上的 ML 工程简介*中使用的。然而，这一次，已经应用了多个数据清理和转换步骤来产生更高质量的模型。

1.  指定唯一的 S3 时段名称和前缀。确保在运行下面的代码块之前，用唯一的 S3 存储桶名称替换了`<INSERT S3 BUCKET NAME HERE>`的值:

    ```
    s3_bucket = '<INSERT S3 BUCKET NAME HERE>'
    ```

    ```
    prefix = 'pipeline'
    ```

您可以使用在前几章中创建的 S3 存储桶之一，并用 S3 存储桶名称更新`s3_bucket`的值。如果您计划创建并使用一个新的 S3 存储桶，请确保使用一个尚不存在的存储桶名称来更新`s3_bucket`的值。之后，运行以下命令:

```
!aws s3 mb s3://{s3_bucket}
```

注意，只有当我们计划创建一个新的 S3 存储桶时，才应该执行这个命令。

注意

将 S3 存储桶名称复制到本地机器上的文本编辑器中。我们将在本章的*测试我们的 ML 推理端点*部分使用它。

1.  让我们准备好上传 CSV 文件的路径:

    ```
    source_path = f's3://{s3_bucket}/{prefix}' + \
    ```

    ```
                   '/source/dataset.all.csv'
    ```

2.  最后，让我们使用`aws s3 cp`命令:

    ```
    !aws s3 cp tmp/bookings.all.csv {source_path}
    ```

    将`bookings.all.csv`文件上传到 S3 存储桶

这里，CSV 文件在上传到 S3 存储桶时被重命名为`dataset.all.csv`文件(因为我们在`source_path`变量中指定了这一点)。

准备好先决条件后，我们现在可以开始运行我们的第一个管道了！

# 使用 SageMaker 管道运行我们的第一条管道

在 [*第一章*](B18638_01.xhtml#_idTextAnchor017) 、*AWS 上的 ML 工程介绍*中，我们安装并使用**autoglon**在 AWS Cloud9 环境中训练多个 ML 模型(使用 **AutoML** )。除此之外，我们使用各种工具和库手动执行了 ML 过程的不同步骤。在本章中，我们将把这些手动执行的步骤转换成一个自动化的管道，这样我们所需要做的就是提供一个输入数据集，ML 管道将为我们完成剩下的工作(并将训练好的模型存储在模型注册表中)。

注意

我们将使用内置的**自动增长-列表**算法，而不是准备一个定制的 Docker 容器映像来使用自动增长来训练 ML 模型。有了可用的内置算法，我们需要担心的只是超参数值和我们将用于配置训练作业的额外配置参数。

也就是说，本节分为两部分:

*   *定义和准备我们的第一个 ML 管道*——我们将在这里通过以下步骤定义和准备管道:
    *   `PrepareData`–这利用一个 **SageMaker 处理**作业来处理输入数据集，并将其分成训练、验证和测试集。
    *   `TrainModel`–这利用**自动生成表格**内置算法来训练分类模型。
    *   `RegisterModel`–将训练好的 ML 模型注册到 **SageMaker 模型注册表**。
*   *运行我们的第一个 ML 管道*——这是我们将使用`start()`方法来执行管道的地方。

记住这些，让我们从准备 ML 管道开始。

## 定义和准备我们的首个 ML 管道

我们将准备的第一个管道是一个相对简单的管道，有三个步骤——包括数据准备步骤、模型训练步骤和模型注册步骤。为了帮助我们想象我们第一个使用 **SageMaker 管道**的 ML 管道将会是什么样子，让我们快速查看一下*图 11.7* :

![Figure 11.7 – Our first ML pipeline using SageMaker Pipelines

](img/B18638_11_007.jpg)

图 11.7–我们第一个使用 SageMaker 管道的 ML 管道

在这里，我们可以看到我们的管道接受一个输入数据集，并将这个数据集分成训练集、验证集和测试集。然后，使用训练和验证集来训练 ML 模型，然后将其注册到 **SageMaker 模型注册表**。

现在我们对管道的样子有了一个很好的想法，让我们在下一组步骤中运行我们的`Machine Learning Pipelines with SageMaker Pipelines.ipynb` Jupyter 笔记本中的以下代码块:

1.  让我们从`boto3`和`sagemaker`开始导入构建模块:

    ```
    import boto3
    ```

    ```
    import sagemaker
    ```

    ```
    from sagemaker import get_execution_role
    ```

    ```
    from sagemaker.sklearn.processing import (
    ```

    ```
        SKLearnProcessor
    ```

    ```
    )
    ```

    ```
    from sagemaker.workflow.steps import (
    ```

    ```
        ProcessingStep, 
    ```

    ```
        TrainingStep
    ```

    ```
    )
    ```

    ```
    from sagemaker.workflow.step_collections import (
    ```

    ```
        RegisterModel
    ```

    ```
    )
    ```

    ```
    from sagemaker.processing import (
    ```

    ```
        ProcessingInput, 
    ```

    ```
        ProcessingOutput
    ```

    ```
    )
    ```

    ```
    from sagemaker.workflow.parameters import (
    ```

    ```
        ParameterString
    ```

    ```
    )
    ```

    ```
    from sagemaker.inputs import TrainingInput
    ```

    ```
    from sagemaker.estimator import Estimator
    ```

    ```
    from sagemaker.workflow.pipeline import Pipeline
    ```

2.  将SageMaker执行角色 ARN 存储在`role`变量:

    ```
    role = get_execution_role()
    ```

    中

注意

`get_execution_role()`函数应该返回我们在本章的*准备必要先决条件*一节中修改的 IAM 角色的 ARN。

1.  另外，让我们准备 SageMaker `Session`对象:

    ```
    session = sagemaker.Session()
    ```

2.  让我们初始化一个`ParameterString`对象，它映射到指向输入数据集存储位置的`Pipeline`参数:

    ```
    input_data = ParameterString(
    ```

    ```
        name="RawData",
    ```

    ```
        default_value=source_path, 
    ```

    ```
    )
    ```

3.  让我们准备`ProcessingInput`对象，它包含`ProcessingOutput`对象的输入源的配置，该配置映射到 **SageMaker 处理**作业的输出结果的配置:

    ```
    input_raw = ProcessingInput(
    ```

    ```
        source=input_data,
    ```

    ```
        destination='/opt/ml/processing/input/'
    ```

    ```
    )
    ```

    ```
    output_split = ProcessingOutput(
    ```

    ```
        output_name="split",
    ```

    ```
        source='/opt/ml/processing/output/', 
    ```

    ```
        destination=f's3://{s3_bucket}/{prefix}/output/'
    ```

    ```
    )
    ```

4.  让我们初始化`SKLearnProcessor`对象以及相应的`ProcessingStep`对象:

    ```
    processor = SKLearnProcessor(
    ```

    ```
        framework_version='0.20.0',
    ```

    ```
        role=role,
    ```

    ```
        instance_count=1,
    ```

    ```
        instance_type='ml.m5.large'
    ```

    ```
    )
    ```

    ```
    step_process = ProcessingStep(
    ```

    ```
        name="PrepareData",  
    ```

    ```
        processor=processor,
    ```

    ```
        inputs=[input_raw],
    ```

    ```
        outputs=[output_split],
    ```

    ```
        code="processing.py",
    ```

    ```
    )
    ```

为了帮助我们可视化如何配置`ProcessingStep`对象，让我们快速检查*图 11.8* :

![Figure 11.8 – Configuring and preparing the ProcessingStep

](img/B18638_11_008.jpg)

图 11.8–配置和准备处理步骤

这里，我们使用已配置的`SKLearnProcessor`对象以及`inputs`、`outputs`和`code`参数的参数值来初始化`ProcessingStep`对象。

1.  接下来，让我们准备好`model_path`变量，以指向 SageMaker 训练任务完成后模型将被上传到的位置(当 ML 管道在后面的步骤中被执行时):

    ```
    model_path = f"s3://{s3_bucket}/{prefix}/model/"
    ```

2.  此外，让我们准备`model_id`变量来存储我们将使用的 ML 模型的ID:

    ```
    model_id = "autogluon-classification-ensemble"
    ```

3.  让我们在`region_name` :

    ```
    region_name = "us-west-2"
    ```

    中指定我们正在使用的区域
4.  使用`image_uris.retrieve()`得到我们训练图像的 ECR 容器图像 URI:

    ```
    from sagemaker import image_uris
    ```

    ```
    train_image_uri = image_uris.retrieve(
    ```

    ```
        region=region_name,
    ```

    ```
        framework=None,
    ```

    ```
        model_id=model_id,
    ```

    ```
        model_version="*",
    ```

    ```
        image_scope="training",
    ```

    ```
        instance_type="ml.m5.xlarge",
    ```

    ```
    )
    ```

如果您想知道`train_image_uri`的值是什么，它应该有一个等于(或类似于)的字符串值:`'763104351884.dkr.ecr.us-west-2.amazonaws.com/autogluon-training:0.4.0-cpu-py38'.`

1.  使用`script_uris.retrieve()`获取与模型相关联的脚本 S3·URI(给定`model_id`、`model_version`和`script_scope`的值):

    ```
    from sagemaker import script_uris
    ```

    ```
    train_source_uri = script_uris.retrieve(
    ```

    ```
        model_id=model_id, 
    ```

    ```
        model_version="*", 
    ```

    ```
        script_scope="training"
    ```

    ```
    )
    ```

注意`train_source_uri`应该有一个与`'s3://jumpstart-cache-prod-us-west-2/source-directory-tarballs/autogluon/transfer_learning/classification/v1.0.1/sourcedir.tar.gz'`相等(或相似)的字符串值。

注意

这个`sourcedir.tar.gz`文件里面是什么？如果调用`script_uris.retrieve()`时使用的`script_scope`值是`"training"`，`sourcedir.tar.gz`文件应该包含训练 ML 模型时使用`autogluon.tabular.TabularPredictor`的代码。请注意，`sourcedir.tar.gz`的内容会根据调用`script_uris.retrieve()`时指定的参数而变化。

1.  使用`model_uris.retrieve()`到获得与模型相关联的模型工件 S3 URI (给定`model_id`、`model_version`和`model_scope`的值):

    ```
    from sagemaker import model_uris
    ```

    ```
    train_model_uri = model_uris.retrieve(
    ```

    ```
        model_id=model_id, 
    ```

    ```
        model_version="*", 
    ```

    ```
        model_scope="training"
    ```

    ```
    )
    ```

注意`train_model_uri`应该有一个等于(或类似于)于`'s3://jumpstart-cache-prod-us-west-2/autogluon-training/train-autogluon-classification-ensemble.tar.gz'`的字符串值。

1.  有了`train_image_uri`、`train_source_uri`、`train_model_uri`和`model_path`的值，我们现在可以初始化`Estimator`对象:

    ```
    from sagemaker.estimator import Estimator
    ```

    ```
    estimator = Estimator(
    ```

    ```
        image_uri=train_image_uri,
    ```

    ```
        source_dir=train_source_uri,
    ```

    ```
        model_uri=train_model_uri,
    ```

    ```
        entry_point="transfer_learning.py",
    ```

    ```
        instance_count=1,
    ```

    ```
        instance_type="ml.m5.xlarge",
    ```

    ```
        max_run=900,
    ```

    ```
        output_path=model_path,
    ```

    ```
        session=session,
    ```

    ```
        role=role
    ```

    ```
    )
    ```

这里，`entry_point`值指向存储在`sourcedir.tar.gz`中的`transfer_learning.py`脚本文件，该文件包含用于训练模型的相关脚本。

1.  接下来，让我们使用`retrieve_default()`函数为我们的**自动引导**分类模型检索默认的超参数集:

    ```
    from sagemaker.hyperparameters import retrieve_default
    ```

    ```
    hyperparameters = retrieve_default(
    ```

    ```
        model_id=model_id, 
    ```

    ```
        model_version="*"
    ```

    ```
    )
    ```

    ```
    hyperparameters["verbosity"] = "3"
    ```

    ```
    estimator.set_hyperparameters(**hyperparameters) 
    ```

2.  准备初始化时使用`Estimator`对象作为参数值之一的`TrainingStep`对象:

    ```
    s3_data = step_process         \
    ```

    ```
        .properties                \
    ```

    ```
        .ProcessingOutputConfig    \
    ```

    ```
        .Outputs["split"]          \
    ```

    ```
        .S3Output.S3Uri            \
    ```

    ```
    step_train = TrainingStep(
    ```

    ```
        name="TrainModel",
    ```

    ```
        estimator=estimator,
    ```

    ```
        inputs={
    ```

    ```
            "training": TrainingInput(
    ```

    ```
                s3_data=s3_data,
    ```

    ```
            )
    ```

    ```
        },
    ```

    ```
    )
    ```

这里，`s3_data`包含一个`Properties`对象，指向`s3_data`的输出文件所在的路径。使用`s3_data.__dict__`，我们应该得到一个类似如下的字典:

```
{'step_name': 'PrepareData',

 'path':  "ProcessingOutputConfig.Outputs['split']

           .S3Output.S3Uri",

 '_shape_names': ['S3Uri'],

 '__str__': 'S3Uri'} 
```

为了帮助用户形象化我们如何配置`TrainingStep`对象，让快速检查*图 11.9* :

![Figure 11.9 – Configuring and preparing the TrainingStep object

](img/B18638_11_009.jpg)

图 11.9–配置和准备训练步骤对象

这里，我们使用已配置的`Estimator`对象以及`name`和`inputs`参数的参数值来初始化`TrainingStep`对象。

1.  现在，让使用`image_uris.retrieve()`和`script_uris.retrieve()`来检索容器图像 URI 和脚本 URI，用于部署自动分类模型:

    ```
    deploy_image_uri = image_uris.retrieve(
    ```

    ```
        region=region_name,
    ```

    ```
        framework=None,
    ```

    ```
        image_scope="inference",
    ```

    ```
        model_id=model_id,
    ```

    ```
        model_version="*",
    ```

    ```
        instance_type="ml.m5.xlarge",
    ```

    ```
    )
    ```

    ```
    deploy_source_uri = script_uris.retrieve(
    ```

    ```
        model_id=model_id, 
    ```

    ```
        model_version="*", 
    ```

    ```
        script_scope="inference"
    ```

    ```
    )
    ```

2.  使用`aws s3 cp`命令将`sourcedir.tar.gz`文件下载到`tmp`目录:

    ```
    !aws s3 cp {deploy_source_uri} tmp/sourcedir.tar.gz
    ```

3.  接下来，从`tmp`目录上传`sourcedir.tar.gz`文件到你的 S3 桶:

    ```
    updated_source_uri = f's3://{s3_bucket}/{prefix}' + \
    ```

    ```
                          '/sourcedir/sourcedir.tar.gz'
    ```

    ```
    !aws s3 cp tmp/sourcedir.tar.gz {updated_source_uri}
    ```

4.  让我们定义一下`random_string()`的功能:

    ```
    import uuid
    ```

    ```
    def random_string():
    ```

    ```
        return uuid.uuid4().hex.upper()[0:6]
    ```

这个函数应该返回一个随机的字母数字字符串(6 个字符)。

1.  有了`deploy_image_uri`、`updated_source_uri`和`model_data`的值，我们现在可以初始化`Model`对象:

    ```
    from sagemaker.model import Model
    ```

    ```
    from sagemaker.workflow.pipeline_context import \
    ```

    ```
        PipelineSession
    ```

    ```
    pipeline_session = PipelineSession()
    ```

    ```
    model_data = step_train    \
    ```

    ```
        .properties            \
    ```

    ```
        .ModelArtifacts        \
    ```

    ```
        .S3ModelArtifacts      \
    ```

    ```
    model = Model(image_uri=deploy_image_uri, 
    ```

    ```
                  source_dir=updated_source_uri,
    ```

    ```
                  model_data=model_data,
    ```

    ```
                  role=role,
    ```

    ```
                  entry_point="inference.py",
    ```

    ```
                  sagemaker_session=pipeline_session,
    ```

    ```
                  name=random_string())
    ```

这里，我们使用在上一步中定义的`random_string()`函数作为`Model`对象的名称标识符。

1.  接下来，我们准备初始化时使用`model.register()`输出的`ModelStep`对象:

    ```
    from sagemaker.workflow.model_step import ModelStep
    ```

    ```
    model_package_group_name = "AutoGluonModelGroup"
    ```

    ```
    register_args = model.register(
    ```

    ```
        content_types=["text/csv"],
    ```

    ```
        response_types=["application/json"],
    ```

    ```
        inference_instances=["ml.m5.xlarge"],
    ```

    ```
        transform_instances=["ml.m5.xlarge"],
    ```

    ```
        model_package_group_name=model_package_group_name,
    ```

    ```
        approval_status="Approved",
    ```

    ```
    )
    ```

    ```
    step_model_create = ModelStep(
    ```

    ```
        name="CreateModel",
    ```

    ```
        step_args=register_args
    ```

    ```
    )
    ```

2.  现在，让我们用不同的步骤初始化对象，我们在前面的步骤中准备了

    ```
    pipeline_name = f"PARTIAL-PIPELINE"
    ```

    ```
    partial_pipeline = Pipeline(
    ```

    ```
        name=pipeline_name,
    ```

    ```
        parameters=[
    ```

    ```
            input_data
    ```

    ```
        ],
    ```

    ```
        steps=[
    ```

    ```
            step_process, 
    ```

    ```
            step_train,
    ```

    ```
            step_model_create,
    ```

    ```
        ],
    ```

    ```
    )
    ```

3.  最后，让我们使用`upsert()`方法来创建我们的 ML 管道:

    ```
    partial_pipeline.upsert(role_arn=role)
    ```

注意

注意，`upsert()`方法也可以用来更新现有的 ML 管道。

现在我们的初始管道已经准备好了，我们可以继续运行 ML 管道了！

## 运行我们的首个 ML 管道

一旦`Pipeline`对象被初始化并创建，我们就可以使用`start()`方法立即运行它，这类似于我们在下面的代码行中所做的:

```
execution = partial_pipeline.start()
```

如果我们希望覆盖管道输入的默认参数(例如，使用的输入数据)，我们可以在调用`start()`方法时指定参数值，类似于下面的代码块:

```
execution = partial_pipeline.start(

    parameters=dict(

        RawData="<INSERT NEW SOURCE PATH>",

    )

)
```

一旦管道执行开始，我们就可以使用`execution.wait()`来等待管道完成运行。

记住这一点，让我们在下一组步骤中运行 ML 管道:

1.  一切准备就绪，让我们使用`start()`方法:

    ```
    execution = partial_pipeline.start()
    ```

    ```
    execution.describe()
    ```

    运行(部分)ML 管道
2.  让我们使用`wait()`方法等待流水线完成，然后再继续:

    ```
    execution.wait()
    ```

注意

这大约需要 10-15 分钟才能完成。在等待的时候，请随意喝杯咖啡或茶！

1.  运行下面的代码块来获得最终的模型包 ARN:

    ```
    steps = execution.list_steps()
    ```

    ```
    steps[0]['Metadata']['RegisterModel']['Arn']
    ```

这将产生一个格式类似于`arn:aws:sagemaker:us-west-2:<ACCOUNT ID>:model-package/autogluonmodelgroup/1`的 ARN。将该值复制到文本编辑器中。我们将在本章的*为部署创建 Lambda 函数*一节中测试我们的 Lambda 函数时使用这个模型包 ARN。

1.  找到并点击 SageMaker Studio 左侧边栏底部附近的三角形图标( **SageMaker 资源**)(如图*图 11.10* 中突出显示的内容):

![Figure 11.10 – Opening the SageMaker resources pane 

](img/B18638_11_010.jpg)

图 11.10–打开 SageMaker 资源窗格

这将打开 **SageMaker 资源**窗格，我们可以在其中查看和检查各种 SageMaker 资源。

1.  从 **SageMaker resources** 窗格的下拉菜单中的可用选项列表中选择**管道**。
2.  之后，双击映射到我们刚刚创建的`PARTIAL-PIPELINE`管道的行。之后，双击映射到我们在调用`partial_pipeline.start()`后触发的管道执行的行。
3.  一旦执行完成，您应该会看到一个类似于图 11.11 所示的图形:

![Figure 11.11 – Completed pipeline execution

](img/B18638_11_011.jpg)

图 11.11-完成的管道执行

请随意点击圆角矩形，查看以下每个步骤的详细信息:

*   **输入**–输入文件、参数和配置
*   **输出**–输出文件和指标(如果有)
*   **日志**–生成的日志
*   **信息**–任何附加信息/元数据

1.  导航回与带有 SageMaker Pipelines.ipynb 笔记本的**机器学习管道对应的选项卡。**
2.  让我们回顾一下使用`list_steps()`方法在(部分)管道运行期间执行的步骤:

    ```
    execution.list_steps()
    ```

这应该会返回映射到管道执行步骤的字典列表。

我们还没完呢！此时，我们只完成了 ML 管道的一半。请确保您没有关闭 SageMaker Studio 中正在运行的应用程序和实例，因为我们稍后将在`Machine Learning Pipelines with SageMaker Pipelines.ipynb`笔记本中运行更多代码块来完成我们的管道。

注意

如果您需要休息，您可以关闭正在运行的实例和应用程序(以管理成本)，然后在继续进行本章的*完成端到端 ML 管道*部分之前，再次运行`Machine Learning Pipelines with SageMaker Pipelines.ipynb`笔记本中的所有单元。

# 创建用于部署的 Lambda 函数

我们的第二个(更完整的)管道将需要一些额外的资源来帮助我们部署 ML 模型。在此部分，我们将创建以下 Lambda 函数:

*   `check-if-endpoint-exists`–这是一个 Lambda 函数，接受 ML 推理端点的名称作为输入，如果端点已经存在，则返回`True`。
*   `deploy-model-to-new-endpoint`–这是一个 Lambda 函数，它接受模型包 ARN 作为输入(以及角色和端点名称)，并将模型部署到一个新的推理端点中
*   `deploy-model-to-existing-endpoint`–这是一个 Lambda 函数，它接受模型包 ARN 作为输入(以及角色和端点名称)，并将模型部署到现有的推理端点中(通过更新 ML 实例中部署的模型)

我们将在稍后的*完成端到端 ML 管道*部分使用这些函数来部署我们将在 SageMaker 模型注册表中注册的 ML 模型(使用`ModelStep`)。

## 准备用于将模型部署到新端点的 Lambda 函数

我们将创建的第一个 **AWS Lambda** 函数将被配置和编程为将模型部署到新的端点。为了帮助我们形象化我们的函数将如何工作，让我们快速检查一下*图 11.12* :

![Figure 11.12 – Deploying a model to a new endpoint

](img/B18638_11_012.jpg)

图 11.12–将模型部署到新的端点

该函数将接受以下输入参数:IAM 角色、端点名称和模型包 ARN。在接收到这些输入参数之后，该函数将创建将模型(从模型包)部署到新的 ML 推断端点所需的相应资源集。

在下一组步骤中，我们将创建一个 Lambda 函数，用于将 ML 模型部署到一个新的推理端点:

1.  导航到 AWS 管理控制台搜索栏中的`lambda`，然后单击搜索结果列表中的**λ**链接。

注意

在本章中，我们将在`us-west-2`区域创建和管理我们的资源。在继续下一步之前，请确保您已经设置了正确的区域。

1.  找到并点击`deploy-model-to-new-endpoint`
2.  `Python 3.9`
3.  **权限** > **改变默认执行角色**
4.  `Use an existing role`
5.  `pipeline-lambda-role`
6.  向下滚动到页面底部，然后点击**创建功能**按钮。

注意

单击**创建函数**按钮后，您应该会看到以下成功通知:**成功创建了函数 deploy-model-to-new-endpoint**。你现在可以改变它的代码和配置。要用测试事件调用您的函数，选择**测试**。

1.  导航到`1024` MB
2.  `15`分钟`0`秒

之后，点击**保存**按钮。

1.  在另一个浏览器标签中打开以下链接:[https://raw . githubusercontent . com/packt publishing/Machine-Learning-Engineering-on-AWS/main/chapter 11/utils . py](https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter11/utils.py)。使用 *Ctrl* + *A* 然后 *Ctrl* + *C* 将页面内容复制到剪贴板中(或者，如果您使用的是 Mac，也可以使用 *CMD* + *A* 然后 *CMD* + *C* )。
2.  回到显示 Lambda 控制台的浏览器选项卡，导航到`Untitled1`。
3.  在新选项卡(不包含代码)中，粘贴复制到剪贴板的代码。打开`utils.py`作为**文件名**字段值，然后点击**保存**。
4.  导航到选项卡，在这里我们可以修改`lambda_function.py`中的代码。继续之前，删除当前存储在`lambda_function.py`中的样板代码。

注意

在`lambda_function.py`内的后续步骤中键入(或复制)代码块。你可以在[https://github . com/packt publishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter 11/deploy-model-to-new-endpoint . py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/deploy-model-to-new-endpoint.py)找到 Lambda 函数的代码副本。

1.  在`lambda_function.py`文件中，导入我们将训练好的 ML 模型部署到新的 ML 推理端点所需的函数:

    ```
    import json
    ```

    ```
    from utils import (
    ```

    ```
        create_model, 
    ```

    ```
        create_endpoint_config, 
    ```

    ```
        create_endpoint, 
    ```

    ```
        random_string,
    ```

    ```
        block
    ```

    ```
    )
    ```

2.  现在，让我们用定义一下`lambda_handler()`的功能:

    ```
    def lambda_handler(event, context):
    ```

    ```
        role = event['role']
    ```

    ```
        endpoint_name = event['endpoint_name']
    ```

    ```
        package_arn = event['package_arn']
    ```

    ```
        model_name = 'model-' + random_string()
    ```

    ```
        with block('CREATE MODEL'):
    ```

    ```
            create_model(
    ```

    ```
                model_name=model_name,
    ```

    ```
                package_arn=package_arn,
    ```

    ```
                role=role
    ```

    ```
            )
    ```

    ```
        with block('CREATE ENDPOINT CONFIG'):
    ```

    ```
            endpoint_config_name = create_endpoint_config(
    ```

    ```
                model_name
    ```

    ```
            )
    ```

    ```
        with block('CREATE ENDPOINT'):
    ```

    ```
            create_endpoint(
    ```

    ```
                endpoint_name=endpoint_name, 
    ```

    ```
                endpoint_config_name=endpoint_config_name
    ```

    ```
            )
    ```

    ```
        return {
    ```

    ```
            'statusCode': 200,
    ```

    ```
            'body': json.dumps(event),
    ```

    ```
            'model': model_name
    ```

    ```
        }
    ```

3.  点击**部署**按钮。
4.  点击**测试**按钮。
5.  在事件名称下的`test`，然后事件 JSON 下指定以下 JSON 值:

    ```
    {
    ```

    ```
      "role": "<INSERT SAGEMAKER EXECUTION ROLE ARN>",
    ```

    ```
      "endpoint_name": "AutoGluonEndpoint",
    ```

    ```
      "package_arn": "<INSERT MODEL PACKAGE ARN>"
    ```

    ```
    }
    ```

请确保替换以下值:

*   `<INSERT SAGEMAKER EXECUTION ROLE ARN>`–用`arn:aws:iam::1234567890:role/service-role/AmazonSageMaker-ExecutionRole-20220000T000000`替换该占位符值。
*   `<INSERT MODEL PACKAGE ARN>`–用`arn:aws:sagemaker:us-west-2:1234567890:model-package/autogluonmodelgroup/1`替换该占位符值。

1.  将这个测试事件 JSON 值复制到本地机器上的文本编辑器中。稍后在测试我们的λ函数时，我们将再次使用这个测试事件 JSON。
2.  之后，点击**保存**按钮。
3.  一切准备就绪后，让我们点击**测试**按钮。这将打开一个新的选项卡，显示几分钟后的执行结果。

注意

完成此步骤可能需要 5-15 分钟。请随意喝杯咖啡或茶！

1.  等待时，向上滚动并找到**功能概述**窗格。将**函数的 ARN** 值复制到你的文本编辑器中。我们将在本章的*完成端到端 ML 流水线*部分使用这个**函数 ARN** 值。

一旦`deploy-model-to-new-endpoint` Lambda 函数完成运行，我们应该已经在 ML 推理端点中部署了 ML 模型。注意我们只是在测试 Lambda 函数，在运行完整的 ML 管道之前，我们将在后面的步骤中删除 ML 推断端点(由`deploy-model-to-new-endpoint` Lambda 函数启动)。

## 准备用于检查端点是否存在的 Lambda 函数

我们将创建的第二个 **AWS Lambda** 函数将被配置和编程，以检查端点是否已经存在(给定端点名称)。为了帮助我们直观地了解我们的函数将如何工作，让我们快速检查一下*图 11.13* :

![Figure 11.13 – Check whether an endpoint exists already

](img/B18638_11_013.jpg)

图 11.13–检查端点是否已经存在

该函数将接受一个输入参数—ML 推理端点的名称。接收到输入参数后，该函数将使用`boto3`库列出该区域中所有正在运行的端点，并检查这些端点之一的名称是否与输入参数值匹配。

在下一组步骤中，我们将创建一个 Lambda 函数，用于检查 ML 推理端点是否已经存在:

1.  打开一个新的浏览器选项卡，导航到 Lambda 管理控制台的**功能**页面。
2.  找到并点击**创建功能**按钮(位于**功能**页面的左上角)，然后指定以下配置值:
    *   **作者从零开始**
    *   `check-if-endpoint-exists`
    *   `Python 3.9`
    *   **权限** > **改变默认执行角色**
    *   `Use an existing role`
    *   `pipeline-lambda-role`
3.  向下滚动到页面底部，然后点击**创建功能**按钮。

注意

将代码块键入(或复制)到`lambda_function.py`内的后续步骤中。你可以在[https://github . com/packt publishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter 11/check-if-endpoint-exists . py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/check-if-endpoint-exists.py)找到 Lambda 函数的代码副本。

1.  在`lambda_function.py`文件中，导入`boto3`并初始化 SageMaker 服务的客户端:

    ```
    import boto3
    ```

    ```
    sm_client = boto3.client('sagemaker')
    ```

2.  接下来，我们来定义一下`endpoint_exists()`的功能:

    ```
    def endpoint_exists(endpoint_name):
    ```

    ```
        response = sm_client.list_endpoints(
    ```

    ```
            NameContains=endpoint_name
    ```

    ```
        )
    ```

    ```
        results = list(
    ```

    ```
            filter(
    ```

    ```
                lambda x: \
    ```

    ```
                x['EndpointName'] == endpoint_name, 
    ```

    ```
                response['Endpoints']
    ```

    ```
            )
    ```

    ```
        )
    ```

    ```
        return len(results) > 0
    ```

3.  现在，让我们用来定义`lambda_handler()`函数，它利用`endpoint_exists()`函数来检查一个 ML 推理端点是否存在(给定端点名称):

    ```
    def lambda_handler(event, context):
    ```

    ```
        endpoint_name = event['endpoint_name']
    ```

    ```
        return {
    ```

    ```
            'endpoint_exists': endpoint_exists(
    ```

    ```
                endpoint_name=endpoint_name
    ```

    ```
            )
    ```

    ```
        }
    ```

4.  点击**部署**按钮。
5.  点击**事件名称**下的`test`，然后在**事件 JSON** 下指定以下 JSON 值:

    ```
    {
    ```

    ```
      "endpoint_name": "AutoGluonEndpoint"
    ```

    ```
    }
    ```

6.  之后，点击**保存**按钮。
7.  一切准备就绪后，让我们点击**测试**按钮。这将打开一个新的选项卡，几秒钟后将显示执行结果。我们应该在测试 Lambda 函数后得到如下响应值:

    ```
    {
    ```

    ```
      "endpoint_exists": true
    ```

    ```
    }
    ```

8.  最后，向上滚动并找到**功能概述**窗格。将**函数的 ARN** 值复制到你的文本编辑器中。我们将在本章的*完成端到端 ML 流水线*部分使用这个**函数 ARN** 值。

现在我们已经完成了准备和测试`check-if-endpoint-exists` Lambda 函数，我们可以继续创建最后一个 Lambda 函数(`deploy-model-to-existing-endpoint`)。

## 准备用于将模型部署到现有端点的 Lambda 函数

我们将创建的第三个 **AWS Lambda** 函数将被配置和编程为将模型部署到现有的端点。为了帮助我们形象化我们的函数将如何工作，让我们快速检查*图 11.14* :

![Figure 11.14 – Deploying a model to an existing endpoint

](img/B18638_11_014.jpg)

图 11.14–将模型部署到现有的端点

该函数将接受三个输入参数 IAM 角色、端点名称和模型包 ARN。在接收到这些输入参数之后，该函数将执行必要的步骤，以使用所提供的模型包中的模型来更新部署在现有 ML 推理端点中的模型。

在下一组步骤中，我们将创建一个 Lambda 函数，用于将 ML 模型部署到现有的推理端点:

1.  打开一个新的浏览器选项卡，导航到 Lambda 管理控制台的**功能**页面。
2.  找到并点击**创建功能**按钮(位于**功能**页面的左上角)，然后指定以下配置值:
    *   **作者从零开始**
    *   `deploy-model-to-existing-endpoint`
    *   `Python 3.9`
    *   **权限** > **改变默认执行角色**
    *   `Use an existing role`
    *   `pipeline-lambda-role`
3.  向下滚动到页面底部，然后点击**创建功能**按钮。
4.  导航到`1024` MB
5.  `15`分钟`0`秒
6.  之后，点击**保存**按钮。
7.  在另一个浏览器标签中打开以下链接:[https://raw . githubusercontent . com/packt publishing/Machine-Learning-Engineering-on-AWS/main/chapter 11/utils . py](https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter11/utils.py)。使用 *Ctrl* + *A* ，然后使用 *Ctrl* + *C* 将页面内容复制到剪贴板中(或者，如果您使用的是 Mac，也可以使用 *CMD* + *A* ，然后使用 *CMD* + *C* )。
8.  回到显示 Lambda 控制台的浏览器选项卡，导航到`Untitled1`。在新选项卡(不包含代码)中，粘贴复制到剪贴板的代码。
9.  打开`utils.py`作为**文件名**字段值，然后点击**保存**。
10.  将导航到选项卡，在这里我们可以修改`lambda_function.py`中的代码。继续之前，删除当前存储在`lambda_function.py`中的样板代码。

注意

在`lambda_function.py`内的后续步骤中键入(或复制)代码块。你可以在[https://github . com/packt publishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter 11/deploy-model-to-existing-endpoint . py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/deploy-model-to-existing-endpoint.py)找到 Lambda 函数的代码副本。

1.  在`lambda_function.py`文件中，导入我们需要更新已部署模型的现有端点的函数:

    ```
    import json
    ```

    ```
    from utils import (
    ```

    ```
        create_model, 
    ```

    ```
        create_endpoint_config, 
    ```

    ```
        update_endpoint, 
    ```

    ```
        random_string,
    ```

    ```
        block
    ```

    ```
    )
    ```

2.  现在，让我们使用下面的代码块来定义`lambda_handler()`函数:

    ```
    def lambda_handler(event, context):
    ```

    ```
        role = event['role']
    ```

    ```
        endpoint_name = event['endpoint_name']
    ```

    ```
        package_arn = event['package_arn']
    ```

    ```
        model_name = 'model-' + random_string()
    ```

    ```
        with block('CREATE MODEL'):
    ```

    ```
            create_model(
    ```

    ```
                model_name=model_name,
    ```

    ```
                package_arn=package_arn,
    ```

    ```
                role=role
    ```

    ```
            )
    ```

    ```
        with block('CREATE ENDPOINT CONFIG'):
    ```

    ```
            endpoint_config_name = create_endpoint_config(
    ```

    ```
                model_name
    ```

    ```
            )
    ```

    ```
        with block('UPDATE ENDPOINT'):
    ```

    ```
            update_endpoint(
    ```

    ```
                endpoint_name=endpoint_name, 
    ```

    ```
                endpoint_config_name=endpoint_config_name
    ```

    ```
        return {
    ```

    ```
            'statusCode': 200,
    ```

    ```
            'body': json.dumps(event),
    ```

    ```
            'model': model_name
    ```

    ```
            'model': model_name
    ```

3.  点击**部署**按钮。
4.  点击**事件名称**和下的`test`，然后在**事件 JSON** 下指定以下 JSON 值:

    ```
    {
    ```

    ```
      "role": "<INSERT SAGEMAKER EXECUTION ROLE ARN>",
    ```

    ```
      "endpoint_name": "AutoGluonEndpoint",
    ```

    ```
      "package_arn": "<INSERT MODEL PACKAGE ARN>"
    ```

    ```
    }
    ```

请确保替换以下值:

*   `<INSERT SAGEMAKER EXECUTION ROLE ARN>`–在本章的*准备必要先决条件*一节中，用复制到您的文本编辑器中的**执行角色 ARN** 替换该占位符值。
*   `<INSERT MODEL PACKAGE ARN>`–在本章的*使用 SageMaker 管道运行我们的第一条管道*一节中，用复制到您的文本编辑器中的**模型包 ARN** 替换该占位符值。

此外，在测试我们的`deploy-model-to-new-endpoint` Lambda 函数时，您可以使用我们复制到文本编辑器中的相同测试事件 JSON 值。

1.  之后，点击**保存**按钮。
2.  一切准备就绪后，让我们点击**测试**按钮。这将打开一个新的标签，几分钟后将显示执行结果。

注意

完成此步骤可能需要 5-15 分钟。请随意喝杯咖啡或茶！

1.  等待时，向上滚动并找到**功能概述**窗格。将**函数的 ARN** 值复制到你的文本编辑器中。我们将在本章的*完成端到端 ML 流水线*部分使用这个**函数 ARN** 值。

准备好所有的 Lambda 函数后，我们现在可以继续测试我们的 ML 推理端点(在完成端到端 ML 管道之前)。

注意

此时，我们应该有 3 个λ函数、`deploy-model-to-new-endpoint`λ函数和`deploy-model-to-existing-endpoint`λ函数。我们将在本章的*完成端到端 ML 管道*部分使用这些 ARN 值。

# 测试我们的 ML 推理端点

当然，我们需要检查 ML 推断端点是否工作！在下一组步骤中，我们将下载并运行一个 Jupyter 笔记本(名为`Test Endpoint and then Delete.ipynb`)，它使用测试数据集测试我们的 ML 推理端点:

1.  让我们首先在另一个浏览器标签中打开以下链接:[https://bit.ly/3xyVAXz](https://bit.ly/3xyVAXz)
2.  右键单击页面的任何部分以打开上下文菜单，然后选择`Test Endpoint then Delete.ipynb`，然后将其下载到本地机器上的`Downloads`文件夹(或类似的文件夹)中。
3.  导航回您的`CH11`文件夹，类似于我们在*图 11.15* 中的内容:

![Figure 11.15 – Uploading the test endpoint and then the Delete.ipynb file

](img/B18638_11_015.jpg)

图 11.15–上传测试端点，然后是 Delete.ipynb 文件

1.  点击上传按钮(在*图 11.15* 中高亮显示)，然后选择我们在之前的步骤中下载的`Test Endpoint then Delete.ipynb`文件。

注意

这将把`Test Endpoint then Delete.ipynb`笔记本文件从你的本地机器上传到 SageMaker Studio 环境中(在`CH11`文件夹中)。

1.  双击**文件树**中的`Test Endpoint then Delete.ipynb`文件，在**主工作区**打开笔记本(包含打开的笔记本、文件、终端的标签页)。
2.  用`Machine Learning Pipelines with SageMaker Pipelines.ipynb`笔记本中使用的 S3 桶的名称更新第一个单元格:

    ```
    s3_bucket = '<INSERT S3 BUCKET HERE>'
    ```

确保用我们在本章的*准备必要的先决条件*一节中复制到我们的文本编辑器的 S3 桶名替换`<INSERT S3 BUCKET HERE>`。

1.  打开`Test Endpoint then Delete.ipynb`笔记本。

注意

运行 Jupyter 笔记本中的所有单元大约需要 1-2 分钟。在等待的时候，请随意喝杯咖啡或茶！

1.  一旦`Test Endpoint then Delete.ipynb`笔记本中的所有单元格都被执行，定位包含以下代码块的单元格(以及返回的输出):

    ```
    from sklearn.metrics import accuracy_score
    ```

    ```
    accuracy_score(actual_list, predicted_list)
    ```

验证模型的准确度得分等于或接近`0.88`(或 88%)。

此时，ML 推断端点应该处于删除状态，因为`Test` `Endpoint then Delete.ipynb` Jupyter 笔记本在计算 ML 模型指标后也运行`predictor.delete_endpoint()`行。

# 完成端到端 ML 管道

在本节中，我们将在本章*使用 SageMaker 管道*运行我们的第一条管道中准备的(部分)管道的基础上进行构建。除了用于构建我们的部分管道的步骤和资源，我们还将利用我们创建的 Lambda 函数(在*创建用于部署的 Lambda 函数*部分)来完成我们的 ML 管道。

## 定义和准备完整的 ML 管道

我们准备的第二条管道将比第一条管道稍长。为了帮助我们形象化使用 **SageMaker 管道**的第二个 ML 管道，让我们快速查看*图 11.16* :

![Figure 11.16 – Our second ML pipeline using SageMaker Pipelines

](img/B18638_11_016.jpg)

图 11.16–我们使用 SageMaker 管道的第二个 ML 管道

这里，我们可以看到我们的管道接受两个输入参数——输入数据集和端点名称。当管道运行时，输入数据集首先被分成定型集、验证集和测试集。然后，使用训练和验证集来训练 ML 模型，然后将该模型注册到 **SageMaker 模型注册表**。之后，管道检查具有所提供端点名称的 ML 推断端点是否已经存在。如果端点还不存在，那么模型被部署到一个新的端点。否则，使用在管道执行期间训练的模型来更新现有端点的模型(具有所提供的端点名称)。

在下一组步骤中，我们将使用`Machine Learning Pipelines with SageMaker Pipelines.ipynb`笔记本中配置的步骤和资源创建一个新的 ML 管道:

1.  导航回与`Machine Learning Pipelines with SageMaker Pipelines.ipynb`笔记本对应的选项卡。

注意

我们将在`Machine Learning Pipelines with SageMaker Pipelines.ipynb`笔记本中的后续步骤中运行代码块(在现有的一组单元格之后)。如果您在运行了*使用 SageMaker Pipelines* 运行我们的第一个管道一节中的命令后关闭了内核和/或 SageMaker Studio 实例，请确保通过从 **Run** 菜单下的选项列表中选择 **Run All Cells** 再次运行所有单元(并等待管道完成运行)。

1.  让我们将映射到`Pipeline`参数的`ParameterString`对象初始化为 ML 推断端点的名称(将在 ML 管道运行结束后创建或更新):

    ```
    input_endpoint_name = ParameterString(
    ```

    ```
        name="EndpointName",
    ```

    ```
        default_value=f'AutoGluonEndpoint', 
    ```

    ```
    )
    ```

2.  接下来，让我们导入我们将需要完成端到端 ML 管道的类:

    ```
    from sagemaker.workflow.lambda_step import (
    ```

    ```
        LambdaStep, 
    ```

    ```
        LambdaOutput, 
    ```

    ```
        LambdaOutputTypeEnum
    ```

    ```
    )
    ```

    ```
    from sagemaker.lambda_helper import (
    ```

    ```
        Lambda
    ```

    ```
    )
    ```

    ```
    from sagemaker.workflow.conditions import (
    ```

    ```
        ConditionEquals
    ```

    ```
    )
    ```

    ```
    from sagemaker.workflow.condition_step import (
    ```

    ```
        ConditionStep, 
    ```

    ```
        JsonGet
    ```

    ```
    )
    ```

3.  准备`LambdaOutput`对象，它将(稍后)映射到`LambdaStep`对象的输出:

    ```
    output_endpoint_exists = LambdaOutput(
    ```

    ```
        output_name="endpoint_exists", 
    ```

    ```
        output_type=LambdaOutputTypeEnum.Boolean
    ```

    ```
    )
    ```

4.  初始化`LambdaStep`对象，该对象映射到Lambda 函数，该函数检查指定的 ML 推断端点是否已经存在(给定端点名称):

    ```
    package_arn = step_model_create \
    ```

    ```
        .properties.ModelPackageArn
    ```

    ```
    endpoint_exists_lambda = LambdaStep(
    ```

    ```
        name="CheckIfEndpointExists",
    ```

    ```
        lambda_func=Lambda(
    ```

    ```
            function_arn="<INSERT FUNCTION ARN>"
    ```

    ```
        ),
    ```

    ```
        inputs={
    ```

    ```
            "endpoint_name": input_endpoint_name,
    ```

    ```
            "package_arn": package_arn
    ```

    ```
        },
    ```

    ```
        outputs=[output_endpoint_exists]
    ```

    ```
    )
    ```

确保用我们复制到文本编辑器中的`check-if-endpoint-exists` Lambda 函数的 ARN 替换`<INSERT FUNCTION ARN>`。它应该具有类似于`arn:aws:lambda:us-west-2:<ACCOUNT ID>:function:check-if-endpoint-exists`的格式。

1.  接下来，初始化对象`LambdaStep`，该对象映射到 Lambda 函数，该函数将经过训练的 ML 模型部署到现有的 ML 推理端点:

    ```
    step_lambda_deploy_to_existing_endpoint = LambdaStep(
    ```

    ```
        name="DeployToExistingEndpoint",
    ```

    ```
        lambda_func=Lambda(
    ```

    ```
            function_arn="<INSERT FUNCTION ARN>"
    ```

    ```
        ),
    ```

    ```
        inputs={
    ```

    ```
            "role": role,
    ```

    ```
            "endpoint_name": input_endpoint_name,
    ```

    ```
            "package_arn": package_arn
    ```

    ```
        },
    ```

    ```
        outputs=[]
    ```

    ```
    )
    ```

确保用我们复制到文本编辑器中的`deploy-model-to-existing-endpoint` Lambda 函数的 ARN 替换`<INSERT FUNCTION ARN>`。它的格式应该类似于`arn:aws:lambda:us-west-2:<ACCOUNT ID>:function:` `deploy-model-to-existing-endpoint`。

1.  之后，初始化`LambdaStep`对象，该对象映射到 Lambda 函数，该函数将经过训练的 ML 模型部署到新的 ML 推理端点:

    ```
    step_lambda_deploy_to_new_endpoint = LambdaStep(
    ```

    ```
        name="DeployToNewEndpoint",
    ```

    ```
        lambda_func=Lambda(
    ```

    ```
            function_arn="<INSERT FUNCTION ARN>"
    ```

    ```
        ),
    ```

    ```
        inputs={
    ```

    ```
            "role": role,
    ```

    ```
            "endpoint_name": input_endpoint_name,
    ```

    ```
            "package_arn": package_arn
    ```

    ```
        },
    ```

    ```
        outputs=[]
    ```

    ```
    )
    ```

确保用我们复制到文本编辑器中的`deploy-model-to-new-endpoint` Lambda 函数的 ARN 替换`<INSERT FUNCTION ARN>`。它应该具有类似于`arn:aws:lambda:us-west-2:<ACCOUNT ID>:function: deploy-model-to-new-endpoint`的格式。

1.  准备好三个`LambdaStep`对象后，让准备`ConditionStep`对象，它检查一个端点是否已经存在(使用`endpoint_exists_lambda` `LambdaStep`对象的输出):

    ```
    left = endpoint_exists_lambda \
    ```

    ```
        .properties               \
    ```

    ```
        .Outputs['endpoint_exists']
    ```

    ```
    cond_equals = ConditionEquals(
    ```

    ```
        left=left,
    ```

    ```
        right=True
    ```

    ```
    )
    ```

    ```
    if_steps = [step_lambda_deploy_to_existing_endpoint]
    ```

    ```
    else_steps = [step_lambda_deploy_to_new_endpoint]
    ```

    ```
    step_endpoint_exists_condition = ConditionStep(
    ```

    ```
        name="EndpointExists",
    ```

    ```
        conditions=[cond_equals],
    ```

    ```
        if_steps=if_steps,
    ```

    ```
        else_steps=else_steps
    ```

    ```
    )
    ```

这一步告诉 ML 管道执行以下操作:

*   如果端点尚不存在，则将模型部署到新端点。
*   如果端点已经存在，则将模型部署到现有端点。

为了帮助我们直观地了解如何配置`ConditionStep`对象，让我们快速检查一下*图 11.17* :

![Figure 11.17 – Configuring and preparing the ConditionStep object

](img/B18638_11_017.jpg)

图 11.17–配置和准备 ConditionStep 对象

在这里，我们可以看到`ConditionStep`对象是用几个参数初始化的——`conditions`、`if_steps`和`else_steps`(除了端点的`name`)。如果`EndpointExists` `LambdaStep`返回`True`，则`DeployToExistingEndpoint` `LambdaStep`被执行。否则，改为执行`DeployToNewEndpoint` `LambdaStep`。

1.  准备好所有的步骤后，让我们使用我们准备的不同步骤对象来初始化一个新的`Pipeline`对象:

    ```
    pipeline_name = f"COMPLETE-PIPELINE"
    ```

    ```
    complete_pipeline = Pipeline(
    ```

    ```
        name=pipeline_name,
    ```

    ```
        parameters=[
    ```

    ```
            input_data,
    ```

    ```
            input_endpoint_name
    ```

    ```
        ],
    ```

    ```
        steps=[
    ```

    ```
            step_process, 
    ```

    ```
            step_train,
    ```

    ```
            step_model_create,
    ```

    ```
            endpoint_exists_lambda, 
    ```

    ```
            step_endpoint_exists_condition
    ```

    ```
        ],
    ```

    ```
    )
    ```

    ```
    complete_pipeline.upsert(role_arn=role)
    ```

请注意，该管道不同于我们在本章*使用 SageMaker 管道*运行第一条管道一节中准备的(部分)管道。一旦我们在下一节中运行这个管道，我们应该会看到它还有一些额外的步骤。

## 运行完整的 ML 管道

万事俱备，我们现在可以运行端到端的 ML 管道了。与我们在本章的*使用 SageMaker 管道*运行我们的第一个管道一节中执行的(部分)管道相比，我们的(完整)管道允许我们指定 ML 推理端点的可选名称(*注意:不要运行下面的代码块*):

```
execution = complete_pipeline.start(

    parameters=dict(

        EndpointName="<INSERT NEW ENDPOINT NAME>",

    )

)
```

如果未指定端点名称，管道将在管道执行期间使用默认端点名称值(即`AutoGluonEndpoint`)继续执行。

在下一组步骤中，我们将运行我们的管道，等待它将经过训练的 ML 模型部署到新的推理端点，然后使用测试数据集测试部署的模型:

1.  在运行完`Machine Learning Pipelines with SageMaker Pipelines.ipynb`笔记本中的最后一个代码块后，继续，让我们使用下面的代码块运行端到端的 ML 管道:

    ```
    execution = complete_pipeline.start()
    ```

    ```
    execution.describe()
    ```

2.  接下来，让我们使用`wait()`方法等待整个流水线完成:

    ```
    execution.wait()
    ```

注意

完成管道执行大约需要 15-30 分钟。在等待的时候，请随意喝杯咖啡或茶！

1.  等待时，找到并点击 SageMaker Studio 左侧边栏底部附近的三角形图标( **SageMaker resources** )。这将打开 **SageMaker 资源**窗格，我们可以在其中查看和检查各种 SageMaker 资源。
2.  从 **SageMaker resources** 窗格的下拉菜单中的可用选项列表中选择 **Pipelines** 。
3.  之后，双击映射到我们刚刚创建的`COMPLETE-PIPELINE`管道的行。之后，双击映射到我们触发的管道执行的行。您应该会看到类似于图 11.18 中所示的图表:

![Figure 11.18 – The ML pipeline is currently running the TrainModel step

](img/B18638_11_018.jpg)

图 11.18–ML 管道当前正在运行 TrainModel 步骤

在这里，我们可以看到`COMPLETE-PIPELINE`管道比我们在本章的*使用 SageMaker 管道*运行我们的第一个管道一节中执行的`PARTIAL-PIPELINE`管道有更多的步骤。

1.  几分钟后,图中应该有更多完成的步骤，类似于图 11.19 中的步骤:

![Figure 11.19 – The ML pipeline proceeds with running the DeployToNewEndpoint step 

](img/B18638_11_019.jpg)

图 11.19–ML 管道继续运行部署新端点步骤

在这里，我们可以看到，由于 ML 端点还不存在(因为我们在运行`Test Endpoint then Delete.ipynb`笔记本时删除了它，ML 管道继续运行**deploytowendpoint**步骤。请注意，对于后续运行，如果 ML 端点已经存在，则应该改为运行**deploytexistingendpoint**步骤。

重要说明

如果在运行 Lambda 函数时遇到以下错误，请确保执行角色(附加到附加的`AWSLambda_FullAccess`权限策略:**client error:User:<ARN>无权对资源执行 lambda:InvokeFunction:<arn>，因为没有基于身份的策略允许 Lambda:invoke function 操作**。请随意查看本章的*准备基本先决条件*一节，了解如何更新执行角色权限的分步说明。

1.  等待管道执行完成。一旦管道完成运行，我们的自动机模型应该部署在一个 ML 推理端点(名为`AutoGluonEndpoint`)中。
2.  导航回与`Test Endpoint then Delete.ipynb`笔记本对应的选项卡。打开`Test Endpoint then Delete.ipynb`笔记本。请注意，运行笔记本中的所有单元还会在所有单元完成运行后删除现有的 ML 推断端点(名为`AutoGluonEndpoint`)。

注意

运行 Jupyter 笔记本中的所有单元需要 1-2 分钟。在等待的时候，请随意喝杯咖啡或茶！

1.  一旦执行完`Test Endpoint then Delete.ipynb`笔记本中的所有单元格，定位包含以下代码块的单元格(以及返回的输出):

    ```
    from sklearn.metrics import accuracy_score
    ```

    ```
    accuracy_score(actual_list, predicted_list)
    ```

验证我们的模型获得了等于或接近`0.88`(或 88%)的准确度分数。注意，这应该类似于我们在本章前面的*测试我们的 ML 推断端点*部分中获得的结果。

*我们可以用这条管道做什么？*有了这个管道，通过为每个管道运行指定不同的端点名称，我们将能够训练和部署一个模型到多个端点。这将有助于我们处理需要为不同环境(比如`production`和`staging`环境)管理专用 ML 推理端点的场景。例如，我们可以同时拥有两个运行的 ML 推理端点— `AutoGluonEndpoint-production`和`AutoGluonEndpoint-staging`。如果我们希望从新的数据集生成一个新的模型，我们可以触发一个管道运行，并为`staging`环境而不是`production`环境指定端点名称。这将帮助我们测试和验证在`staging`环境中部署的新模型的质量，并确保`production`环境始终处于稳定状态。一旦我们需要更新`production`环境，我们可以简单地触发另一个管道运行，并在训练和部署新模型时指定与`production`环境相关联的端点名称。

注意

有几种方法来管理这些类型的部署，这是 ML 工程师和数据科学家可用的选项之一。

差不多就是这样！恭喜你能够完成一个相对更复杂的 ML 管道！在这一章中，我们已经完成了很多，我们应该准备好设计和构建我们自己的定制管道。

# 清理

现在我们已经完成了本章的动手解决方案，是时候清理并关闭我们不再使用的资源了。在下一组步骤中，我们将在 **SageMaker Studio** 中找到并关闭任何剩余的运行实例:

1.  确保检查并删除 **SageMaker resources** 下所有正在运行的推理端点(如果有的话)。要检查是否有正在运行的推理端点，点击 **SageMaker 资源**图标，然后从下拉菜单的选项列表中选择**端点**。
2.  打开**文件**菜单，从可用选项列表中选择**关机**。这应该会关闭 SageMaker Studio 中所有正在运行的实例。

需要注意的是，这个清理操作需要在使用 **SageMaker Studio** 之后进行。即使在不活动期间，SageMaker 也不会自动关闭这些资源。在继续下一部分之前，请确保检查所有删除操作是否都已成功。

注意

也可以随意清理和删除 AWS 帐户中的所有其他资源(例如，Cloud9 环境和我们创建的 VPCs 和 Lambda 函数)。

# 推荐的策略和最佳实践

在我们结束本章(和本书)之前，让我们快速讨论一下使用 SageMaker 管道准备自动化 ML 工作流时的一些推荐策略和最佳实践。*我们可以对最初的管道做哪些改进？*以下是我们可以实施的一些可能的升级，以使我们的设置更具可扩展性、更安全，并更有能力处理不同类型的 ML 和 ML 工程要求:

*   在创建时配置和设置 ML 推断端点的**自动缩放**(自动缩放)，以动态调整用于处理(ML 推断请求的)传入流量的资源数量。
*   允许 ML 模型也部署在**无服务器**和**异步**端点中(取决于额外的管道输入参数的值)，以帮助为各种用例提供额外的模型部署选项。
*   在流水线中添加额外的步骤(或多个步骤),该步骤使用测试集自动评估已训练的 ML 模型，并且如果目标度量值低于指定的阈值分数，则拒绝模型的部署。
*   在管道中增加一个额外的步骤，使用 **SageMaker Clarify** 检查偏差和漂移。
*   一旦通过 **Amazon EventBridge** 发生一个事件(比如一个文件被上传到 Amazon S3 桶中)，就触发一个管道执行。
*   缓存特定的管道步骤，以加速重复的管道执行。
*   当管道执行过程中出现异常和错误时，利用**重试策略**自动重试特定的管道步骤。
*   使用 **SageMaker Pipelines** 和 **SageMaker 项目**构建完整的 ML 工作流，其中可能涉及 CI/CD 功能(使用 AWS 服务，如 **AWS CodeCommit** 和 **AWS CodePipeline** )。
*   使用更严格的权限集更新本章中使用的 IAM 角色，以提高设置的安全性。
*   为了管理运行 SageMaker 资源的长期成本，我们可以利用**机器学习节约计划**，这涉及到在做出长期承诺(例如，1 年或 3 年的承诺)后降低运行资源的总体成本

还有更多的我们可以添加到这个列表，但这些应该做了！请确保您也查看并检查了第 9 章*安全、治理和合规性策略*中推荐的解决方案和策略。

# 总结

在本章中，我们使用了 **SageMaker 管道**来构建端到端的自动化 ML 管道。我们首先准备了一个相对简单的管道，包含三个步骤——包括数据准备步骤、模型训练步骤和模型注册步骤。在准备和定义管道后，我们继续触发管道执行，在管道执行完成运行后，将新训练的模型注册到 **SageMaker 模型注册表**。

然后，我们准备了三个 AWS Lambda 函数，用于第二个 ML 管道的模型部署步骤。在准备好 Lambda 函数之后，我们通过添加一些额外的步骤来完成端到端的 ML 管道，以将模型部署到新的或现有的 ML 推理端点。最后，我们讨论了使用我们在本章中使用的技术栈来保护、扩展和管理 ML 管道的相关最佳实践和策略。

你终于看到这本书的结尾了！祝贺您完成了本书中讨论的所有章节，包括实践示例和解决方案。从开始到结束，这是一个令人惊奇的旅程，如果你也能和别人分享这个旅程，那就太好了。

# 延伸阅读

在这一点上，你可能想要通过检查前面每一章的*进一步阅读*部分中列出的参考资料来更深入地探讨相关的子主题。除此之外，您还可以查看以下资源:

*   *亚马逊 SageMaker 模型构建管道——管道步骤*([https://docs . AWS . Amazon . com/SageMaker/latest/DG/build-and-manage-Steps . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.xhtml))
*   *boto 3–SageMaker 客户端*([https://boto 3 . Amazon AWS . com/v1/documentation/API/latest/reference/services/SageMaker . XHTML](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.xhtml)
*   *亚马逊 SageMaker–autoglon-Tabular 算法*([https://docs . AWS . Amazon . com/SageMaker/latest/DG/autoglon-Tabular . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/autogluon-tabular.xhtml))
*   *使用 SageMaker 项目自动执行 mlop*([https://docs . AWS . Amazon . com/sage maker/latest/DG/sage maker-Projects . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects.xhtml))
*   *机器学习储蓄计划*(【https://aws.amazon.com/savingsplans/ml-pricing/】T2)
*   *SageMaker-Amazon event bridge 集成*([https://docs . AWS . Amazon . com/SageMaker/latest/DG/pipeline-event bridge . XHTML](https://docs.aws.amazon.com/sagemaker/latest/dg/pipeline-eventbridge.xhtml))