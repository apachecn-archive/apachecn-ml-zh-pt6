<html><head/><body>









<title>Chapter 7: SageMaker Deployment Solutions</title>







<div><div><h1 class="chapter-number" id="_idParaDest-141"><a id="_idTextAnchor150"/> <a id="_idTextAnchor151"/> 7</h1>

<h1 id="_idParaDest-142"><a id="_idTextAnchor152"/> SageMaker部署解决方案</h1>

<p>在训练完我们的<strong class="bold">机器学习</strong> ( <strong class="bold"> ML </strong>)模型之后，我们可以继续将它部署到web API。然后，其他应用程序(例如，移动应用程序)可以调用该API来执行“预测”或推断。例如，我们在<a href="B18638_01.xhtml#_idTextAnchor017"> <em class="italic">第1章</em> </a>、<em class="italic">AWS上的ML工程介绍</em>中训练的ML模型，可以部署到web API，然后在给定一组输入的情况下，用于预测客户取消预订或不取消预订的可能性。将ML模型部署到web API允许不同的应用程序和系统访问ML模型。</p>

<p>几年前，ML从业者不得不花时间构建一个定制的后端API来从零开始托管和部署模型。如果您有这个需求，您可能会使用Python框架，如<strong class="bold"> Flask </strong>、<strong class="bold"> Pyramid </strong>或<strong class="bold"> Django </strong>来部署ML模型。构建一个自定义API作为推理端点可能需要一周左右的时间，因为大多数应用程序逻辑需要从头开始编写。如果我们要为API设置<strong class="bold"> A/B测试</strong>、<strong class="bold">自动缩放</strong>或<strong class="bold">模型监控</strong>，那么除了最初花费的时间之外，我们可能还需要花费几周时间来设置基础API。ML工程师和软件开发人员通常低估了构建和维护ML推理端点所需的工作量。需求随着时间的推移而发展，随着需求和解决方案的堆积，定制的应用程序代码变得更加难以管理。此时，你可能会问:“有没有更好更快的方法来做这件事？”。好消息是，如果我们使用<strong class="bold"> SageMaker </strong>来部署我们的模型，我们可以在“不到一天”的时间内完成所有的工作！SageMaker已经自动化了大部分工作，我们需要做的只是指定正确的配置参数，而不是从头开始构建一切。如果需要，SageMaker允许我们定制某些组件，我们可以轻松地用我们自己的定制实现替换一些默认的自动化解决方案。</p>

<p>使用SageMaker时的一个误解是，ML模型需要先在SageMaker中接受培训，然后才能部署在<strong class="bold"> SageMaker托管服务</strong>中。值得注意的是“这不是真的”,因为该服务是为支持不同的场景而设计和构建的，包括立即部署预先训练好的模型。这意味着，如果我们有一个在SageMaker之外训练的预训练模型，那么我们<em class="italic">可以</em>继续部署它，而不必再次经历训练步骤。在本章中，您将发现在执行模型部署时，使用<strong class="bold"> SageMaker Python SDK </strong>是多么容易。在短短几行代码中，我们将向您展示如何将我们预训练的模型部署到各种推理端点类型中——<strong class="bold">实时</strong>、<strong class="bold">无服务器</strong>和<strong class="bold">异步推理端点</strong>。在本章的后面，我们还将讨论何时最好使用这些推理端点类型。同时，我们将讨论在SageMaker中执行模型部署时的不同策略和最佳实践。</p>

<p>也就是说，我们将涵盖以下主题:</p>

<ul>

<li>SageMaker中的模型部署入门</li>

<li>准备预训练的模型工件</li>

<li>准备SageMaker脚本模式先决条件</li>

<li>将预训练模型部署到实时推理端点</li>

<li>将预训练模型部署到无服务器推理端点</li>

<li>将预训练模型部署到异步推理端点</li>

<li>清理</li>

<li>部署策略和最佳实践</li>

</ul>

<p>我们将快速讨论部署ML模型时的其他选择和选项。在你完成了本章的动手解决方案后，你将会更有信心在SageMaker中部署不同类型的ML模型。一旦您对使用SageMaker Python SDK达到了一定的熟悉和掌握程度，您应该能够在短短几个小时，甚至几分钟内建立一个ML推断端点！</p>

<h1 id="_idParaDest-143"><a id="_idTextAnchor153"/>技术要求</h1>

<p>开始之前，请务必准备好以下内容:</p>

<ul>

<li>网络浏览器(最好是Chrome或Firefox)</li>

<li>访问本书第一章中使用的AWS帐户和<strong class="bold"> SageMaker Studio </strong>域</li>

</ul>

<p>Jupyter笔记本、源代码和其他用于每章的文件都可以在这个资源库中获得:<a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS">https://github . com/packt publishing/Machine-Learning-Engineering-on-AWS</a>。</p>

<p class="callout-heading">重要说明</p>

<p class="callout">运行本书中的示例时，建议使用具有有限权限的IAM用户，而不是root帐户。我们将在第9章 、<em class="italic">安全、治理和遵从策略</em>中详细讨论这一点以及其他安全最佳实践。如果您刚刚开始使用AWS，您可以同时继续使用root帐户。</p>

<h1 id="_idParaDest-144"><a id="_idTextAnchor154"/>sage maker中的模型部署入门</h1>

<p>在<a href="B18638_06.xhtml#_idTextAnchor132"> <em class="italic">第六章</em> </a>、<em class="italic"> SageMaker训练和调试解决方案</em>中，我们使用<strong class="bold"> SageMaker Python SDK </strong>训练和部署了<a id="_idIndexMarker733"/>一个图像分类模型。在那一章中，我们在着手解决<a id="_idIndexMarker734"/>问题时使用了内置算法。当使用内置算法<a id="_idIndexMarker735"/>时，我们只需准备好训练数据集，并指定一些配置参数，就万事俱备了！请注意，如果我们想使用我们最喜欢的ML框架(如TensorFlow和PyTorch)训练一个定制模型，那么我们可以准备我们的定制<a id="_idIndexMarker736"/>脚本，并使用<strong class="bold">脚本模式</strong>使它们在SageMaker中工作。这给了我们更多的灵活性，因为我们可以通过一个定制的脚本来调整SageMaker与我们的模型的接口，这个脚本允许我们在训练我们的模型时使用不同的库和框架。如果我们希望训练脚本运行的环境具有最高级别的灵活性，那么我们可以选择使用我们自己的定制容器映像。SageMaker有自己的一套在训练模型时使用的预构建容器图像。然而，如果需要的话，我们可以决定构建和使用我们自己的。</p>

<div><div><img alt="Figure 7.1 – The different options when training and deployment models&#10;&#10;" height="647" src="img/B18638_07_001.jpg" width="1148"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.1–培训和部署模型时的不同选项</p>

<p>正如我们在<em class="italic">图7.1 </em>中看到的，在SageMaker中训练ML <a id="_idIndexMarker737"/>模型时可用的选项在使用SageMaker托管服务部署模型<a id="_idIndexMarker738"/>时也可用。在这里，我们给每个方法或选项贴上任意的标签(例如，<strong class="bold"> T1 </strong>或<strong class="bold"> T2 </strong>)来帮助我们更详细地讨论这些选项。当在SageMaker中执行模型部署时，我们可以选择使用内置算法的容器(<strong class="bold"> D1 </strong>)来部署模型。我们还可以选择使用<strong class="bold">脚本模式</strong> ( <strong class="bold"> D2 </strong>)部署我们的深度<a id="_idIndexMarker739"/>学习模型。有了这个选项，我们需要准备将在预建的<strong class="bold">深度学习容器</strong>内运行的定制脚本。我们还可以选择为将要部署ML模型的环境提供和使用我们自己的定制容器映像(<strong class="bold"> D3 </strong>)。</p>

<p class="callout-heading">重要说明</p>

<p class="callout">选择使用哪种选项组合通常取决于执行ML实验和部署时所需的定制级别(以定制脚本和容器映像的形式)。在开始使用SageMaker时，建议使用SageMaker内置算法，以便在训练模型(<strong class="bold"> T1 </strong>)和部署模型(<strong class="bold"> D1 </strong>)时更好地了解事情是如何工作的。如果我们需要在带有SageMaker的AWS托管基础设施之上使用TensorFlow、PyTorch或MXNet等框架，我们将需要准备一组定制脚本，以便在培训(<strong class="bold"> T2 </strong>)和部署(<strong class="bold"> D2 </strong>)期间使用。最后，当我们需要更大的灵活性时，我们可以准备定制的容器映像，并在训练模型(<strong class="bold"> T3 </strong>)和部署模型(<strong class="bold"> D3 </strong>)时使用它们。</p>

<p class="callout">值得注意的是，在训练和部署模型时，我们可以组合和使用不同的选项。例如，我们可以使用脚本模式(<strong class="bold"> T2 </strong>)训练一个ML模型，并在模型部署期间使用一个定制的容器映像(<strong class="bold"> D3 </strong>)。另一个例子包括在SageMaker ( <strong class="bold"> T4 </strong>)之外训练一个模型，并在模型部署(<strong class="bold"> D1 </strong>)期间为内置算法使用预先构建的推理容器映像。</p>

<p>现在，让我们讨论一下<a id="_idIndexMarker740"/>模型部署如何使用<a id="_idIndexMarker741"/>sage maker托管服务:</p>

<div><div><img alt="Figure 7.2 – Deploying a model using the SageMaker hosting services&#10;&#10;" height="559" src="img/B18638_07_002.jpg" width="1359"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.2–使用SageMaker托管服务部署模型</p>

<p>在<em class="italic">图7.2 </em>中，我们有一个使用SageMaker托管服务的模型部署工作方式的高级图表。假设在训练步骤之后，<code>model.tar.gz</code>文件(包含ML模型工件和输出文件)已经被上传到S3存储桶，那么<code>model.tar.gz</code>文件从S3存储桶被下载到ML计算实例中，该实例用作ML推理端点的专用服务器。在这个ML compute实例中，存储在<code>model.tar.gz</code>文件中的模型工件被加载到一个包含推理代码的运行容器<a id="_idIndexMarker742"/>中，该容器可以加载<a id="_idIndexMarker743"/>模型并使用它对传入的请求进行推理。如前所述，推理代码和用于推理的容器映像既可以由AWS(内置或预建)提供，也可以由ML工程师使用SageMaker(自定义)提供。</p>

<p>让我们展示几个示例代码块来帮助我们解释这些概念。我们的第一个示例涉及使用内置的<strong class="bold">主成分分析</strong> ( <strong class="bold"> PCA </strong>)算法来训练<a id="_idIndexMarker744"/>和部署模型，该算法可用于诸如降维和数据压缩等用例:</p>

<pre class="source-code">from sagemaker import PCA

<strong class="bold"># [1] TRAINING</strong>

estimator = PCA(

    role=role,

    instance_count=1,

    instance_type='ml.c4.xlarge',

    num_components=2,

    sagemaker_session=session

)

estimator.fit(...)

<strong class="bold"># [2] DEPLOYMENT</strong>

predictor = estimator.<strong class="bold">deploy</strong>(

    initial_instance_count=1,

    instance_type='ml.t2.medium'

)</pre>

<p>在这里，SageMaker在训练和部署PCA模型时使用了一个预构建的容器映像。这个容器映像是由AWS团队准备的，因此我们在使用内置算法时不必担心自己实现它。注意，我们也可以跳过<a id="_idIndexMarker745"/>训练步骤，继续进行<a id="_idIndexMarker746"/>SageMaker中的部署步骤，只要我们有一个预先训练好的模型，并且这个模型与内置算法的预先构建的容器兼容。</p>

<p>现在，让我们快速看一下在SageMaker中部署模型时如何使用定制脚本的例子:</p>

<pre class="source-code">from sagemaker.pytorch.model import PyTorchModel

<strong class="bold"># [1] HERE, WE DON'T SHOW THE TRAINING STEP</strong>

model_data = estimator.model_data

<strong class="bold"># [2] DEPLOYMENT</strong>

model = PyTorchModel(

    model_data=model_data, 

    role=role, 

    source_dir="scripts",

    entry_point='<strong class="bold">inference.py</strong>', 

    framework_version='1.6.0',

    py_version="py3"

)

predictor = model.<strong class="bold">deploy</strong>(

    instance_type='ml.m5.xlarge', 

    initial_instance_count=1

)</pre>

<p>在这个例子中，SageMaker利用预先构建的深度学习容器映像来部署PyTorch模型。如<a href="B18638_03.xhtml#_idTextAnchor060"> <em class="italic">第三章</em> </a>、<em class="italic">深度学习容器</em>中所讨论的，相关的包和依赖项已经安装在这些容器镜像里面了。在部署步骤中，容器运行在<code>PyTorchModel</code>对象初始化期间提供的自定义<code>inference.py</code>脚本中指定的自定义代码。定制的<a id="_idIndexMarker747"/>代码将加载模型<a id="_idIndexMarker748"/>，并在处理发送到SageMaker推断端点的请求时使用它。</p>

<p class="callout-heading">注意</p>

<p class="callout">在提供的例子中，我们初始化了一个<code>PyTorchModel</code>对象，并使用<code>deploy()</code>方法将模型部署到一个实时推理端点。在推断端点内部，使用PyTorch推断容器映像的容器将运行装载模型并使用它进行推断的推断代码。注意，对于其他库和框架，我们也有相应的<code>Model</code>类，比如<code>TensorFlowModel</code>、<code>SKLearnModel</code>和<code>MXNetModel</code>。一旦调用了<code>deploy()</code>方法，适当的推理容器(带有相关的安装包和依赖项)将在推理端点中使用。</p>

<p>如果我们想要指定和使用我们自己的自定义容器图像，我们可以使用下面的代码块:</p>

<pre class="source-code">from sagemaker.model import Model

<strong class="bold"># [1] HERE, WE DON'T SHOW THE TRAINING STEP</strong>

model_data = estimator.model_data

<strong class="bold"># [2] DEPLOYMENT</strong>

image_uri = "<strong class="bold">&lt;INSERT ECR URI OF CUSTOM CONTAINER IMAGE&gt;</strong>"

model = Model(

    image_uri=image_uri, 

    model_data=model_data,

    role=role,

    sagemaker_session=session

)

predictor = model.<strong class="bold">deploy</strong>(

    initial_instance_count=1, 

    instance_type='ml.m5.xlarge'

)</pre>

<p>在这个例子中，SageMaker利用了存储在<code>image_uri</code>变量中指定位置的<a id="_idIndexMarker750"/>中的自定义<a id="_idIndexMarker749"/>容器图像。这里，假设我们已经准备并测试了定制容器映像，并且在执行<a id="_idIndexMarker751"/>模型部署步骤之前，我们已经将该容器映像推送到<strong class="bold">Amazon Elastic Container Registry</strong>存储库中。</p>

<p class="callout-heading">注意</p>

<p class="callout">在准备定制脚本和定制容器映像时需要一点试错(类似于我们如何在<a href="B18638_03.xhtml#_idTextAnchor060"> <em class="italic">第3章</em> </a>、<em class="italic">深度学习容器</em>中准备和测试我们的定制容器映像)。如果您正在使用一个笔记本实例，那么您可以使用SageMaker <strong class="bold">本地模式</strong>，这为我们提供了一种在托管ML实例中运行这些之前在本地环境中测试定制<a id="_idIndexMarker752"/>脚本和定制容器映像的方法。</p>

<p>本节中显示的代码示例假设我们将在实时推理端点中部署ML模型。然而，在SageMaker中部署ML模型时，有不同的选项<a id="_idIndexMarker753"/>可供选择:</p>

<ul>

<li>第一个选项<a id="_idIndexMarker754"/>涉及在一个<strong class="bold">实时推理端点</strong>中部署和托管我们的模型。</li>

<li>第二个选项<a id="_idIndexMarker755"/>涉及到在使用SageMaker Python SDK将我们的模型部署到<strong class="bold">无服务器推理端点</strong>时对配置进行一点调整。</li>

<li>第三个选项<a id="_idIndexMarker756"/>是在一个<strong class="bold">异步推理端点</strong>中托管我们的模型。</li>

</ul>

<p>我们将在本章的实践部分讨论这些选项，并且我们还将讨论每个选项的相关使用案例和场景。</p>

<p class="callout-heading">注意</p>

<p class="callout">值得注意的是，不需要<a id="_idIndexMarker757"/>设置推理端点，也可以用模型进行推理。这涉及到使用<strong class="bold">批量转换</strong>，其中一个模型被加载并用于处理多个输入有效载荷值并一次性执行预测。要查看批量转换的工作示例<a id="_idIndexMarker758"/>，请随意查看以下链接:<a href="https://bit.ly/3A9wrVy">https://bit.ly/3A9wrVy</a>。</p>

<p>现在我们对SageMaker模型部署<a id="_idIndexMarker760"/>的工作方式有了更好的了解<a id="_idIndexMarker759"/>，让我们继续本章的实践部分。在下一节中，我们将准备包含ML模型工件的<code>model.tar.gz</code>文件，我们将在本章的模型部署解决方案中使用它。</p>

<h1 id="_idParaDest-145"><a id="_idTextAnchor155"/>准备预训练的模型工件</h1>

<p>在<a href="B18638_06.xhtml#_idTextAnchor132"> <em class="italic">第6章</em> </a>，<em class="italic"> SageMaker培训和调试解决方案</em>中，我们创建了<a id="_idIndexMarker761"/>一个名为<code>CH06</code>的新文件夹，以及一个使用所创建文件夹中的<code>Data Science</code>图像的新笔记本。在本节中，我们将创建一个新文件夹(名为<code>CH07</code>)，并在创建的文件夹中创建一个新笔记本。代替<code>Data Science</code>图像，我们将使用<code>PyTorch 1.10 Python 3.8 CPU Optimized</code>图像作为笔记本中使用的图像，因为我们将下载预训练<code>transformers</code>库的模型<a id="_idIndexMarker762"/>工件。一旦笔记本<a id="_idIndexMarker763"/>准备好，我们将使用拥抱脸<code>transformers</code>库下载一个预先训练好的模型，可以用于情感分析。最后，我们将模型工件压缩到一个<code>model.tar.gz</code>文件中，并上传到一个S3桶中。</p>

<p class="callout-heading">注意</p>

<p class="callout">在继续之前，确保您已经完成了第1章 、<em class="italic">AWS</em>上的ML工程介绍<em class="italic">sage maker和SageMaker Studio </em>部分中的动手解决方案。需要注意的是，本章的动手操作部分并不是我们在<a href="B18638_06.xhtml#_idTextAnchor132"> <em class="italic">第6章</em> </a>、<em class="italic"> SageMaker培训和调试解决方案</em>中所完成内容的延续。只要我们有SageMaker工作室成立，我们应该很好去。</p>

<p>在下一组<a id="_idIndexMarker764"/>步骤中，我们将准备包含模型工件的<code>model.tar.gz</code>文件，然后将它上传到S3存储桶:</p>

<ol>

<li>导航到<code>sagemaker studio</code>进入AWS管理控制台的搜索栏<a id="_idIndexMarker765"/>，然后从<strong class="bold">功能</strong>下的结果列表中选择<strong class="bold"> SageMaker Studio </strong>。我们点击侧边栏中<strong class="bold"> SageMaker Domain </strong>下的<strong class="bold"> Studio </strong>，然后从<strong class="bold"> Launch app </strong>下拉菜单(在<strong class="bold"> Users </strong>窗格上)下的下拉选项列表中选择<strong class="bold"> Studio </strong>。等待一两分钟，等待SageMaker Studio界面加载。</li>

</ol>

<p class="callout-heading">重要说明</p>

<p class="callout">本章假设我们在使用服务管理和创建不同类型的资源时使用了<code>us-west-2</code>区域。您可以使用不同的区域，但请确保在需要将某些资源转移到所选区域时进行必要的调整。</p>

<ol>

<li value="2">右击<code>CH07</code>中的空白区域。最后，双击侧边栏中的文件夹名称，导航到<code>CH07</code>目录。</li>

<li>点击<code>PyTorch 1.10 Python 3.8 CPU Optimized</code>创建一个新的笔记本</li>

<li><code>Python 3</code></li>

<li><code>No script</code></li>



<li>之后点击<strong class="bold">选择</strong>按钮。</li>

</ol>

<p class="callout-heading">注意</p>

<p class="callout">等待内核启动。在配置ML实例以运行Jupyter笔记本单元时，此步骤可能需要大约3到5分钟。</p>

<ol>

<li value="5">将笔记本从<code>Untitled.ipynb</code>重命名为<code>01 - Prepare model.tar.gz file.ipynb</code>。</li>

<li>现在我们的笔记本<a id="_idIndexMarker766"/>已经准备好了，我们可以继续生成预先训练好的模型工件，并将它们存储在一个<code>model.tar.gz</code>文件中。在Jupyter笔记本的第一个单元格中，让我们运行下面的代码，它将安装拥抱脸<code>transformers</code>库:<pre class="source-code">!pip3 install <strong class="bold">transformers</strong>==4.4.2</pre></li>

<li>同样使用<code>pip</code>安装<code>ipywidgets</code>:<pre class="source-code">!pip3 install ipywidgets --quiet</pre></li>

<li>接下来，让我们运行下面的代码块来重启内核:<pre class="source-code">import IPython</pre> <pre class="source-code">kernel = IPython.Application.instance().kernel</pre> <pre class="source-code">kernel.<strong class="bold">do_shutdown</strong>(True)</pre></li>

</ol>

<p class="list-inset">这将产生类似于<code>{'status': 'ok', 'restart': True}</code>的输出值，并相应地重启内核，以确保我们在使用刚刚安装的包时不会遇到问题。</p>

<ol>

<li value="9">让我们使用<code>transformers</code>库下载<a id="_idIndexMarker767"/>一个预先训练好的模型。我们将下载一个模型，该模型可用于情绪分析，并对陈述是<em class="italic">正面</em>还是<em class="italic">负面</em>进行分类。运行下面的代码块，将预先训练好的<code>distilbert</code>模型的工件下载到当前目录:<pre class="source-code">from transformers import AutoModelForSequenceClassification as AMSC</pre> <pre class="source-code">pretrained = "<strong class="bold">distilbert-base-uncased-finetuned-sst-2-english</strong>"</pre> <pre class="source-code">model = AMSC.from_pretrained(pretrained)</pre> <pre class="source-code">model.<strong class="bold">save_pretrained</strong>(save_directory=".")</pre></li>

</ol>

<p class="list-inset">这将在与<code>.ipynb</code>笔记本文件相同的目录下生成<a id="_idIndexMarker768"/>两个文件:</p>

<ul>

<li><code>config.json</code></li>

<li><code>pytorch_model.bin</code></li>

</ul>

<p class="callout-heading">注意</p>

<p class="callout">这应该如何工作？例如，如果我们有一个“<code>I love reading the book MLE on AWS!</code>”语句，经过训练的模型应该将其归类为<em class="italic">正</em>语句。如果我们有一个“<code>This is the worst spaghetti I've had</code>”语句，那么经过训练的模型应该将其归类为一个<em class="italic">否定</em>语句。</p>

<ol>

<li value="10">使用下面的代码块准备<code>model.tar.gz</code>(压缩存档)文件，该文件包含在前面的步骤中生成的模型工件文件:<pre class="source-code">import tarfile</pre> <pre class="source-code">tar = tarfile.open("<strong class="bold">model.tar.gz</strong>", "w:gz")</pre> <pre class="source-code">tar.add("<strong class="bold">pytorch_model.bin</strong>")</pre> <pre class="source-code">tar.add("<strong class="bold">config.json</strong>")</pre> <pre class="source-code">tar.close()</pre></li>

<li>使用<code>rm</code>命令，通过删除前面步骤生成的模型工件来清理模型文件:<pre class="source-code">%%bash</pre> <pre class="source-code">rm <strong class="bold">pytorch_model.bin</strong></pre> <pre class="source-code">rm <strong class="bold">config.json</strong></pre></li>

<li>指定S3铲斗<a id="_idIndexMarker769"/>的名称和前缀。确保在运行下面的代码块之前，用唯一的S3存储桶名称替换<code>&lt;INSERT S3 BUCKET NAME HERE&gt;</code>的值:<pre class="source-code">s3_bucket = "<strong class="bold">&lt;INSERT S3 BUCKET NAME HERE&gt;</strong>"</pre> <pre class="source-code">prefix = "chapter07"</pre></li>

</ol>

<p class="list-inset">确保为尚不存在的S3时段指定一个时段名称。如果您想要重用在前面章节中创建的一个bucket，您可以这样做，但是要确保在安装和配置SageMaker Studio的同一个区域中使用S3 bucket。</p>

<ol>

<li value="13">使用<code>aws s3 mb</code>命令:<pre class="source-code">!aws s3 mb s3://{s3_bucket}</pre>创建一个新的S3桶</li>

</ol>

<p class="list-inset">如果您计划重用在前面章节中创建的现有S3存储桶之一，可以跳过这一步。</p>

<ol>

<li value="14">准备上传模型文件的S3路径:<pre class="source-code"><strong class="bold">model_data</strong> = "s3://{}/{}/model/<strong class="bold">model.tar.gz</strong>".format(</pre> <pre class="source-code">    s3_bucket, prefix</pre> <pre class="source-code">)</pre></li>

</ol>

<p class="list-inset">注意，此时在指定的S3路径中还不存在一个<code>model.tar.gz</code>文件。在这里，我们只是准备了S3的位置(字符串)来上传<code>model.tar.gz</code>文件。</p>

<ol>

<li value="15">现在，让我们使用<code>aws s3 cp</code>命令将<code>model.tar.gz</code>文件复制并上传到S3桶:<pre class="source-code">!<strong class="bold">aws s3 cp</strong> model.tar.gz {<strong class="bold">model_data</strong>}</pre></li>

<li>使用<code>%store</code>魔法存储<code>model_data</code>、<code>s3_bucket</code>和<code>prefix</code> : <pre class="source-code">%store <strong class="bold">model_data</strong></pre> <pre class="source-code">%store <strong class="bold">s3_bucket</strong></pre> <pre class="source-code">%store <strong class="bold">prefix</strong></pre>的变量值</li>

</ol>

<p class="list-inset">这应该<a id="_idIndexMarker770"/>允许我们在本章的一个或多个后续章节中使用这些变量值，类似于<em class="italic">图7.3 </em>中的内容:</p>

<div><div><img alt="Figure 7.3 – The %store magic&#10;&#10;" height="631" src="img/B18638_07_003.jpg" width="1192"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.3–商店魔力百分比</p>

<p>确保不要重启内核<a id="_idIndexMarker771"/>，否则我们会丢失使用<code>%store</code>魔法保存的变量值。</p>

<h1 id="_idParaDest-146"><a id="_idTextAnchor156"/>准备SageMaker脚本模式先决条件</h1>

<p>在这一章中，我们将准备<a id="_idIndexMarker772"/>一个自定义脚本来使用预训练的模型进行预测。在<a id="_idIndexMarker773"/>之前，我们可以继续使用<strong class="bold"> SageMaker Python SDK </strong>将我们的预训练模型部署到推理端点，我们需要确保所有脚本模式先决条件都已准备好。</p>

<div><div><img alt="Figure 7.4 – The desired file and folder structure&#10;&#10;" height="764" src="img/B18638_07_004.jpg" width="1284"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.4–所需的文件和文件夹结构</p>

<p>在<em class="italic">图7.4 </em>中，我们可以看到我们需要准备三个先决条件:</p>

<ul>

<li><code>inference.py</code></li>

<li><code>requirements.txt</code></li>

<li><code>setup.py</code></li>

</ul>

<p>我们将把这些先决条件存储在<code>scripts</code>目录中。我们将在本章的后续章节中详细讨论这些先决条件<a id="_idIndexMarker774"/>。事不宜迟，让我们从准备<code>inference.py</code>脚本文件开始吧！</p>

<h2 id="_idParaDest-147"><a id="_idTextAnchor157"/>准备推论. py文件</h2>

<p>在本节中，我们将准备一个定制的Python脚本<a id="_idIndexMarker775"/>，SageMaker将在处理推理请求时使用它。在这里，我们可以影响如何反序列化输入请求，如何加载定制模型，如何执行预测步骤，以及如何序列化输出预测并将其作为响应返回。要做到这一切，我们需要在脚本文件中覆盖以下推理处理函数:<code>model_fn()</code>、<code>input_fn()</code>、<code>predict_fn()</code>和<code>output_fn()</code>。我们稍后将讨论这些函数是如何工作的。</p>

<p>在下一组步骤中，我们将准备我们的自定义Python脚本，并覆盖推理处理器函数的默认实现:</p>

<ol>

<li value="1">右键单击<strong class="bold">文件浏览器</strong>侧栏窗格中的空白区域，打开类似于<em class="italic">图7.5 </em>所示的上下文菜单:</li>

</ol>

<div><div><img alt="Figure 7.5 – Creating a new folder inside the CH07 directory&#10;&#10;" height="290" src="img/B18638_07_005.jpg" width="722"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.5–在CH07目录中创建新文件夹</p>

<p class="list-inset">从上下文菜单的可用选项列表中选择<strong class="bold">新文件夹</strong>，如图7.5 中的<em class="italic">所示。注意，我们也可以按下<strong class="bold"> + </strong>按钮旁边的信封按钮(带加号)来创建一个新文件夹。</em></p>

<ol>

<li value="2">将新文件夹命名为<code>scripts</code>。</li>

<li>接下来，双击<code>scripts</code>文件夹导航到该目录。</li>

<li>点击<strong class="bold">文件</strong>菜单，并从<strong class="bold">新建</strong>子菜单下的选项列表中选择<strong class="bold">文本文件</strong>，创建一个新的文本文件。</li>

<li>右击<code>inference.py</code>上的<a id="_idIndexMarker776"/>。</li>

<li>点击<code>inference.py</code>脚本:</li>

</ol>

<div><div><img alt="Figure 7.6 – Getting ready to add code to the inference.py file in the Editor pane&#10;&#10;" height="393" src="img/B18638_07_006.jpg" width="975"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.6–准备向编辑器窗格中的inference.py文件添加代码</p>

<p class="list-inset">我们将把随后的代码块添加到<code>inference.py</code>文件中。确保每个代码块后面都有一个额外的空行。</p>

<ol>

<li value="7">在<code>inference.py</code>文件中:<pre class="source-code">import json</pre><pre class="source-code">from transformers import AutoModelForSequenceClassification as AMSC</pre><pre class="source-code">from transformers import Trainer</pre><pre class="source-code">from transformers import TrainingArguments</pre><pre class="source-code">from torch.nn import functional as F</pre><pre class="source-code">from transformers import AutoTokenizer</pre><pre class="source-code">from time import sleep</pre></li>

<li>指定标记符:<pre class="source-code">TOKENIZER = "<strong class="bold">distilbert-base-uncased-finetuned-sst-2-english</strong>"</pre></li>

</ol>

<p class="list-inset">这里，我们为模型指定<a id="_idIndexMarker777"/>适当的记号化器，我们将在后面的步骤中使用它来执行预测。</p>

<p class="callout-heading">注意</p>

<p class="callout">什么是记号赋予器？一个<code>"I am hungry"</code>，然后记号赋予器会把它拆分成记号<code>"I"</code>、<code>"am"</code>和<code>"</code>、<code>hungry"</code>。请注意，这是一个简化的示例，除了几句话可以解释的内容之外，还有更多需要标记化的内容。更多<a id="_idIndexMarker779"/>详情，可以随意查看以下链接:<a href="https://huggingface.co/docs/transformers/main_classes/tokenizer">https://hugging face . co/docs/transformers/main _ classes/tokenizer</a>。</p>

<ol>

<li value="9">定义<code>model_fn()</code>功能:<pre class="source-code">def <strong class="bold">model_fn</strong>(model_dir):</pre> <pre class="source-code">    model = AMSC.<strong class="bold">from_pretrained</strong>(model_dir)</pre> <pre class="source-code">    </pre> <pre class="source-code">    return model</pre></li>

</ol>

<p class="list-inset">这里，我们定义了一个模型函数，它返回一个用于执行预测和处理推理请求的模型对象。由于我们计划加载并使用一个预先训练好的模型，我们使用了来自<code>transformers</code>库的<code>AutoModelForSequenceClassification</code>的<code>from_pretrained()</code>方法来加载指定模型目录中的模型工件。然后<code>from_pretrained()</code>方法返回一个可以在预测步骤中使用的模型对象。</p>

<ol>

<li value="10">现在，我们来定义一下<code>humanize_prediction()</code>的功能:<pre class="source-code">def <strong class="bold">humanize_prediction</strong>(output):</pre><pre class="source-code">    class_a, class_b = F.softmax(</pre><pre class="source-code">        output[0][0], </pre><pre class="source-code">        dim = 0</pre><pre class="source-code">    ).tolist()</pre><pre class="source-code">    </pre><pre class="source-code">    prediction = "-"</pre><pre class="source-code">    </pre><pre class="source-code">    if class_a &gt; class_b:</pre><pre class="source-code">        prediction = "<strong class="bold">NEGATIVE</strong>"</pre><pre class="source-code">    else:</pre><pre class="source-code">        prediction = "<strong class="bold">POSITIVE</strong>"</pre><pre class="source-code">        </pre><pre class="source-code">    return prediction</pre></li>

</ol>

<p class="list-inset"><code>humanize_prediction()</code>函数简单地<a id="_idIndexMarker780"/>在预测步骤中处理输入有效载荷后接受模型产生的原始输出。它向调用函数返回一个<code>"POSITIVE"</code>或<code>"NEGATIVE"</code>预测。我们将在下一步定义这个<em class="italic">调用函数</em>。</p>

<ol>

<li value="11">接下来，让我们使用下面的代码块来定义<code>predict_fn()</code>:<pre class="source-code">def <strong class="bold">predict_fn</strong>(input_data, model):</pre><pre class="source-code">    # sleep(30)</pre><pre class="source-code">    </pre><pre class="source-code">    sentence = input_data['text']</pre><pre class="source-code">    </pre><pre class="source-code">    tokenizer = AutoTokenizer.from_pretrained(</pre><pre class="source-code">        TOKENIZER</pre><pre class="source-code">    )</pre><pre class="source-code">    </pre><pre class="source-code">    batch = tokenizer(</pre><pre class="source-code">        [sentence],</pre><pre class="source-code">        padding=True,</pre><pre class="source-code">        truncation=True,</pre><pre class="source-code">        max_length=512,</pre><pre class="source-code">        return_tensors="pt"</pre><pre class="source-code">    )</pre><pre class="source-code">    output = model(**batch)</pre><pre class="source-code">    prediction = <strong class="bold">humanize_prediction</strong>(output)</pre><pre class="source-code">    </pre><pre class="source-code">    return prediction</pre></li>

</ol>

<p class="list-inset"><code>predict_fn()</code>函数接受<a id="_idIndexMarker781"/>反串行化的输入请求数据和加载的模型作为输入。然后，它使用这两个参数值来产生预测。怎么会？由于加载的模型作为第二个参数可用，我们简单地使用它来执行预测。这个预测步骤的输入有效负载是反序列化的请求数据，它可以作为<code>predict_fn()</code>函数的第一个参数。在输出返回之前，我们利用<code>humanize_prediction()</code>函数将原始输出转换为<code>"POSITIVE"</code>或<code>"NEGATIVE"</code>。</p>

<p class="callout-heading">注意</p>

<p class="callout">为什么我们有一个包含<code>sleep(30)</code>的注释行？稍后在<em class="italic">将预训练模型部署到异步推理端点部分</em>中，我们将使用人工的30秒延迟来模拟具有相对较长处理时间的推理端点。现在，我们将保留这一行的注释，我们将在这一部分的后面撤销它。</p>

<ol>

<li value="12">让我们也定义一下<code>input_fn()</code>函数，它用于将序列化的输入请求<a id="_idIndexMarker782"/>数据转换成反序列化的形式。这种反串行化的形式将在稍后阶段用于预测:<pre class="source-code">def <strong class="bold">input_fn</strong>(serialized_input_data, </pre> <pre class="source-code">             content_type='application/json'):</pre> <pre class="source-code">    if content_type == 'application/json':</pre> <pre class="source-code">        input_data = json.loads(serialized_input_data)</pre> <pre class="source-code">        </pre> <pre class="source-code">        return input_data</pre> <pre class="source-code">    else:</pre> <pre class="source-code">        raise <strong class="bold">Exception</strong>('Unsupported Content Type')</pre></li>

</ol>

<p class="list-inset">在<code>input_fn()</code>函数中，我们还通过为不支持的内容类型引发<code>Exception</code>来确保指定的内容类型在我们定义的支持内容类型列表中。</p>

<ol>

<li value="13">最后，我们来定义一下<code>output_fn()</code>:<pre class="source-code">def <strong class="bold">output_fn</strong>(prediction_output, </pre><pre class="source-code">              accept='application/json'):</pre><pre class="source-code">    if accept == 'application/json':</pre><pre class="source-code">        return json.dumps(prediction_output), accept</pre><pre class="source-code">    </pre><pre class="source-code">    raise Exception('Unsupported Content Type')</pre></li>

</ol>

<p class="list-inset"><code>output_fn()</code>的作用是将预测结果序列化为指定的内容类型。这里，我们还通过为不支持的内容类型引发<code>Exception</code>来确保指定的内容类型在我们定义的支持内容类型列表中。</p>

<p class="callout-heading">注意</p>

<p class="callout">我们可以将<em class="italic">序列化</em>和<em class="italic">反序列化</em>视为将数据<a id="_idIndexMarker784"/>从一种形式转换为另一种形式的数据转换步骤<a id="_idIndexMarker783"/>。例如，输入请求数据可以作为有效的JSON <em class="italic">字符串</em>传递给推理端点。该输入请求数据通过<code>input_fn()</code>函数，该函数将其转换为<em class="italic"> JSON </em>或<em class="italic">字典</em>。这个反序列化的值然后作为有效载荷传递给<code>predict_fn()</code>函数。此后，<code>predict_fn()</code>函数返回一个预测作为结果。然后使用<code>output_fn()</code>函数将这个结果转换成指定的内容类型。</p>

<ol>

<li value="14">通过按下<em class="italic"> CTRL </em> + <em class="italic"> S </em>保存更改<a id="_idIndexMarker785"/>。</li>

</ol>

<p class="callout-heading">注意</p>

<p class="callout">如果你用的是Mac，用<em class="italic"> CMD </em> + <em class="italic"> S </em>代替。或者，你可以点击<strong class="bold">文件</strong>菜单下选项列表下的<strong class="bold">保存Python文件</strong>。</p>

<p>此时，您可能想知道所有这些是如何组合在一起的。为了帮助我们理解推理处理函数如何与数据交互以及如何相互交互，让我们快速查看一下图7.7 中所示的图表:</p>

<div><div><img alt="Figure 7.7 – The inference handler functions&#10;&#10;" height="665" src="img/B18638_07_007.jpg" width="1084"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.7-推理处理器功能</p>

<p>在<em class="italic">图7.7 </em>中，我们可以看到<code>model_fn()</code>函数用于加载ML模型对象。一旦有请求进来，<code>predict_fn()</code>函数将使用这个模型对象<a id="_idIndexMarker786"/>来执行预测。当请求进来时，<code>input_fn()</code>函数处理序列化的请求数据，并将其转换成反序列化的形式。这个反序列化的请求数据然后被传递给<code>predict_fn()</code>函数，该函数然后使用加载的ML模型来执行一个使用请求数据作为有效负载的预测。然后，<code>predict_fn()</code>函数返回输出预测，该预测由<code>output_fn()</code>函数序列化。</p>

<p class="callout-heading">注意</p>

<p class="callout">关于这个话题的更多信息<a id="_idIndexMarker787"/>，可以随意查看以下链接:<a href="https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.xhtml#serve-a-pytorch-model">https://sagemaker . readthe docs . io/en/stable/frameworks/py torch/using _ py torch . XHTML</a>。</p>

<p>现在我们已经准备好了推理脚本，让我们继续准备下一部分的<code>requirements.txt</code>文件！</p>

<h2 id="_idParaDest-148"><a id="_idTextAnchor158"/>准备requirements.txt文件</h2>

<p>由于<code>transformers</code>包<a id="_idIndexMarker788"/>没有包含在SageMaker PyTorch Docker容器中，我们需要通过一个<code>requirements.txt</code>文件来包含它，SageMaker使用这个文件在运行时安装额外的包。如果这是您第一次处理一个<code>requirements.txt</code>文件，它只是一个文本文件，包含一个要使用<code>pip install</code>命令安装的软件包列表。如果您的<code>requirements.txt</code>文件包含一行(例如<code>transformers==4.4.2</code>，那么这将在安装步骤中映射到一个<code>pip install transformers==4.4.2</code>。如果<code>requirements.txt</code>文件包含多行，那么将使用<code>pip install</code>命令安装列出的每个包。</p>

<p class="callout-heading">注意</p>

<p class="callout">我们可以选择使用<code>==</code> (equal)将<em class="italic">列出的包和依赖项固定到一个特定的版本。或者，我们也可以使用<code>&lt;</code>(小于)、<code>&gt;</code>(大于)和其他变体来管理要安装的包的版本号的上限值和下限值。</em></p>

<p>在下一组步骤中，我们将在<code>scripts</code>目录中创建并准备<code>requirements.txt</code>文件:</p>

<ol>

<li value="1">点击<strong class="bold">文件</strong>菜单，并从<strong class="bold">新建</strong>子菜单下的选项列表中选择<strong class="bold">文本文件</strong>，创建一个新的文本文件:</li>

</ol>

<div><div><img alt="Figure 7.8 – Creating a new text file inside the scripts directory&#10;&#10;" height="227" src="img/B18638_07_008.jpg" width="807"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.8–在脚本目录中创建新的文本文件</p>

<p class="list-inset">在<em class="italic">图7.8 </em>中，我们可以看到我们在<code>scripts</code>目录(<code>scripts</code>目录)。</p>

<ol>

<li value="2">重命名文件<code>requirements.txt</code></li>

<li>在<code>requirements.txt</code>文件中进行如下操作:<pre class="source-code"><strong class="bold">transformers==4.4.2</strong></pre></li>

<li>确保通过按下<em class="italic"> CTRL </em> + <em class="italic"> S </em>保存更改。</li>

</ol>

<p class="callout-heading">注意</p>

<p class="callout">如果你用的是Mac，用<em class="italic"> CMD </em> + <em class="italic"> S </em>代替。或者，您可以点击<strong class="bold">文件</strong>菜单下选项列表下的<strong class="bold">保存Python文件</strong>。</p>

<p>那不是很容易吗？现在让我们继续最后一个先决条件——文件。</p>

<h2 id="_idParaDest-149"><a id="_idTextAnchor159"/>准备setup.py文件</h2>

<p>除了<code>requirements.txt</code>文件，我们还将准备<a id="_idIndexMarker790"/>一个<code>setup.py</code>文件，它将包含一些额外的信息和元数据。</p>

<p class="callout-heading">注意</p>

<p class="callout">我们就不深究<code>requirements.txt</code>和<code>setup.py</code>文件用法的区别了。想了解更多信息，请点击下面的链接:<a href="https://docs.python.org/3/distutils/setupscript.xhtml">https://docs.python.org/3/distutils/setupscript.xhtml</a>。</p>

<p>在下一组步骤中，我们将在<code>scripts</code>目录中创建并准备<code>setup.py</code>文件:</p>

<ol>

<li value="1">使用与上一节相同的步骤，创建一个新的文本文件，并将其重命名为<code>setup.py</code>。确保该文件与<code>inference.py</code>和<code>requirements.txt</code>文件在同一个目录下(<code>scripts</code>)。</li>

<li>更新<code>setup.py</code>文件的内容以包含以下代码块:<pre class="source-code">from setuptools import setup, find_packages</pre><pre class="source-code"><strong class="bold">setup</strong>(<strong class="bold">name</strong>='distillbert',</pre><pre class="source-code">      <strong class="bold">version</strong>='1.0',</pre><pre class="source-code">      <strong class="bold">description</strong>='distillbert',</pre><pre class="source-code">      packages=find_packages(</pre><pre class="source-code">          exclude=('tests', 'docs')</pre><pre class="source-code">     ))</pre></li>

</ol>

<p class="list-inset">设置脚本简单地利用<code>setup()</code>函数来描述模块分布。这里，我们在调用<code>setup()</code>函数时指定了<code>name</code>、<code>version</code>和<code>description</code>等元数据。</p>

<ol>

<li value="3">最后，确保通过按下<em class="italic"> CTRL </em> + <em class="italic"> S </em>保存更改。</li>

</ol>

<p class="callout-heading">注意</p>

<p class="callout">如果您使用的是Mac，请改用<em class="italic"> CMD </em> + <em class="italic"> S </em>。或者，您可以点击<strong class="bold">文件</strong>菜单下选项列表中的<strong class="bold">保存Python文件</strong>。</p>

<p>至此，我们已经准备好了运行本章所有后续章节所需的所有先决条件。至此，让我们在下一节继续将我们的预训练模型部署到实时推理端点！</p>

<h1 id="_idParaDest-150"><a id="_idTextAnchor160"/>将预训练模型部署到实时推理端点</h1>

<p>在这一节中，我们将使用SageMaker Python SDK <a id="_idIndexMarker793"/>将预先训练好的模型<a id="_idIndexMarker794"/>部署到实时推理端点。从名称本身，我们可以看出实时推理端点可以处理输入负载并实时执行预测。如果您之前已经构建了一个API端点(例如，它可以处理GET和POST请求)，那么我们可以将推理端点视为接受输入请求并作为响应的一部分返回预测的API端点。预测是如何做出的？推理端点只是将模型加载到内存中，并使用它来处理输入负载。这将产生一个作为响应返回的输出。例如，如果我们在实时推理端点中部署了预先训练的情感分析ML模型，那么它将根据请求中提供的输入字符串有效负载返回响应<code>"POSITIVE"</code>或<code>"</code> <code>NEGATIVE"</code>。</p>

<p class="callout-heading">注意</p>

<p class="callout">假设我们的推理端点通过POST请求接收到语句<code>"I love reading the book MLE on AWS!"</code>。然后，推理端点将处理请求输入数据，并使用ML模型进行推理。ML模型推断步骤的结果(例如，代表一个<code>"POSITIVE"</code>结果的数值)将作为响应的一部分返回。</p>

<div><div><img alt="Figure 7.9 – The desired file and folder structure&#10;&#10;&#10;&#10;&#10;&#10;" height="703" src="img/B18638_07_009.jpg" width="1424"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.9–所需的文件和文件夹结构</p>

<p>为了实现这一点，我们只需要确保<a id="_idIndexMarker795"/>在使用SageMaker Python SDK准备实时推理端点之前，包括<a id="_idIndexMarker796"/>推理脚本文件(例如<code>inference.py</code>)和<code>requirements.txt</code>文件在内的先决条件文件已经准备好。在继续本节中的动手解决方案之前，确保检查并查看图7.9 中的文件夹结构。</p>

<p>在下一组步骤中，我们将使用SageMaker Python SDK将我们的预训练模型部署到实时推理端点:</p>

<ol>

<li value="1">并使用<code>Data Science</code>映像创建一个新的笔记本。重命名笔记本<code>02 - Deploying a real-time inference endpoint.ipynb</code>。</li>

</ol>

<p class="callout-heading">注意</p>

<p class="callout">新笔记本应该在<code>01 - Prepare model.tar.gz file.ipynb</code>旁边，类似于<em class="italic">图7.9 </em>所示。</p>

<ol>

<li value="2">让我们在新笔记本的第一个单元格中运行下面的代码块:<pre class="source-code">%store -r <strong class="bold">model_data</strong></pre> <pre class="source-code">%store -r <strong class="bold">s3_bucket</strong></pre> <pre class="source-code">%store -r <strong class="bold">prefix</strong></pre></li>

</ol>

<p class="list-inset">这里，我们使用<code>%store</code>魔术来加载<code>model_data</code>、<code>s3_bucket</code>和<code>prefix</code>的变量值。</p>

<ol>

<li value="3">接下来，让我们准备<a id="_idIndexMarker797"/>IAM执行角色<a id="_idIndexMarker798"/>供SageMaker使用:<pre class="source-code">from sagemaker import get_execution_role </pre> <pre class="source-code">role = get_execution_role()</pre></li>

<li>初始化<code>PyTorchModel</code>对象:<pre class="source-code">from sagemaker.pytorch.model import PyTorchModel</pre><pre class="source-code">model = <strong class="bold">PyTorchModel</strong>(</pre><pre class="source-code">    model_data=model_data, </pre><pre class="source-code">    role=role, </pre><pre class="source-code">    source_dir="<strong class="bold">scripts</strong>",</pre><pre class="source-code">    entry_point='<strong class="bold">inference.py</strong>', </pre><pre class="source-code">    framework_version='1.6.0',</pre><pre class="source-code">    py_version="py3"</pre><pre class="source-code">)</pre></li>

</ol>

<p class="list-inset">让我们看看<em class="italic">图7.10 </em>来帮助我们直观地了解前面的代码块中发生了什么:</p>

<div><div><img alt="Figure 7.10 – Deploying a real-time inference endpoint&#10;&#10;" height="372" src="img/B18638_07_010.jpg" width="1212"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.10–部署实时推理端点</p>

<p class="list-inset">在<em class="italic">图7.10 </em>中，我们可以看到<a id="_idIndexMarker799"/>我们在初始化步骤中通过传递几个配置参数初始化了一个<code>Model</code>对象<a id="_idIndexMarker800"/>:(1)模型数据，(2)框架版本，(3)到<code>inference.py</code>脚本文件的路径。我们还可以设置其他参数，但我们会将事情简化一点，集中在这三个参数上。为了让SageMaker知道如何使用预先训练好的模型进行推理，<code>inference.py</code>脚本文件应该包含定制逻辑，它加载ML模型并使用它来执行预测。</p>

<p class="callout-heading">注意</p>

<p class="callout">值得注意的是，我们并不局限于命名推理脚本文件<code>inference.py</code>。我们可以使用不同的命名约定，只要我们指定正确的<code>entry_point</code>值。</p>

<p class="list-inset">如果我们在部署ML模型时使用SageMaker的脚本模式，就会出现这种情况。请注意，还有其他可用的选项，例如使用定制的容器图像，而不是传递脚本，我们将传递我们提前准备好的容器图像。当部署使用SageMaker的<strong class="bold">内置算法</strong>训练的ML模型时，我们可以立即着手部署这些模型，而无需任何定制脚本或容器映像，因为SageMaker已经提供了部署所需的所有先决条件。</p>

<ol>

<li value="5">使用<code>deploy()</code>方法<a id="_idIndexMarker801"/>将模型部署到实时推理<a id="_idIndexMarker802"/>端点:<pre class="source-code">%%time</pre><pre class="source-code">from sagemaker.serializers import JSONSerializer</pre><pre class="source-code">from sagemaker.deserializers import JSONDeserializer</pre><pre class="source-code">predictor = model.<strong class="bold">deploy</strong>(</pre><pre class="source-code">    instance_type='ml.m5.xlarge', </pre><pre class="source-code">    initial_instance_count=1,</pre><pre class="source-code">    serializer=<strong class="bold">JSONSerializer()</strong>,</pre><pre class="source-code">    deserializer=<strong class="bold">JSONDeserializer()</strong></pre><pre class="source-code">)</pre></li>

</ol>

<p class="list-inset">完成此步骤大约需要3到8分钟。</p>

<p class="callout-heading">注意</p>

<p class="callout">当使用<code>deploy()</code>方法通过SageMaker Python SDK部署ML模型时，我们可以指定实例类型。为模型选择正确的实例类型是很重要的，并且在成本和性能之间找到最佳平衡不是一个简单的过程。有许多实例类型和大小可供选择，ML工程师在SageMaker托管的<a id="_idIndexMarker803"/>服务中部署模型时，最终可能会有一个次优的设置。好消息是SageMaker有一个名为<strong class="bold"> SageMaker推理推荐器</strong>的功能，它可以帮助您决定使用哪个<a id="_idIndexMarker804"/>实例类型。更多信息可以查看以下链接:<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml">https://docs . AWS . Amazon . com/sage maker/latest/DG/inference-recommender . XHTML</a>。</p>

<ol>

<li value="6">现在我们的实时推理端点正在运行，让我们使用<code>predict()</code>方法执行一个样本预测:<pre class="source-code">payload = {</pre> <pre class="source-code">    "text": "I love reading the book MLE on AWS!"</pre> <pre class="source-code">}</pre> <pre class="source-code">predictor.<strong class="bold">predict</strong>(payload)</pre></li>

</ol>

<p class="list-inset">这将产生一个输出值<code>'POSITIVE'</code>。</p>

<ol>

<li value="7">我们再来测试一个负面的场景:<pre class="source-code">payload = {</pre> <pre class="source-code">    "text": "This is the worst spaghetti I've had"</pre> <pre class="source-code">}</pre> <pre class="source-code">predictor.<strong class="bold">predict</strong>(payload)</pre></li>

</ol>

<p class="list-inset">这将产生一个<code>'NEGATIVE'</code>的输出值。在下一步删除端点之前，请随意测试不同的值。</p>

<ol>

<li value="8">最后，让我们使用<code>delete_endpoint()</code>方法删除推理端点:<pre class="source-code">predictor.<strong class="bold">delete_endpoint()</strong></pre></li>

</ol>

<p class="list-inset">这将有助于我们避免任何未使用的推理端点的意外费用。</p>

<p>那不是很容易吗？使用SageMaker Python SDK将预先训练的模型<a id="_idIndexMarker806"/>部署到实时推理端点(在具有指定实例类型的ML实例中)是如此简单！许多工程工作已经为我们自动化了，我们需要做的就是调用<code>Model</code>对象的<code>deploy()</code>方法。</p>

<h1 id="_idParaDest-151"><a id="_idTextAnchor161"/>将预训练模型部署到无服务器推理端点</h1>

<p>在本书的前几章<a id="_idIndexMarker807"/>中，我们已经使用了几种无服务器<a id="_idIndexMarker808"/>服务来管理和降低成本。如果您想知道在SageMaker中部署ML模型时是否有无服务器选项，那么答案将是肯定的。当您处理间歇性和不可预测的流量时，使用无服务器推理端点来托管ML模型可能是一个更具成本效益的选择。假设我们可以容忍<strong class="bold">冷启动</strong>(在一段时间的不活动后，请求需要更长的时间来处理),并且我们每天只希望有几个请求——那么，我们可以利用无服务器推理端点来代替实时选项。当我们可以最大化推理端点时，实时推理端点是最好的。如果您希望您的端点大部分时间都得到利用，那么实时选项可能会达到目的。</p>

<div><div><img alt="Figure 7.11 – The desired file and folder structure&#10;&#10;" height="591" src="img/B18638_07_011.jpg" width="972"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.11–所需的文件和文件夹结构</p>

<p>使用SageMaker Python SDK将预训练的ML模型部署到无服务器推理端点类似于为实时推理端点所做的。唯一的主要区别如下:</p>

<ul>

<li><code>ServerlessInferenceConfig</code>对象的初始化</li>

<li>当调用<code>Model</code>对象的<code>deploy()</code>方法时，将该对象作为参数传递</li>

</ul>

<p>在下一组步骤中，我们将使用SageMaker Python SDK <a id="_idIndexMarker809"/>将我们预先训练的模型<a id="_idIndexMarker810"/>部署到无服务器推理端点:</p>

<ol>

<li value="1">并使用<code>Data Science</code>映像创建一个新的笔记本。重命名笔记本<code>03 - Deploying a serverless inference endpoint.ipynb</code>。</li>

</ol>

<p class="callout-heading">注意</p>

<p class="callout">新笔记本应该在<code>01 - Prepare model.tar.gz file.ipynb</code>旁边，类似于<em class="italic">图7.11 </em>所示。</p>

<ol>

<li value="2">在新笔记本的第一个单元格中，让我们运行下面的代码块来加载<code>model_data</code>、<code>s3_bucket</code>和<code>prefix</code>的变量值:<pre class="source-code">%store -r <strong class="bold">model_data</strong></pre> <pre class="source-code">%store -r <strong class="bold">s3_bucket</strong></pre> <pre class="source-code">%store -r <strong class="bold">prefix</strong></pre></li>

</ol>

<p class="list-inset">如果您在运行这段代码时遇到错误，请确保您已经完成了本章<em class="italic">准备预训练模型工件</em>一节中指定的步骤。</p>

<ol>

<li value="3">准备SageMaker使用的IAM执行角色:<pre class="source-code">from sagemaker import get_execution_role </pre> <pre class="source-code">role = get_execution_role()</pre></li>

<li>初始化并配置<code>ServerlessInferenceConfig</code>对象:<pre class="source-code">from sagemaker.serverless import ServerlessInferenceConfig</pre> <pre class="source-code">serverless_config = <strong class="bold">ServerlessInferenceConfig</strong>(</pre> <pre class="source-code">  memory_size_in_mb=4096,</pre> <pre class="source-code">  max_concurrency=5,</pre> <pre class="source-code">)</pre></li>

<li>初始化<code>PyTorchModel</code>对象<a id="_idIndexMarker811"/>并使用<code>deploy()</code>方法<a id="_idIndexMarker812"/>将模型部署到无服务器推理端点:<pre class="source-code">from sagemaker.pytorch.model import PyTorchModel</pre><pre class="source-code">from sagemaker.serializers import JSONSerializer</pre><pre class="source-code">from sagemaker.deserializers import JSONDeserializer</pre><pre class="source-code">model = <strong class="bold">PyTorchModel</strong>(</pre><pre class="source-code">    model_data=model_data, </pre><pre class="source-code">    role=role, </pre><pre class="source-code">    source_dir="<strong class="bold">scripts</strong>",</pre><pre class="source-code">    entry_point='<strong class="bold">inference.py</strong>', </pre><pre class="source-code">    framework_version='1.6.0',</pre><pre class="source-code">    py_version="py3"</pre><pre class="source-code">)</pre><pre class="source-code">predictor = model.<strong class="bold">deploy</strong>(</pre><pre class="source-code">    instance_type='ml.m5.xlarge', </pre><pre class="source-code">    initial_instance_count=1,</pre><pre class="source-code">    serializer=JSONSerializer(),</pre><pre class="source-code">    deserializer=JSONDeserializer(),</pre><pre class="source-code">    <strong class="bold">serverless_inference_config=serverless_config</strong></pre><pre class="source-code">)</pre></li>

</ol>

<p class="callout-heading">注意</p>

<p class="callout">完成模型部署大约需要3到8分钟。</p>

<ol>

<li value="6">现在我们的实时推理端点<a id="_idIndexMarker813"/>正在运行，让我们使用<code>predict()</code>方法执行<a id="_idIndexMarker814"/>一个样本预测:<pre class="source-code">payload = {</pre> <pre class="source-code">    "text": "I love reading the book MLE on AWS!"</pre> <pre class="source-code">}</pre> <pre class="source-code">predictor.<strong class="bold">predict</strong>(payload)</pre></li>

</ol>

<p class="list-inset">这将产生一个输出值<code>'POSITIVE'</code>。</p>

<ol>

<li value="7">我们再来测试一个负面场景:<pre class="source-code">payload = {</pre> <pre class="source-code">    "text": "This is the worst spaghetti I've had"</pre> <pre class="source-code">}</pre> <pre class="source-code">predictor.predict(payload)</pre></li>

</ol>

<p class="list-inset">这将产生一个输出值<code>'NEGATIVE'</code>。在下一步删除端点之前，请随意测试不同的值。</p>

<ol>

<li value="8">最后，让我们使用<code>delete_endpoint()</code>方法删除推断端点:<pre class="source-code">predictor.<strong class="bold">delete_endpoint()</strong></pre></li>

</ol>

<p class="list-inset">这将有助于我们避免任何未使用的推理端点的意外费用。</p>

<p>如你所见，除了<code>ServerlessInferenceConfig</code>对象的初始化和用法之外，一切都几乎相同。使用无服务器端点时，SageMaker为我们管理计算资源，并自动执行以下操作:</p>

<ul>

<li>自动分配与我们在初始化<code>ServerlessInferenceConfig</code>时指定的<code>memory_size_in_mb</code>参数值成比例的计算资源</li>

<li>使用配置的最大并发值来管理可以同时发生多少个并发调用</li>

<li>如果没有请求，资源会自动缩减到零</li>

</ul>

<p>一旦你看到更多关于如何使用SageMaker Python SDK的例子<a id="_idIndexMarker815"/>，你就会开始<a id="_idIndexMarker816"/>意识到这个SDK的设计和实现有多好。</p>

<h1 id="_idParaDest-152"><a id="_idTextAnchor162"/>将预训练模型部署到异步推理端点</h1>

<p>除了实时<a id="_idIndexMarker817"/>和无服务器推理端点之外，SageMaker在部署模型时还提供了第三种选择—<strong class="bold">异步推理端点</strong>。为什么叫异步？首先，不是期望结果立即可用，而是请求被排队，并且结果异步可用<em class="italic"/>。这适用于涉及以下一项或多项的ML要求:</p>

<ul>

<li>大型输入负载(高达1 GB)</li>

<li>预测处理持续时间长(长达15分钟)</li>

</ul>

<p>异步推理端点的一个很好的用例是用于检测大型视频文件中的对象的ML模型(可能需要60多秒才能完成)。在这种情况下，一个推论可能需要几分钟而不是几秒钟。</p>

<p><em class="italic">我们如何使用异步推理端点？</em>为了调用<a id="_idIndexMarker819"/>一个异步推理端点，我们执行<a id="_idIndexMarker820"/>以下操作:</p>

<ol>

<li value="1">请求负载被上传到亚马逊S3存储桶。</li>

<li>当调用<code>AsyncPredictor</code>对象的<code>predict_async()</code>方法(映射或表示ML推理端点)时，S3路径或位置(存储请求负载的位置)被用作参数值。</li>

<li>在调用端点时，异步推断端点将请求排队等待处理(一旦端点可以处理)。</li>

<li>处理完请求后，输出推理结果被存储并上传到输出S3位置。</li>

<li>发送SNS通知(例如，成功或错误通知)(如果已设置)。</li>

</ol>

<p>在这一节中，我们将把NLP模型部署到一个异步推理端点。为了模拟延迟，我们将在我们的推理脚本中调用<code>sleep()</code>函数，以便预测步骤比通常花费更长的时间。一旦我们可以让这个相对简单的设置工作起来，处理更复杂的需求，比如视频文件的对象检测，肯定会更容易。</p>

<div><div><img alt="Figure 7.12 – The file and folder structure&#10;&#10;" height="689" src="img/B18638_07_012.jpg" width="1330"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.12–文件和文件夹结构</p>

<p>为了让这个设置工作，我们需要<a id="_idIndexMarker821"/>准备一个包含<a id="_idIndexMarker822"/>的文件，一个类似于<em class="italic">图7.12 </em>所示的输入有效载荷(例如，<em class="italic">数据</em>或<code>input.json</code>)。一旦输入文件准备好了，我们将把它上传到一个亚马逊S3桶，然后继续把我们预先训练好的ML模型部署到一个异步推理端点。</p>

<p>记住这一点，让我们继续创建输入JSON文件！</p>

<h2 id="_idParaDest-153"><a id="_idTextAnchor163"/>创建输入JSON文件</h2>

<p>在下一组步骤中，我们将<a id="_idIndexMarker823"/>创建一个包含输入JSON <a id="_idIndexMarker824"/>值的样本文件，该文件将在下一节调用异步推理端点时使用:</p>

<ol>

<li value="1">右键单击<strong class="bold">文件浏览器</strong>侧栏窗格中的空白区域，打开类似于<em class="italic">图7.13 </em>所示的上下文菜单:</li>

</ol>

<div><div><img alt="Figure 7.13 – Creating a new folder&#10;&#10;" height="351" src="img/B18638_07_013.jpg" width="706"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.13–创建新文件夹</p>

<p class="list-inset">在执行该步骤之前，确保<a id="_idIndexMarker825"/>在<strong class="bold">文件浏览器</strong>中的<code>CH07</code>目录<a id="_idIndexMarker826"/>中。</p>

<ol>

<li value="2">重命名文件夹<code>data</code>。</li>

<li>双击<strong class="bold">文件浏览器</strong>侧栏窗格中的<code>data</code>文件夹，导航到该目录。</li>

<li>点击<strong class="bold">文件</strong>菜单并从<strong class="bold">新建</strong>子菜单下的选项列表中选择<strong class="bold">文本文件</strong>，创建一个新的文本文件:</li>

</ol>

<div><div><img alt="Figure 7.14 – Creating a new text file&#10;&#10;" height="226" src="img/B18638_07_014.jpg" width="822"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.14–创建新的文本文件</p>

<p class="list-inset">当创建一个新的文本文件时，确保你在<code>data</code>目录中，类似于<em class="italic">图7.14 </em>。</p>

<ol>

<li value="5">重命名文件<code>input.json</code>，如图<em class="italic">图7.15 </em>:</li>

</ol>

<div><div><img alt="Figure 7.15 – Renaming the text file&#10;&#10;" height="281" src="img/B18638_07_015.jpg" width="609"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.15–重命名文本文件</p>

<p class="list-inset">要重命名<code>untitled.txt</code>文件，右击<code>input.json</code>中的文件<a id="_idIndexMarker827"/>，替换默认的名称值。</p>

<ol>

<li value="6">在具有以下JSON值的<code>input.json</code>文件中:<pre class="source-code">{"text": "I love reading the book MLE on AWS!"}</pre></li>

<li>确保通过按下<em class="italic"> CTRL </em> + <em class="italic"> S </em>保存您的更改。</li>

</ol>

<p class="callout-heading">注意</p>

<p class="callout">如果您使用的是Mac，请改用<em class="italic"> CMD </em> + <em class="italic"> S </em>。或者，你可以点击<strong class="bold">文件</strong>菜单下选项列表下的<strong class="bold">保存Python文件</strong>。</p>

<p>同样，输入文件<a id="_idIndexMarker829"/>仅在我们计划<a id="_idIndexMarker830"/>将ML模型部署到异步推理端点时才需要。有了这些准备，我们现在可以进行下一组步骤。</p>

<h2 id="_idParaDest-154"><a id="_idTextAnchor164"/>向推理脚本添加人为延迟</h2>

<p>在使用SageMaker Python SDK将我们的预训练模型部署到异步推理端点之前，我们将向预测步骤添加一个人工延迟。这将帮助我们模拟需要一点时间才能完成的推断或预测请求。</p>

<p class="callout-heading">注意</p>

<p class="callout">当对异步推理端点进行故障排除时，您可以选择先测试一个在几秒钟内执行预测的ML模型。这将有助于您立即知道是否有问题，因为输出预计将在几秒钟内上传到S3输出路径(而不是几分钟)。也就是说，如果您在让设置工作时遇到问题，您可能需要暂时消除人为延迟。</p>

<p>在下一组步骤中，我们将更新<code>inference.py</code>脚本，在执行预测时添加30秒的延迟:</p>

<ol>

<li value="1">继续上一节我们离开的地方，让我们在<strong class="bold">文件浏览器</strong>中导航到<code>CH07</code>目录:</li>

</ol>

<div><div><img alt="Figure 7.16 – Navigating to the CH07 directory&#10;&#10;" height="181" src="img/B18638_07_016.jpg" width="546"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.16–导航至CH07目录</p>

<p class="list-inset">这里，我们单击<code>CH07</code>链接，如图7.16 中的<em class="italic">所示。</em></p>

<ol>

<li value="2">双击<code>scripts</code>文件夹<a id="_idIndexMarker833"/>，如图<em class="italic">图7.17 </em>所示，导航到目录:</li>

</ol>

<div><div><img alt="Figure 7.17 – Navigating to the scripts directory&#10;&#10;" height="256" src="img/B18638_07_017.jpg" width="583"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.17–导航到脚本目录</p>

<p class="list-inset">在继续下一步之前，确保您已经完成了<em class="italic">准备SageMaker脚本模式先决条件</em>一节中的实际操作步骤。<code>scripts</code>目录应该包含三个文件:</p>

<ul>

<li><code>inference.py</code></li>

<li><code>requirements.txt</code></li>

<li><code>setup.py</code></li>

</ul>

<ol>

<li value="3">双击并打开<code>inference.py</code>文件，如图<em class="italic">图7.18 </em>所示。找到<code>predict_fn()</code>函数，取消包含<code>sleep(30)</code>的代码行的注释:</li>

</ol>

<div><div><img alt="" height="354" src="img/B18638_07_018.jpg" width="964"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.18-更新推论. py文件</p>

<p class="list-inset">要取消代码行的注释，只需删除<code>sleep(30)</code>前的散列和空格(<code># </code>)，类似于我们在<em class="italic">图7.18 </em>中看到的。</p>

<ol>

<li value="4">确保通过按下<em class="italic"> CTRL </em> + <em class="italic"> S </em>保存更改。</li>

</ol>

<p class="callout-heading">注意</p>

<p class="callout">如果您使用的是Mac，请改用<em class="italic"> CMD </em> + <em class="italic"> S </em>。或者，您可以点击<strong class="bold">文件</strong>菜单下选项列表下的<strong class="bold">保存Python文件</strong>。</p>

<p>既然我们已经添加了<a id="_idIndexMarker834"/>一个人工的30秒延迟，让我们继续使用SageMaker Python SDK来部署我们的异步推理端点。</p>

<h2 id="_idParaDest-155"><a id="_idTextAnchor165"/>部署和测试异步推理端点</h2>

<p>使用SageMaker Python SDK将预训练的ML模型<a id="_idIndexMarker835"/>部署到异步推理端点<a id="_idIndexMarker836"/>类似于对实时和无服务器推理端点的部署。唯一的主要区别是(1)初始化<code>AsyncInferenceConfig</code>对象，以及(2)在调用<code>Model</code>对象的<code>deploy()</code>方法时将该对象作为参数传递。</p>

<p>在下一组步骤中，我们将使用SageMaker Python SDK将我们的预训练模型部署到异步推理端点:</p>

<ol>

<li value="1">继续我们在<em class="italic">中停止的<a id="_idIndexMarker837"/>向推理脚本部分</em>添加人工延迟，让我们导航到<code>Data Science</code>图像中的<code>CH07</code>目录。重命名笔记本<code>04 - Deploying an asynchronous inference endpoint.ipynb</code>。</li>

</ol>

<p class="callout-heading">注意</p>

<p class="callout">新笔记本应该在<code>01 - Prepare model.tar.gz file.ipynb</code>旁边。</p>

<ol>

<li value="2">在新笔记本的第一个单元格中，让我们运行下面的代码块来加载<code>model_data</code>、<code>s3_bucket</code>和<code>prefix</code>的变量值:<pre class="source-code">%store -r <strong class="bold">model_data</strong></pre>、<pre class="source-code">%store -r <strong class="bold">s3_bucket</strong></pre>、T5】</li>

</ol>

<p class="list-inset">如果您在运行这段代码时遇到错误，请确保您已经完成了本章<em class="italic">准备预训练模型工件</em>一节中指定的步骤。</p>

<ol>

<li value="3">准备上传推理输入文件的路径:<pre class="source-code">input_data = "s3://{}/{}/data/input.json".format(</pre> <pre class="source-code">    s3_bucket,</pre> <pre class="source-code">    prefix</pre> <pre class="source-code">)</pre></li>

<li>使用<code>aws s3 cp</code>命令将<code>input.json</code>文件上传到S3桶:<pre class="source-code">!aws s3 cp data/input.json {input_data}</pre></li>

<li>准备IAM <a id="_idIndexMarker839"/>执行角色，供SageMaker: <pre class="source-code">from sagemaker import get_execution_role </pre> <pre class="source-code">role = get_execution_role()</pre>使用<a id="_idIndexMarker840"/></li>

<li>初始化<code>AsyncInferenceConfig</code>对象:<pre class="source-code">from sagemaker.async_inference import AsyncInferenceConfig</pre>T17<pre class="source-code">async_config = <strong class="bold">AsyncInferenceConfig</strong>(</pre>T19<pre class="source-code">)</pre></li>

</ol>

<p class="list-inset">在初始化<code>AsyncInferenceConfig</code>对象时，我们指定了保存结果的<code>output_path</code>参数的值。</p>

<ol>

<li value="7">接下来，让我们初始化<code>PyTorchModel</code>对象:<pre class="source-code">from sagemaker.pytorch.model import PyTorchModel</pre><pre class="source-code">model = <strong class="bold">PyTorchModel</strong>(</pre><pre class="source-code">    model_data=model_data, </pre><pre class="source-code">    role=role, </pre><pre class="source-code">    source_dir="<strong class="bold">scripts</strong>",</pre><pre class="source-code">    entry_point='<strong class="bold">inference.py</strong>', </pre><pre class="source-code">    framework_version='1.6.0',</pre><pre class="source-code">    py_version="py3"</pre><pre class="source-code">)</pre></li>

</ol>

<p class="list-inset">这里我们指定参数的配置值，如<code>model_data</code>、<code>role</code>、<code>source_dir</code>、<code>entry_point</code>、<code>framework_version</code>、<code>py_version</code>。</p>

<ol>

<li value="8">使用<code>deploy()</code>方法<a id="_idIndexMarker841"/>将模型<a id="_idIndexMarker842"/>部署到异步推理端点:<pre class="source-code">%%time</pre><pre class="source-code">from sagemaker.serializers import JSONSerializer</pre><pre class="source-code">from sagemaker.deserializers import JSONDeserializer</pre><pre class="source-code">predictor = model.<strong class="bold">deploy</strong>(</pre><pre class="source-code">    instance_type='ml.m5.xlarge', </pre><pre class="source-code">    initial_instance_count=1,</pre><pre class="source-code">    serializer=JSONSerializer(),</pre><pre class="source-code">    deserializer=JSONDeserializer(),</pre><pre class="source-code">    <strong class="bold">async_inference_config=async_config</strong></pre><pre class="source-code">)</pre></li>

</ol>

<p class="list-inset">这里，我们将在上一步中启动的<code>AsyncInferenceConfig</code>对象指定为<code>async_inference_config</code>的参数值。</p>

<div><div><img alt="Figure 7.19 – Deploying an asynchronous inference endpoint&#10;&#10;" height="523" src="img/B18638_07_019.jpg" width="1129"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.19–部署异步推理端点</p>

<p class="list-inset">在<em class="italic">图7.19 </em>中，我们可以看到<code>deploy()</code>方法接受<a id="_idIndexMarker843"/>参数值<a id="_idIndexMarker844"/>为SageMaker配置异步推理端点，而不是实时推理端点。</p>

<p class="callout-heading">注意</p>

<p class="callout">完成模型部署大约需要3到8分钟。</p>

<ol>

<li value="9">一旦推断端点准备就绪，让我们使用<code>predict_async()</code>方法来执行预测:<pre class="source-code">response = predictor.<strong class="bold">predict_async</strong>(</pre> <pre class="source-code">    input_path=<strong class="bold">input_data</strong></pre> <pre class="source-code">)</pre></li>

</ol>

<p class="list-inset">这应该使用存储在S3的<code>input.json</code>文件中的数据调用异步推理端点。</p>

<div><div><img alt="Figure 7.20 – How the predict_async() method works&#10;&#10;" height="347" src="img/B18638_07_020.jpg" width="930"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.20–predict _ async()方法如何工作</p>

<p class="list-inset">在<em class="italic">图7.20 </em>中，我们可以看到<a id="_idIndexMarker845"/>异步推理端点<a id="_idIndexMarker846"/>的输入负载来自S3桶。然后，在端点处理请求之后，输出被保存到S3。如果您的输入负载很小(例如，小于<em class="italic"> 1 MB </em>)，这可能没有任何意义。然而，如果输入负载涉及更大的文件，比如视频文件，那么将它上传到S3并利用异步推断端点进行预测会更有意义。</p>

<ol>

<li value="10">使用<code>sleep()</code>函数等待40秒后调用<code>response</code>对象的<code>get_result()</code>函数:<pre class="source-code">from time import sleep</pre> <pre class="source-code">sleep(40)</pre> <pre class="source-code">response.<strong class="bold">get_result()</strong></pre></li>

</ol>

<p class="list-inset">这将产生一个输出值<code>'POSITIVE'</code>。</p>

<p class="callout-heading">注意</p>

<p class="callout">为什么要等40秒？由于我们在预测步骤中人为添加了30秒的延迟，因此在输出文件在指定的S3位置可用之前，我们必须至少等待30秒。</p>

<ol>

<li value="11">将S3路径字符串值存储在<code>output_path</code>变量:<pre class="source-code">output_path = response.output_path</pre></li>

<li>使用<code>aws s3 cp</code>命令<a id="_idIndexMarker847"/>将输出文件<a id="_idIndexMarker848"/>的副本下载到Studio笔记本实例:<pre class="source-code">!<strong class="bold">aws s3 cp</strong> {output_path} sample.out</pre></li>

<li>现在我们已经下载了输出文件，让我们使用<code>cat</code>命令来检查它的内容:<pre class="source-code">!cat sample.out</pre></li>

</ol>

<p class="list-inset">这应该给我们一个输出值<code>'POSITIVE'</code>，类似于我们在前面的步骤中使用<code>get_result()</code>方法后得到的结果。</p>

<ol>

<li value="14">让我们通过使用<code>rm</code>命令:<pre class="source-code">!rm sample.out</pre>删除输出文件的副本来做一个快速清理</li>

<li>最后，让我们使用<code>delete_endpoint()</code>方法删除推理端点:<pre class="source-code">predictor.<strong class="bold">delete_endpoint()</strong></pre></li>

</ol>

<p class="list-inset">这将有助于我们避免任何未使用的推理端点的意外费用。</p>

<p>值得注意的是，在生产设置中，最好将架构<a id="_idIndexMarker849"/>更新为更受事件驱动，并且在初始化<code>AsyncInferenceConfig</code>对象时，必须使用适当的值字典更新<code>notification_config</code>参数值。更多<a id="_idIndexMarker850"/>信息，可以随意查看以下链接:<a href="https://sagemaker.readthedocs.io/en/stable/overview.xhtml#sagemaker-asynchronous-inference">https://sagemaker . readthe docs . io/en/stable/overview . XHTML # sagemaker-asynchronous-inference</a>。</p>

<p class="callout-heading">注意</p>

<p class="callout">SNS是什么？SNS是一个完全托管的消息服务<a id="_idIndexMarker851"/>，它允许架构是事件驱动的。来自一个源(<em class="italic">发布者</em>)的消息可以散开并被发送到不同的接收者(<em class="italic">订阅者</em>)。如果我们要配置SageMaker异步推断端点来将通知消息推送到SNS，那么最好也注册并设置一个订阅者，在预测步骤完成后等待成功(或错误)通知消息。一旦结果可用，该订户就继续执行预定义的操作。</p>

<h1 id="_idParaDest-156"><a id="_idTextAnchor166"/>清理</h1>

<p>现在我们已经完成了本章的动手解决方案，是时候清理和关闭我们不再使用的资源了。在下一组步骤中，我们将在SageMaker Studio中找到并关闭任何剩余的运行实例:</p>

<ol>

<li value="1">点击侧边栏中的<strong class="bold">运行实例和内核</strong>图标，如图<em class="italic">图7.21 </em>所示:</li>

</ol>

<div><div><img alt="Figure 7.21 – Turning off the running instance&#10;&#10;" height="234" src="img/B18638_07_021.jpg" width="500"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.21–关闭正在运行的实例</p>

<p class="list-inset">点击<strong class="bold">运行实例和内核</strong>图标将打开并显示SageMaker Studio中的运行实例、应用和终端。</p>

<ol>

<li value="2">点击<strong class="bold">关闭</strong>按钮，关闭<strong class="bold">运行实例</strong>下的所有运行实例，如图<em class="italic">图7.21 </em>中高亮显示。单击<strong class="bold">关闭</strong>按钮将打开一个弹出窗口，确认实例关闭操作。点击<strong class="bold">关闭所有</strong>按钮继续。</li>

<li>确保检查并删除<strong class="bold"> SageMaker资源</strong>下所有正在运行的推理端点(如果有):</li>

</ol>

<div><div><img alt="Figure 7.22 – Checking the list of running inference endpoints&#10;&#10;" height="579" src="img/B18638_07_022.jpg" width="960"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.22–检查正在运行的推理端点列表</p>

<p class="list-inset">要检查是否有<a id="_idIndexMarker853"/>正在运行推理端点，点击<strong class="bold"> SageMaker资源</strong>图标，如图<em class="italic">图7.22 </em>所示，然后从下拉菜单的选项列表中选择<strong class="bold">端点</strong>。</p>

<p>需要注意的是，这个清理操作需要在使用SageMaker Studio之后执行。即使在不活动期间，SageMaker也不会自动关闭这些资源。</p>

<p class="callout-heading">注意</p>

<p class="callout">如果您正在寻找在SageMaker中运行ML工作负载时降低成本的其他方法，您可以查看<a id="_idIndexMarker854"/>如何利用其他特性和功能，例如<strong class="bold"> SageMaker节约计划</strong>(这有助于降低成本，以换取1年或3年期的一致使用承诺<a id="_idIndexMarker855"/>)<strong class="bold">sage maker Neo</strong>(这有助于优化ML模型以进行部署，加快推理并降低成本)，以及<strong class="bold"> SageMaker推理推荐器</strong>(这有助于您选择最佳实例我们不会在本书中进一步详细讨论这些，所以请随意<a id="_idIndexMarker857"/>查看下面的链接，了解有关这些主题的更多信息:<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/inference-cost-optimization.xhtml">https://docs . AWS . Amazon . com/sage maker/latest/DG/inference-cost-optimization . XHTML</a>。</p>

<h1 id="_idParaDest-157"><a id="_idTextAnchor167"/>部署策略和最佳实践</h1>

<p>在本节中，我们将讨论使用SageMaker托管服务时的相关部署策略和最佳实践。让我们从讨论调用现有SageMaker推理端点的不同方式开始。到目前为止，我们使用的解决方案包括使用SageMaker Python SDK来调用现有端点:</p>

<pre class="source-code">from sagemaker.predictor import <strong class="bold">Predictor</strong>

from sagemaker.serializers import <strong class="bold">JSONSerializer</strong>

from sagemaker.deserializers import <strong class="bold">JSONDeserializer</strong>

endpoint_name = "<strong class="bold">&lt;INSERT NAME OF EXISTING ENDPOINT&gt;</strong>"

predictor = <strong class="bold">Predictor</strong>(endpoint_name=endpoint_name)

predictor.serializer = <strong class="bold">JSONSerializer</strong>() 

predictor.deserializer = <strong class="bold">JSONDeserializer</strong>()

payload = {

^  "text": "I love reading the book MLE on AWS!"

}

predictor.<strong class="bold">predict</strong>(payload)</pre>

<p>这里，我们初始化一个<code>Predictor</code>对象<a id="_idIndexMarker860"/>，并在初始化步骤中将其指向一个现有的推理端点<a id="_idIndexMarker861"/>。然后我们使用这个<code>Predictor</code>对象的<code>predict()</code>方法来调用推理端点。</p>

<p>注意，我们还可以使用<strong class="bold"> boto3 </strong>库调用<a id="_idIndexMarker862"/>同一个端点，类似于下面的代码块所示:</p>

<pre class="source-code">import boto3 

import json

endpoint_name = "<strong class="bold">&lt;INSERT NAME OF EXISTING ENDPOINT&gt;</strong>"

runtime = boto3.Session().client('<strong class="bold">sagemaker-runtime</strong>')

payload = {

    "text": "I love reading the book MLE on AWS!"

}

response = sagemaker_client.<strong class="bold">invoke_endpoint</strong>(

    EndpointName=endpoint_name, 

    ContentType='application/json', 

    Body=json.dumps(payload)

)

json.loads(<strong class="bold">response['Body']</strong>.read().decode('utf-8'))</pre>

<p>这里，当使用现有的ML推断端点执行预测和推断时，我们使用<code>invoke_endpoint()</code>方法。如您所见，即使没有安装SageMaker Python SDK，我们也应该能够使用<code>InvokeEndpoint</code> API从一个<code>POST</code>请求中调用一个现有的ML推理端点。</p>

<p class="callout-heading">注意</p>

<p class="callout">如果您的后端应用程序代码使用了Python之外的语言(例如，Ruby、Java或JavaScript)，那么您需要做的就是查找该语言的现有SDK以及要调用的相应函数或方法。要了解更多信息，您可以查看以下包含不同工具的链接，以及每种语言可用的SDK:<a href="https://aws.amazon.com/tools/">https://aws.amazon.com/tools/</a>。</p>

<p>如果您想准备一个HTTP API <a id="_idIndexMarker868"/>来调用现有的SageMaker推理端点并与之接口，那么有几种可能的解决方案。以下是可能解决方案的快速列表:</p>

<ul>

<li><em class="italic">选项1 </em> : <em class="italic">亚马逊API网关HTTP API + AWS Lambda函数+ boto3 + SageMaker ML推理端点</em>—<code>boto3</code>库调用<a id="_idIndexMarker870"/>sage maker ML推理端点。</li>

<li><em class="italic">选项2 </em> : <em class="italic"> AWS Lambda函数+ boto3 + SageMaker ML推理端点(Lambda函数URL)</em>–直接从Lambda函数URL(是触发Lambda函数的专用端点)调用AWS Lambda函数。AWS Lambda函数然后使用<code>boto3</code>库来调用SageMaker ML推理端点。</li>

<li><em class="italic">选项3 </em> : <em class="italic">亚马逊API网关HTTP API + SageMaker ML推理端点(API网关映射模板)</em>–亚马逊API网关HTTP API接收<a id="_idIndexMarker871"/>HTTP请求，直接使用<strong class="bold"> API网关映射模板</strong>(不使用Lambda函数)调用SageMaker ML推理端点。</li>

<li><em class="italic">选项4 </em> : <em class="italic">自定义基于容器的web应用程序，在EC2实例+ boto3 + SageMaker ML推断端点</em>内使用web框架(例如Flask或Django)–web应用程序(在<code>boto3</code>库中的容器内运行，以调用SageMaker ML推断端点。</li>

<li><em class="italic">选项5 </em> : <em class="italic">在弹性容器服务(ECS) + boto3 + SageMaker ML推断端点</em>内使用web框架(例如Flask或Django)定制基于容器的web应用程序——web应用程序(在容器内运行，使用<code>boto3</code>库调用SageMaker ML推断端点。</li>

<li><em class="italic">选项6 </em> : <em class="italic">使用web框架(例如Flask或Django)的定制的基于容器的web应用程序，具有弹性Kubernetes服务(EKS) + boto3 + SageMaker ML推断端点</em>——web应用程序(运行在<code>boto3</code>库内以调用SageMaker ML推断端点。</li>

<li><em class="italic">选项7</em>:<em class="italic">AWS app sync(graph QL API)+AWS Lambda函数+ boto3 + SageMaker ML推理端点</em>—<code>boto3</code>库调用SageMaker ML推理端点。</li>

</ul>

<p>注意，这不是一个详尽的<a id="_idIndexMarker876"/>列表，肯定还有其他方法来设置HTTP API <a id="_idIndexMarker877"/>调用现有的SageMaker推断端点。当然，也有一些场景，我们希望从另一个AWS服务资源中直接调用现有的推理端点。这意味着我们不再需要准备一个单独的HTTP API作为两个服务之间的中间人。</p>

<p>重要的是<a id="_idIndexMarker878"/>要注意到<a id="_idIndexMarker879"/>我们也可以直接从<strong class="bold">亚马逊极光</strong>、<strong class="bold">亚马逊雅典娜</strong>、<strong class="bold">亚马逊Quicksight </strong>或<strong class="bold">亚马逊红移</strong>调用SageMaker推断<a id="_idIndexMarker880"/>端点。在<a href="B18638_04.xhtml#_idTextAnchor079"> <em class="italic">第四章</em> </a>、<em class="italic">AWS上的无服务器数据管理</em>中，我们使用了红移<a id="_idIndexMarker881"/>和Athena来查询我们的数据。除了使用这些服务已经可用的数据库查询之外，我们可以使用类似于以下代码块中的语法直接执行ML推断(Athena的示例查询):</p>

<pre class="source-code">USING EXTERNAL FUNCTION <strong class="bold">function_name</strong>(value INT)

RETURNS DOUBLE

SAGEMAKER '<strong class="bold">&lt;INSERT EXISTING ENDPOINT NAME&gt;</strong>'

SELECT label, value, <strong class="bold">function_name</strong>(value) AS alias

FROM athena_db.athena_table</pre>

<p>在这里，我们定义并使用一个定制函数，该函数在使用<a id="_idIndexMarker883"/> Amazon Athena时调用现有的SageMaker推理端点<a id="_idIndexMarker882"/>进行预测。如需更多<a id="_idIndexMarker884"/>信息，请随时查看以下资源和链接:</p>

<ul>

<li><strong class="bold">亚马逊雅典娜</strong> + <strong class="bold">亚马逊sage maker</strong>:<a href="https://docs.aws.amazon.com/athena/latest/ug/querying-mlmodel.xhtml">https://docs . AWS . Amazon . com/Athena/latest/ug/query-ml model . XHTML</a>。</li>

<li><strong class="bold">亚马逊红移</strong> + <strong class="bold">亚马逊SageMaker</strong>:<a href="https://docs.aws.amazon.com/redshift/latest/dg/machine_learning.xhtml">https://docs . AWS . Amazon . com/Redshift/latest/DG/machine _ learning . XHTML</a>。</li>

<li><strong class="bold">亚马逊Aurora </strong> + <strong class="bold">亚马逊sage maker</strong>:<a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ml.xhtml">https://docs . AWS . Amazon . com/Amazon rds/latest/Aurora user guide/Aurora-ml . XHTML</a>。</li>

<li><strong class="bold">亚马逊QuickSight </strong> + <strong class="bold">亚马逊SageMaker</strong>:<a href="https://docs.aws.amazon.com/quicksight/latest/user/sagemaker-integration.xhtml">https://docs . AWS . Amazon . com/quick sight/latest/user/SageMaker-integration . XHTML</a>。</li>

</ul>

<p>如果我们想在SageMaker托管服务之外部署一个模型，我们也可以这么做。例如，我们可以使用SageMaker来训练我们的模型，然后从S3存储桶中下载<code>model.tar.gz</code>文件，其中包含了在训练过程中生成的模型工件文件。生成的模型工件文件可以部署在SageMaker之外，类似于我们在<a href="B18638_02.xhtml#_idTextAnchor041"> <em class="italic">第二章</em> </a>、<em class="italic">深度学习AMIs </em>、<a href="B18638_03.xhtml#_idTextAnchor060"> <em class="italic">第三章</em> </a>、<em class="italic">深度学习容器</em>中部署和调用模型的方式。此时，您可能会问自己:为什么使用SageMaker托管服务部署ML模型？如果您要在SageMaker托管服务中部署ML模型，以下是您可以轻松执行和设置的快速列表:</p>

<ul>

<li>设置用于托管<a id="_idIndexMarker887"/>ML模型的基础设施资源(ML实例)的自动缩放(<strong class="bold">自动缩放</strong>)。当流量或工作负载增加时，自动缩放会自动添加新的ML实例，而当流量或工作负载减少时，自动缩放会减少所提供的ML实例的数量。</li>

<li>使用SageMaker的<strong class="bold">多模型端点</strong> ( <strong class="bold"> MME </strong>)和<strong class="bold">多容器端点</strong> ( <strong class="bold"> MCE </strong>)支持，在单个推理端点部署多个<a id="_idIndexMarker888"/> ML模型。<a id="_idIndexMarker889"/>还可以在单个端点<a id="_idIndexMarker890"/>后建立一个<strong class="bold">串行推理管道</strong>，它涉及一系列用于处理ML推理请求的容器(例如，预处理、预测和后处理)。</li>

<li>通过将流量<a id="_idIndexMarker891"/>分配给单个推理端点下的多个变量，设置ML模型的<strong class="bold"> A/B测试</strong>。</li>

<li>使用SageMaker Python SDK，只需几行代码即可设置自动化模型监控并监控(1)数据质量、(2)模型质量、(3)偏差漂移和(4)要素属性漂移。我们将在<a href="B18638_08.xhtml#_idTextAnchor172"> <em class="italic">第八章</em> </a>、<em class="italic">模型监控和管理解决方案</em>中深入探讨模型监控。</li>

<li>在部署模型时使用<strong class="bold">弹性推理</strong>将<a id="_idIndexMarker892"/>推理加速添加到SageMaker推理端点，以提高吞吐量并减少延迟。</li>

<li>在更新部署模型时执行蓝/绿部署时，使用各种流量转换模式。如果我们想一次性将所有流量从旧设置转移到新设置，我们可以使用<strong class="bold">一次性</strong>流量转移模式<a id="_idIndexMarker893"/>。如果我们想分两步将<a id="_idIndexMarker894"/>流量从旧设置转移到新设置，我们可以使用<strong class="bold">金丝雀</strong>流量转移模式。这涉及在第一次移位中仅移位一部分流量，而在第二次移位中移位剩余的流量。最后，我们可以使用<strong class="bold">线性</strong>业务转移模式，以预定数量的步骤将业务从旧设置迭代转移<a id="_idIndexMarker895"/>到新设置。</li>

<li>设置<strong class="bold"> CloudWatch </strong>警报和<a id="_idIndexMarker896"/> SageMaker自动回滚配置，以自动化部署回滚过程。</li>

</ul>

<p>如果我们要使用SageMaker进行模型<a id="_idIndexMarker898"/>部署，所有这些都相对容易设置。当使用这些特性和功能时，我们需要担心的只是配置步骤，因为大部分工作已经被SageMaker自动化了。</p>

<p>到目前为止，我们已经讨论了在云中部署ML模型时的不同选项和解决方案。在结束本节之前，让我们快速地<a id="_idIndexMarker899"/>讨论一下在<strong class="bold">边缘设备</strong>上的ML模型部署，例如移动设备和智能摄像机。这种方法有几个优点，包括实时预测延迟、隐私保护和与网络连接相关的成本降低。当然，在边缘设备上运行和管理ML模型存在挑战，因为涉及到资源限制<a id="_idIndexMarker900"/>，例如计算和内存。这些挑战可以通过<strong class="bold"> SageMaker Edge Manager </strong>来解决，这是一种在边缘设备上优化、运行、监控和更新ML <a id="_idIndexMarker903"/>模型时，使<a id="_idIndexMarker901"/>使用其他几个<a id="_idIndexMarker902"/>服务、功能和特性(如<strong class="bold"> SageMaker Neo </strong>、<strong class="bold"> IoT Greengrass </strong>和<strong class="bold"> SageMaker Model Monitor </strong>)的能力。我们不会更深入地探究细节，所以请随意查看https://docs.aws.amazon.com/sagemaker/latest/dg/edge.xhtml以获得更多关于这个话题的信息。</p>

<h1 id="_idParaDest-158"><a id="_idTextAnchor168"/>总结</h1>

<p>在本章中，我们讨论并关注了使用SageMaker的几个部署选项和解决方案。我们将预先训练的模型部署到三种不同类型的推理端点中——( 1)实时推理端点，(2)无服务器推理端点，以及(3)异步推理端点。我们还讨论了每种方法的不同之处，以及在部署ML模型时每种方法的最佳使用时机。在本章的末尾，我们讨论了一些部署策略，以及使用SageMaker进行模型部署的最佳实践。</p>

<p>在下一章中，我们将深入探讨<strong class="bold"> SageMaker模型注册表</strong>和<strong class="bold"> SageMaker模型监视器</strong>，它们是SageMaker的功能，可以帮助我们管理和监视生产中的模型。</p>

<h1 id="_idParaDest-159"><a id="_idTextAnchor169"/>延伸阅读</h1>

<p>有关本章主题的更多信息，请随时查阅以下资源:</p>

<ul>

<li><em class="italic">抱脸蒸馏模型</em>(<a href="https://huggingface.co/docs/transformers/model_doc/distilbert">https://Hugging Face . co/docs/transformers/model _ doc/蒸馏模型</a>)</li>

<li><em class="italic">sage maker——部署模型进行推理</em>(https://docs . AWS . Amazon . com/sage maker/latest/DG/deploy-model . XHTML)</li>

<li><em class="italic">SageMaker–推理推荐器</em>(<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml">https://docs . AWS . Amazon . com/SageMaker/latest/DG/Inference-Recommender . XHTML</a>)</li>

<li><em class="italic">SageMaker-Deployment guardrails</em>(<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails.xhtml">https://docs . AWS . Amazon . com/SageMaker/latest/DG/Deployment-guardrails . XHTML</a>)</li>

</ul>

</div>

<div><div/>

</div>

</div>



</body></html>