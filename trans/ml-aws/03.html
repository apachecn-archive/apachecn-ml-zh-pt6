<html><head/><body>









<title>Chapter 2: Deep Learning AMIs</title>







<div><div><h1 class="chapter-number" id="_idParaDest-40"><a id="_idTextAnchor041"/> <a id="_idTextAnchor042"/> 2</h1>

<h1 id="_idParaDest-41"><a id="_idTextAnchor043"/>深度学习AMIs</h1>

<p>在<a href="B18638_01.xhtml#_idTextAnchor017"> <em class="italic">第一章</em> </a>、<em class="italic">AWS上的ML工程介绍</em>的<em class="italic">必要前提</em>部分，我们大概花了一个小时左右的时间来设置我们的Cloud9环境。在我们能够处理实际的<strong class="bold">机器学习</strong> ( <strong class="bold"> ML </strong>)需求之前，我们必须花一些时间安装几个包，以及一些依赖项。最重要的是，我们必须确保我们对某些包使用了正确的版本，以避免遇到各种各样的问题。如果您认为这很容易出错并且很乏味，那么想象一下，您被分配了为一组数据科学家准备20 ML环境的任务！让我重复一遍……<em class="italic">二十</em>！重复做同样的事情会花费我们大约15到20个小时。在使用你准备的ML环境一周后，数据科学家然后要求你也在这些环境中安装深度学习框架<strong class="bold"> TensorFlow </strong>、<strong class="bold"> PyTorch </strong>和<strong class="bold"> MXNet </strong>，因为他们将使用这些ML框架测试不同的深度学习模型。此时，你可能已经在问自己，“<em class="italic">有更好的方法吗？</em>”。好消息是，有多种方法可以更有效地处理这些类型的需求。一个可能的解决方案是利用<strong class="bold">亚马逊机器映像</strong> ( <strong class="bold"> AMIs </strong>)，特别是AWS <strong class="bold">深度学习AMIs </strong> ( <strong class="bold"> DLAMIs </strong>)到<a id="_idIndexMarker110"/>显著加快准备ML环境的过程。当启动新实例时，这些ami将充当包含相关软件和环境配置的预配置模板。</p>

<p>在<strong class="bold"> DLAMIs </strong>存在之前，ML工程师必须花费数小时在EC2实例中安装和配置<a id="_idIndexMarker111"/>深度学习框架，然后才能在AWS云中运行ML工作负载。从零开始手动准备这些ML环境的过程既繁琐又容易出错。一旦DLAMIs可用，数据科学家和ML工程师就能够使用他们首选的深度学习框架直接运行他们的ML实验。</p>

<p>在本章中，我们将看到使用特定于框架的深度学习AMI来设置GPU实例是多么方便。然后我们将在这个环境中使用<strong class="bold"> TensorFlow </strong>和<strong class="bold"> Keras </strong>训练一个深度学习模型。一旦训练步骤完成，我们将使用测试数据集评估模型。之后，我们将执行清理步骤并终止EC2实例。在本章末尾，我们还将简短讨论AWS定价如何适用于EC2实例。这将帮助您掌握管理在这些实例中运行ML工作负载的总成本所需的知识。</p>

<p>也就是说，我们将在本章中讨论以下主题:</p>

<ul>

<li>深度学习人工智能入门</li>

<li>使用深度学习AMI启动EC2实例</li>

<li>下载样本数据集</li>

<li>训练一个ML模型</li>

<li>加载和评估模型</li>

<li>清理</li>

<li>了解AWS定价如何适用于EC2实例</li>

</ul>

<p>本章中的实践解决方案将帮助您将任何现有的<strong class="bold"> TensorFlow </strong>、<strong class="bold"> PyTorch </strong>和<strong class="bold"> MXNet </strong>脚本和模型迁移到AWS cloud。除了前面提到的成本讨论，我们还将讨论一些安全指南和最佳实践，以帮助我们确保我们设置的环境具有良好的初始安全配置。考虑到这些，我们开始吧！</p>

<h1 id="_idParaDest-42"><a id="_idTextAnchor044"/>技术要求</h1>

<p>在我们开始之前，我们必须有一个网络浏览器(最好是Chrome或Firefox)和一个AWS帐户来使用本章中的动手解决方案。确保您可以访问您在第1章 、<em class="italic">AWS上的ML工程简介</em>中使用的AWS帐户。</p>

<p>Jupyter笔记本、源代码和其他用于每章的文件都可以在本书的GitHub资源库中找到:<a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS">https://GitHub . com/packt publishing/Machine-Learning-Engineering-on-AWS</a>。</p>

<h1 id="_idParaDest-43"><a id="_idTextAnchor045"/>深度学习人工智能入门</h1>

<p>在我们谈论ami之前，我们必须对ami有一个很好的概念。我们可以把AMI <a id="_idIndexMarker112"/>想象成一个有机体的“DNA”。使用这个类比，有机体将对应并映射到一个或多个EC2实例:</p>

<div><div><img alt="Figure 2.1 – Launching EC2 instances using Deep Learning AMIs&#10;&#10;" height="453" src="img/B18638_02_001.jpg" width="1104"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.1–使用深度学习AMIs启动EC2实例</p>

<p>如果我们使用相同的AMI启动两个EC2实例(类似于图2.1<em class="italic">中所示)，那么在实例启动时，两个实例将具有相同的安装包、框架、工具和操作系统。当然，并非一切都需要相同，因为这些实例可能有不同的实例类型、不同的安全组和其他可配置的属性。</em></p>

<p>AMIs允许工程师在一致的环境中轻松启动EC2实例，而不必花费数小时安装不同的包和工具。除了安装步骤之外，这些EC2实例需要进行配置和优化，然后才能用于特定的工作负载。DLAMIs等预建的AMIs已经预装了<strong class="bold"> TensorFlow </strong>、<strong class="bold"> PyTorch </strong>、<strong class="bold"> MXNet </strong>等流行的深度学习框架。这意味着数据科学家、开发人员和ML工程师可以继续进行ML实验和部署，而不必担心安装和设置过程。</p>

<p>如果我们必须准备安装了这些深度学习框架的20 ML环境，我很确定我们不会花20个小时或更多时间来完成。如果我们使用DLAMIs，大概2到3个小时就足够完成工作了。<em class="italic">你不相信我？</em> <em class="italic">在下一节中，我们将这么做！</em>当然，我们将只准备一个ML环境，而不是20个。在学习本章中的动手解决方案时，您会注意到在设置和配置运行ML实验所需的先决条件时，速度有了显著的提高。</p>

<p class="callout-heading">注意</p>

<p class="callout">值得注意的是，我们可以选择在现有的非盟特派团基础上构建，并准备我们自己的定制非盟特派团。然后，我们可以在启动新的EC2实例时使用这些定制的ami。</p>

<h1 id="_idParaDest-44"><a id="_idTextAnchor046"/>使用深度学习AMI启动EC2实例</h1>

<p>从DLAMI启动EC2实例非常简单。一旦我们有了使用哪个DLAMI的想法，<a id="_idIndexMarker114"/>剩下的步骤将集中在配置和启动EC2实例上。这里很酷的一点是，我们并不局限于从现有映像启动单个实例。在配置阶段，在从AMI启动一个实例之前，重要的是要注意我们可以为要启动的实例数量指定所需的值(例如，<code>20</code>)。这意味着我们不是启动一个实例，而是同时启动20个实例。</p>

<div><div><img alt="Figure 2.2 – Steps to launch an EC2 instance using a DLAMI&#10;&#10;" height="195" src="img/B18638_02_002.jpg" width="1048"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.2–使用DLAMI启动EC2实例的步骤</p>

<p>我们将把这一部分分成四个部分。如上图所示，我们将首先在<code>p3.2xlarge</code>中定位特定于框架的深度学习AMI，作为实例类型。然后，我们将配置实例要使用的安全设置，包括网络安全设置。最后，我们将启动实例，并使用<strong class="bold"> EC2实例连接</strong>从浏览器连接到它。</p>

<h2 id="_idParaDest-45"><a id="_idTextAnchor047"/>定位特定于框架的DLAMI</h2>

<p>在<a id="_idIndexMarker118"/>寻找AMI时，我们首先应该查看的是<strong class="bold"> AWS AMI目录</strong>。在AMI目录中，我们应该可以找到各种类型的AMI。这些数据语言可以分为多框架数据语言或特定于框架的数据语言。<em class="italic">有什么区别？</em>多框架AMI包括单个AMI中的多个框架，如<strong class="bold"> TensorFlow </strong>、<strong class="bold"> PyTorch </strong>或<strong class="bold"> MXNet </strong>。这使得开发人员、ML工程师和数据科学家可以轻松地试验和探索几种框架。另一方面，特定于框架的DLAMIs更适合生产环境，并且只支持单个框架。在本章中，我们将使用特定于框架(TensorFlow)的深度学习AMI。</p>

<p>在下一组步骤中，我们将导航到AMI目录，并使用特定于框架的(TensorFlow)深度学习AMI来启动一个实例:</p>

<ol>

<li>导航到AWS管理控制台，然后在搜索栏中键入<code>ec2</code>。从结果列表中选择<strong class="bold"> EC2 </strong>:</li>

</ol>

<div><div><img alt="Figure 2.3 – Navigating to the EC2 console&#10;&#10;" height="475" src="img/B18638_02_003.jpg" width="1098"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.3–导航至EC2控制台</p>

<p class="list-inset">我们应该会看到一个匹配结果列表，如<strong class="bold"> EC2 </strong>、<strong class="bold"> EC2映像构建器</strong>和<strong class="bold"> AWS计算优化器</strong>，类似于<em class="italic">图2.2 </em>中所示。从这个列表中，我们将<a id="_idIndexMarker120"/>选择第一个，这将把我们重定向到EC2控制台。</p>

<ol>

<li value="2">在侧边栏中，找到并点击<strong class="bold">图像</strong>下的<strong class="bold"> AMI目录</strong>，导航至<strong class="bold"> EC2 </strong> &gt; <strong class="bold"> AMI目录</strong>页面。</li>

<li>接下来，在<strong class="bold"> AMI目录</strong>页面的搜索栏中输入<code>deep learning ami</code>。确保您按下<strong class="bold">输入</strong>来搜索与搜索查询相关的相关ami:</li>

</ol>

<div><div><img alt="Figure 2.4 – Searching for the framework-specific Deep Learning AMI&#10;&#10;" height="554" src="img/B18638_02_004.jpg" width="1650"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.4–搜索特定于框架的深度学习AMI</p>

<p class="list-inset">如前面的截图所示，我们应该在<strong class="bold"> Quickstart AMIs </strong>下有几个匹配结果。在<strong class="bold"> AWS Marketplace AMIs </strong>和<strong class="bold"> Community AMIs </strong>下也应该有匹配结果。快速启动AMI包括常用于关键工作负载的AMI，如<strong class="bold">亚马逊Linux 2 </strong> AMI、<strong class="bold">Ubuntu Server 20.04 LTS</strong>AMI、<strong class="bold">深度学习AMI </strong>(亚马逊Linux 2) AMI等等。AWS Marketplace AMIs包括几个由AWS创建的ami，以及由可信的第三方来源创建的ami。这些应该包括AMI，如<strong class="bold"> OpenVPN访问服务器</strong> AMI、<strong class="bold"> Kali Linux </strong> AMI和<strong class="bold"> Splunk Enterprise </strong> AMI。所有公开的ami都可以在<strong class="bold">社区ami</strong>下找到。</p>

<ol>

<li value="4">向下滚动<strong class="bold">quick start AMI</strong>列表，找到特定于框架的深度学习AMI，如以下截图所示:</li>

</ol>

<div><div><img alt="Figure 2.5 – Locating the TensorFlow DLAMI&#10;&#10;" height="454" src="img/B18638_02_005.jpg" width="1528"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.5–定位TensorFlow DLAMI</p>

<p class="list-inset">这里，我们为<strong class="bold">亚马逊Linux 2 </strong>选择特定于框架的(TensorFlow)深度学习AMI，因为我们将在本章稍后使用TensorFlow训练一个ML模型。通过阅读AMI的名称和描述来验证选择。然后，点击<strong class="bold">选择</strong>按钮。</p>

<ol>

<li value="5">在上一步中单击了<strong class="bold">选择</strong>按钮后，向上滚动到页面顶部并单击<strong class="bold">启动带有AMI的实例</strong>按钮，如下面的屏幕截图所示:</li>

</ol>

<div><div><img alt="Figure 2.6 – Launch Instance with AMI&#10;&#10;" height="309" src="img/B18638_02_006.jpg" width="973"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.6–使用AMI启动实例</p>

<p class="list-inset">正如我们<a id="_idIndexMarker122"/>所见，带有AMI 按钮的<strong class="bold">启动实例就在带有AMI </strong>按钮的<strong class="bold">创建模板旁边。</strong></p>

<p class="callout-heading">重要说明</p>

<p class="callout">使用<strong class="bold"> AWS深度学习AMIs </strong>没有额外费用。这个<a id="_idIndexMarker123"/>意味着我们只需要考虑与创建的基础设施资源相关的成本。但是，其他ami的使用可能不是免费的。例如，其他公司创建的ami(来自<strong class="bold">AWS market place ami</strong>下的列表)可能会按使用小时收取额外费用。也就是说，检查使用这些ami启动的基础设施资源上的任何额外费用是很重要的。</p>

<p class="list-inset">点击<strong class="bold"> Launch Instance with AMI </strong>按钮应该会将您重定向到<strong class="bold">Launch a Instance</strong>页面，如下图所示:</p>

<div><div><img alt="Figure 2.7 – The Launch an instance page&#10;&#10;" height="1013" src="img/B18638_02_007.jpg" width="1650"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.7–启动实例页面</p>

<p class="list-inset">由于AWS会定期更新在控制台中启动和管理资源的体验，因此在执行下一组步骤时，您可能会看到一些差异。然而，无论您在本部分工作时控制台是什么样子，期望的最终配置将是<a id="_idIndexMarker124"/>相同的。</p>

<ol>

<li value="6">在<strong class="bold">名称</strong>字段的<code>MLE-CH02-DLAMI</code>下。</li>

</ol>

<p>在设置了<strong class="bold"> Name </strong>字段的值之后，下一步包括为我们的EC2实例选择所需的实例类型。在我们继续选择所需的实例类型之前，我们必须快速讨论一下哪些实例可用，哪些类型的实例适合大规模ML工作负载。</p>

<h2 id="_idParaDest-46"><a id="_idTextAnchor048"/>选择实例类型</h2>

<p>在进行<a id="_idIndexMarker125"/>深度学习实验时，数据科学家和ML <a id="_idIndexMarker126"/>工程师一般更喜欢GPU实例而不是CPU实例。<strong class="bold">图形处理单元</strong>(<strong class="bold">GPU</strong>)有助于显著加快深度学习实验的速度，因为GPU可以用于同时处理多个并行计算。由于GPU实例通常比CPU实例更昂贵，数据科学家和ML工程师在处理ML需求时使用两种类型的组合<a id="_idIndexMarker127"/>。例如<a id="_idIndexMarker128"/>, ML从业者可能会限制GPU实例的使用，仅用于训练深度学习模型。这意味着CPU实例将用于部署训练模型的推理端点。这在大多数情况下是足够的，一旦考虑到成本，这将被认为是一个非常实际的举措。</p>

<div><div><img alt="Figure 2.8 – CPU instances versus GPU instances&#10;&#10;" height="541" src="img/B18638_02_008.jpg" width="1019"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.8–CPU实例与GPU实例的对比</p>

<p>也就是说，我们需要确定哪些实例属于GPU实例组，哪些实例属于CPU实例组。上图展示了一些GPU实例的例子，包括<code>p3.2xlarge</code>、<code>dl1.24xlarge</code>、<code>g3.4xlarge</code>、<code>p2.8xlarge</code>和<code>g4ad.8xlarge</code>。还有其他GPU实例类型的例子不在此列表中，但是您应该能够通过检查实例系列来识别它们。例如，我们确信<code>p3.8xlarge</code>是一个GPU实例类型，因为它与<code>p3.2xlarge</code>实例类型属于同一个系列。</p>

<p>既然我们<a id="_idIndexMarker129"/>对CPU和GPU实例有了更好的了解，让我们继续从实例类型的选项列表中定位和选择<code>p3.2xlarge</code>:</p>

<ol>

<li value="1">继续我们在<em class="italic">定位特定于框架的DLAMI </em>部分停止的地方，让我们定位并点击<strong class="bold">实例类型</strong>窗格下的<strong class="bold">比较实例类型</strong>链接。这会将您重定向到<strong class="bold">比较实例类型</strong>页面，如下面的屏幕截图所示:</li>

</ol>

<div><div><img alt="Figure 2.9 – Compare instance types&#10;&#10;" height="698" src="img/B18638_02_009.jpg" width="1102"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.9–比较实例类型</p>

<p class="list-inset">在这里，我们可以看到不同的实例类型，以及它们相应的规格和每小时的成本。</p>

<ol>

<li value="2">单击搜索字段(带有<strong class="bold">过滤器实例类型</strong>占位符文本)。这将打开一个下拉选项列表，如下面的屏幕截图所示:</li>

</ol>

<div><div><img alt="Figure 2.10 – Using the Filter instance types search field&#10;&#10;" height="434" src="img/B18638_02_010.jpg" width="859"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.10–使用过滤器实例类型搜索字段</p>

<p class="list-inset">从选项列表中找到并选择<strong class="bold">GPU</strong>。这将打开<strong class="bold">为GPU添加过滤器</strong>窗口。</p>

<ol>

<li value="3">在<code>0</code>旁边的文本字段中。之后点击<strong class="bold">确认</strong>按钮。</li>

</ol>

<p class="callout-heading">注意</p>

<p class="callout">我们应用的过滤器应该将结果集限制在GPU实例中。我们应该会找到几个加速计算实例家族，比如<em class="italic"> P3 </em>、<em class="italic"> P2 </em>、<em class="italic"> G5 </em>、<em class="italic"> G4dn </em>和<em class="italic"> G3 </em>等等。</p>

<ol>

<li value="4">接下来，让我们单击<strong class="bold">首选项</strong>按钮，如下图所示:</li>

</ol>

<div><div><img alt="Figure 2.11 – Opening the Preferences window&#10;&#10;" height="707" src="img/B18638_02_011.jpg" width="1650"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.11–打开首选项窗口</p>

<p class="list-inset">这将<a id="_idIndexMarker132"/>打开<strong class="bold">首选项</strong>窗口。在<strong class="bold">属性列</strong>下，确保<strong class="bold">GPU</strong>单选按钮处于打开状态，如下图所示:</p>

<div><div><img alt="Figure 2.12 – Displaying the GPUs attribute column&#10;&#10;" height="561" src="img/B18638_02_012.jpg" width="1022"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.12–显示GPU属性列</p>

<p class="list-inset">之后点击<strong class="bold">确认</strong>按钮。这将更新表格列表显示，并显示列表中每种实例类型的GPU数量，如下所示:</p>

<div><div><img alt="Figure 2.13 – GPUs of each instance type&#10;&#10;" height="546" src="img/B18638_02_013.jpg" width="1083"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.13–每种实例类型的GPU</p>

<p class="list-inset">在这里，我们应该看到一种模式，即在同一个实例族中，随着实例类型变得“更大”, GPU的数量通常会增加。</p>

<ol>

<li value="5">定位并选择对应于<strong class="bold"> p3.2xlarge </strong>实例类型的行。记下可用GPU的<a id="_idIndexMarker133"/>数量，以及<strong class="bold"> p3.2xlarge </strong>实例类型的每小时成本(按需Linux定价)。</li>

<li>之后点击<strong class="bold">选择实例类型</strong>按钮(位于屏幕右下方)。</li>

</ol>

<p>这将关闭<strong class="bold">比较实例类型</strong>窗口，并返回到<strong class="bold">启动实例</strong>页面。</p>

<h2 id="_idParaDest-47"><a id="_idTextAnchor049"/>确保默认安全配置</h2>

<p>当启动EC2 <a id="_idIndexMarker134"/>实例时，我们需要管理安全配置，这将影响如何访问实例。这包括配置以下内容:</p>

<ul>

<li><strong class="bold">密钥对</strong>:包含用于安全访问实例的<a id="_idIndexMarker135"/>凭证的文件(例如，使用SSH)</li>

<li><strong class="bold">虚拟私有云</strong> ( <strong class="bold"> VPC </strong>):一个逻辑上隔离的虚拟网络，它规定了如何访问<a id="_idIndexMarker136"/>资源以及资源如何相互通信</li>

<li><strong class="bold">安全组</strong>:虚拟<a id="_idIndexMarker137"/>防火墙，它使用基于配置的协议和端口过滤流量的规则来控制进出EC2实例的流量</li>

</ul>

<p>也就是说，在我们<a id="_idIndexMarker138"/>启动EC2实例之前，让我们继续完成剩余的配置参数:</p>

<ol>

<li value="1">继续我们在<em class="italic">选择实例类型</em>部分离开的地方，让我们继续创建一个新的密钥对。在<strong class="bold">密钥对(登录)</strong>下，定位并点击<strong class="bold">创建新的密钥对</strong>。</li>

<li>在<code>dlami-key</code>中为<code>RSA</code></li>

<li><code>.pem</code></li>



<li>点击本地机器上的<code>.pem</code>文件。注意，对于本章的动手解决方案，我们不需要这个<code>.pem</code>文件，因为我们稍后将使用<strong class="bold"> EC2实例连接</strong>(通过浏览器)来访问实例。</li>

</ol>

<p class="callout-heading">重要说明</p>

<p class="callout">不要共享下载的密钥文件，因为这是用来通过SSH访问实例的。对于生产环境，也可以考虑在正确配置的VPC中隐藏非公共实例。当谈到保护我们的ML环境时，有很多要讨论的。我们将在<a href="B18638_09.xhtml#_idTextAnchor187"> <em class="italic">第9章</em> </a>、<em class="italic">安全性、治理和遵从性策略</em>中详细讨论安全性。</p>

<ol>

<li value="4">在<code>vpc-xxxxxxxx (default)</code>下</li>

<li><code>Enable</code></li>

<li><code>Create security group</code></li>



<li>在<strong class="bold">网络设置</strong>的<strong class="bold">入站安全组规则</strong>下，指定一组安全组规则，类似于以下截图中的配置:</li>

</ol>

<div><div><img alt="Figure 2.14 – Inbound security groups rules&#10;&#10;" height="802" src="img/B18638_02_014.jpg" width="908"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.14–入站安全组规则</p>

<p class="list-inset">如您所见，我们将使用以下规则配置新的安全组:</p>

<ul>

<li><code>SSH</code>；<code>TCP</code>；<code>22</code>；<code>Anywhere</code>|<code>0.0.0.0/0</code>；<code>SSH</code>–允许任何“计算机”，比如您的本地机器，通过端口22上的<strong class="bold">安全外壳</strong> (SSH)协议连接到EC2实例</li>

<li><code>Custom TCP</code>；<code>TCP</code>；<code>6006</code>；<code>Anywhere</code>|<code>0.0.0.0/0</code>；<code>Tensorboard</code>–允许<a id="_idIndexMarker140"/>任何“计算机”如您的本地机器访问EC2实例的端口6006(它可能正在运行一个应用程序如<strong class="bold"> TensorBoard </strong>)</li>

<li><code>Custom TCP</code>；<code>TCP</code>；<code>8888</code>；<code>Anywhere</code>|<code>0.0.0.0/0</code>；<code>Jupyter</code>–允许任何“计算机”如您的本地机器访问EC2实例的端口8888(它可能正在运行一个应用程序，如<strong class="bold"> Jupyter Notebook </strong> app)</li>

</ul>

<p class="list-inset">一旦使用<strong class="bold">安全组名称-必填</strong>和<strong class="bold">描述-必填</strong>以及相关的<strong class="bold">入站安全组规则集</strong>配置了新的安全组，您就可以继续下一步。</p>

<p class="callout-heading">注意</p>

<p class="callout">请注意，一旦我们需要为生产使用准备设置，就需要进一步检查和保护该配置。首先，<code>0.0.0.0/0</code>)因为这种配置允许任何计算机或服务器通过开放端口访问我们的实例。也就是说，我们只能有限地访问本地机器的IP地址。同时，我们拥有的配置应该可以完成任务，因为一旦我们完成本章，我们将立即删除实例。</p>

<ol>

<li value="6">找到并点击<strong class="bold">配置存储</strong>下的<strong class="bold">添加新卷</strong>按钮:</li>

</ol>

<div><div><img alt="Figure 2.15 – Configuring the storage settings&#10;&#10;" height="591" src="img/B18638_02_015.jpg" width="1431"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.15–配置存储设置</p>

<p class="list-inset">在<strong class="bold"> 1x </strong>和<strong class="bold">钩</strong>之间的文本字段中指定<code>35</code>。类似于我们在前面的<a id="_idIndexMarker141"/>截图。</p>

<p>我们还可以配置和调整更多选项(在<strong class="bold">高级细节</strong>下)，但我们将保持默认值不变。</p>

<h2 id="_idParaDest-48"><a id="_idTextAnchor050"/>启动实例并使用EC2实例连接进行连接</h2>

<p>有不同的方法可以连接到EC2实例。前面，我们将实例配置为<a id="_idIndexMarker142"/>可以使用一个密钥文件通过SSH访问它(例如，从本地机器的终端)。另一个可能的选择是使用<strong class="bold"> EC2实例连接</strong>通过浏览器访问实例。我们还可以使用<strong class="bold">会话管理器</strong>通过SSH访问实例。在本节中，我们将使用EC2实例连接来访问我们的实例。</p>

<p>继续<a id="_idIndexMarker143"/>我们在<em class="italic">确保默认安全配置</em>部分停止的地方，让我们继续启动EC2实例并从浏览器访问它:</p>

<ol>

<li value="1">配置完存储设置后，在<strong class="bold">摘要</strong>窗格(位于屏幕右侧)下找到并点击<strong class="bold">启动实例</strong>按钮。请确保在实例启动后的一小时内终止该实例，因为这些类型的实例的每小时速率相对于其他类型的实例要高一些。您可以查看本章的<em class="italic">清理</em>部分了解更多详情。</li>

</ol>

<p class="callout-heading">注意</p>

<p class="callout">确保<code>1</code>中指定的值。从技术上讲，通过将这个值设置为<code>20</code>，我们可以一次启动20个实例。然而，我们不想这样做，因为这将是非常昂贵和浪费。现在，让我们坚持使用<code>1</code>，因为这对于处理本章中的深度学习实验来说应该绰绰有余。</p>

<ol>

<li value="2">您应该会看到一个成功通知，以及正在启动的资源的实例ID，类似于下面的屏幕截图:</li>

</ol>

<div><div><img alt="Figure 2.16 – Launch success notification&#10;&#10;" height="546" src="img/B18638_02_016.jpg" width="1174"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.16–启动成功通知</p>

<p class="list-inset">单击包含实例ID ( <code>i-xxxxxxxxxxxxxxxxx</code>)的链接，如前面的屏幕截图中突出显示的那样，导航到出现在实例列表中的<code>MLE-CH02-DLAMI</code>。</p>

<p class="callout-heading">注意</p>

<p class="callout">等待一两分钟，然后再进行下一步。如果您在启动实例时遇到一个<code>InsufficientInstanceCapacity</code>错误，请随意使用一个不同的<code>p3</code>实例。要进一步解决此问题，您也可以参考<a href="https://aws.amazon.com/premiumsupport/knowledge-center/ec2-insufficient-capacity-errors/">https://AWS . Amazon . com/premium support/knowledge-center/ec2-unlimited-capacity-errors/</a>了解更多信息。</p>

<ol>

<li value="3">通过切换下图中突出显示的复选框来选择<a id="_idIndexMarker145"/>实例。之后点击<strong class="bold">连接</strong>按钮:</li>

</ol>

<div><div><img alt="Figure 2.17 – Connecting to the instance directly&#10;&#10;" height="249" src="img/B18638_02_017.jpg" width="1064"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.17–直接连接到实例</p>

<p class="list-inset">在这里，我们可以<a id="_idIndexMarker146"/>看到有一个使用浏览器直接连接到实例的选项。</p>

<ol>

<li value="4">在<code>AA.BB.CC.DD</code>中)到本地机器上的文本编辑器中。请注意，您将获得不同的公共IP地址值。我们将在本章稍后访问<code>root</code>时使用该IP地址值，然后点击<strong class="bold">连接</strong>:</li>

</ol>

<div><div><img alt="Figure 2.18 – EC2 Instance Connect&#10;&#10;" height="501" src="img/B18638_02_018.jpg" width="860"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.18–EC2实例连接</p>

<p class="list-inset">这应该会打开一个新的选项卡，允许我们直接从浏览器运行终端命令。如果您收到一条<strong class="bold">连接到实例</strong>有问题的错误消息，请等待大约2到3分钟，然后刷新页面或单击<strong class="bold">重试</strong>按钮:</p>

<div><div><img alt="Figure 2.19 – EC2 Instance Connect terminal&#10;&#10;" height="556" src="img/B18638_02_019.jpg" width="1650"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.19–EC2实例连接终端</p>

<p class="list-inset">我们可以看到，<code>TensorFlow 2.9.1</code>和其他实用程序库都安装在<code>/usr/local/bin/python3.9</code>中。请注意，您可能会得到不同的TensorFlow和Python版本，这取决于您用来启动实例的DLAMI的版本。</p>

<p>那不是很容易吗？此时，我们现在应该能够使用TensorFlow执行深度学习实验，而不必在EC2实例中安装额外的工具和库。</p>

<p class="callout-heading">注意</p>

<p class="callout">注意，使用<strong class="bold">启动模板</strong>可以进一步加速<a id="_idIndexMarker149"/>从AMI启动实例的过程，启动模板已经指定了实例配置信息，比如AMI ID、实例类型、密钥对和安全组。我们不会在本书中介绍启动模板的用法，因此可以随时查看以下链接了解更多详细信息:<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-templates.xhtml">https://docs . AWS . Amazon . com/AWS C2/latest/user guide/ec2-Launch-Templates . XHTML</a>。</p>

<h1 id="_idParaDest-49"><a id="_idTextAnchor051"/>下载样本数据集</h1>

<p>在本章接下来的章节中，我们将使用一个非常简单的合成数据集，其中<a id="_idIndexMarker150"/>只包含两列—<em class="italic">x</em>和<em class="italic"> y </em>。这里，<em class="italic"> x </em>可以表示一个物体在<em class="italic"> X </em>轴上的相对位置，而<em class="italic"> y </em>可以表示同一物体在<em class="italic"> Y </em>轴上的位置。下面的屏幕截图显示了数据的一个示例:</p>

<div><div><img alt="Figure 2.20 – Sample dataset&#10;&#10;" height="314" src="img/B18638_02_020.jpg" width="961"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.20–样本数据集</p>

<p>ML是关于寻找模式的。有了这个数据集，我们将构建一个模型，在本章后面给出<em class="italic"> x </em>的值的情况下，尝试预测<em class="italic"> y </em>的值。一旦我们能够用这样一个简单的例子建立模型，处理包含两列以上的更真实的数据集就容易多了，类似于我们在<a href="B18638_01.xhtml#_idTextAnchor017"> <em class="italic">第1章</em> </a>、<em class="italic">AWS</em>上的ML工程介绍中所处理的。</p>

<p class="callout-heading">注意</p>

<p class="callout">在这本书里，我们不会仅仅局限于表格数据和简单的数据集。在<a href="B18638_06.xhtml#_idTextAnchor132"> <em class="italic">第六章</em> </a>，<em class="italic"> SageMaker培训和调试解决方案</em>中，例如，我们将使用带标签的<a id="_idIndexMarker151"/>图像数据，并使用<strong class="bold"> Amazon SageMaker </strong>的几个功能和特性构建两个图像分类模型。在<a href="B18638_07.xhtml#_idTextAnchor151"> <em class="italic">第7章</em> </a>、<em class="italic"> SageMaker部署解决方案</em>中，我们将处理文本数据，并使用<a id="_idIndexMarker152"/>多种部署选项部署一个<strong class="bold">自然语言处理</strong> ( <strong class="bold"> NLP </strong>)模型。</p>

<p>也就是说，让我们从<em class="italic">启动实例并使用EC2实例连接</em>部分中停止的地方继续<a id="_idIndexMarker153"/>并继续下载我们将在本章中用于训练深度学习模型的数据集:</p>

<ol>

<li value="1">在<code>data</code>目录中:<pre class="source-code">mkdir -p <strong class="bold">data</strong></pre></li>

<li>使用<code>wget</code>命令:<pre class="source-code">wget <strong class="bold">https://bit.ly/3h1KBx2</strong> -O <strong class="bold">data/training_data.csv</strong></pre> <pre class="source-code">wget <strong class="bold">https://bit.ly/3gXYM6v</strong> -O<strong class="bold"> data/validation_data.csv</strong></pre> <pre class="source-code">wget <strong class="bold">https://bit.ly/35aKWem</strong> -O <strong class="bold">data/test_data.csv</strong></pre>下载训练、验证和测试数据集</li>

<li>可选地，我们可以使用<code>yum</code>包管理工具:<pre class="source-code">yum install <strong class="bold">tree</strong></pre>安装<code>tree</code>实用程序</li>

</ol>

<p class="list-inset">如果这是您第一次遇到<code>tree</code>命令，它用于以树状结构列出目录<a id="_idIndexMarker154"/>和文件。</p>

<p class="callout-heading">注意</p>

<p class="callout">也可以从EC2实例创建一个定制AMI。如果我们要从我们现在使用的EC2实例创建一个定制AMI，我们将能够从新的定制AMI启动新的EC2实例，并且已经安装了以下内容:(1)来自DLAMI的已安装框架、库和工具，以及(2)我们在定制AMI创建之前安装的<code>tree</code>实用程序。</p>

<ol>

<li value="4">使用<code>tree</code>命令查看当前设置的目录和当前目录下的文件:<pre class="source-code"><strong class="bold">tree</strong></pre></li>

</ol>

<p class="list-inset">这将产生一个树状结构，类似于下面的屏幕截图所示:</p>

<div><div><img alt="Figure 2.21 – Results after using the tree command&#10;&#10;" height="248" src="img/B18638_02_021.jpg" width="1365"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.21–使用tree命令后的结果</p>

<p class="list-inset">在这里，我们可以看到我们已经使用前面的<code>wget</code>命令成功下载了CSV文件。</p>

<ol>

<li value="5">现在，让我们验证并检查我们下载的一个CSV文件的内容。使用<code>head</code>命令查看<code>training_data.csv</code>文件的前几行:<pre class="source-code">head <strong class="bold">data/training_data.csv</strong></pre></li>

</ol>

<p class="list-inset">这将为我们提供多行<em class="italic"> (x，y)对</em>，类似于下面的截图所示:</p>

<div><div><img alt="Figure 2.22 – The first few rows of the training_data.csv file&#10;&#10;" height="420" src="img/B18638_02_022.jpg" width="1650"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.22–training _ data . CSV文件的前几行</p>

<p class="list-inset">您也可以使用<code>head</code>命令<a id="_idIndexMarker155"/>检查<code>validation_data.csv</code>和<code>test_data.csv</code>的内容。</p>

<p class="callout-heading">注意</p>

<p class="callout">值得注意的是，本例中的第一列是<em class="italic"> y </em>列。一些ML实践者遵循一个约定，其中第一列被用作目标列(包含我们想要使用数据集的其他列预测的值的列)。当使用某些算法，如<strong class="bold"> SageMaker </strong>的<strong class="bold"> XGBoost </strong>和<strong class="bold">线性学习器</strong>内置算法时，第一列被假定为目标列。如果您使用自己的定制脚本来加载数据，那么您可以遵循您喜欢的任何约定，因为您可以自由选择如何从文件中加载和解释数据。</p>

<p>你可能已经注意到，到目前为止，在这本书里，我们一直在使用干净的和预处理过的数据集。在真实的ML项目中，您将处理带有各种问题的原始数据<a id="_idIndexMarker156"/>，比如缺失值和重复行。在<a href="B18638_05.xhtml#_idTextAnchor105"> <em class="italic">第5章</em> </a>、<em class="italic">实用数据处理和分析</em>中，我们将使用“更脏”版本的<em class="italic">预订</em>数据集，并使用各种AWS服务和功能<a id="_idIndexMarker157"/>，如<strong class="bold"> AWS Glue DataBrew </strong>和<strong class="bold">Amazon SageMaker Data Wrangler</strong>来分析、清理和处理数据。然而，在本章中，我们将使用“干净”的数据集，因为我们需要专注于使用<strong class="bold"> TensorFlow </strong>和<strong class="bold"> Keras </strong>训练深度学习模型。也就是说，让我们继续生成一个接受<em class="italic"> x </em>作为输入并返回预测的<em class="italic"> y </em>值作为输出的模型。</p>

<h1 id="_idParaDest-50"><a id="_idTextAnchor052"/>训练一个ML模型</h1>

<p>在<a href="B18638_01.xhtml#_idTextAnchor017"> <em class="italic">第1章</em> </a>，<em class="italic">AWS上的ML工程简介</em>中，我们训练了一个二元分类器模型，旨在<a id="_idIndexMarker159"/>使用可用信息预测酒店预订是否会被取消。在本章中，我们将使用从<em class="italic">下载样本数据集</em>得到的(有意简化的)数据集，并训练一个回归模型，该模型将在给定<em class="italic"> x </em>的值的情况下预测<em class="italic"> y </em>(连续变量)的值。我们将不再依赖现成的AutoML工具和服务，而是使用自定义脚本:</p>

<div><div><img alt="Figure 2.23 – Model life cycle&#10;&#10;" height="360" src="img/B18638_02_023.jpg" width="1051"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.23–模型生命周期</p>

<p>编写自定义培训脚本时，我们通常遵循类似于上图所示的顺序。我们从定义和编译模型开始。之后，我们加载数据并使用它来训练和评估模型。最后，我们将模型序列化并保存到一个文件中。</p>

<p class="callout-heading">注意</p>

<p class="callout">模型保存后会发生什么？模型文件可以在推理端点中使用和加载，推理端点是一个web服务器，它使用经过训练的ML模型来执行预测(例如，预测的<em class="italic"> y </em>值)，给定一组输入值(例如，输入<em class="italic"> x </em>值)。在本章的<em class="italic">加载和评估模型</em>部分，我们将使用<code>tf.keras.models</code>中的<code>load_model()</code>函数将生成的模型文件加载到Jupyter笔记本中。然后，我们将使用<code>predict()</code>方法，使用提供的测试数据集执行样本预测。</p>

<p>在本章中，我们将使用一个脚本文件，该文件使用<strong class="bold"> TensorFlow </strong>和<strong class="bold"> Keras </strong>来构建一个<strong class="bold">神经网络</strong>模型——一组相互连接的节点，可以学习输入和输出之间的复杂模式<a id="_idIndexMarker160"/>。由于我们将在本书中使用神经网络和深度学习概念，我们必须对以下概念有一个基本的了解:</p>

<ul>

<li><strong class="bold">神经元</strong>:这些是神经网络的构建模块，接受并处理输入值<a id="_idIndexMarker161"/>以产生输出值。<em class="italic">如何计算输出值？</em>通过神经元的每个输入值乘以相关的<strong class="bold">权重</strong>值<a id="_idIndexMarker162"/>，然后加上一个数值(也称为<strong class="bold">偏差</strong>)。称为<strong class="bold">激活函数</strong>的非线性<a id="_idIndexMarker163"/>函数随后被应用于结果值，这将产生输出。这个非线性函数帮助神经网络学习输入值和输出值之间的复杂模式。我们可以在下图中看到神经元的表示:</li>

</ul>

<div><div><img alt="Figure 2.24 – A representation of a neuron&#10;&#10;" height="431" src="img/B18638_02_024.jpg" width="999"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.24–一个神经元的代表</p>

<p class="list-inset">在这里，我们可以看到，我们可以使用一个公式来计算<em class="italic"> y </em>的值，该公式包括<em class="italic"> x </em>输入值、相应的权重值、偏差和激活函数。也就是说，我们<a id="_idIndexMarker164"/>可以将神经元视为“数学函数”,将神经网络视为“一堆数学函数”,试图通过不断更新权重和偏差值来映射输入值和输出值。</p>

<ul>

<li><strong class="bold">层</strong>:层是由位于神经网络中特定位置或深度的一组神经元组成的<a id="_idIndexMarker165"/>；</li>

</ul>

<div><div><img alt="Figure 2.25 – An input layer, output layer, and multiple hidden layers&#10;&#10;" height="433" src="img/B18638_02_025.jpg" width="870"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.25–一个输入层、一个输出层和多个隐藏层</p>

<p class="list-inset">在这里，我们可以看到神经网络的不同层次。<strong class="bold">输入层</strong>是接收<a id="_idIndexMarker167"/>输入值的层，而<strong class="bold">输出层</strong>是产生输出值的层<a id="_idIndexMarker168"/>。在输入层<a id="_idIndexMarker169"/>和输出层之间是被称为<strong class="bold">隐藏层</strong>的处理层，其处理和转换来自输入层的数据到<a id="_idIndexMarker170"/>输出层。(隐层超过一两层的神经网络一般称为<strong class="bold">深度神经网络</strong>。)</p>

<ul>

<li><strong class="bold">正向传播</strong>:这个<a id="_idIndexMarker171"/>是指信息从输入层到隐藏层，再到输出层的正向流动，产生输出值。</li>

<li><strong class="bold">成本函数</strong>:该函数用于计算预测计算值<a id="_idIndexMarker172"/>与实际值的偏差。假设训练神经网络的目标是生成尽可能接近实际值的预测值，我们应该使用优化算法(如<strong class="bold">梯度下降</strong>)来寻找成本<a id="_idIndexMarker173"/>函数(代表模型的误差)的最小值。</li>

<li><strong class="bold">反向传播</strong>:这是<a id="_idIndexMarker174"/>根据预测值和实际值之间的差异调整神经网络中的权重的过程(包括计算<strong class="bold">梯度</strong>或对每层的权重进行小的更新):</li>

</ul>

<div><div><img alt="Figure 2.26 – Backpropagation&#10;&#10;" height="344" src="img/B18638_02_026.jpg" width="860"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.26–反向传播</p>

<p class="list-inset">这里，我们可以看到<a id="_idIndexMarker175"/>反向传播涉及将计算的误差从输出层反向传播到输入层(并相应地更新权重)。</p>

<ul>

<li><strong class="bold">学习率</strong>:在训练神经网络时，这会影响用于调整网络中与<a id="_idIndexMarker176"/>损失梯度相关的权重的数量。</li>

<li><strong class="bold">历元</strong>:这是一个训练迭代，涉及使用整个训练数据集的一个正向和一个反向传播<a id="_idIndexMarker177"/>。在每次训练迭代之后，更新神经网络的权重，并且期望神经网络在将该组输入值映射到该组输出值方面表现得更好。</li>

</ul>

<p class="callout-heading">注意</p>

<p class="callout">我们不会在本书中深入探讨深度学习和神经网络的细节。如果你有兴趣了解更多关于这些话题的知识，网上有几本书可供选择:<a href="https://www.amazon.com/Neural-Network/s?k=Neural+Network">https://www.amazon.com/Neural-Network/s?k=Neural+Network</a>。</p>

<p>现在我们对什么是神经网络有了更好的了解，我们可以继续训练神经<a id="_idIndexMarker178"/>网络模型。在接下来的一组步骤中，我们将使用自定义脚本，通过上一节下载的数据来训练深度学习模型:</p>

<ol>

<li value="1">首先，让我们使用<code>mkdir</code>命令:<pre class="source-code">mkdir -p <strong class="bold">logs</strong></pre>创建一个名为<code>logs</code>的目录</li>

<li>接下来，使用<code>wget</code>命令下载<code>train.py</code>文件:<pre class="source-code">wget <strong class="bold">https://bit.ly/33D0iYC</strong> -O <strong class="bold">train.py</strong></pre></li>

<li>使用<code>tree</code>命令快速检查文件和目录结构:<pre class="source-code"><strong class="bold">tree</strong></pre></li>

</ol>

<p class="list-inset">这将产生一个树状结构，类似于下面的屏幕截图所示:</p>

<div><div><img alt="Figure 2.27 – Results after using the tree command&#10;&#10;" height="338" src="img/B18638_02_027.jpg" width="1462"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.27–使用tree命令后的结果</p>

<p class="list-inset">请注意，数据和日志目录与<code>train.py</code>文件处于同一级别。</p>

<ol>

<li value="4">在运行<code>train.py</code>文件之前，执行以下命令:<pre class="source-code"><strong class="bold">for a in /sys/bus/pci/devices/*; do echo 0 | sudo tee -a $a/numa_node; done</strong></pre></li>

</ol>

<p class="list-inset">这将有助于我们在本章稍后列出GPU设备时，避免出现<strong class="bold">从SysFS成功读取NUMA节点具有负值(-1) </strong>警告消息。</p>

<ol>

<li value="5">在运行下载的<code>train.py</code>脚本之前，让我们在单独的浏览器选项卡中打开<a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter02/train.py">https://github . com/packt publishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter 02/train . py</a>来检查其内容:</li>

</ol>

<div><div><img alt="Figure 2.28 – The train.py file&#10;&#10;" height="597" src="img/B18638_02_028.jpg" width="1026"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.28–train . py文件</p>

<p class="list-inset">在前面的<a id="_idIndexMarker179"/>截图中，我们可以看到我们的<code>train.py</code>脚本执行了以下操作:</p>

<ul>

<li>(<code>prepare_model()</code>功能</li>

<li>(<code>load_data()</code>功能</li>

<li>(<strong class="bold"> 3 </strong>)准备<strong class="bold">张量板</strong>回调对象</li>

<li>(<code>fit()</code>方法并传递<code>callback</code>参数值</li>

<li>(<code>save()</code>方法</li>

</ul>

<p class="callout-heading">注意</p>

<p class="callout">值得注意的是，我们的<code>train.py</code>脚本中的<code>prepare_model()</code>函数执行<em class="italic">定义模型</em>和<em class="italic">编译模型</em>步骤。该函数中定义的神经网络是一个具有五层的样本序列模型。要了解更多信息，请在<a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter02/train.py#L24">https://github . com/packt publishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter 02/train . py</a>查看<code>prepare_model()</code>函数的实现。</p>

<ol>

<li value="6">让我们通过在EC2实例连接终端中运行以下命令来开始训练步骤:<pre class="source-code">python3.9 <strong class="bold">train.py</strong></pre></li>

</ol>

<p class="list-inset">这应该<a id="_idIndexMarker180"/>产生一组日志，类似于下面的截图所示:</p>

<div><div><img alt="Figure 2.29 – train.py script logs&#10;&#10;" height="426" src="img/B18638_02_029.jpg" width="903"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.29–train . py脚本日志</p>

<p class="list-inset">请注意，完成培训步骤可能需要大约5分钟。一旦<code>train.py</code>脚本执行完毕，您可以使用<code>tree</code>命令检查在<code>logs</code>和<code>model</code>目录中生成的新文件。</p>

<p class="callout-heading">注意</p>

<p class="callout">这里发生了什么事？这里，我们在<code>train.py</code>中定义的模型的<code>fit()</code>方法是用设置为<code>500</code>的历元(迭代)数来训练模型。对于每次迭代，我们都在更新神经网络的权重，以最小化实际值和预测值之间的“误差”(例如，使用交叉验证数据)。</p>

<ol>

<li value="7">接下来，运行下面的命令来运行<code>tensorBoard</code>应用程序，它可以帮助<a id="_idIndexMarker181"/>可视化和调试ML实验:<pre class="source-code"><strong class="bold">tensorboard --logdir=logs --bind_all</strong></pre></li>

<li>打开新的浏览器标签，打开<code>http://&lt;IP ADDRESS&gt;:6006</code>。在<em class="italic">使用深度学习AMI启动EC2实例</em>一节中，将<code>&lt;IP ADDRESS&gt;</code>替换为我们复制到文本编辑器中的公共IP地址:</li>

</ol>

<div><div><img alt="Figure 2.30 – TensorBoard&#10;&#10;" height="1143" src="img/B18638_02_030.jpg" width="1650"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.30–张量板</p>

<p class="list-inset">这将加载一个web应用程序，类似于前面的屏幕截图。我们<a id="_idIndexMarker182"/>不会深入探究我们能用TensorBoard做什么，所以请随意查看<a href="https://www.tensorflow.org/tensorboard">https://www.tensorflow.org/tensorboard</a>获取更多信息。</p>

<p class="callout-heading">注意</p>

<p class="callout">我们如何解读这些图表？如图<em class="italic">图2.30 </em>所示，培训和验证损失一般会随着时间的推移而减少。在第一个图表(顶部)中，<em class="italic">X</em>-轴对应于纪元编号，而<em class="italic">Y</em>-轴显示训练和验证损失。应该注意的是，在这个图表中，训练和验证“学习曲线”是重叠的，并且随着时期或迭代次数的增加，两者都继续减少到某一点。应该注意的是，这些类型的图表有助于诊断ML模型的性能，这将<a id="_idIndexMarker183"/>有助于避免诸如<strong class="bold">过度拟合</strong>(其中经过训练的<a id="_idIndexMarker184"/>模型在训练数据上表现良好，但在看不见的数据上表现不佳)和<strong class="bold">欠拟合</strong>(其中经过训练的模型在训练数据集和看不见的数据上表现不佳)的问题。我们不会详细讨论这一点，所以可以随意查看其他可用的ML和深度学习资源。</p>

<ol>

<li value="9">导航<a id="_idIndexMarker185"/>回到<strong class="bold"> EC2实例内容</strong>终端，用<em class="italic"> Ctrl </em> + <em class="italic"> C </em>停止正在运行的<strong class="bold"> TensorBoard </strong>应用进程。</li>

</ol>

<p>此时，我们应该在<code>model</code>目录中有一个经过训练的模型的工件。在下一节中，我们将在Jupyter笔记本环境中加载和评估这个模型。</p>

<h1 id="_idParaDest-51"><a id="_idTextAnchor053"/>加载和评估模型</h1>

<p>在上一节中，我们使用终端训练了我们的深度学习模型。当进行ML实验时，通常使用基于网络的交互环境更方便，例如<strong class="bold"> Jupyter笔记本</strong>。从技术上讲，我们可以在终端中运行所有后续的代码块，但是为了方便起见，我们将使用Jupyter笔记本。</p>

<p>在下一组步骤中，我们将从命令行启动Jupyter笔记本。然后，我们将运行几个代码块来加载和评估我们在上一节中训练的ML模型。让我们开始吧:</p>

<ol>

<li value="1">继续我们在<em class="italic">训练一个ML模型</em>部分中离开的地方，让我们在<strong class="bold"> EC2实例连接</strong>终端中运行下面的命令:<pre class="source-code"><strong class="bold">jupyter notebook --allow-root --port 8888 --ip 0.0.0.0</strong></pre></li>

</ol>

<p class="list-inset">这将启动Jupyter笔记本，并使其可以通过端口<code>8888</code>访问:</p>

<div><div><img alt="Figure 2.31 – Jupyter Notebook token&#10;&#10;" height="393" src="img/B18638_02_031.jpg" width="1650"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.31–Jupyter笔记本令牌</p>

<p class="list-inset">确保从运行<code>jupyter notebook</code>命令后生成的日志中复制生成的随机令牌。请参考前面的屏幕截图，了解如何获取生成的令牌。</p>

<ol>

<li value="2">打开一个新的<a id="_idIndexMarker187"/>浏览器标签，并打开<code>http://&lt;IP ADDRESS&gt;:8888</code>。在<em class="italic">使用深度学习AMI启动EC2实例</em>部分，用我们复制到文本编辑器的公共IP地址替换<code>&lt;IP ADDRESS&gt;</code>:</li>

</ol>

<div><div><img alt="Figure 2.32 – Accessing the Jupyter Notebook&#10;&#10;" height="791" src="img/B18638_02_032.jpg" width="1650"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.32–访问Jupyter笔记本</p>

<p class="list-inset">在这里，我们可以看到我们被要求输入密码或令牌，然后才能使用<strong class="bold"> Jupyter笔记本</strong>。只需输入从上一步生成的日志中获得的令牌。</p>

<p class="callout-heading">重要说明</p>

<p class="callout">请注意，该设置还不能用于生产环境。有关如何保护Jupyter笔记本服务器的更多信息，请查看https://Jupyter-Notebook . readthedocs . io/en/stable/Security . XHTML。我们还将在<a href="B18638_09.xhtml#_idTextAnchor187"> <em class="italic">第9章</em> </a>、<em class="italic">安全、治理和合规性策略</em>中讨论一些提高此设置安全性的策略。</p>

<ol>

<li value="3">通过点击<strong class="bold">新建</strong>并从<a id="_idIndexMarker189"/>下拉选项列表中选择<strong class="bold"> Python 3 (ipykernel) </strong>来创建一个新的笔记本，类似于下面的截图所示:</li>

</ol>

<div><div><img alt="Figure 2.33 – Creating a new Jupyter Notebook&#10;&#10;" height="353" src="img/B18638_02_033.jpg" width="1056"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.33–创建新的Jupyter笔记本</p>

<p class="list-inset">这将打开一个空白的笔记本，我们可以在那里运行我们的Python代码。</p>

<ol>

<li value="4">导入<code>tensorflow</code>，然后使用<code>list_physical_devices()</code>列出我们实例中可见的GPU:<pre class="source-code">import tensorflow as tf</pre><pre class="source-code">tf.config.list_physical_devices('GPU')</pre></li>

</ol>

<p class="list-inset">这将向<a id="_idIndexMarker190"/>返回一个带有单个<code>PhysicalDevice</code>对象的列表，类似于<code>[PhysicalDevice(name='/physical_device:GPU:0',device_type='GPU')]</code>。</p>

<p class="callout-heading">注意</p>

<p class="callout">由于我们使用的是一个<code>p3.2xlarge</code>实例，前面的代码块返回了一个可见的GPU设备。如果我们启动了一个<code>p3.16xlarge</code>实例，我们应该得到8个可见的GPU设备。请注意，我们可以通过并行技术，如<strong class="bold">数据并行</strong>(其中在每个GPU中使用相同的模型，但使用<a id="_idIndexMarker192"/>数据集的不同块进行训练)和<strong class="bold">模型并行</strong>(其中模型被分成与GPU数量相等的几个部分)，同时利用多个GPU设备<a id="_idIndexMarker191"/>来显著减少训练时间。当然，ML实验脚本需要修改以利用多个GPU。有关如何在TensorFlow中使用GPU的更多<a id="_idIndexMarker193"/>信息，请随时查看以下链接了解更多细节:<a href="https://www.tensorflow.org/guide/gpu">https://www.tensorflow.org/guide/gpu</a>。</p>

<ol>

<li value="5">使用<code>tf.keras.models.load_model()</code>加载模型。使用<code>model.summary()</code>:<pre class="source-code">model = tf.keras.models.<strong class="bold">load_model</strong>('model')</pre>:<pre class="source-code">model.<strong class="bold">summary</strong>()</pre>检查模型</li>

</ol>

<p class="list-inset">这应该<a id="_idIndexMarker194"/>产生一个模型摘要，如下面的屏幕截图所示:</p>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>

<div><div><img alt="Figure 2.34 – Model summary&#10;&#10;" height="459" src="img/B18638_02_034.jpg" width="1231"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.34–模型总结</p>

<p class="list-inset">该模型摘要应反映我们在<em class="italic">培训ML模型</em>部分准备和培训的模型的属性。</p>

<p class="callout-heading">重要说明</p>

<p class="callout">确保使用<code>load_model()</code>函数(以及其他类似的函数)只加载来自可信来源的ML模型。攻击者可以很容易地准备一个模型(带有恶意的有效负载)，当加载该模型时，攻击者可以访问运行ML脚本的服务器(例如，通过<strong class="bold">反向外壳</strong>)。关于这个主题的更多信息，你可以查看作者关于如何黑客攻击和保护ML环境和系统的演讲:<a href="https://speakerdeck.com/arvslat/pycon-apac-2022-hacking-and-securing-machine-learning-environments-and-systems?slide=21">https://speaker deck . com/ARV slat/pycon-APAC-2022-hacking-and-securing-machine-learning-environments-and-systems？slide=21 </a>。</p>

<ol>

<li value="6">定义<a id="_idIndexMarker195"/><code>load_data()</code>函数，返回指定文件位置的CSV文件的值:<pre class="source-code"><strong class="bold">import</strong> numpy <strong class="bold">as</strong> np</pre><pre class="source-code"><strong class="bold">def load_data</strong>(training_data_location):</pre><pre class="source-code">    fo = open(training_data_location, "rb")</pre><pre class="source-code">    result = np.loadtxt(fo, delimiter=",")</pre><pre class="source-code">    </pre><pre class="source-code">    y = result[:, 0]</pre><pre class="source-code">    x = result[:, 1]</pre><pre class="source-code">    </pre><pre class="source-code">    <strong class="bold">return</strong> (x, y)</pre></li>

<li>现在，让我们测试加载的模型是否可以执行作为一组输入值给出的预测。使用<code>load_data()</code>加载测试数据，并使用<code>model.predict()</code> : <pre class="source-code">x, y = <strong class="bold">load_data</strong>("data/test_data.csv")</pre> <pre class="source-code">predictions = model.<strong class="bold">predict</strong>(x[0:5])</pre> <pre class="source-code">predictions</pre>执行一些样本预测</li>

</ol>

<p class="list-inset">这应该<a id="_idIndexMarker196"/>产生一个浮点值数组，类似于下面的屏幕截图所示:</p>

<div><div><img alt="Figure 2.35 – Prediction results&#10;&#10;" height="135" src="img/B18638_02_035.jpg" width="925"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.35–预测结果</p>

<p class="list-inset">这里，我们有预测的<em class="italic"> y </em>目标值的数组，对应于五个输入<em class="italic"> x </em>值的每一个。请注意，这些预测的<em class="italic"> y </em>值与从<code>test_data.csv</code>文件加载的实际<em class="italic"> y </em>值不同。</p>

<ol>

<li value="8">使用<code>model.evaluate()</code> : <pre class="source-code">results = model.evaluate(x, y, batch_size=128)</pre> <pre class="source-code">results</pre>评估<a id="_idIndexMarker197"/>加载的模型</li>

</ol>

<p class="list-inset">这应该给我们一个类似于或接近于<code>2.705784797668457</code>的值。如果您想知道这个数字的含义，这是对应于<em class="italic">预测值与实际值的距离</em>的数值:</p>

<div><div><img alt="Figure 2.36 – How model evaluation works&#10;&#10;" height="251" src="img/B18638_02_036.jpg" width="686"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.36-模型评估如何工作</p>

<p>在这里，我们<a id="_idIndexMarker198"/>可以看到模型评估<a id="_idIndexMarker199"/>如何处理回归问题的例子。首先，诸如<strong class="bold">均方根误差</strong> ( <strong class="bold"> RMSE </strong>)、<strong class="bold">均方误差</strong> ( <strong class="bold"> MSE </strong>)和<strong class="bold">平均绝对误差</strong> ( <strong class="bold"> MAE </strong>)的评估<a id="_idIndexMarker200"/>度量在计算单个评估度量值之前，计算<em class="italic"> y </em>的实际值和预测值之间的差<a id="_idIndexMarker201"/>。这意味着，与RMSE值较高的模型相比，RMSE值较低的模型通常会犯较少的错误。</p>

<p>此时，您可能决定利用前面的代码块以及Python web框架(如<strong class="bold"> Flask </strong>、<strong class="bold"> Pyramid </strong>或<strong class="bold"> Django </strong>)来构建一个定制的后端API。但是，您可能<a id="_idIndexMarker202"/>想要先检查其他内置解决方案，例如<strong class="bold">tensor flow Serving</strong>(tensor flow模型的ML模型服务系统)，它是为生产环境设计的。</p>

<p>如果你仔细想想，我们已经在最后几节<em class="italic">中完成了一个完整的ML实验，而不需要安装任何额外的库、包或框架</em>(除了可选的<code>tree</code>实用程序)。这样，你就知道了<strong class="bold">深度学习人工智能</strong>是多么有用和强大了！同样，如果我们必须设置20个或更多这样的ML环境，我们可能需要不到2个小时来设置和准备好一切。</p>

<h1 id="_idParaDest-52"><a id="_idTextAnchor054"/>清理</h1>

<p>既然我们已经<a id="_idIndexMarker203"/>完成了一个端到端的ML实验，是时候执行清理步骤来帮助我们管理成本了:</p>

<ol>

<li value="1">关闭包含<strong class="bold"> EC2实例连接</strong>终端会话的浏览器选项卡。</li>

<li>导航到我们使用深度学习AMI启动的实例的<strong class="bold"> EC2实例</strong>页面。点击<strong class="bold">实例状态</strong>打开下拉选项列表，然后点击<strong class="bold">终止实例</strong>:</li>

</ol>

<div><div><img alt="Figure 2.37 – Terminating the instance&#10;&#10;" height="381" src="img/B18638_02_037.jpg" width="860"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.37–终止实例</p>

<p class="list-inset">正如我们所见，还有其他选项可用，如<strong class="bold">停止实例</strong>和<strong class="bold">重启实例</strong>。如果您还不想删除该实例，您可能希望停止该实例，并在以后的日期和时间启动它。请注意，停止的实例会产生成本，因为EC2实例停止时不会删除连接的EBS卷。也就是说，如果EBS卷中没有存储任何关键文件，最好终止实例并删除任何附加的EBS卷。</p>

<ol>

<li value="3">在<strong class="bold">中终止实例？</strong>窗口，点击<strong class="bold">终止</strong>。这将删除EC2实例，以及附加了卷的<a id="_idIndexMarker204"/>。</li>

</ol>

<p>当不再需要管理和降低成本时，应关闭、终止或删除未使用的资源。由于我们的ML和ML工程要求需要更多的资源，我们将不得不利用几种成本优化策略来管理成本。我们将在下一节讨论其中的一些策略。</p>

<h1 id="_idParaDest-53"><a id="_idTextAnchor055"/>了解AWS定价如何适用于EC2实例</h1>

<p>在我们结束本章之前，我们必须对AWS定价在处理EC2实例时如何工作有一个好的了解。我们还需要了解架构和设置如何影响在云中运行ML工作负载的总成本。</p>

<p>假设我们最初在俄勒冈地区有一个全天24小时运行的单个<code>p2.xlarge</code>实例。在这个实例中，数据科学团队定期运行一个脚本，该脚本使用首选的ML框架来训练深度学习模型。这个训练脚本一般每周运行两次，每次3小时左右。鉴于新数据可用性的不可预测的时间表<a id="_idIndexMarker207"/>，很难<a id="_idIndexMarker208"/>知道训练脚本将在何时运行以产生新的模型。然后，生成的ML模型被立即部署到web API服务器，该服务器充当同一个实例中的推理端点。根据这些信息，安装费用是多少？</p>

<div><div><img alt="Figure 2.38 – Approximate cost of running a p2.xlarge instance per month&#10;&#10;" height="485" src="img/B18638_02_038.jpg" width="956"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.38–每月运行一个p2.xlarge实例的大概成本</p>

<p>在这里，我们可以看到这种设置的总成本大约至少为每月<em class="italic">648美元</em>。我们是如何得到这个号码的？我们首先寻找在俄勒冈地区运行一个<code>p2.xlarge</code>实例每小时的按需成本(使用以下链接作为参考:<a href="https://aws.amazon.com/ec2/pricing/on-demand/">https://aws.amazon.com/ec2/pricing/on-demand/</a>)。在本文撰写之时<a id="_idIndexMarker209"/>，在俄勒冈州(<code>us-west-2</code>)地区，一个<code>p2.xlarge</code>实例的每小时按需成本将是<em class="italic">$ 0.90/小时</em>。由于我们将一整月24/7运行这个实例，我们必须计算<em class="italic">估计的每月总小时数</em>。假设我们每个月有大约30天，那么我们在——也就是<code>24 hours per day x 30 days = 720 hours</code>——一个月总共大约有<em class="italic"> 720个小时。</em></p>

<p>请注意，我们也可以使用<em class="italic"> 730.001小时</em>作为每月总小时数的更准确值。然而，我们现在将坚持720小时，以稍微简化事情。下一步是将运行EC2实例每小时的<em class="italic">成本</em> ( <code>$0.90 per hour</code>)和每月的<em class="italic">总小时数</em> ( <code>720 hours per month</code>)相乘。这将给我们一个月内运行EC2实例的总成本(<code>$0.90 x 720 = $648</code>)。</p>

<p class="callout-heading">注意</p>

<p class="callout">为了简化本节的计算，我们将只考虑使用EC2实例的每小时成本。在现实生活中，我们需要考虑与使用其他资源相关的成本，如EBS卷、VPC资源(NAT网关)等。为了获得更准确的估计，请确保使用<strong class="bold"> AWS价格计算器</strong>:https://calculator.aws/.</p>

<p>过了一会儿，<a id="_idIndexMarker211"/>数据科学团队决定在同一个实例中训练<a id="_idIndexMarker212"/>另一个模型，在这个实例中我们已经运行了一个训练脚本和一个web服务器(推理端点)。由于担心在同时运行两个训练脚本时可能会遇到性能问题和瓶颈，团队请求将<code>p2.xlarge</code>实例升级到<code>p2.8xlarge</code>实例。根据这些信息，新的设置会花费多少钱？</p>

<div><div><img alt="Figure 2.39 – Approximate cost of running a p2.8xlarge instance per month&#10;&#10;" height="454" src="img/B18638_02_039.jpg" width="968"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.39–每月运行一个2.39大型实例的大概成本</p>

<p>在这里，我们可以看到这种设置的总成本大约至少为每月<em class="italic">5184美元</em>。我们是如何得到这个数字的？我们必须遵循与前面的<a id="_idIndexMarker213"/>示例类似的步骤，并寻找运行一个<code>p2.8xlarge</code>实例每小时的按需<a id="_idIndexMarker214"/>成本。在这里，我们可以看到运行一个<code>p2.8xlarge</code>实例的成本(<em class="italic"> $7.20每小时</em>)是运行一个<code>p2.xlarge</code>实例的成本(<em class="italic"> $0.90每小时</em>)的八倍。也就是说，我们预计总成本将是之前初始设置的八倍。将运行p2.8xlarge实例每小时的<em class="italic">成本</em> ( <code>$7.20 per hour</code>)和每月的<em class="italic">总小时数</em> ( <code>720 hours per month</code>)相乘后，我们应该得到单月运行<code>p2.8xlarge</code>实例的总成本(<code>$7.20 x 720 = $5,184</code>)。</p>

<h2 id="_idParaDest-54"><a id="_idTextAnchor056"/>使用多个较小的实例来降低运行ML工作负载的总体成本</h2>

<p>在这一点上，您可能想知道是否有更好的方法来设置事情，以便在运行相同的ML工作负载集时显著降低成本。好消息是，有多种方法可以改善我们目前的状况，并将成本从每月<em class="italic">5184美元</em>降低到更低的水平，例如每月<em class="italic">86.40美元</em>！请注意，与运行原始设置的成本相比，这也要小得多(<em class="italic"> $648每月</em>)。我们如何做到这一点？</p>

<p>我们需要做的第一件事是利用多个“较小的”实例，而不是单个<code>p2.8xlarge</code>实例。一种可能的设置是为每个训练脚本使用一个<code>p2.xlarge</code>实例(<em class="italic"> $0.90每小时</em>)。因为我们正在使用两个训练脚本，所以我们总共有两个<code>p2.xlarge</code>实例。除此之外，我们将使用一个<code>m6i.large</code>实例(<em class="italic"> $0.096每小时</em>)来托管部署模型的推理端点。由于训练脚本只在有新数据可用时运行(大约每周两次)，我们可以让<code>p2.xlarge</code>实例只在需要运行训练脚本时运行。这意味着，如果我们每月大约有<em class="italic"> 720小时</em>，那么与其中一个培训脚本相关联的<code>p2.xlarge</code>实例应该每月总共只运行大约<em class="italic"> 24小时</em>(大多数时间实例是关闭的)。</p>

<p class="callout-heading">注意</p>

<p class="callout"><em class="italic">我们怎么得到这个数字</em>？由于训练脚本预计每周运行两次，每次大约3小时，因此公式为<code>[3 hours per run] x [2 times per week] x [4 weeks]</code>，得出的值为24小时。这意味着，如果每个<code>p2.xlarge</code>实例在一个月内总共只运行大约24小时，那么每个<code>p2.xlarge</code>实例每月将花费大约<em class="italic">$ 21.60</em>。</p>

<p>即使这些<code>p2.xlarge</code>实例大部分时间都是关闭的，我们的ML推理端点<a id="_idIndexMarker216"/>仍然会在其专用的<code>m6i.large</code>实例中全天候运行。运行<code>m6i.large</code>实例一整月的成本大约是每月<em class="italic">69.12美元</em>(使用<code>[$0.096 per hour] x [720 hours per month]</code>公式):</p>

<div><div><img alt="Figure 2.40 – Using multiple smaller instances to reduce the overall cost&#10;&#10;" height="659" src="img/B18638_02_040.jpg" width="1184"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.40–使用多个较小的实例来降低总成本</p>

<p>也就是说，我们应该能够将总成本降低到大约每月<em class="italic">$ 112.32</em>，类似于上图所示。我们是如何得到这个数字的？我们简单地添加了一个月内运行每个实例的预期成本:<code>$21.60 + $21.60 + $69.12 = $112.32</code>。</p>

<h2 id="_idParaDest-55"><a id="_idTextAnchor057"/>使用spot实例降低运行培训作业的成本</h2>

<p>值得注意的是，我们可以通过利用用于运行<a id="_idIndexMarker217"/>训练脚本的<code>p2.xlarge</code>实例来进一步降低成本。使用spot实例，我们可以通过利用AWS中可用的备用计算能力，将使用特定EC2实例类型的成本降低大约60%到90%。这意味着在运行<code>p2.xlarge</code>实例时，我们可能只支付<em class="italic">每小时0.36美元</em>，而不是支付<em class="italic">每小时0.90美元</em>，假设我们使用spot实例将节省大约60%。<em class="italic">使用spot实例时有什么问题？</em>当使用spot实例时，在这些实例中运行的应用程序有可能被中断！这意味着我们应该只运行在意外中断后可以恢复的任务(比如ML培训任务)。</p>

<p class="callout-heading">注意</p>

<p class="callout">我们是怎么得到这个号码的？60%的节省相当于每小时的按需成本(<em class="italic">$ 0.90/小时</em>)乘以0.40。这将给我们<code>[$0.90 per hour] x [0.40] = [$0.36 per hour]</code>。</p>

<p>由于使用spot实例时可能会出现中断，因此不建议您将它们用于web服务器(推理端点)正在运行的24/7 <code>m6i.large</code>实例:</p>

<div><div><img alt="Figure 2.41 – Using spot instances to reduce the cost of running training jobs&#10;&#10;" height="689" src="img/B18638_02_041.jpg" width="1185"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图2.41–使用spot实例降低运行培训工作的成本</p>

<p>一旦我们为<code>p2.xlarge</code>实例使用了spot实例，我们就能够将总成本降低到大约每月<em class="italic">86.40美元</em>，类似于我们在前面的<a id="_idIndexMarker218"/>图表中所得到的。同样，这个最终值不包括其他成本，以稍微简化计算。然而，正如您所看到的，这个值比运行单个<code>p2.8xlarge</code>实例的成本要小得多(每月<em class="italic">5184美元</em>)。</p>

<p>是不是很神奇？！我们只是稍微改变了一下架构，我们能够将成本从每月<em class="italic">5184美元</em>降低到每月<em class="italic">86.40美元</em>！请注意，还有其他方法可以优化在云中运行ML工作负载的总体成本(例如，利用<strong class="bold">计算节省计划</strong>)。你在这一节学到的应该足够了，因为我们将在本书接下来的几章中继续这些类型的讨论。</p>

<h1 id="_idParaDest-56"><a id="_idTextAnchor058"/>总结</h1>

<p>在这一章中，我们能够使用一个<strong class="bold">深度学习AMI </strong>来启动一个EC2实例。这使得我们可以立即拥有一个可以执行ML实验的环境，而不用担心安装和设置步骤。然后，我们继续使用<strong class="bold"> TensorFlow </strong>来训练和评估我们的深度学习模型，以解决一个回归问题。我们简要讨论了AWS定价如何适用于EC2实例，从而结束了本章。</p>

<p>在下一章，我们将关注<strong class="bold"> AWS深度学习容器</strong>如何帮助显著加快ML实验和部署过程。</p>

<h1 id="_idParaDest-57"><a id="_idTextAnchor059"/>延伸阅读</h1>

<p>对于深度学习人工智能，我们只是触及了表面。除了具有预安装框架的便利性，DLAMIs还使ML工程师能够轻松利用其他优化解决方案，如<strong class="bold"> AWS推理</strong>、<strong class="bold"> AWS神经元</strong>、<strong class="bold">分布式训练</strong>和<strong class="bold">弹性织物适配器</strong>。有关更多信息，请随时查看以下资源:</p>

<ul>

<li><em class="italic">什么是AWS深度学习AMI？</em>(<a href="https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.xhtml">https://docs . AWS . Amazon . com/dlami/latest/dev guide/what-is-dlami . XHTML</a>)</li>

<li><em class="italic">AWS定价如何工作</em>(<a href="https://docs.aws.amazon.com/whitepapers/latest/how-aws-pricing-works/how-aws-pricing-works.pdf">https://docs . AWS . Amazon . com/whites/latest/How-AWS-Pricing-Works/How-AWS-Pricing-Works . pdf</a>)</li>

<li><em class="italic">弹力织物适配器</em>(<a href="https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-efa.xhtml">https://docs . AWS . Amazon . com/dlami/latest/dev guide/tutorial-EFA . XHTML</a>)</li>

</ul>

</div>

</div>



</body></html>