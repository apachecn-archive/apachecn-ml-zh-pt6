<html><head/><body>





<style type="text/css">body{margin:1em;background-color:transparent!important;}#sbo-rt-content *{text-indent:0pt!important;}#sbo-rt-content .bq{margin-right:1em!important;}#sbo-rt-content *{word-wrap:break-word!important;word-break:break-word!important;}#sbo-rt-content table,#sbo-rt-content pre{overflow-x:unset!important;overflow:unset!important;overflow-y:unset!important;white-space:pre-wrap!important;}</style>
<div><div><h1 id="_idParaDest-58"><em class="italic"> <a id="_idTextAnchor058"/>第四章</em>:与TPOT一起探索分类</h1>
			<p>在这一章中，你将继续学习自动化机器学习的实例。通过浏览三个完整的数据集，您将了解如何使用TPOT以自动化的方式处理分类任务。</p>
			<p>我们将涵盖基本主题，如数据集加载、清理、必要的数据准备和探索性数据分析。然后，我们将深入探讨TPOT的分类。您将学习如何训练和评估自动分类模型。</p>
			<p>在自动训练模型之前，您将看到如何使用基本分类算法(如逻辑回归)获得好的模型。这个模型将作为TPOT超越市场的基准。</p>
			<p>本章将涵盖以下主题:</p>
			<ul>
				<li>将自动分类建模应用于虹膜数据集</li>
				<li>将自动分类建模应用于Titanic数据集</li>
			</ul>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor059"/>技术要求</h1>
			<p>要完成本章，您需要在安装了Python和TPOT的计算机中安装Python和TPOT。参见<a href="B16954_02_Final_SK_ePub.xhtml#_idTextAnchor036"> <em class="italic">第二章</em> </a>、<em class="italic">深入TPOT </em>，了解环境设置的详细说明。如果分类的概念对你来说是全新的，参考<a href="B16954_01_Final_SK_ePub.xhtml#_idTextAnchor014"> <em class="italic">第一章</em> </a>、<em class="italic">机器学习和自动化的思想</em>。</p>
			<p>你可以在这里下载本章的源代码和数据集:<a href="https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter04">https://github . com/packt publishing/Machine-Learning-Automation-with-TPOT/tree/main/chapter 04</a><a href="https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter4%20"/></p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor060"/>将自动分类模型应用于虹膜数据集</h1>
			<p>让我们从最基本的数据集之一——虹膜数据集(<a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">https://en.wikipedia.org/wiki/Iris_flower_data_set</a>)开始，<a id="_idIndexMarker229"/>从简单的开始。这里的挑战不是建立一个自动化的模型，而是建立一个能够超越基线模型的模型。虹膜数据集非常简单，即使是最基本的分类算法也可以达到很高的精度。</p>
			<p>因此，在这一节中，您应该将重点放在了解分类基础知识上。稍后您将有足够的时间来担心性能:</p>
			<ol>
				<li>As with the regression section, the first thing you should do is import the required libraries and load the dataset. You'll need <code>n</code><code>umpy</code>, <code>pandas</code>, <code>matplotlib</code>, and <code>seaborn</code> for starters. The <code>matplotlib.rcParams</code> module is imported to tweak the default stylings.<p>下面是库导入和数据集加载的代码片段:</p><pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import rcParams
rcParams['axes.spines.top'] = False
rcParams['axes.spines.right'] = False
df = pd.read_csv('data/iris.csv')
df.head()</pre><p>下面是由<code>head()</code>函数返回的输出:</p><div><img src="img/B16954_04_01.jpg" alt="Figure 4.1 – Head of the Iris dataset&#13;&#10;" width="600" height="224"/></div><p class="figure-caption">图4.1–虹膜数据集的头部</p><p>太好了，这正是我们开始工作所需要的。</p></li>
				<li>The next <a id="_idIndexMarker231"/>step is to check if data quality is <a id="_idIndexMarker232"/>good enough to be passed to a machine learning algorithm. The first step here is to check for missing values. The following code snippet does just that:<pre>df.isnull().sum()</pre><p>输出如下图所示:</p><div><img src="img/B16954_04_02.jpg" alt="Figure 4.2 – Missing value counts per column for the Iris dataset&#13;&#10;" width="200" height="135"/></div><p class="figure-caption">图4.2–Iris数据集每列的缺失值计数</p><p>似乎没有丢失的值，所以我们可以继续。</p></li>
				<li>Let's now check for class distribution in the target variable. This refers to the number of instances belonging to each class – <code>setosa</code>, <code>virginica</code>, and <code>versicolor</code>, in this case. Machine learning models are known to perform poorly if a severe class imbalance is present.<p>以下代码片段可视化了类分布:</p><pre>ax = df.groupby('species').count().plot(kind='bar', figsize=(10, 6), fontsize=13, color='#4f4f4f')
ax.set_title('Iris Dataset target variable distribution', size=20, pad=30)
ax.set_ylabel('Count', fontsize=14)
ax.set_xlabel('Species', fontsize=14)
ax.get_legend().remove()</pre><p><a id="_idIndexMarker233"/>可视化如下图<a id="_idIndexMarker234"/>所示:</p><div><img src="img/B16954_04_03.jpg" alt="Figure 4.3 – Iris dataset target variable distribution&#13;&#10;" width="1000" height="770"/></div><p class="figure-caption">图4.3–Iris数据集目标变量分布</p><p>虹膜数据集就像它们来的时候一样好——所以再一次，我们没有什么要准备的。</p></li>
				<li>The final step in the data exploratory analysis and preparation is to check for correlation. A high correlation between features typically means there's some redundancy in the dataset, at least to a degree.<p>以下代码片段绘制了一个带有注释的关联矩阵:</p><pre>plt.figure(figsize=(12, 9))
plt.title('Correlation matrix', size=20)
sns.heatmap(df.corr(), annot=True, cmap='Blues');</pre><p>相关矩阵如下图所示:</p><div><img src="img/B16954_04_04.jpg" alt="Figure 4.4 – Correlation matrix of the Iris dataset&#13;&#10;" width="1089" height="905"/></div><p class="figure-caption">图4.4–虹膜数据集的相关矩阵</p><p>不出所料，大多数特征之间有很强的相关性。</p><p>您现在已经熟悉了Iris数据集，这意味着我们可以继续下一步的建模。</p></li>
				<li>Let's build a baseline model with a logistic regression algorithm first. It will serve as a starting model that TPOT needs to outperform.<p>该过程的第一步是训练/测试分割。下面的代码片段就是这样做的，它还打印了两个集合中的实例数:</p><pre>from sklearn.model_selection import train_test_split
X = df.drop('species', axis=1)
y = df['species']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=3
)
y_train.shape, y_test.shape</pre><p><a id="_idIndexMarker239"/>实例数量如下图<a id="_idIndexMarker240"/>所示:</p><div><img src="img/B16954_04_05.jpg" alt="Figure 4.5 – Number of instances in train and test sets&#13;&#10;" width="230" height="34"/></div><p class="figure-caption">图4.5–训练和测试集中的实例数量</p><p>接下来让我们构建基线模型。</p></li>
				<li>As mentioned earlier, we'll use logistic regression for the job. The code snippet below fits a logistic regression model, makes the predictions on the test set, and prints a confusion matrix of actual and predicted values:<pre>from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
lm = LogisticRegression(random_state=42)
lm.fit(X_train, y_train)
lm_preds = lm.predict(X_test)
print(confusion_matrix(y_test, lm_preds))</pre><p><a id="_idIndexMarker241"/>对应的混乱<a id="_idIndexMarker242"/>矩阵如下图所示:</p><div><img src="img/B16954_04_06.jpg" alt="Figure 4.6 – Logistic regression confusion matrix for the Iris dataset&#13;&#10;" width="184" height="89"/></div><pre>from sklearn.metrics import accuracy_score
print(accuracy_score(y_test, lm_preds))</pre><p>准确性得分显示在下图中:</p><div><img src="img/B16954_04_07.jpg" alt="Figure 4.7 – Accuracy on the test set with logistic regression for the Iris dataset&#13;&#10;" width="287" height="24"/></div><p class="figure-caption">图4.7–Iris数据集逻辑回归测试集的准确性</p><p>现在你有了它——97%的准确率，只有一个开箱即用的错误分类，使用最简单的分类算法。让我们看看接下来TPOT是否能超越它。</p></li>
				<li>Let's build an automated classification model next. We'll optimize for accuracy and train for 10 minutes – similar to what we did in <a href="B16954_03_Final_SK_ePub.xhtml#_idTextAnchor051"><em class="italic">Chapter 3</em></a>, <em class="italic">Exploring Regression with TPOT</em>. The <a id="_idIndexMarker243"/>code snippet below imports TPOT, instantiates a pipeline optimizer, and trains the <a id="_idIndexMarker244"/>model on the training datasets:<pre>from tpot import TPOTClassifier
pipeline_optimizer = TPOTClassifier(
    scoring='accuracy',
    max_time_mins=10,
    random_state=42,
    verbosity=2
)
pipeline_optimizer.fit(X_train, y_train)</pre><p>TPOT设法在我的机器上安装了18代，如下图所示:</p><div><img src="img/B16954_04_08.jpg" alt="" width="1046" height="1070"/></div><p class="figure-caption">图4.8–Iris数据集上TPOT管道优化的输出</p></li>
				<li>Let's see if training <a id="_idIndexMarker245"/>an automated model managed <a id="_idIndexMarker246"/>to increase accuracy. You can use the following snippet to obtain the accuracy score:<pre>tpot_preds = pipeline_optimizer.predict(X_test)
accuracy_score(y_test, tpot_preds)</pre><p>准确性得分如下图所示:</p><div><img src="img/B16954_04_09.jpg" alt="Figure 4.9 – Accuracy on the test set with an automated model for the Iris dataset&#13;&#10;" width="285" height="27"/></div><p class="figure-caption">图4.9–Iris数据集自动化模型测试集的准确性</p><p>如您所见，测试集的准确性没有提高。如果你要做一个目标变量和特征的散点图，你会看到<em class="italic"> virginica </em>和<em class="italic"> versicolor </em>类有一些重叠。这是最有可能的情况，再多的训练也无法正确地对这一个例进行分类。</p></li>
				<li>There's only two things left to do here, and both are optional. The <a id="_idIndexMarker247"/>first one is to see what TPOT <a id="_idIndexMarker248"/>declared as an optimal pipeline after 10 minutes of training. The following code snippet will output that pipeline to the console:<pre>pipeline_optimizer.fitted_pipeline_</pre><p>对应的管道如下图所示:</p><div><img src="img/B16954_04_10.jpg" alt="Figure 4.10 – Optimal TPOT pipeline for the Iris dataset&#13;&#10;" width="1000" height="124"/></div><p class="figure-caption">图4.10–Iris数据集的最佳TPOT流水线</p></li>
				<li>As always, you can also export the pipeline with the <code>export()</code> function:<pre>pipeline_optimizer.export('iris_pipeline.py')</pre><p>下图显示了完整的Python代码:</p></li>
			</ol>
			<div><div><img src="img/B16954_04_11.jpg" alt="Figure 4.11 – Python code for an optimal TPOT pipeline for the Iris dataset&#13;&#10;" width="1100" height="631"/>
				</div>
			</div>
			<p class="figure-caption">图4.11–虹膜数据集最佳TPOT管道的Python代码</p>
			<p>现在<a id="_idIndexMarker249"/>你有了它——你的第一个全自动<a id="_idIndexMarker250"/>TPOT分类模型。是的，数据集是最基本的，但是原理总是一样的。接下来，我们将在更复杂的数据集上制作自动化模型，因此将有时间让您亲自动手。</p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor061"/>将自动分类建模应用于titanic数据集</h1>
			<p>我们<a id="_idIndexMarker251"/>现在要将自动TPOT分类<a id="_idIndexMarker252"/>建模应用到稍微复杂一点的数据集。你会被泰坦尼克号数据集(<a href="https://gist.githubusercontent.com/michhar/2dfd2de0d4f8727f873422c5d959fff5/raw/fa71405126017e6a37bea592440b4bee94bf7b9e/titanic.csv">https://gist . githubusercontent . com/michhar/2 DFD 2 de 0d 4 f 8727 f 873422 C5 d 959 fff 5/raw/fa 71405126017 e 6 a 37 bea 592440 B4 bee 94 BF 7 b 9 e/Titanic . CSV</a>)弄脏双手——这个数据集包含泰坦尼克号事故中幸存和未幸存乘客的各种属性和描述。</p>
			<p>目标是建立一个自动化模型，能够根据各种输入特征，如乘客类别、性别、年龄、客舱、兄弟姐妹、配偶、父母和子女的数量等，预测乘客是否会在事故中幸存。</p>
			<p>接下来，我们将从加载库和数据集开始:</p>
			<ol>
				<li value="1">As always, the first step is to load in the libraries and the dataset. You'll need <code>numpy</code>, <code>pandas</code>, <code>matplotlib</code>, and <code>seaborn</code> to get you started. The <code>Matplotlib.rcParams</code> module is also imported, just to make the visualizations a bit more appealing.<p>下面的代码片段<a id="_idIndexMarker254"/>导入库，加载到数据集中，并显示前五行:</p><pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import rcParams
rcParams['axes.spines.top'] = False
rcParams['axes.spines.right'] = False
df = pd.read_csv('data/titanic.csv')
df.head()</pre><p>调用<code>head()</code>函数返回数据集的前五行。它们如下图所示:</p><div><img src="img/B16954_04_12.jpg" alt="Figure 4.12 – Head of the Titanic dataset&#13;&#10;" width="1288" height="208"/></div><p class="figure-caption">图4.12–泰坦尼克号数据集的头部</p><p>现在，您可以继续进行探索性数据分析和准备。</p></li>
				<li>The first step in the exploratory data analysis and preparation is to check for missing values. The following code line does just that:<pre>df.isnull().sum()</pre><p>前面一行代码报告了数据集中每列缺失值的<a id="_idIndexMarker255"/>数量，如下图所示:</p><div><img src="img/B16954_04_13.jpg" alt="Figure 4.13 – Missing values count per column for the Titanic dataset&#13;&#10;" width="280" height="328"/></div><p class="figure-caption">图4.13–Titanic数据集的每列缺失值计数</p><p>如您所见，数据集中存在大量缺失值。大多数丢失的值都在<code>Age</code>和<code>Cabin</code>属性中。对于<code>Cabin</code>来说很容易理解——如果乘客没有自己的客舱，那么这个值就是缺失的。</p><p>我们稍后将处理这些丢失的值，但是现在，让我们将注意力转移到数据可视化上，这样您可以更好地理解数据集。</p></li>
				<li>To avoid code duplication, let's define a single function for displaying a bar chart. The function shows a bar chart with column counts on top of the bars. It also allows you to specify for which dataset column you want to draw a bar chart, values for the title, <em class="italic">x</em>-axis label, and <em class="italic">y</em>-axis label, and also offsets for the counts.<p>你可以在这里找到这个<a id="_idIndexMarker257"/>函数的代码:</p><pre>def make_bar_chart(column, title, ylabel, xlabel, y_offset=10, x_offset=0.2):
    ax = df.groupby(column).count()[['PassengerId']].plot(
        kind='bar', figsize=(10, 6), fontsize=13, color='#4f4f4f'
    )
    ax.set_title(title, size=20, pad=30)
    ax.set_ylabel(ylabel, fontsize=14)
    ax.set_xlabel(xlabel, fontsize=14)
    ax.get_legend().remove()
                  
    for i in ax.patches:
        ax.text(i.get_x() + x_offset, i.get_height() + y_offset, i.get_height(), fontsize=20)
    return ax</pre><p>在接下来的几页中，您将广泛使用这个函数。目标是可视化分类变量是如何分布的，这样您可以更好地理解数据集。</p></li>
				<li>To start, let's visualize how many passengers have survived and how many haven't. The previously declared <code>make_bar_chart()</code> function comes in handy for the job.<p>下面的<a id="_idIndexMarker258"/>代码片段使<a id="_idIndexMarker259"/>可视化:</p><pre>make_bar_chart(
    column='Survived',
    title='Distribution of the Survived variable',
    ylabel='Count',
    xlabel='Has the passenger survived? (0 = No, 1 = Yes)'
);</pre><p>可视化显示在下图中:</p><div><img src="img/B16954_04_14.jpg" alt="Figure 4.14 – Target class distribution for the Titanic dataset&#13;&#10;" width="1149" height="754"/></div><p class="figure-caption">图4.14–Titanic数据集的目标类别分布</p><p>正如你所看到的，大多数乘客没有在泰坦尼克号事故中幸存。这些信息本身并不能告诉你太多，因为你不知道每个性别、乘客等级和其他属性有多少乘客幸存。</p><p>你可以使用<code>make_bar_chart()</code>功能进行这种类型的可视化。</p></li>
				<li>Let's continue our data visualization journey by visualizing the number of passengers in each passenger class. You can use the same <code>make_bar_chart()</code> function for this visualization. Just make sure to change the parameters accordingly.<p>下面的代码片段显示了每个乘客级别的乘客数量。班级人数越少越好——更贵的机票，更好的服务，谁知道呢，也许生存的机会更高:</p><pre>make_bar_chart(
    column='Pclass',
    title='Distirbution of the Passenger Class variable',
    ylabel='Count',
    xlabel='Passenger Class (smaller is better)',
    x_offset=0.15
);</pre><p><a id="_idIndexMarker262"/>可视化是下图所示的<a id="_idIndexMarker263"/>:</p><div><img src="img/B16954_04_15.jpg" alt="Figure 4.15 – Number of passengers per passenger class&#13;&#10;" width="1140" height="751"/></div><p class="figure-caption">图4.15-每个乘客等级的乘客数量</p><p>正如你所看到的，大多数<a id="_idIndexMarker264"/>乘客属于<a id="_idIndexMarker265"/>三等舱。这是意料之中的，因为船上的工人比富人多。</p></li>
				<li>For the next step in the data visualization phase, let's see how the <code>Sex</code> attribute is distributed. This will give us insight into whether there were more women or men on board and how large the difference was.<p>以下代码片段实现了可视化:</p><pre>make_bar_chart(
    column='Sex',
    title='Distirbution of the Sex variable',
    ylabel='Count',
    xlabel='Gender'
);</pre><p>可视化如下图所示:</p><div><img src="img/B16954_04_16.jpg" alt="Figure 4.16 – Number of passengers per gender&#13;&#10;" width="1252" height="893"/></div><p class="figure-caption">图4.16-每种性别的乘客数量</p><p>如你所见，船上肯定有更多的男人。这与之前的可视化中得出的结论有关，我们得出的结论是船上有许多工人。</p><p>大多数工人都是男性，所以这种可视化是有意义的。</p></li>
				<li>Let's take a little break from the bar charts and visualize a continuous variable for change. The goal is to make a histogram of the <code>Fare</code> attribute, which will show the distribution of the amounts paid for the ticket.<p>以下代码片段为提到的属性绘制了一个直方图:</p><pre>plt.figure(figsize=(12, 7))
plt.title('Fare cost distribution', size=20)
plt.xlabel('Cost', size=14)
plt.ylabel('Count', size=14)
plt.hist(df['Fare'], bins=15, color='#4f4f4f', ec='#040404');</pre><p><a id="_idIndexMarker268"/>直方图如下图<a id="_idIndexMarker269"/>所示:</p><div><img src="img/B16954_04_17.jpg" alt="Figure 4.17 – Distribution of the Fare variable&#13;&#10;" width="1251" height="756"/></div><p class="figure-caption">图4.17-票价变量的分布</p><p>看起来大多数乘客花了30美元或更少的钱买了一张票。一如既往的有极端的<a id="_idIndexMarker270"/>案例。似乎一名单身乘客为这次旅行支付了大约500美元。考虑到事情的结局，这不是一个明智的决定。</p></li>
				<li>Let's do something a bit different now. The <code>Name</code> attribute is more or less useless in this format. But if you take a closer look, you can see that every value in the mentioned attribute is formatted identically.<p>这意味着我们可以保留第一个逗号后的单词，并将其存储在一个新的变量中。我们称这个变量为<code>Title</code>,因为它代表乘客的头衔(例如，先生，小姐。，以此类推)。</p><p>下面的代码片段将Title值提取为一个新属性，并使用<code>make_bar_chart()</code>函数直观地表示泰坦尼克号乘客的不同头衔:</p><pre>df['Title'] = df['Name'].apply(lambda x: x.split(',')[1].strip().split(' ')[0])
make_bar_chart(
    column='Title',
    title='Distirbution of the Passenger Title variable',
    ylabel='Count',
    xlabel='Title',
    x_offset=-0.2
);</pre><p>结果如下图所示:</p><div><img src="img/B16954_04_18.jpg" alt="Figure 4.18 – Distribution of the passenger titles&#13;&#10;" width="1168" height="861"/></div><p class="figure-caption">图4.18–乘客头衔的分布</p><p>同样，这些是预期的<a id="_idIndexMarker272"/>结果。大多数<a id="_idIndexMarker273"/>乘客都有共同的头衔，比如先生和小姐。他们中只有少数人有独特的头衔。您可以保持该列不变，或者将其转换为二进制列——如果标题是常用的，则值为零，否则值为一。接下来您将看到如何做到这一点。</p></li>
				<li>That's about enough with regards to the exploratory data analysis. We've made quite a few visualizations, but you can always make more on your own.<p>现在是时候为机器学习准备数据集了。这些步骤描述如下:</p><p>a)删除无用的列—<code>Ticket</code>和<code>PassengerId</code>。第一个只是虚拟字母和数字的集合，对预测建模毫无用处。第二个是一个任意的ID，很可能是用数据库序列生成的。你可以通过调用<code>drop()</code>函数来移除这两者。</p><p>b)将<code>Sex</code>属性中的值重新映射为整数。文本值<em class="italic">男</em>和<em class="italic">女</em>不能直接传递给机器学习算法。某种形式的转换是必须的——所以用0代替男性，用1代替女性。<code>replace()</code>功能是这项工作的完美候选。</p><p>c)使用之前生成的<code>Title</code>列，并将其转换为二进制1–如果标题是<a id="_idIndexMarker275"/>常见的(例如，先生，小姐),则值为<a id="_idIndexMarker274"/>零。和夫人)和一个否则。然后，您可以将该列重命名为更合适的名称，比如<code>Title_Unusal</code>。不再需要<code>Name</code>列，因此将其删除。</p><p>d)通过将该属性转换为二进制1来处理<code>Cabin</code>列中的缺失值——如果舱室的值缺失，则该值为零，否则为1。将这个新列命名为<code>Cabin_Known</code>。之后，您可以删除<code>Cabin</code>列，因为不再需要它，并且它不能传递给机器学习模型。</p><p>e)用<code>Embarked</code>属性创建虚拟变量。此属性指示乘客上船的港口。你可以判断这个属性是否必要，但是我们将让TPOT来决定。声明虚拟变量后，将它们连接到原始数据集并删除<code>Embarked</code>列。</p><p>f)以某种方式处理<code>Age</code>属性中缺失的值。有许多复杂的方法，如<em class="italic"> KNN估算</em>或<em class="italic"> MissForest估算</em>，但为了简单起见，只需用简单的平均值估算缺失值。</p><p>以下代码片段向您展示了如何应用所有提到的转换:</p><pre>df.drop(['Ticket', 'PassengerId'], axis=1, inplace=True)
gender_mapper = {'male': 0, 'female': 1}
df['Sex'].replace(gender_mapper, inplace=True)
df['Title'] = [0 if x in ['Mr.', 'Miss.', 'Mrs.'] else 1 for x in df['Title']]
df = df.rename(columns={'Title': 'Title_Unusual'})
df.drop('Name', axis=1, inplace=True)
df['Cabin_Known'] = [0 if str(x) == 'nan' else 1 for x in df['Cabin']]
df.drop('Cabin', axis=1, inplace=True)
emb_dummies = pd.get_dummies(df['Embarked'], drop_first=True, prefix='Embarked')
df = pd.concat([df, emb_dummies], axis=1)
df.drop('Embarked', axis=1, inplace=True)
df['Age'] = df['Age'].fillna(int(df['Age'].mean()))
df.head()</pre><p>您可以通过检查下图来查看准备好的数据集:</p><div><img src="img/B16954_04_19.jpg" alt="Figure 4.19 – Prepared Titanic dataset&#13;&#10;" width="1267" height="268"/></div><p class="figure-caption">图4.19-准备好的泰坦尼克号数据集</p><p>关于数据准备，这就是你要做的全部工作。不需要缩放/标准化，因为TPOT将决定该步骤是否必要。</p><p>我们将很快开始预测建模——只剩一步了。</p></li>
				<li>Before you can train a classification <a id="_idIndexMarker276"/>model, you'll have to split the dataset into training and testing <a id="_idIndexMarker277"/>subsets. Keep in mind the <code>random_state</code> parameter – use the same value if you want the same data split:<pre>from sklearn.model_selection import train_test_split
X = df.drop('Survived', axis=1)
y = df['Survived']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)
y_train.shape, y_test.shape</pre><p>最后一行代码打印训练和测试子集中的实例数量。您可以在下图中看到这些数字:</p><div><img src="img/B16954_04_20.jpg" alt="Figure 4.20 – Number of instances in training and testing sets (Titanic)&#13;&#10;" width="264" height="33"/></div><p class="figure-caption">图4.20–训练和测试集中的实例数量(Titanic)</p><p>现在，您已经准备好训练预测模型了。</p></li>
				<li>Let's start with a baseline model – logistic regression. We'll train it on the train set and evaluate <a id="_idIndexMarker278"/>it on the test set. The <a id="_idIndexMarker279"/>following code snippet trains the model and prints the confusion matrix:<pre>from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
lm = LogisticRegression(random_state=42)
lm.fit(X_train, y_train)
lm_preds = lm.predict(X_test)
print(confusion_matrix(y_test, lm_preds))</pre><p>您可以在下图中看到混淆矩阵:</p><div><img src="img/B16954_04_21.jpg" alt="Figure 4.21 – Logistic regression confusion matrix (Titanic)&#13;&#10;" width="169" height="58"/></div><p class="figure-caption">图4.21-逻辑回归混淆矩阵(Titanic)</p><p>看起来假阳性和假阴性的数量是一样的(23)。如果我们把比率考虑进去，就会有更多的假阴性。在翻译中，基线模型更有可能说乘客幸存了，即使他们没有。</p></li>
				<li>Interpreting the confusion matrix is great, but what if you want to look at a concrete number instead? Since this is a classification problem, you could use accuracy. But there's a "better" metric – <strong class="bold">F1 score</strong>. The value for this metric ranges between 0 and 1 (higher is better) and represents a harmonic mean between precision and recall.<p>下面是用Python计算的方法:</p><pre>from sklearn.metrics import f1_score
print(f1_score(y_test, lm_preds))</pre><p>测试集的F1分值如下图所示:</p><div><img src="img/B16954_04_22.jpg" alt="Figure 4.22 – Logistic regression F1 score on the test set (Titanic)&#13;&#10;" width="288" height="28"/></div><p class="figure-caption">图4.22–测试集上的逻辑回归F1得分(Titanic)</p><p>对于基线<a id="_idIndexMarker281"/>模型来说，0.74的值并不坏。TPOT能超越它吗？让我们训练一个自动化模型，看看会发生什么。</p></li>
				<li>In a similar fashion as before, we'll train an automated classification model for 10 minutes. Instead of accuracy, we'll optimize for the F1 score. By doing so, we can compare the F1 scores of an automated model with the baseline one.<p>下面的代码片段在定型集上对模型进行定型:</p><pre>from tpot import TPOTClassifier
pipeline_optimizer = TPOTClassifier(
    scoring='f1',
    max_time_mins=10,
    random_state=42,
    verbosity=2
)
pipeline_optimizer.fit(X_train, y_train)</pre><p>在下图中，您可以看到培训期间打印在笔记本上的输出。TPOT设法在10分钟内<a id="_idIndexMarker282"/>训练了7代<a id="_idIndexMarker283"/>，分数随着模型的训练而增加:</p><div><img src="img/B16954_04_23.jpg" alt="Figure 4.23 – TPOT pipeline optimization output (Titanic)&#13;&#10;" width="1032" height="395"/></div><p class="figure-caption">图4.23-TPOT管道优化输出(泰坦尼克号)</p><p>你可以让模型训练超过10分钟。尽管如此，这个时间框架应该足以超越基线模型。</p></li>
				<li>Let's take a look at the value of the F1 score on the test set now. Remember, anything above 0.7415 means TPOT outperformed the baseline model.<p>以下代码片段打印F1分数:</p><pre>pipeline_optimizer.score(X_test, y_test)</pre><p>对应的F1分数如下图所示:</p><div><img src="img/B16954_04_24.jpg" alt="Figure 4.24 – TPOT optimized model F1 score on the test set (Titanic)&#13;&#10;" width="308" height="33"/></div><p class="figure-caption">图4.24–TPOT优化模型F1在测试集(Titanic)上的得分</p><p>看起来TPOT的表现超过了基线模型——正如预期的那样。</p></li>
				<li>In case you're <a id="_idIndexMarker284"/>more trustworthy of basic <a id="_idIndexMarker285"/>metrics, such as accuracy, here's how you can compare it between baseline and automated models:<pre>tpot_preds = pipeline_optimizer.predict(X_test)
from sklearn.metrics import accuracy_score
print(f'Baseline model accuracy: {accuracy_score(y_test, lm_preds)}')
print(f'TPOT model accuracy: {accuracy_score(y_test, tpot_preds)}')</pre><p>相应的准确度分数如下图所示:</p><div><img src="img/B16954_04_25.jpg" alt="Figure 3.25 – Accuracies of the baseline model and TPOT optimized model on the test set (Titanic)&#13;&#10;" width="690" height="62"/></div><p class="figure-caption">图3.25–基准模型和TPOT优化模型在测试集(Titanic)上的精确度</p><p>正如你所看到的，简单的准确性指标讲述了一个相似的故事——TPOT建立的模型仍然比基线模型好。</p></li>
				<li>We are near the end of this practical example. There are two optional things left to do. The first one is to take a look at the optimal pipeline. You can obtain it with the following line of code:<pre>pipeline_optimizer.fitted_pipeline_</pre><p>最佳管道如下图所示:</p><div><img src="img/B16954_04_26.jpg" alt="Figure 4.26 – TPOT optimized pipeline (Titanic)&#13;&#10;" width="1274" height="416"/></div><p class="figure-caption">图4.26-TPOT优化管道(泰坦尼克号)</p><p>正如你所见<a id="_idIndexMarker286"/>，TPOT使用极端梯度<a id="_idIndexMarker287"/>提升来解决这个分类问题。</p></li>
				<li>Finally, you can convert the optimal pipeline into Python code. Doing so makes the process of sharing the code that much easier. You can find the code for doing so here:<pre>pipeline_optimizer.export('titanic_pipeline.py')</pre><p>下图显示了自动化管道的完整源代码:</p></li>
			</ol>
			<div><div><img src="img/B16954_04_27.jpg" alt="Figure 4.27 – Source code for the optimized TPOT pipeline (Titanic)&#13;&#10;" width="1000" height="538"/>
				</div>
			</div>
			<p class="figure-caption">图4.27-优化的TPOT管道(泰坦尼克号)的源代码</p>
			<p>而<a id="_idTextAnchor062"/> <a id="_idIndexMarker288"/>以自动化的方式解决了泰坦尼克号数据集上的分类问题<a id="_idIndexMarker289"/>。现在，您已经构建了两个全自动分类机器学习解决方案。接下来让我们总结这一章。</p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor063"/>总结</h1>
			<p>这是本书的第二个实践章节。通过两个著名数据集上的深入示例，您已经了解了如何以自动化方式解决分类机器学习任务。毫无疑问，您现在已经准备好使用TPOT来解决任何类型的分类问题。</p>
			<p>到目前为止，您已经知道如何解决回归和分类任务。但是平行训练呢？神经网络呢？接下来的章节，<a href="B16954_05_Final_SK_ePub.xhtml#_idTextAnchor065"> <em class="italic">第五章</em> </a> <em class="italic">，与TPOT和达斯克</em>的平行训练，将教你什么是平行训练，以及如何与TPOT利用平行训练。稍后，在<a href="B16954_06_Final_SK_ePub.xhtml#_idTextAnchor073"> <em class="italic">第6章</em> </a>，<em class="italic">深度学习入门——神经网络速成班</em>中，您将强化您的基本深度学习和神经网络知识。作为锦上添花，你将在<a href="B16954_07_Final_SK_ePub.xhtml#_idTextAnchor086"> <em class="italic">第七章</em></a><em class="italic">中与TPOT一起学习如何使用深度学习，与TPOT一起学习</em>神经网络分类器。</p>
			<p>请使用本章介绍的工具和技术来练习自动解决分类问题。</p>
			<h1 id="_idParaDest-63">问一个</h1>
			<ol>
				<li value="1">你能用条形图研究分类变量的分布吗？解释一下。</li>
				<li>解释混淆矩阵和术语真阳性、真阴性、假阳性和假阴性。</li>
				<li>什么是精准？用一个实际的例子来解释。</li>
				<li>什么是回忆？用一个实际的例子来解释。</li>
				<li>准确率和F1成绩有什么区别？什么时候你会使用F1超过准确性？</li>
				<li>F1成绩中的“1”是什么意思？这个数字能改吗？在这种情况下会发生什么？</li>
				<li>在训练过程中，TPOT会输出训练集或测试集的评分标准值吗？解释一下。</li>
			</ol>
		</div>
	</div>
</body></html>