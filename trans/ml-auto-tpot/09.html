<html><head/><body>





<style type="text/css">body{margin:1em;background-color:transparent!important;}#sbo-rt-content *{text-indent:0pt!important;}#sbo-rt-content .bq{margin-right:1em!important;}#sbo-rt-content *{word-wrap:break-word!important;word-break:break-word!important;}#sbo-rt-content table,#sbo-rt-content pre{overflow-x:unset!important;overflow:unset!important;overflow-y:unset!important;white-space:pre-wrap!important;}</style>
<div><div><h1 id="_idParaDest-72"><em class="italic"> <a id="_idTextAnchor073"/>第六章</em>:深度学习入门:神经网络速成班</h1>
			<p>在本章中，你将学习深度学习和人工神经网络的基础知识。您将发现这些主题背后的基本思想和理论，以及如何用Python训练简单的神经网络模型。这一章将作为即将到来的几章的极好的入门，在这几章中结合了流水线优化和神经网络的思想。</p>
			<p>我们将涵盖深度学习背后的基本主题和思想，为什么它在过去几年中变得流行，以及神经网络比传统机器学习算法更好的案例。您还将获得编写自己的神经网络的实践经验，无论是从零开始还是通过预制的库。</p>
			<p>本章将涵盖以下主题:</p>
			<ul>
				<li>深度学习概述</li>
				<li>介绍人工神经网络</li>
				<li>利用神经网络对手写数字进行分类</li>
				<li>神经网络在回归和分类中的比较</li>
			</ul>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor074"/>技术要求</h1>
			<p>没有深度学习和神经网络的经验是必要的。仅从这一章你就应该能够理解基础知识。以前的经验是有帮助的，因为深度学习不是你一次就能学会的。</p>
			<p>你可以在这里下载本章的源代码和数据集:<a href="https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter06">https://github . com/packt publishing/Machine-Learning-Automation-with-TPOT/tree/main/chapter 06</a>。</p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor075"/>深度学习概述</h1>
			<p>深度学习是机器学习的一个<a id="_idIndexMarker340"/>子领域，专注于神经网络。神经网络作为一个概念并不新鲜——它们早在20世纪40年代就被引入，但直到它们开始赢得数据科学竞赛(大约在2010年)后才变得很受欢迎。</p>
			<p>深度学习和人工智能潜在的最大年份是2016年，这都是由于一个单一的事件。玩棋盘游戏围棋的<a id="_idIndexMarker341"/>计算机程序AlphaGo 击败了世界排名最高的棋手。在这次活动之前，围棋被认为是计算机无法掌握的游戏，因为潜在的棋盘配置非常多。</p>
			<p>如前所述，深度学习基于神经网络。你可以把神经网络想象成<strong class="bold">有向无环图</strong>——由顶点(节点)和边(连接)组成的图<a id="_idIndexMarker342"/>。输入图层(最左侧的第一个图层)接收来自数据集的原始数据，将其传递给一个或多个隐藏图层，然后构造一个输出。</p>
			<p>您可以在下图中看到神经网络的示例体系结构:</p>
			<div><div><img src="img/B16954_06_1.jpg" alt="Figure 6.1 – An example neural network architecture&#13;&#10;" width="999" height="489"/>
				</div>
			</div>
			<p class="figure-caption">图6.1–神经网络架构示例</p>
			<p>最左侧的黑色小节点表示输入数据，即直接来自数据集的数据。然后将这些值与隐藏层及其各自的权重和偏差联系起来。引用这些权重和偏差的一种常见方式是通过<a id="_idIndexMarker343"/>使用术语<strong class="bold">可调参数</strong>。我们将在下一节讨论这一项，并展示如何计算它们。</p>
			<p>神经网络的每一个节点都被称为神经元。让我们来看看单个神经元的架构:</p>
			<div><div><img src="img/B16954_06_2.jpg" alt="Figure 6.2 – Individual neuron&#13;&#10;" width="999" height="696"/>
				</div>
			</div>
			<p class="figure-caption">图6.2-单个神经元</p>
			<p>X对应于输入层或前一个隐藏层的值。这些值相乘(<em class="italic"> x1 * w1 </em>、<em class="italic"> x2 * w2 </em>)，然后相加(<em class="italic"> x1w1 + x2w2 </em>)。在求和之后，添加一个偏置项，最后，一切通过<strong class="bold">激活函数</strong>传递<a id="_idIndexMarker346"/>。这个函数决定了神经元是否会“触发”。用最简单的术语来说，它就像一个开关。</p>
			<p>权重和偏差的简要说明如下所示:</p>
			<ul>
				<li>Weights:<p>a)乘以前一层的<a id="_idIndexMarker347"/>值</p><p>b)可以改变幅度或将值从正值完全翻转为负值</p><p>c)就函数而言–调整权重会改变函数的斜率</p></li>
				<li>Biases:<p>a)解释为功能的<a id="_idIndexMarker348"/>偏移</p><p>b)偏差的增加导致函数的上移</p><p>c)偏差的减小导致函数的下移</p></li>
			</ul>
			<p>除了人工神经网络之外，还有许多类型的神经网络架构<a id="_idIndexMarker349"/>，这里将对它们进行讨论:</p>
			<ul>
				<li><strong class="bold">卷积神经网络</strong>(<strong class="bold">CNN</strong>)——一种最常用于分析图像的<a id="_idIndexMarker350"/>神经网络。它们<a id="_idIndexMarker351"/>基于卷积运算——两个矩阵之间的运算，其中<a id="_idIndexMarker352"/>第二个矩阵滑过(卷积)第一个矩阵，并计算元素级乘法。此操作的目标是找到一个滑动矩阵(内核),它可以从输入图像中提取正确的特征，从而使图像分类任务变得容易。</li>
				<li><strong class="bold">递归神经网络</strong>(<strong class="bold">RNNs</strong>)–一种最常用于序列数据的<a id="_idIndexMarker353"/>神经网络。今天，这些网络被应用在许多任务中，例如手写识别、语音识别、机器翻译和时间序列预测。RNN模型一次处理序列中的一个元素。处理之后，新的更新单元的状态被传递到下一个时间步。想象一下，根据前面的<em class="italic"> n个</em>字符预测单个字符；这是总的要点。</li>
				<li><strong class="bold">生成对抗网络</strong>(<strong class="bold">GANs</strong>)——一种最常用于从真实数据中学习后创建新<a id="_idIndexMarker356"/>样本的<a id="_idIndexMarker355"/>神经网络。GAN架构包括两个独立的模型——发生器和鉴别器。生成器模型的工作是制作假图像，并将其发送给鉴别器。鉴别器像法官一样工作，试图辨别一幅图像是假的还是假的。</li>
				<li><strong class="bold">自动编码器</strong>–无监督<a id="_idIndexMarker357"/>学习<a id="_idIndexMarker358"/>技术，旨在学习高维数据集的低维表示。在某种程度上，它们<a id="_idIndexMarker359"/>的工作方式类似于<strong class="bold">主成分分析</strong> ( <strong class="bold"> PCA </strong>)。</li>
			</ul>
			<p>这四个深度学习概念，本书不会涉及。我们将只关注人工神经网络，但如果你想自己更深入地研究，最好知道它们的存在。</p>
			<p>下一节着眼于人工神经网络，并向您展示如何用Python实现它们，包括从头开始和使用数据科学库。</p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor076"/>引入人工神经网络</h1>
			<p><a id="_idIndexMarker360"/>人工神经网络的基本构件是神经元。单个神经元本身是没有用的，但当组合成更复杂的网络时，它可以具有很强的预测能力。</p>
			<p>如果你不能推理为什么，想想你的大脑和它是如何工作的。就像人工神经网络一样，它也是由数百万个神经元组成的，只有在它们之间有通信时才会发挥作用。由于人工神经网络试图模仿人脑，因此需要以某种方式复制大脑中的神经元以及它们之间的连接(权重)。在本节中，这种联系将变得不那么抽象。</p>
			<p>今天，人工神经网络可以用来解决常规机器学习算法可以解决的任何问题。简而言之，如果你能用线性或逻辑回归解决一个问题，你就能用神经网络解决它。</p>
			<p>在我们探索整个网络的复杂性和内部运作之前，我们必须从简单的开始——从单个神经元的理论开始。</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor077"/>单个神经元理论</h2>
			<p>用Python建模单个神经元很容易。例如，假设一个<a id="_idIndexMarker362"/>神经元从其他五个神经元(输入，或X)接收值。在用代码实现它之前，让我们直观地研究一下这个行为。下图显示了单个神经元接收来自前几层中五个神经元的值时的样子(我们正在对右侧的神经元进行建模):</p>
			<div><div><img src="img/B16954_06_3.jpg" alt="Figure 6.3 – Modeling a single neuron&#13;&#10;" width="682" height="587"/>
				</div>
			</div>
			<p class="figure-caption">图6.3–模拟单个神经元</p>
			<p>X表示来自原始数据或之前隐藏图层的输入要素。每个输入要素都分配有一个权重，用W表示。将相应的输入值和权重相乘并求和，然后将偏差项(b)添加到结果之上。</p>
			<p>计算我们神经元输出值的公式如下:</p>
			<div><div><img src="img/Formula_06_001.jpg" alt="" width="1158" height="67"/>
				</div>
			</div>
			<p>让我们用具体的值来让这个概念更清晰一点。下图看起来与图6.3相同，但用实际数字代替了变量:</p>
			<div><div><img src="img/B16954_06_4.jpg" alt="Figure 6.4 – Neuron value calculation&#13;&#10;" width="539" height="464"/>
				</div>
			</div>
			<p class="figure-caption">图6.4–神经元值计算</p>
			<p>我们可以将值<a id="_idIndexMarker363"/>直接代入前面的<a id="_idIndexMarker364"/>公式来计算值:</p>
			<div><div><img src="img/Formula_06_002.jpg" alt="" width="962" height="55"/>
				</div>
			</div>
			<div><div><img src="img/Formula_06_003.jpg" alt="" width="1299" height="52"/>
				</div>
			</div>
			<div><div><img src="img/Formula_06_004.jpg" alt="" width="775" height="45"/>
				</div>
			</div>
			<div><div><img src="img/Formula_06_005.jpg" alt="" width="270" height="47"/>
				</div>
			</div>
			<p>在现实中，单个神经元从先前层中潜在的成千上万个神经元中获取值，因此手动计算值并可视化表达是不实际的。</p>
			<p>即使你决定这样做，那也只是一次向前传球。神经网络在反向传递过程中学习，手动计算要复杂得多。</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor078"/>编码单个神经元</h2>
			<p>接下来，让我们看看如何用Python来<a id="_idIndexMarker365"/>半自动地计算神经元值:</p>
			<ol>
				<li>To start, let's declare <a id="_idIndexMarker366"/>input values, their respective weights, and a value for the bias term. The first two are lists, and the bias is just a number:<pre>inputs = [5, 4, 2, 1, 6]
weights = [0.1, 0.3, 0.05, 0.4, 0.9]
bias = 4</pre><p>这就是计算输出值所需的全部内容。让我们看看你接下来有什么选择。</p></li>
				<li>There are three simple methods for calculating neuron output values. The first one is the most manual, and that is to explicitly multiply corresponding inputs and weights, adding them together with the bias. <p>下面是一个Python实现:</p><pre>output = (inputs[0] * weights[0] + 
          inputs[1] * weights[1] + 
          inputs[2] * weights[2] +
          inputs[3] * weights[3] +
          inputs[4] * weights[4] + 
          bias)
output</pre><p>执行完这段代码后，您应该会看到打印出一个值<code>11.6</code>。更准确的说，数值应该是<code>11.600000000000001</code>，不过不用担心这个计算误差。</p><p>下一个方法更具可伸缩性，它归结为同时遍历输入和权重，并递增前面为输出声明的变量。循环结束后，添加偏置项。下面是实现这种计算方法的方法:</p><pre>output = 0
for x, w in zip(inputs, weights):
    output += x * w
output += bias
output</pre><p>输出仍然相同，但是<a id="_idIndexMarker367"/>您可以立即看到这个选项的可伸缩性有多大。想象一下，如果之前的网络层有1000个神经元，那么使用第一个选项——一点也不方便。</p><p>第三种也是首选的方法<a id="_idIndexMarker368"/>是使用科学计算库，比如NumPy。有了它，你就可以计算矢量点积，加上偏项。方法如下:</p><pre>import numpy as np
output = np.dot(inputs, weights) + bias
output</pre><p>无论是编写还是执行，这个选项都是最快的，所以它是首选。</p></li>
			</ol>
			<p>你现在知道如何对单个神经元进行编码，但神经网络采用多层神经元。接下来，您将了解关于图层的更多信息。</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor079"/>单层理论</h2>
			<p>为了使事情更简单，可以把层看作向量或者简单的组。层不是复杂或抽象的数据结构。用代码术语来说，你可以把它们想象成列表。它们包含许多神经元。</p>
			<p>对单层神经元进行编码与对单个神经元进行编码非常相似。我们仍然有相同的输入，因为它们要么来自先前的隐藏层，要么来自输入层。什么变化是权重和偏差。在代码中，权重不再被视为列表，而是列表的列表(或矩阵)。类似地，bias现在是一个列表，而不是一个标量值。</p>
			<p>简而言之，你的权重矩阵<a id="_idIndexMarker371"/>的行数将与新层中的神经元数一样多，列数将与前一层中的神经元数一样多。让我们来看一个示例图，让这个概念不那么抽象:</p>
			<div><div><img src="img/B16954_06_5.jpg" alt="Figure 6.5 – Layer of neurons&#13;&#10;" width="503" height="494"/>
				</div>
			</div>
			<p class="figure-caption">图6.5–神经元层</p>
			<p>权重值<a id="_idIndexMarker372"/>故意没有放在前一张图中，因为它看起来很乱。要在代码中实现这一层，您需要具有以下结构:</p>
			<ul>
				<li>输入向量(1行，5列)</li>
				<li>权重矩阵(2行，5列)</li>
				<li>偏差向量(1行，2列)</li>
			</ul>
			<p>来自线性代数的矩阵乘法规则规定，两个矩阵需要具有(m，n)和(n，p)的形状，以便在乘法后产生(m，p)矩阵。记住这一点，您可以通过转置权重矩阵来轻松执行矩阵乘法。</p>
			<p>从数学上来说，您可以使用下面的公式<a id="_idIndexMarker373"/>来计算输出层的值:</p>
			<div><div><img src="img/Formula_06_006.jpg" alt="" width="473" height="66"/>
				</div>
			</div>
			<p>在这里，以下情况适用:</p>
			<ul>
				<li><img src="img/Formula_06_007.png" alt="" width="13" height="13"/>是输入的矢量。</li>
				<li><img src="img/Formula_06_008.png" alt="" width="17" height="14"/>是权重矩阵。</li>
				<li><img src="img/Formula_06_009.png" alt="" width="12" height="17"/>是偏差的向量。</li>
			</ul>
			<p>让我们声明所有这些的值，看看如何计算输出层的值:</p>
			<div><div><img src="img/Formula_06_010.jpg" alt="" width="1174" height="256"/>
				</div>
			</div>
			<p>前面提到的公式现在可用于计算输出图层的值:</p>
			<div><div><img src="img/Formula_06_011.jpg" alt="" width="453" height="62"/>
				</div>
			</div>
			<div><div><img src="img/Formula_06_012.jpg" alt="" width="972" height="239"/>
				</div>
			</div>
			<div><div><img src="img/Formula_06_013.jpg" alt="" width="660" height="249"/>
				</div>
			</div>
			<div><div><img src="img/Formula_06_014.jpg" alt="" width="1357" height="106"/>
				</div>
			</div>
			<div><div><img src="img/Formula_06_015.jpg" alt="" width="510" height="111"/>
				</div>
			</div>
			<div><div><img src="img/Formula_06_016.jpg" alt="" width="367" height="107"/>
				</div>
			</div>
			<p>这基本上就是你如何<a id="_idIndexMarker375"/>计算整个层的输出。对于实际的神经网络来说,<a id="_idIndexMarker376"/>计算的规模将会增长，因为每层都有数千个神经元，但数学背后的逻辑是相同的。</p>
			<p>您可以看到手动计算图层输出是多么繁琐。接下来，您将学习如何在Python中计算值。</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor080"/>编码单层</h2>
			<p>现在让我们检查三种<a id="_idIndexMarker377"/>方法，在这些方法中，您可以计算<a id="_idIndexMarker378"/>单个图层的输出值。与单个神经元一样，我们将从手动方法开始，以NumPy一行程序结束。</p>
			<p>您必须首先声明输入、权重和偏差的值，下面是如何做的:</p>
			<pre>inputs = [5, 4, 2, 1, 6]
weights = [
    [0.1, 0.3, 0.05, 0.4, 0.9],
    [0.3, 0.15, 0.4, 0.7, 0.2]
]
biases = [4, 2]</pre>
			<p>让我们继续计算输出图层的值:</p>
			<ol>
				<li value="1">Let's start with the manual approach. No, we won't do the same procedure as with neurons. You could, of course, but it would look too messy and impractical. Instead, we'll immediately use the <code>zip()</code> function to iterate over the <code>weights</code> matrix and <code>biases</code> array and calculate the value of a single output neuron.<p>对<a id="_idIndexMarker379"/>重复该过程，无论有多少个神经元，每个输出神经元都被附加到表示输出层的列表中。</p><p>以下是完整的代码片段:</p><pre>layer = []
for n_w, n_b in zip(weights, biases):
    output = 0
    for x, w in zip(inputs, n_w):
        output += x * w
    output += n_b
    layer.append(output)
    
layer</pre><p>结果是一个值为<code>[11.6, 6.8]</code>的列表，这与我们之前通过手动<a id="_idIndexMarker380"/>计算得到的结果相同。</p><p>虽然这种方法可行，但它仍然不是最佳的。接下来看看怎么改进。</p></li>
				<li>You'll now calculate the values of the output layer by taking the vector dot product between input values and every row of the <code>weights</code> matrix. The bias term will be added after this operation is completed.<p>让我们看看它是如何工作的:</p><pre>import numpy as np
layer = []
for n_w, n_b in zip(weights, biases):
    layer.append(np.dot(inputs, n_w) + n_b)
layer</pre><p>层值仍然是相同的—<code>[11.6, 6.8]</code>,并且这种方法比之前的<a id="_idIndexMarker381"/>方法更具可扩展性。它仍然可以改进。接下来看看如何。</p></li>
				<li>You can <a id="_idIndexMarker382"/>perform a matrix multiplication between inputs and transposed weights and add the corresponding biases with a single line of Python code. Here's how:<pre>layer = np.dot(inputs, np.transpose(weights)) + biases
layer</pre><p>如果出于某种原因，您想要手动计算输出，这是推荐的方法。NumPy处理完全，所以同时也是最快的一个。</p></li>
			</ol>
			<p>您现在知道如何计算单个神经元和神经网络单层的输出值。到目前为止，我们还没有涵盖神经网络中决定神经元是否会“激发”或“激活”的关键思想。这些被称为激活函数，我们将在接下来讨论它们。</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor081"/>激活功能</h2>
			<p>激活函数<a id="_idIndexMarker383"/>对于神经网络的<a id="_idIndexMarker384"/>输出是必不可少的，因此对于深度学习模型的输出也是如此。它们只不过是数学方程式，准确地说是相对简单的方程式。激活函数是那些决定神经元是否应该被“激活”的函数。</p>
			<p>另一种思考激活函数的方式是将它视为一种门，位于进入当前神经元的输入和进入下一层的输出之间。激活函数可以像阶跃函数一样简单(打开或关闭神经元)，也可以稍微复杂一些，并且是非线性的。事实证明，非线性函数在学习复杂数据和提供准确预测方面非常有用。</p>
			<p>接下来我们将讨论几个最常见的激活函数。</p>
			<h3>阶跃函数</h3>
			<p>阶跃函数是基于阈值<a id="_idIndexMarker386"/>的<a id="_idIndexMarker385"/>。如果输入的值高于阈值，神经元就会被激活。这就是为什么我们可以说阶跃函数是一个开关——中间没有值。</p>
			<p>您可以轻松地使用Python和NumPy来声明和可视化一个基本的step函数。步骤如下所示:</p>
			<ol>
				<li value="1">To start, you'll have to define a step function. The typical threshold value is 0, so the neuron will activate if and only if the value passed in to the function is greater than 0 (the input value being the sum of the previous inputs multiplied by the weights and added bias).<p>这种逻辑在Python中实现起来很简单:</p><pre>def step_function(x):
    return 1 if x &gt; 0 else 0</pre></li>
				<li>您现在可以声明一个值列表，作为这个函数的输入，然后将<code>step_function()</code>应用到这个列表。这里有一个例子:<pre>xs = np.arange(-10, 10, step=0.1) ys = [step_function(x) for x in xs]</pre></li>
				<li>Finally, you can visualize the function with the help of the Matplotlib library in just two lines of code:<pre>plt.plot(xs, ys, color='#000000', lw=3)
plt.title('Step activation function', fontsize=20)</pre><p>您可以在下图中直观地看到该函数的工作方式:</p></li>
			</ol>
			<div><div><img src="img/B16954_06_6.jpg" alt="Figure 6.6 – Step activation function&#13;&#10;" width="1389" height="724"/>
				</div>
			</div>
			<p class="figure-caption">图6.6–步进激活功能</p>
			<p><a id="_idIndexMarker387"/>阶跃函数的最大问题是<a id="_idIndexMarker388"/>不允许多输出——只有两个。接下来我们将深入一组非线性函数，您将看到它们的不同之处。</p>
			<h3>Sigmoid函数</h3>
			<p>sigmoid <a id="_idIndexMarker389"/>激活功能通常被称为<a id="_idIndexMarker390"/>逻辑功能。在神经网络和深度学习领域，这是一个非常受欢迎的函数。它本质上是将输入转换为0到1之间的值。</p>
			<p>稍后您将看到该函数的工作原理，并且您会立即注意到与阶跃函数相比的一个优势–梯度是平滑的，因此输出值中没有跳跃。例如，如果值稍微改变(例如，从-0.000001到0.000001)，您不会从0跳到1。</p>
			<p>sigmoid函数确实遇到了深度学习中的一个常见问题—<strong class="bold">消失梯度</strong>。这是一个<a id="_idIndexMarker391"/>问题，经常发生在反向传播过程中(神经网络中的学习过程，远远超出了本章的范围)。简而言之，梯度在反向传递过程中“消失”，使网络无法学习(调整权重和偏差)，因为建议的调整过于接近零。</p>
			<p>你可以使用Python和NumPy来<a id="_idIndexMarker392"/>轻松声明和可视化sigmoid函数。步骤如下所示:</p>
			<ol>
				<li value="1">To start, you'll have to define the sigmoid function. Its formula is pretty well established: <em class="italic">(1 / (1 + exp(-x)))</em>, where <em class="italic">x</em> is the input value.<p>下面是如何用Python实现这个公式:</p><pre>def sigmoid_function(x):
     return 1 / (1 + np.exp(-x))</pre></li>
				<li>现在您可以<a id="_idIndexMarker393"/>声明一个值列表，作为这个函数的输入，然后将<code>sigmoid_function()</code>应用到这个列表。这里有一个例子:<pre>xs = np.arange(-10, 10, step=0.1) ys = [step_function(x) for x in xs]</pre></li>
				<li>Finally, you can visualize the function with the help of the Matplotlib library in just two lines of code:<pre>plt.plot(xs, ys, color='#000000', lw=3)
plt.title(Sigmoid activation function', fontsize=20)</pre><p>您可以在下图中直观地看到该函数的工作方式:</p></li>
			</ol>
			<div><div><img src="img/B16954_06_7.jpg" alt="Figure 6.7 – Sigmoid activation function&#13;&#10;" width="1389" height="724"/>
				</div>
			</div>
			<p class="figure-caption">图6.7–s形激活功能</p>
			<p>一个很大的缺点是，<a id="_idIndexMarker395"/> sigmoid函数返回的值不是以零为中心的。这是一个问题，因为对高度负面或高度正面的输入建模变得更加困难。双曲正切函数解决了这个问题。</p>
			<h3>双曲正切函数</h3>
			<p>双曲正切函数(或双曲正切函数)与sigmoid函数密切相关。这也是一种类型的激活函数，<a id="_idIndexMarker397"/>受到消失梯度问题的困扰，但其输出以零为中心——因为该函数的范围从-1到+1。</p>
			<p>这使得对高度负面或高度正面的输入进行建模变得容易得多。可以使用Python和NumPy轻松声明和可视化双曲正切函数。步骤如下所示:</p>
			<ol>
				<li value="1">To start, you'll have to define the hyperbolic tangent function. You can use the <code>tanh()</code> function from NumPy for the implementation.<p>下面是如何用Python实现它:</p><pre>def tanh_function(x):
    return np.tanh(x)</pre></li>
				<li>现在您可以<a id="_idIndexMarker398"/>声明一个值列表，作为这个函数的输入，然后将<code>tanh_function()</code>应用到这个列表。这里有一个例子:<pre>xs = np.arange(-10, 10, step=0.1) ys = [step_function(x) for x in xs]</pre></li>
				<li>Finally, you can visualize the <a id="_idIndexMarker399"/>function with the help of the Matplotlib library in just two lines of code:<pre>plt.plot(xs, ys, color='#000000', lw=3)
plt.title(Tanh activation function', fontsize=20)</pre><p>您可以在下图中直观地看到该函数的工作方式:</p></li>
			</ol>
			<div><div><img src="img/B16954_06_8.jpg" alt="Figure 6.8 – Hyperbolic tangent activation function&#13;&#10;" width="1388" height="712"/>
				</div>
			</div>
			<p class="figure-caption">图6.8–双曲正切激活函数</p>
			<p>为了有效地训练和<a id="_idIndexMarker400"/>优化神经网络，您需要一个激活函数，该函数充当线性函数，但本质上是<a id="_idIndexMarker401"/>非线性的，允许网络学习数据中的复杂关系。这就是本节最后一个激活函数的作用。</p>
			<h3>校正线性单位函数</h3>
			<p><a id="_idIndexMarker402"/>整流线性单元(或ReLU)功能是一种激活功能，你可以在大多数现代深度学习<a id="_idIndexMarker403"/>架构中看到。简单地说，它返回0和x之间的两个值中较大的一个，其中x是输入值。</p>
			<p>ReLU是计算效率最高的函数之一，它允许相对快速地找到收敛点。接下来您将看到如何用Python实现它:</p>
			<ol>
				<li value="1">To start, you'll have to define the ReLU function. This can be done entirely from scratch or with NumPy, as you only have to find the larger values of the two (0 and x).<p>下面是如何用Python实现ReLU:</p><pre>def relu_function(x):
    return np.maximum(0, x)</pre></li>
				<li>现在可以声明一个值列表，作为这个函数的输入，然后对这个列表应用<code>relu_function()</code>。这里有一个例子:<pre>xs = np.arange(-10, 10, step=0.1) ys = [step_function(x) for x in xs]</pre></li>
				<li>Finally, you can visualize the function with the help of the Matplotlib library in just two lines of code:<pre>plt.plot(xs, ys, color='#000000', lw=3)
plt.title(ReLU activation function', fontsize=20)</pre><p>您可以在下图中直观地看到该函数的工作方式:</p></li>
			</ol>
			<div><div><img src="img/B16954_06_9.jpg" alt="Figure 6.9 – ReLU activation function&#13;&#10;" width="1388" height="728"/>
				</div>
			</div>
			<p class="figure-caption">图6.9–ReLU激活功能</p>
			<p>简而言之就是这样。您可以使用默认版本或任何变体(例如，leaky ReLU或parametric ReLU)，这取决于用例。</p>
			<p>现在你已经了解了足够多的<a id="_idIndexMarker404"/>理论，可以用Python编写一个基本的神经网络了。我们还没有涵盖所有的理论主题，所以诸如损失、梯度下降、反向传播等术语可能仍然感觉抽象。我们将在接下来的动手示例中尝试揭开它们的神秘面纱。</p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor082"/>利用神经网络对手写数字进行分类</h1>
			<p>深度学习的“hello world”正在训练一个可以对手写数字进行分类的模型。这正是你在这一部分要做的事情。用TensorFlow库实现它只需要几行代码。</p>
			<p>在继续之前，您必须安装TensorFlow。这个过程有一点不同，取决于你是在Windows、macOS还是Linux上，以及你是否有一个CUDA兼容的GPU。可以<a id="_idIndexMarker407"/>参考官方安装说明:<a href="https://www.tensorflow.org/install">https://www.tensorflow.org/install</a>。本节的其余部分假设您已经安装了TensorFlow 2.x。以下是要遵循的步骤:</p>
			<ol>
				<li value="1">To start, you'll have to import the <a id="_idIndexMarker408"/>TensorFlow library along with some additional modules. The <code>datasets</code> module enables you to download data straight from the notebook. The <code>layers</code> and <code>models</code> modules will be used later to design the architecture of the neural network.<p>下面是导入的代码片段:</p><pre>import tensorflow as tf
from tensorflow.keras import datasets, layers, models</pre></li>
				<li>You can now proceed with data gathering and preparation. A call to <code>datasets.mnist.load_data()</code> will download train and test images alongside the train and test labels. The images are grayscale and 28x28 pixels in size. This means you'll have a bunch of 28x28 matrices with values ranging from 0 (black) to 255 (white).<p>然后，您可以通过重新调整图像来进一步准备数据集，即将这些值除以255，使所有值都在0到1的范围内:</p><pre>(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()
train_images, test_images = train_images / 255.0, test_images / 255.0</pre><p>以下是您应该在笔记本上看到的内容:</p><div><img src="img/B16954_06_10.jpg" alt="Figure 6.10 – Downloading the MNIST dataset&#13;&#10;" width="1462" height="88"/></div><p class="figure-caption">图6.10-下载MNIST数据集</p></li>
				<li>Furthermore, you can inspect the matrix values for one of the images to see if you can spot the pattern inside. <p>下面一行代码使得<a id="_idIndexMarker409"/>检查矩阵变得很容易——它打印出矩阵并将所有浮点数舍入到一个小数点:</p><pre>print('\n'.join([''.join(['{:4}'.format(round(item, 1)) for item in row]) for row in train_images[0]]))</pre><p>结果如以下截图所示:</p><div><img src="img/B16954_06_11.jpg" alt="Figure 6.11 – Inspecting a single image matrix&#13;&#10;" width="1644" height="902"/></div><p class="figure-caption">图6.11–检查单个图像矩阵</p><p>你注意到在图像中找出一个5是多么容易吗？你可以执行<code>train_labels[0]</code>来验证。</p></li>
				<li>You can continue with laying out the neural network architecture next. As mentioned earlier, the input images are 28x28 pixels in size. Artificial neural networks can't process a matrix directly, so you'll have to convert this matrix to a vector. This process is <a id="_idIndexMarker410"/>known as <code>layers.Dense()</code> to construct a layer.<p>这个隐藏层也会<a id="_idIndexMarker411"/>需要一个激活函数，所以可以用ReLU。</p><p>最后，您可以添加最终(输出)层，该层需要有与不同类别一样多的神经元——在本例中是10个。</p><p>以下是网络架构的完整代码片段:</p><pre>model = models.Sequential([
  layers.Flatten(input_shape=(28, 28)),
  layers.Dense(128, activation='relu'),
  layers.Dense(10)
])</pre><p><code>models.Sequential</code>功能允许你一层接一层的堆叠，并且，嗯，用单独的层组成一个网络。</p><p>您可以通过调用模型上的<code>summary()</code>方法来查看模型的架构:</p><pre>model.summary()</pre><p>结果如以下截图所示:</p><div><img src="img/B16954_06_12.jpg" alt="Figure 6.12 – Neural network architecture&#13;&#10;" width="1044" height="490"/></div><p class="figure-caption">图6.12–神经网络架构</p></li>
				<li>There's still one thing you need to do before model training, and that is to compile the model. During the <a id="_idIndexMarker412"/>compilation, you'll have to specify values for the optimizer, loss, and the optimization metrics. <p>本章没有涉及到这些内容，但下面将对每个内容进行简要说明:</p><ul><li><em class="italic">优化器</em>–用于改变神经网络属性以减少损失的算法。这些属性包括权重、学习率等等。</li><li><em class="italic">损失</em>–一种用于计算梯度的方法，然后用于更新神经网络中的权重。</li><li><em class="italic">Metrics</em> – the metric(s) you're optimizing for (for example, accuracy).<p>深入这些话题已经超出了本书的范围。有大量的资源可以发现深度学习背后的理论。这一章的目的只是涵盖基本的基础知识。</p><p>您可以通过执行以下代码来编译您的神经网络:</p><pre>model.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)</pre></li></ul></li>
				<li>Now you're ready to train the model. The training subset will be used to train the network, and the testing <a id="_idIndexMarker413"/>subset will be used for the evaluation. The network will be trained for 10 epochs (10 complete passes through the entire training data).<p>您可以使用下列程式码片段来训练模型:</p><pre>history = model.fit(
    train_images, 
    train_labels, 
    epochs=10, 
    validation_data=(test_images, test_labels)
)</pre><p>执行前面的代码将开始训练过程。这需要多长时间取决于你的硬件以及你使用的是GPU还是CPU。您应该会看到类似于以下截图的内容:</p><div><img src="img/B16954_06_13.jpg" alt="Figure 6.13 – MNIST model training&#13;&#10;" width="1387" height="460"/></div><p class="figure-caption">图6.13-MNIST模型培训</p><p>经过10个时期后，验证集的准确率为97.7%——如果我们考虑到常规神经网络对图像的处理不太好，这已经是非常好的了。</p></li>
				<li>To test your model on a <a id="_idIndexMarker414"/>new instance, you can use the <code>predict()</code> method. It returns an array that tells you how likely it is that the prediction for a given class is correct. There will be 10 items in this array, as there were 10 classes. <p>然后，您可以调用<code>np.argmax()</code>来获取具有最高值的项目:</p><pre>import numpy as np
prediction = model.predict(test_images[0].reshape(-1, 784))
print(f'True digit = {test_labels[0]}')
print(f'Predicted digit = {np.argmax(prediction)}')</pre><p>结果如以下截图所示:</p></li>
			</ol>
			<div><div><img src="img/B16954_06_14.jpg" alt="Figure 6.14 – Testing the MNIST model&#13;&#10;" width="317" height="74"/>
				</div>
			</div>
			<p class="figure-caption">图6.14–测试MNIST模型</p>
			<p>如你所见，预测是正确的。</p>
			<p>而用TensorFlow之类的库训练神经网络就是这么简单。请记住，这种处理图像分类的方式在现实世界中并不推荐，因为我们已经展平了一个28x28的图像，并且立即丢失了所有的二维信息。CNN将是一种更好的图像分类方法，因为它们可以从二维数据中提取有用的特征。我们的人工神经网络在这里工作得很好，因为MNIST是一个简单而干净的数据集——不是你在工作中会得到的东西。</p>
			<p>在下一节中，您将了解使用神经网络处理分类和回归任务的不同之处。</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor083"/>回归与分类中的神经网络</h1>
			<p>如果你用scikit-learn做过任何机器学习，你就会知道有专门的类和模型用于回归和分类数据集。例如，如果您想将决策树<a id="_idIndexMarker416"/>算法应用于分类数据集，您可以使用<code>DecisionTreeClassifier</code>类。同样，您可以将<code>DecisionTreeRegressor</code>类用于回归任务。</p>
			<p>但是你用<a id="_idIndexMarker417"/>神经网络做什么呢？分类和回归任务没有专用的类或层。</p>
			<p>相反，你可以通过调整输出层中神经元的数量来适应。简单地说，如果你正在处理回归任务，在输出层必须有一个单一的神经元。如果您正在处理分类任务，那么输出层中的神经元数量将与目标变量中的不同类数量一样多。</p>
			<p>例如，您看到了上一节中的<a id="_idIndexMarker419"/>神经网络在输出层有10个神经元。原因是有10个不同的数字，从0到9。如果你是在预测某样东西的价格(回归)，那么在输出层将只有一个神经元。</p>
			<p>神经网络的任务是学习适当的参数值(权重和偏差)以产生最佳输出值，而不管您正在解决的问题的类型。</p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor084"/>总结</h1>
			<p>如果这是你第一次接触深度学习和神经网络，这一章可能很难理解。反复阅读材料可能会有所帮助，但这不足以完全理解主题。整本书都是关于深度学习的，甚至是深度学习的一小部分。因此，在一章中涵盖所有内容是不可能的。</p>
			<p>尽管如此，你应该有神经元、层和激活函数概念背后的基本理论，并且你总是可以自己学习更多。下一章，<a href="B16954_07_Final_SK_ePub.xhtml#_idTextAnchor086"> <em class="italic">第7章</em> </a> <em class="italic">，带TPOT </em>的神经网络分类器，将向您展示如何连接神经网络和管道优化，这样您就可以以完全自动化的方式构建最先进的模型。</p>
			<p>一如既往，请随意自行探索深度学习和神经网络的理论和实践。这绝对是一个值得进一步探索的研究领域。</p>
			<h1 id="_idParaDest-84">问&amp;答</h1>
			<ol>
				<li value="1">你如何定义“深度学习”这个术语？</li>
				<li>传统的机器学习算法和深度学习中使用的算法有什么区别？</li>
				<li>列出并简要描述五种类型的神经网络。</li>
				<li>给定每层神经元的数量，你能想出如何计算网络中可训练参数的数量吗？例如，具有架构[10，8，8，2]的神经网络总共有178个可训练参数(160个权重和18个偏差)。</li>
				<li>说出四种不同的激活功能，并简要解释。</li>
				<li>用自己的话描述神经网络中的<em class="italic">损失</em>。</li>
				<li>解释为什么用常规的人工神经网络建模想象分类模型不是一个好主意。</li>
			</ol>
		</div>
	</div>
</body></html>