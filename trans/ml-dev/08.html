<html><head/><body>


    
        <title>Recent Models and Developments</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">最近的模型和发展</h1>
                
            
            
                
<p class="mce-root">在前面的章节中，我们已经探索了大量用于机器学习模型的训练机制，从简单的传递机制开始，例如众所周知的前馈神经网络。然后，我们看了一个更复杂和受现实约束的机制，接受一个确定的输入序列作为训练输入，使用<strong>递归神经网络</strong> ( <strong> RNNs </strong>)。</p>
<p>现在是时候来看看最近的两个融入了现实世界其他方面的玩家了。在第一种情况下，我们将不仅有一个单一的网络优化其模型，而且还有另一个参与者，他们都将改善彼此的结果。<strong>生成对抗网络</strong> ( <strong> GANs </strong>)就是这种情况。</p>
<p>在第二种情况下，我们将讨论一种不同的模型，它将试图确定一组最佳步骤来最大化一个奖励:<strong>强化学习</strong>。</p>


            

            
        
    






    
        <title>GANs</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">甘斯</h1>
                
            
            
                
<p>GANs是一种新的无监督学习模型，是过去十年中为数不多的颠覆性模型之一。在整个迭代过程中，他们有两个相互竞争和改进的模型。</p>
<p>这种架构最初是基于监督学习和博弈论，其主要目标是基本上学会从同一类元素的原始数据集生成逼真的样本。</p>
<p>值得注意的是，对GANs的研究数量正以近乎指数的速度增长，如下图所示:</p>
<div><img height="371" width="540" src="img/35e6af3d-756a-470f-a9d2-0f7fb907fa3f.jpg"/></div>
<p>来源:甘动物园(https://github.com/hindupuravinash/the-gan-zoo)</p>


            

            
        
    






    
        <title>Types of GAN applications</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">氮化镓应用的类型</h1>
                
            
            
                
<p>GANs允许新的应用程序从以前的样本集中产生新的样本，包括补充缺失的信息。</p>
<p>在下面的截图中，我们描绘了在来自LSUN的五个场景数据集上使用<strong> LSGAN架构</strong>创建的多个样本，包括厨房、教堂、餐厅和会议室:</p>
<div><img height="528" width="668" class="alignnone size-full wp-image-867 image-border" src="img/6289b53f-de4e-4cd8-9116-52c6e6dd56e0.png"/></div>
<p>LSGAN创建了模型</p>
<p>另一个真正有趣的例子是使用<strong>即插即用生成网络(PPGN) </strong>的类条件图像采样，以填充实际227 x 227图像中100 x 100的缺失像素。</p>
<p>下面的屏幕截图比较了PPGN变体和等效的Photoshop图像补全:</p>
<div><img height="264" width="334" src="img/8b3e6c6c-b1df-4a67-9afa-a8869db14d78.jpg"/></div>
<p>PPGN填充示例</p>
<p class="mce-root CDPAlignLeft CDPAlign">PPGN还可以合成生成高分辨率(227 x 227)的火山类图像。不仅许多图像近乎照片般真实，而且本课程中的样本也是多种多样的:</p>
<div><img height="208" width="379" src="img/7e1b41d5-17e8-4dac-9aa0-fc13daa0b726.jpg"/></div>
<p>PPGN生成的火山样本</p>
<p>下面的屏幕截图展示了视觉概念的过程向量算法。它允许使用表示图像中对象的操作数，并且可以添加或删除它们，在特征空间中移动:</p>
<div><img height="290" width="536" src="img/360b4d4c-7ea5-4df5-b87d-f2d8f081c991.png"/></div>
<p>特征空间的向量算术运算</p>


            

            
        
    






    
        <title>Discriminative and generative models</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">判别和生成模型</h1>
                
            
            
                
<p>为了理解敌对网络的概念，我们将首先定义在典型GAN设置中交互的两个模型:</p>
<ul>
<li class="qtext_para"><strong>生成器</strong>:它的任务是从一个标准的随机分布中获取一个样本(例如，一个来自n维高斯分布的样本)，并产生一个看起来像是来自与<em> XX </em>相同的分布的点。可以说，生成器想要欺骗鉴别器输出1。在数学上，它学习一个函数，该函数将输入数据(<em> x </em>)映射到某个期望的输出类标签(<em> y </em>)。在概率方面，它学习输入数据的条件分布<em> P(y|x) </em>。它可以区分两种(或更多种)不同类别的数据，例如，一个卷积神经网络被训练为在给定人脸图像的情况下输出1，否则输出0。</li>
</ul>
<ul>
<li class="qtext_para"><strong>鉴别器</strong>:其任务是区分来自真实数据的样本和由生成器生成的人工数据。每个模型将试图击败另一个模型(生成器的目标是欺骗鉴别器，而鉴别器的目标是不被生成器欺骗):</li>
</ul>
<div><img height="320" width="367" src="img/d5b720c1-e3b6-4fea-9c47-d20cd6587eee.png"/></div>
<p>GAN的训练过程</p>
<p class="qtext_para">更正式地说，生成器试图同时学习输入数据和标签的联合概率，即<em> P(x，y) </em>。因此，它也可以用于其他用途，比如创建可能的新<em> (x，y) </em>样本。它对数据的类别一无所知。相反，它的目的是生成符合训练数据分布的新数据。</p>
<p class="qtext_para">在一般情况下，发生器和鉴别器都是神经网络，它们以交替的方式被训练。他们的每个目标都可以表示为一个损失函数，我们可以通过梯度下降来优化它。</p>
<p class="qtext_para">最终的结果是，两种模式一前一后地自我完善；生成器生成更好的图像，鉴别器在确定生成的样本是否是假的方面变得更好。在实践中，最终的结果是一个产生真正好的和真实的新样本的模型(例如，自然环境的随机图片)。</p>
<p><strong>概括来说，GANs的主要优点是:</strong></p>
<ul>
<li>甘是生成模型，使用监督学习来近似一个棘手的成本函数</li>
<li>GANs可以模拟许多成本函数，包括用于最大似然的函数</li>
<li>寻找高维、连续、非凸博弈中的纳什均衡是一个重要的开放性研究问题</li>
<li>gan是PPGNs的关键组成部分，它能够从不同的图像类别中生成引人注目的高分辨率样本</li>
</ul>


            

            
        
    






    
        <title>Reinforcement learning</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">强化学习</h1>
                
            
            
                
<p>强化学习是最近重新出现的一个领域，它在控制领域变得更加流行，寻找游戏和情境问题的解决方案，其中必须实施许多步骤来解决问题。</p>
<p>强化学习的正式定义如下:</p>
<p>“强化学习是一个智能体面临的问题，它必须通过与动态环境的反复试验来学习行为。”(Kaelbling等人，1996年)。</p>
<p>为了对我们想要解决的问题类型有一个参考框架，我们将从回到20世纪50年代发展的一个数学概念开始，称为<strong>马尔可夫决策过程</strong>。</p>


            

            
        
    






    
        <title>Markov decision process</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">马尔可夫决策过程</h1>
                
            
            
                
<p>在解释强化学习技术之前，我们将解释我们将用它们来解决的问题的类型。</p>
<p>当谈到强化学习时，我们希望优化马尔可夫决策过程的问题。它由一个数学模型组成，在结果部分是随机的，部分在代理人控制下的情况下帮助决策。</p>
<p>该模型的主要元素是一个<strong>代理</strong>、一个<strong>环境</strong>和一个<strong>状态</strong>，如下图所示:</p>
<div><img height="167" width="278" src="img/2335ec23-3edc-413c-98a7-c9c96d4bedda.png"/></div>
<p>强化学习过程的简化方案</p>
<p>代理可以执行某些操作(例如向左或向右移动挡板)。这些行为有时会导致奖励<em>r<sub>t</sub>T11】，奖励可以是积极的，也可以是消极的(比如分数的增减)。动作改变了环境，可以导致新的状态<em>s<sub>t+1</sub>T15】，此时代理可以执行另一个动作<em>a<sub>t+1</sub>T19】。状态、动作和奖励的集合，以及从一个状态转换到另一个状态的规则，构成了马尔可夫决策过程。</em></em></em></p>


            

            
        
    






    
        <title>Decision elements</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">决策要素</h1>
                
            
            
                
<p>为了理解这个问题，让我们将自己置于解决问题的环境中，看看主要因素:</p>
<ul>
<li>状态的集合</li>
<li>要采取的行动是从一个地方到另一个地方</li>
<li>奖励函数是由边表示的值</li>
<li>政策是完成任务的途径</li>
<li>折扣系数，决定未来奖励的重要性</li>
</ul>
<p>与传统形式的监督和非监督学习的主要区别是计算奖励所花费的时间，而在强化学习中这不是即时的；它发生在一系列步骤之后。</p>
<p>因此，下一个状态依赖于当前状态和决策者的行动，而该状态不依赖于所有先前的状态(它没有记忆)，因此它符合马尔可夫性质。</p>
<p>由于这是一个马尔可夫决策过程，状态<em>s<sub>t+1</sub>T23】的概率只取决于当前状态<em>s<sub>t</sub>T27】和动作<em>a<sub>t</sub>T31】:</em></em></em></p>
<div><img height="146" width="524" src="img/3a39fd08-d2bc-4440-a22c-85a754637b0d.png"/></div>
<p>展开强化机制</p>
<p>整个过程的目标是生成一个使回报最大化的策略<em> P，</em>。训练样本为元组，<em> &lt; s，a，r &gt; </em>。</p>


            

            
        
    






    
        <title>Optimizing the Markov process</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">优化马尔可夫过程</h1>
                
            
            
                
<p>强化学习是一个代理和环境之间的迭代交互。每个时间步都会发生以下情况:</p>
<ul>
<li>流程处于一种状态，决策者可以选择该状态下可用的任何操作</li>
<li>这个过程在下一个时间点随机进入一个新的状态，并给予决策者相应的奖励</li>
<li>流程进入新状态的概率受状态转移函数形式的所选动作的影响</li>
</ul>


            

            
        
    






    
        <title>Basic RL techniques: Q-learning</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">基本RL技术:Q-learning</h1>
                
            
            
                
<p>最著名的强化学习技术之一，也是我们将在示例中实现的技术，是<strong> Q-learning </strong>。</p>
<p>Q-learning可用于在有限马尔可夫决策过程中找到任何给定状态的最优动作。当我们在状态<em> s </em>中执行动作<em> a </em>时，Q-learning试图最大化Q-function的值，该值代表最大的贴现未来回报。</p>
<p>一旦我们知道了Q函数，状态<em> s </em>中的最优动作<em> a </em>就是具有最高Q值的那个。然后，我们可以定义一个策略π<em>(s)</em>，它可以在任何状态下为我们提供最佳操作，如下所示:</p>
<div><img height="58" width="75" src="img/af84694d-ce12-4ccd-9df8-c4ea163fe2c9.png"/></div>
<p>我们可以根据下一点的Q函数(<em> s <sub> t+1 </sub> </em>，<em> a <sub> t+1 </sub> </em>，<em> r <sub> t)来定义一个过渡点的Q函数(<em> s <sub> t </sub>，a <sub> t </sub>，r <sub> t </sub>，s <sub> t+1 </sub> </em>这个方程被称为Q学习的<strong>贝尔曼方程:</strong></sub></em></p>
<div><img height="67" width="358" src="img/0153a9f1-4dc4-4b03-8b8a-108c8090f0c3.png"/></div>
<p>在实践中，我们可以把Q函数想象成一个查找表(称为<strong> Q表</strong>)，其中状态(由<em> s </em>表示)是行，动作(由<em> a </em>表示)是列，元素(由<em> Q(s，a) </em>表示)是当你处于行给出的状态并采取列给出的动作时得到的奖励。在任何状态下采取的最佳行动都是奖励最高的行动:</p>
<pre><strong>initialize Q-table Q</strong><br/><strong>observe initial state s</strong><br/><strong>while (! game_finished):</strong><br/><strong>   select and perform action a</strong><br/><strong>   get reward r <br/>   advance to state s'</strong><br/><strong>   Q(s, a) = Q(s, a) + α(r + γ max_a' Q(s', a') - Q(s, a))</strong><br/><strong>   s = s'</strong></pre>
<p>你会意识到，该算法基本上是在贝尔曼方程上做随机梯度下降，通过状态空间(或插曲)反向传播奖励，并在许多试验(或时期)上求平均。这里，<kbd>α</kbd>是学习率，它决定了应该合并多少以前的Q值和贴现的新的最大Q值之间的差。</p>
<p>我们可以用下面的流程图来表示这个过程:</p>
<div><img height="296" width="211" src="img/deb6d7a0-4377-43e1-bfea-b9b75915f19a.png"/></div>


            

            
        
    






    
        <title>References</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">参考</h1>
                
            
            
                
<ul>
<li>贝尔曼，理查德，<em>一个马尔可夫决策过程。</em>数学与力学学报(1957): 679-684。</li>
<li>Kaelbling，Leslie Pack，Michael L. Littman和Andrew W. Moore，<em>强化学习:一项调查。</em>人工智能研究杂志4期(1996): 237-285。</li>
<li>Goodfellow，Ian等，<em>生成对抗网，a </em> <em>神经信息处理系统的进展，</em> 2014</li>
<li>拉德福德、亚历克、卢克·梅茨和苏史密斯·钦塔拉，<em>利用深度卷积生成对抗网络的无监督表示学习。</em> arXiv预印本arXiv:1511.06434 (2015)。</li>
<li>Isola，Phillip，et al., <em>条件对抗网络的图像到图像翻译</em>，arXiv预印本arXiv:1611.07004 (2016)。</li>
<li>毛，徐东，等，<em>最小二乘生成对抗网络。</em> arXiv预印本ArXiv:1611.04076 (2016)。</li>
<li>埃格巴尔-扎德、哈米德和格哈德·威德默，<em>生成性敌对网络的似然估计。</em> arXiv预印本arXiv:1707.07530 (2017)。</li>
<li>Nguyen，Anh等人，<em>塞&amp;玩生成网络:潜在空间中图像的条件迭代生成</em>。arXiv预印本arXiv:1612.00005 (2016)。</li>
</ul>


            

            
        
    






    
        <title>Summary</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">摘要</h1>
                
            
            
                
<p>在这一章中，我们回顾了最近出现的两个最重要和最具创新性的架构。每天，新的生成和强化模型都以创新的方式应用，无论是从以前已知的类别中生成可行的新元素，还是在策略游戏中战胜职业玩家。</p>
<p>在下一章，我们将提供精确的说明，这样你可以使用和修改提供的代码，以更好地理解你在整本书中获得的不同概念。</p>
<p> </p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    


</body></html>