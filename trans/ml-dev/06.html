<html><head/><body>


    
        <title>Convolutional Neural Networks</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">卷积神经网络</h1>
                
            
            
                
<p>嗯，现在事情变得有趣了！我们的模型现在可以学习更复杂的函数，我们现在准备好围绕更现代和令人惊讶的有效模型进行一次精彩的旅行</p>
<p>在堆叠神经元层成为改进模型的最受欢迎的解决方案后，从基于人类视觉的模型开始，出现了更丰富节点的新思路。它们开始只是研究主题，在图像数据集和更多处理能力变得可用后，它们允许研究人员在分类挑战中达到几乎人类的准确性，我们现在准备在我们的项目中利用这种能力。</p>
<p>我们将在本章中讨论的主题如下:</p>
<ul>
<li>卷积神经网络的起源</li>
<li>离散卷积的简单实现</li>
<li>其他操作类型:汇集、退出</li>
<li>迁移学习</li>
</ul>


            

            
        
    






    
        <title>Origin of convolutional neural networks</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">卷积神经网络的起源</h1>
                
            
            
                
<p><strong>卷积神经网络</strong>(<strong>CNN</strong>)起源久远。他们是在<strong>多层感知器</strong>完善的同时发展起来的，第一个具体的例子就是<strong>神经认知器</strong>。</p>
<p>neocognitron是一个分层的、多层的<strong>人工神经网络</strong> ( <strong> ANN </strong>)，由Fukushima教授在1980年的一篇论文中提出，具有以下主要特征:</p>
<ul>
<li>自组织</li>
<li>容忍输入中的偏移和变形</li>
</ul>
<div><img height="211" width="315" src="img/c29e144d-13cd-46c1-a2fd-a92cc2b6aa99.png"/></div>
<p>这个最初的想法在1986年再次出现在最初的反向传播论文的书籍版本中，并且在1988年也被用于语音识别中的时间信号。</p>
<p>该设计在1998年得到了改进，Ian LeCun <em>、</em> <em>的一篇论文将基于梯度的学习应用于文档识别</em>，提出了LeNet-5网络，这是一种用于分类手写数字的架构。与其他现有模型相比，该模型表现出了更高的性能，特别是在SVM的几个变体上，这是本书出版当年性能最好的操作之一。</p>
<p>然后那篇论文的概括在2003年出现，用“<em>用于图像解释的分层神经网络</em>”。但是总的来说，几乎所有的内核都遵循最初的想法，直到现在。</p>


            

            
        
    






    
        <title>Getting started with convolution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">卷积入门</h1>
                
            
            
                
<p>为了理解卷积，我们将从研究卷积算子的起源开始，然后我们将解释这个概念如何应用于信息。</p>
<p>卷积基本上是两个连续或离散函数之间的运算，实际上，它具有用一个函数过滤另一个函数的效果。</p>
<p>它在不同的领域有许多用途，尤其是在数字信号处理领域，它是整形和过滤音频和图像的首选工具，它甚至用于概率理论，表示两个独立随机变量的和。</p>
<p>而这些过滤能力和机器学习有什么关系呢？答案是，通过过滤器，我们将能够构建能够强调或隐藏我们输入的某些特征的网络节点(通过过滤器的定义)，因此我们可以为所有特征构建自动定制检测器，这些检测器可用于检测确定的模式。我们将在接下来的章节中详细介绍这一点；现在，让我们回顾一下运算的正式定义，以及它是如何计算的摘要。</p>


            

            
        
    






    
        <title>Continuous convolution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">连续卷积</h1>
                
            
            
                
<p>卷积作为一种运算是由达朗贝尔在18世纪微分学发展初期首先创造的。该操作的常见定义如下:</p>
<div><img height="57" width="280" src="img/cfa53da3-92fd-4150-919e-5949eecac3ba.png"/></div>
<p>如果我们试图描述应用该运算所需的步骤，以及它是如何结合两个函数的，我们可以用下面的细节来表达该数学运算所涉及的内容:</p>
<ul>
<li>翻转信号:这是变量的(<em> -τ </em>)部分</li>
<li>移动它:这是由<em> g(τ) </em>的<em> t </em>求和因子给出的</li>
<li>乘以:f 和g 的乘积</li>
<li>对结果曲线进行积分:这是不太直观的部分，因为每个瞬时值都是积分的结果</li>
</ul>
<p>为了理解所涉及的所有步骤，让我们直观地表示在确定点<em> t </em> <sub> <em> 0 </em> </sub>计算两个函数<em> f </em>和<em> g </em>之间的卷积所涉及的所有步骤:</p>
<div><img height="298" width="281" src="img/1827d5c2-accf-4d2f-93bc-713e522cd3eb.png"/></div>
<p>这种对卷积规则的直观近似也适用于函数的离散领域，这是我们将要工作的真实领域。所以，我们先来定义一下。</p>


            

            
        
    






    
        <title>Discrete convolution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">离散卷积</h1>
                
            
            
                
<p>即使有用，我们工作的领域也是非常数字化的，所以我们需要将这种操作转换到离散的领域。两个离散函数<em> f </em>和<em> g </em>的卷积运算是将原始积分转换为以下形式的等价求和:</p>
<div><img height="54" width="330" src="img/c2080076-17fb-4e9f-ab49-c12ada720b31.png"/></div>
<p>这个原始定义可以应用于任意维数的函数。特别是，我们将在2D图像上工作，因为这是大量应用的领域，我们将在本章中进一步描述。</p>
<p>现在是时候学习我们通常应用卷积算子的方式了，通过内核。</p>


            

            
        
    






    
        <title>Kernels and convolutions</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">核和卷积</h1>
                
            
            
                
<p>当在离散域中解决实际问题时，我们通常有有限维的2D函数(例如，它可以是一个图像)，我们希望通过另一个图像过滤它。滤波器开发学科研究不同类型的滤波器通过卷积应用于各种类时的效果。最常见的函数类型是每个维度有两到五个元素，其余元素的值为0。这些代表过滤功能的小矩阵被称为<strong>内核</strong>。</p>
<p>卷积运算从一个<em> n </em>维矩阵(通常是一个代表图像的2D矩阵)的第一个元素与一个核的所有元素开始，将矩阵的中心元素应用于我们要相乘的特定值，并应用核的维度之后的剩余因子。在图像的情况下，最终结果是一个等效的图像，其中它的某些元素(例如，线条和边缘)被突出显示，而其他元素(例如，在模糊的情况下)被隐藏。</p>
<p>在下面的例子中，您将看到特定的3 x 3内核是如何应用于特定的图像元素的。这在扫描模式中对矩阵的所有元素重复:</p>
<div><img height="186" width="369" src="img/8732d877-f163-44ff-8548-701da33e19ad.png"/></div>
<p>内核在应用时还有几个额外的元素需要考虑，特别是步幅和填充，它们完成了适应特殊应用情况的规范。我们来看看步幅和垫高。</p>


            

            
        
    






    
        <title>Stride and padding</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步幅和衬垫</h1>
                
            
            
                
<p>当应用卷积运算时，可以应用于该过程的变化之一是改变核的位移单位。该参数可按尺寸指定，称为<strong>步距</strong>。在下图中，我们展示了几个如何应用stride的示例。在第三种情况下，我们看到一个不兼容的步幅，因为内核不能应用到最后一步。根据库的不同，可以忽略这种类型的警告:</p>
<div><img height="235" width="499" class="alignnone size-full wp-image-753 image-border" src="img/9998091b-8d00-42e9-a034-11f3ebf7f49d.png"/></div>
<p>应用内核时的另一个重要事实是，内核越大，图像/矩阵的边界上就有越多的单元不会收到答案，因为我们需要覆盖整个内核。为了应对这种情况，<strong>填充</strong>参数将为图像添加指定宽度的边界，以允许内核能够均匀地应用于边缘像素/元素。这里，您有填充参数的图形描述:</p>
<div><img height="233" width="198" class="alignnone size-full wp-image-755 image-border" src="img/e8261782-72b9-4c56-8e97-c62b33b19db0.png"/></div>
<p>在描述了卷积的基本概念之后，让我们用一个实际例子来实现卷积，看看它在真实图像中的应用，并对它的效果有一个直观的了解。</p>


            

            
        
    






    
        <title>Implementing the 2D discrete convolution operation in an example</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">在一个例子中实现2D离散卷积运算</h1>
                
            
            
                
<p>为了理解离散卷积运算的机制，让我们对这一概念进行简单直观的实现，并将其应用于具有不同类型内核的样本图像。让我们导入所需的库。因为我们将以尽可能清晰的方式实现算法，所以我们将只使用最少的必要算法，比如NumPy:</p>
<div><pre class="sourceCode python">import matplotlib.pyplot as plt
import imageio
import numpy as np</pre></div>
<p>使用<kbd>imageio</kbd>包的<kbd>imread</kbd>方法，让我们读取图像(导入为三个相等的通道，因为它是灰度的)。然后，我们对第一个通道进行切片，将其转换为浮点，并使用matplotlib显示它:</p>
<div><pre class="sourceCode python">arr = imageio.imread("b.bmp") [:,:,0].astype(np.float)
plt.imshow(arr, cmap=plt.get_cmap('binary_r'))
plt.show()</pre></div>
<div><img height="246" width="330" src="img/e65c77d4-672a-4da5-b364-a6c8dd099a5a.png"/></div>
<p>现在是时候定义内核卷积运算了。正如我们之前所做的，我们将简化3 x 3内核上的操作，以便更好地理解边界条件。<kbd>apply3x3kernel</kbd>将在图像的所有元素上应用内核，返回一个新的等效图像。请注意，为了简单起见，我们将内核限制为3 x 3，因此图像的1像素边界不会有新值，因为我们没有考虑填充:</p>
<div><pre class="sourceCode python">class ConvolutionalOperation:
    def apply3x3kernel(self, image, kernel):  # Simple 3x3 kernel operation
        newimage=np.array(image)
        for m in range(1,image.shape[0]-2):
            for n in range(1,image.shape[1]-2):
                newelement = 0
                for i in range(0, 3):
                    for j in range(0, 3):
                        newelement = newelement + image[m - 1 + i][n - 1+ <br/>                        j]*kernel[i][j]
                newimage[m][n] = newelement
        return (newimage)</pre></div>
<p class="mce-root">正如我们在前面几节中看到的，不同的内核配置突出了原始图像的不同元素和属性，构建的过滤器在经过多次训练后可以专门处理非常高级的特征，如眼睛、耳朵和门。在这里，我们将生成一个以名称为关键字的内核字典，内核的系数排列在一个3 x 3的数组中。<kbd>Blur</kbd>滤镜相当于计算3 x 3点邻域的平均值，<kbd>Identity</kbd>只是按原样返回像素值，<kbd>Laplacian</kbd>是一个经典的导数滤镜，它突出显示边界，然后两个<kbd>Sobel</kbd>滤镜将在第一种情况下标记水平边缘，在第二种情况下标记垂直边缘:</p>
<div><pre class="sourceCode python">kernels = {"Blur":[[1./16., 1./8., 1./16.], [1./8., 1./4., 1./8.], [1./16., 1./8., 1./16.]]
           ,"Identity":[[0, 0, 0], [0., 1., 0.], [0., 0., 0.]]
           ,"Laplacian":[[1., 2., 1.], [0., 0., 0.], [-1., -2., -1.]]
           ,"Left Sobel":[[1., 0., -1.], [2., 0., -2.], [1., 0., -1.]]
           ,"Upper Sobel":[[1., 2., 1.], [0., 0., 0.], [-1., -2., -1.]]}</pre></div>
<p>让我们生成一个<kbd>ConvolutionalOperation</kbd>对象，并生成一个比较内核图表，看看它们是如何比较的:</p>
<div><pre>conv = ConvolutionalOperation()
plt.figure(figsize=(30,30))
fig, axs = plt.subplots(figsize=(30,30))
j=1
for key,value in kernels.items():
    axs = fig.add_subplot(3,2,j)
    out = conv.apply3x3kernel(arr, value)
    plt.imshow(out, cmap=plt.get_cmap('binary_r'))
    j=j+1
plt.show()<br/><br/>&lt;matplotlib.figure.Figure at 0x7fd6a710a208&gt;</pre></div>
<p class="figure">在最终的图像中，您可以清楚地看到我们的内核是如何检测到图像上的几个高细节特征的——在第一个图像中，您看到的是没有变化的图像，因为我们使用了单位内核，然后是拉普拉斯边缘检测器、左边界检测器、上边界检测器，然后是模糊操作符:</p>
<div><img height="577" width="596" src="img/fabe713d-8ade-4606-a804-df420bfbfae5.png"/></div>
<p>回顾了连续场和离散场的卷积运算的主要特征后，我们可以得出这样的结论:基本上，卷积核突出或隐藏了模式。根据训练的或(在我们的例子中)手动设置的参数，我们可以开始发现图像中的许多元素，例如不同维度的方向和边缘。例如，我们也可以通过模糊内核来掩盖一些不想要的细节或异常值。此外，通过堆叠卷积层，我们甚至可以突出更高阶的复合元素，如眼睛或耳朵。</p>
<p>卷积神经网络的这一特性是它们相对于以前的数据处理技术的主要优势:我们可以非常灵活地确定某个数据集的主要成分，并将更多的样本表示为这些基本构件的组合。</p>
<p>现在是时候看看通常与前者结合使用的另一种类型的层了——池层。</p>


            

            
        
    






    
        <title>Subsampling operation (pooling)</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">子采样操作(合并)</h1>
                
            
            
                
<p>二次采样操作包括应用一个核(可变维度)并通过将图像分成<em> mxn </em>个块并取一个代表该块的元素来减小输入维度的范围，从而以某个确定的因子减小图像分辨率。在2 x 2内核的情况下，图像大小将减少一半。最广为人知的操作是最大值(最大池)、平均值(平均池)和最小值(最小池)。<br/>下图向您展示了如何将2 x 2 <kbd>maxpool</kbd>内核应用于单通道16 x 16矩阵。它只是保持它所覆盖的内部区域的最大值:</p>
<div><img height="141" width="281" src="img/854251eb-1ea4-4f3a-aa9b-6e3aa8afaf0a.png"/> </div>
<p>现在我们已经看到了这个简单的机制，让我们问问自己，它的主要目的是什么？二次抽样层的主要目的与卷积层有关:减少信息量和复杂性，同时保留最重要的信息元素。换句话说，他们构建了底层信息的<em> <strong>紧凑表示</strong> </em>。</p>
<p>现在是时候编写一个简单的池操作符了。它比卷积运算符更容易、更直接，在这种情况下，我们将只实现max pooling，它选择4 x 4邻域中最亮的像素，并将其投影到最终图像:</p>
<div><pre class="sourceCode python">class PoolingOperation:
    def apply2x2pooling(self, image, stride):  # Simple 2x2 kernel operation
        newimage=np.zeros((int(image.shape[0]/2),int(image.shape[1]/2)),np.float32)
        for m in range(1,image.shape[0]-2,2):
            for n in range(1,image.shape[1]-2,2):
                newimage[int(m/2),int(n/2)] = np.max(image[m:m+2,n:n+2])
        return (newimage)</pre></div>
<p>让我们应用新创建的池化操作，正如您所看到的，最终的图像分辨率更加块状化，并且细节通常更加明亮:</p>
<div><pre class="sourceCode python">plt.figure(figsize=(30,30))
pool=PoolingOperation()
fig, axs = plt.subplots(figsize=(20,10))
axs = fig.add_subplot(1,2,1)
plt.imshow(arr, cmap=plt.get_cmap('binary_r'))
out=pool.apply2x2pooling(arr,1)
axs = fig.add_subplot(1,2,2)
plt.imshow(out, cmap=plt.get_cmap('binary_r'))
plt.show()</pre></div>
<p class="figure">在这里你可以看到不同之处，尽管它们很细微。最终图像的精度较低，选择的像素(环境的最大值)会产生更亮的图像:</p>
<div><img height="355" width="714" src="img/c37f3486-e074-4d1f-96d4-b1adbd85a921.png"/></div>


            

            
        
    






    
        <title>Improving efficiency with the dropout operation</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">利用下降操作提高效率</h1>
                
            
            
                
<p>正如我们在前面的章节中观察到的，过度拟合是每个模型的潜在问题。神经网络也是如此，数据在训练集上可以做得很好，但在测试集上却不行，这使得它对泛化毫无用处。</p>
<p>为此，2012年，Geoffrey Hinton领导的团队发表了一篇论文，描述了辍学操作。它的操作很简单:</p>
<ul>
<li>选择随机数量的节点(从总数中选择的节点的比率是一个参数)</li>
<li>所选权重的值被审查为零，从而使它们在后续层的先前连接的对等体无效</li>
</ul>


            

            
        
    






    
        <title>Advantages of the dropout layers</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">脱落层的优点</h1>
                
            
            
                
<p>这种方法的主要优点是防止一层中的所有神经元同步优化它们的权重。这种在随机组中进行的适应，防止所有神经元收敛到同一个目标，从而使权重去相关。</p>
<p>对于dropout的应用发现的第二个特性是隐藏单元的激活变得稀疏，这也是一个期望的特性。</p>
<p class="mce-root">在下图中，我们有一个原始的、完全连接的多层神经网络的表示，以及带有辍学的关联网络:</p>
<p>&gt;<img height="181" width="548" src="img/000dfeda-4caa-4272-b4a2-28903b1d4044.png"/></p>


            

            
        
    






    
        <title>Deep neural networks</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">深度神经网络</h1>
                
            
            
                
<p>现在我们已经有了丰富的层次，是时候开始参观神经架构是如何随时间演变的了。从2012年开始，一系列新的、越来越强大的层的组合开始迅速出现，而且势不可挡。这套新的架构采用了术语<strong>深度学习</strong>，我们可以近似地将它们定义为至少包含三层的复杂神经架构。它们也倾向于包括比单层感知机更高级的层，比如卷积层。</p>


            

            
        
    






    
        <title>Deep convolutional network architectures through time</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">穿越时间的深度卷积网络架构</h1>
                
            
            
                
<p>深度学习架构可以追溯到20年前，并在很大程度上受解决人类视觉问题的挑战的指导而发展。让我们看看主要的深度学习架构及其主要的构建模块，然后我们可以为自己的目的重用它们。</p>


            

            
        
    






    
        <title>Lenet 5</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">Lenet 5</h1>
                
            
            
                
<p>正如我们在卷积神经网络的历史介绍中看到的，卷积层是在20世纪80年代发现的。但直到20世纪90年代末，可用的技术还不足以构建复杂的层组合。</p>
<p>大约在1998年，在贝尔实验室，在围绕手写数字解码的研究期间，Ian LeCun形成了一种新的方法——卷积、池化和完全连接层的混合——来解决识别手写数字的问题。</p>
<p>在这个时候，SVM和其他更多的数学定义的问题或多或少地被成功使用，关于CNN的基础论文表明，神经网络可以与当时最先进的方法相比表现得相当好。</p>
<p>在下图中，显示了该体系结构的所有层，该体系结构接收28 x 28灰度图像作为输入，并返回一个10元素向量，输出为每个字符的概率:</p>
<div><img src="img/564b343b-c5aa-444e-b00d-38f719b0c32a.png"/></div>


            

            
        
    






    
        <title>Alexnet</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">Alexnet</h1>
                
            
            
                
<p>经过几年的中断(尽管Lecun正在将他的网络应用于其他任务，如人脸和物体识别)，可用的结构化数据和原始处理能力的指数增长使团队能够增长和调整模型，这在几年前被认为是不可能的。</p>
<p>促进该领域创新的因素之一是名为<strong> Imagenet </strong>的图像识别基准的可用性，该基准由数百万张按类别组织的对象图像组成。</p>
<p>从2012年开始，<strong>大规模视觉识别挑战(LSVRC) </strong>每年都会举办，帮助研究人员创新网络配置，每年都取得越来越好的结果。</p>
<p>Alex Krizhevsky开发的Alexnet 是第一个赢得这项挑战的深度卷积网络，并为未来几年开创了先例。它由一个在结构上类似于Lenet-5的模型组成，但其卷积层的深度为数百个单位，参数总数为数千万。</p>
<p>接下来的挑战见证了一个强有力竞争者的出现，来自牛津大学的<strong>视觉几何小组(VGG) </strong>，带着它的VGG模型。</p>


            

            
        
    






    
        <title>The VGG model</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">VGG模式</h1>
                
            
            
                
<p>VGG网络架构的主要特点是，它将卷积滤波器的大小减少到一个简单的3 x 3矩阵，并按序列组合它们，这与以前的竞争者不同，以前的竞争者具有较大的核维数(高达11 x 11)。<br/>矛盾的是，这一系列小的卷积权重实际上包含了大量的参数(大约几百万个)，因此它必须受到许多措施的限制。</p>
<div><img src="img/7aee123c-c553-4558-a73d-a924b6c642a7.png"/></div>


            

            
        
    






    
        <title>GoogLenet and the Inception model</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">GoogLenet和初始模型</h1>
                
            
            
                
<p><strong> GoogLenet </strong>是在2014年赢得LSVRC的神经网络架构，也是该系列中大型IT公司的第一次真正成功的尝试，自2014年以来，该系列主要由拥有巨额预算的公司赢得。</p>
<p>GoogLenet基本上是九个连锁的初始模块的深度组合，很少或没有修改。下图显示了这些Inception模块中的每一个，它是微小卷积块的混合体，混合了一个3 x 3 max的池节点:</p>
<div><img height="284" width="537" src="img/5c3db9cd-2c4e-47d0-a033-6fd7357c77be.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">即使如此复杂，与两年前发布的Alexnet相比，GoogLenet仍设法减少了所需的参数数量(1100万比6000万)，并提高了准确性(6.7%的误差比16.4%)。此外，先启模块的重用允许敏捷的实验。</p>
<p>但这不是这个架构的最后版本。很快，第二个版本的Inception模块就被创建了，它具有以下特征。</p>


            

            
        
    






    
        <title>Batch-normalized inception V2 and V3</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">批量标准化的初始V2和V3</h1>
                
            
            
                
<p>2015年12月，随着论文<em>重新思考计算机视觉的盗梦架构</em>，谷歌研究院发布了盗梦架构的新迭代。</p>
<p><strong>内部协方差移位问题</strong></p>
<p>最初GoogLenet的主要问题之一是培训不稳定。正如我们前面所看到的，输入归一化基本上包括将所有输入值集中在零上，并将其值除以标准偏差，以便为反向传播的梯度获得良好的基线。</p>
<p>在训练非常大的数据集期间发生的是，在大量训练示例之后，不同值的振荡开始放大平均参数值，就像在共振现象中一样。这种现象被称为<strong>协方差偏移</strong>。</p>
<p>为了缓解这一问题，解决方案是不仅对原始输入值进行归一化，而且对每一层的输出值进行归一化，从而避免在各层开始偏离平均值之前出现不稳定性。</p>
<p>除了批量标准化之外，还向V2提出了一些增量式的添加:</p>
<ul>
<li>将卷积数减少到最大值3 x 3</li>
<li>增加网络的总深度</li>
<li>在每个图层上使用宽度增加技术来改善要素组合</li>
<li>卷积的因子分解</li>
</ul>
<p>Inception V3基本上在同一架构上实现了所有提出的创新，并对网络的辅助分类器添加了批量归一化。</p>
<p>在下图中，我们展示了新的架构。请注意卷积单元的缩减大小:</p>
<div><img height="218" width="218" src="img/ca0f4608-6d08-40a2-ab45-364692d956a2.png"/></div>
<p>2015年底，这一系列架构的最后一次根本性改进来自另一家公司微软，以<strong> ResNets </strong>的形式。</p>


            

            
        
    






    
        <title>Residual Networks (ResNet)</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">剩余网络</h1>
                
            
            
                
<p>这种新的架构出现在2015年12月(或多或少与Inception V3同时)，它有一个简单但新颖的想法——不仅应该使用每个构成层的输出，而且该架构还应该将该层的输出与原始输入相结合。</p>
<p>在下图中，我们观察到一个ResNet模块的简化视图。它清楚地显示了卷积序列末尾的求和运算，以及最终的ReLU运算:</p>
<div><img height="185" width="198" src="img/b626c89d-0474-4e33-aab3-01fdf6cc9f19.png"/></div>
<p>模块的卷积部分包括一个从256个值减少到64个值的特征，一个保持特征数的3×3滤波器层，以及一个从64×256个值扩充1×1层的特征。最初，它跨越了100多个层，但在最近的发展中，ResNet也用于不到30层的深度，但分布更广。</p>
<p>既然我们已经看到了近年来主要发展的概况，让我们直接进入研究人员为CNN发现的主要应用类型。</p>


            

            
        
    






    
        <title>Types of problem solved by deep layers of CNNs</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">CNN深层解决的问题类型</h1>
                
            
            
                
<p>在过去，CNN已经被用来解决各种各样的问题。以下是对主要问题类型的回顾，以及对该体系结构的简短参考:</p>
<ul>
<li>分类</li>
<li>侦查</li>
<li>分割</li>
</ul>


            

            
        
    






    
        <title>Classification</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">分类</h1>
                
            
            
                
<p>正如我们之前看到的，分类模型将图像或其他类型的输入作为参数，并返回一个数组，该数组包含与可能的类别数量一样多的元素，每个元素都有相应的概率。</p>
<p>这种解决方案的正常架构是卷积层和池层的复杂组合，最后是逻辑层，显示任何预训练类的概率。</p>


            

            
        
    






    
        <title>Detection</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">侦查</h1>
                
            
            
                
<p>检测增加了复杂性，因为它需要猜测与图像相关的一个或多个元素的位置，然后尝试对这些信息元素中的每一个进行分类。</p>
<p>对于这项任务，针对单个定位问题的常见策略是将分类和回归问题结合起来——一个问题(分类)用于对象的类别，另一个问题(回归)用于确定被检测对象的坐标——然后将这些损失结合成一个共同的问题。</p>
<p class="mce-root">对于多个元素，第一步是确定感兴趣区域的数量，搜索图像中在统计上显示属于同一对象的信息斑点的位置，然后仅将分类算法应用于检测到的区域，寻找具有高概率的阳性情况。</p>


            

            
        
    






    
        <title>Segmentation</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">分割</h1>
                
            
            
                
<p>分割增加了复杂性，因为模型必须定位图像中的元素，并标记所有已定位对象的确切形状，如下图所示:</p>
<div><img height="402" width="605" class="alignnone size-full wp-image-759 image-border" src="img/9650e822-2c45-41e6-b22f-cf1082db369a.png"/></div>
<p>完成这项任务的最常见方法之一是实施连续的下采样和上采样操作，恢复高分辨率图像，每个像素只有一定数量的可能结果，这些结果标记了该元素的类别号。</p>


            

            
        
    






    
        <title>Deploying a deep neural network with Keras</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">使用Keras部署深度神经网络</h1>
                
            
            
                
<p>在这个练习中，我们将生成先前描述的由Keras应用程序库提供的Inception模型的一个实例。首先，我们将导入所有需要的库，包括Keras模型处理，图像预处理库，用于优化变量的梯度下降，以及几个初始实用程序。此外，我们将使用OpenCV库来调整新的输入图像，以及常见的NumPy和matplotlib库:</p>
<div><pre>from keras.models import Model
from keras.preprocessing import image
from keras.optimizers import SGD
from keras.applications.inception_v3 import InceptionV3, decode_predictions, preprocess_input

import matplotlib.pyplot as plt
import numpy as np
import cv2<br/><br/>Using TensorFlow backend.</pre></div>
<p>Keras使加载模型变得非常简单。您只需调用一个新的<kbd>InceptionV3</kbd>类实例，然后我们将分配一个基于<strong>随机梯度下降</strong>和损失分类交叉熵的优化器，这非常适合图像分类问题:</p>
<div><pre class="sourceCode python">model=InceptionV3()
model.compile(optimizer=SGD(), loss='categorical_crossentropy')</pre></div>
<p>既然模型已经加载到内存中，现在是时候使用<kbd>cv</kbd>库加载和调整照片了，然后我们调用Keras应用程序的预处理输入，这将使值正常化:</p>
<div><pre class="sourceCode python"># resize into VGG16 trained images' format
im = cv2.resize(cv2.imread('blue_jay.jpg'), (299, 299))
im = np.expand_dims(im, axis=0)
im = im /255.
im = im - 0.5
im =  im * 2
plt.figure (figsize=(10,10))
plt.imshow(im[0], cmap=plt.get_cmap('binary_r'))
plt.show()</pre></div>
<p class="figure">这是图像归一化后的样子，请注意我们对图像的结构理解是如何变化的，但从模型的角度来看，这是允许模型收敛的最佳方式:</p>
<div><img height="408" width="414" src="img/4adcfca4-f4b1-4e89-98ba-bee9896d17da.png"/></div>
<p>现在我们将调用模型的<kbd>predict</kbd>方法，这将显示神经网络最后一层的结果，即每个类别的概率数组。<kbd>decode_predictions</kbd>方法读取一个字典，其中所有的类别编号作为索引，类别名称作为值，因此它提供检测到的项目分类的名称，而不是编号:</p>
<div><pre>out = model.predict(im)
print('Predicted:', decode_predictions(out, top=3)[0])
print (np.argmax(out))<br/><br/>Predicted: [('n01530575', 'brambling', 0.18225007), ('n01824575', 'coucal', 0.13728797), ('n01560419', 'bulbul', 0.048493069)]
10</pre></div>
<p>正如你所看到的，通过这个简单的方法，我们从一系列相似的鸟类中得到了一个非常近似的预测。对输入图像和模型本身的额外调整可以导致更精确的答案，因为蓝鸟是包含在1000个可能类别中的一个类别。</p>


            

            
        
    






    
        <title>Exploring a convolutional model with Quiver</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">用颤动探索卷积模型</h1>
                
            
            
                
<p>在这个实际的例子中，我们将在Keras库和箭筒的帮助下加载一个我们之前研究过的模型(在这个例子中是<kbd>Vgg19</kbd>)。然后，我们将观察架构的不同阶段，以及不同层如何工作，并输入一定的信息。</p>


            

            
        
    






    
        <title>Exploring a convolutional network with Quiver</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">用箭图探索卷积网络</h1>
                
            
            
                
<p><strong>颤</strong>(<a href="https://github.com/keplr-io/quiver" target="_blank">https://github.com/keplr-io/quiver</a>)是一个最近非常方便的工具，用来在Keras的帮助下探索模型。它创建了一个可由现代网络浏览器访问的服务器，并允许模型结构的可视化和来自输入层的输入图像的评估，直到最终预测。</p>
<p>使用下面的代码片段，我们将创建一个<kbd>VGG16</kbd>模型的实例，然后我们将允许颤读当前目录中的所有图像，并启动一个web应用程序，该应用程序将允许我们与我们的模型及其参数进行交互:</p>
<pre>from keras.models import Model<br/>from keras.preprocessing import image<br/>from keras.optimizers import SGD<br/>from keras.applications.vgg16 import VGG16<br/>import keras.applications as apps<br/><br/>model=apps.vgg16.VGG16()<br/><br/>from quiver_engine.server import launch<br/>launch(model,input_folder=".") </pre>
<p>该脚本将下载<kbd>VGG16</kbd>模型权重(您需要一个良好的连接，因为它的重量高达数百兆字节)。然后，它将模型加载到内存中，并创建一个监听端口5000的服务器。</p>
<p>Keras库下载的模型权重之前已经使用Imagenet进行了彻底的训练，因此它可以在我们的数据集中的1，000个类别上获得非常好的准确性。</p>
<p>在下面的屏幕截图中，我们看到了加载web应用程序的索引页面后的第一个屏幕。左侧显示了网络架构的交互式图形表示。在中间右侧，您可以选择当前目录中的一张图片，应用程序会自动将其作为输入，打印出输入的五个最可能的结果。</p>
<p>该屏幕截图还显示了第一个网络层，它基本上由代表原始图像的红色、绿色和蓝色成分的三个矩阵组成:</p>
<div><img height="379" width="772" class="alignnone size-full wp-image-779 image-border" src="img/7f08fd1e-24e3-484d-b270-fcf57f39f584.png"/></div>
<p>然后，随着我们进入模型层，我们有了第一个卷积层。在这里，我们可以看到这个阶段主要突出了高级功能，如我们用3 x 3滤镜设置的功能，如不同类型的边框、亮度和对比度:</p>
<div><img height="361" width="793" src="img/d6515d78-8381-4126-884f-38a674246a5d.png"/></div>
<p>让我们再前进一点。我们现在可以看到一个不关注全局特性的中间层。相反，我们看到它已经为中间特征进行了训练，例如不同组的纹理、角度或特征组，例如眼睛和鼻子:</p>
<div><img height="389" width="792" src="img/c7883909-d0bb-40fa-83f0-55f4d1fb386d.png"/></div>
<p>当到达最后的卷积层时，真正抽象的概念出现了。这个阶段显示了我们现在正在训练的模型是多么不可思议的强大，因为现在我们看到的是突出显示的元素，而没有任何有用的(对我们来说)意义。在一些完全连接的层之后，这些新的抽象类别将导致最终的解决方案，这是一个具有浮点概率值的1000元素的数组，该概率值是ImageNet中每个类别的概率值:</p>
<div><img height="360" width="791" src="img/c0ecddc0-353d-4f3b-b020-8a36010b61e2.png"/></div>
<p>我们希望你可以探索不同的例子和层的输出，并尝试发现它们如何突出不同类别图像的不同特征。</p>
<p>现在是时候研究一种新型的机器学习了，它包括应用以前训练过的网络来解决新型问题。这叫做<strong>迁移学习</strong>。</p>


            

            
        
    






    
        <title>Implementing transfer learning</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">实施迁移学习</h1>
                
            
            
                
<p>在本例中，我们将实现之前看到的示例之一，替换预训练卷积神经网络的最后阶段，并为一组新元素训练最后阶段，将其应用于分类。它具有以下优点:</p>
<ul>
<li>它建立在对图像分类任务证明有效的模型上</li>
<li>它减少了训练时间，因为我们可以重复使用系数，其精确度可能需要数周的计算能力才能达到</li>
</ul>
<p>数据集类将是flower17数据集的两种不同的花类型。这是一个17类花卉数据集，每类有80幅图像。选的花是英国一些常见的花。图像具有较大的比例、姿态和光照变化，并且还存在类内图像变化较大且与其他类非常相似的类。在这种情况下，我们将收集前两个类(水仙花和款冬)，并在预训练的VGG16网络上构建一个分类器。</p>
<p>首先，我们将进行图像数据扩充，因为图像的数量可能不足以提取每个物种的所有元素。让我们从导入所有需要的库开始，包括应用程序、预处理、检查点模型和相关对象，以允许我们保存中间步骤，以及用于图像处理和数字基础操作的<kbd>cv2</kbd>和<kbd>NumPy</kbd>库:</p>
<div><pre>from keras import applications
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras.models import Sequential, Model 
from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D
from keras import backend as k 
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping
from keras.models import load_model
from keras.applications.vgg16 import VGG16, decode_predictions,preprocess_input
import cv2
import numpy as np<br/><br/>Using TensorFlow backend.</pre></div>
<p>在本节中，我们将定义影响输入、数据源和训练参数的所有变量:</p>
<div><pre class="sourceCode python">img_width, img_height = 224, 224
train_data_dir = "train"
validation_data_dir = "validation"
nb_train_samples = 300
nb_validation_samples = 100 
batch_size = 16
epochs = 50</pre></div>
<p>现在我们将调用VGG16预训练模型，不包括顶部展平层:</p>
<div><pre class="sourceCode python">model = applications.VGG16(weights = "imagenet", include_top=False, input_shape = (img_width, img_height, 3))

# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.
for layer in model.layers[:5]:
    layer.trainable = False

#Adding custom Layers 
x = model.output
x = Flatten()(x)
x = Dense(1024, activation="relu")(x)
x = Dropout(0.5)(x)
x = Dense(1024, activation="relu")(x)
predictions = Dense(2, activation="softmax")(x)

# creating the final model 
model_final = Model(input = model.input, output = predictions)</pre></div>
<p>现在是时候编译模型并为训练和测试数据集创建图像数据扩充对象了:</p>
<div><pre class="sourceCode python"># compile the model 
model_final.compile(loss = "categorical_crossentropy", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=["accuracy"])

# Initiate the train and test generators with data Augumentation 
train_datagen = ImageDataGenerator(
rescale = 1./255,
horizontal_flip = True,
fill_mode = "nearest",
zoom_range = 0.3,
width_shift_range = 0.3,
height_shift_range=0.3,
rotation_range=30)

test_datagen = ImageDataGenerator(
rescale = 1./255,
horizontal_flip = True,
fill_mode = "nearest",
zoom_range = 0.3,
width_shift_range = 0.3,
height_shift_range=0.3,
rotation_range=30)</pre></div>
<p>现在，我们将生成新的扩充数据:</p>
<div><pre>train_generator = train_datagen.flow_from_directory(
train_data_dir,
target_size = (img_height, img_width),
batch_size = batch_size, 
class_mode = "categorical")

validation_generator = test_datagen.flow_from_directory(
validation_data_dir,
target_size = (img_height, img_width),
class_mode = "categorical")

# Save the model according to the conditions  
checkpoint = ModelCheckpoint("vgg16_1.h5", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)
early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')<br/><br/>Found 120 images belonging to 2 classes.
Found 40 images belonging to 2 classes.</pre></div>
<p>是时候为模型添加新的最终图层了:</p>
<div><pre>model_final.fit_generator(
train_generator,
samples_per_epoch = nb_train_samples,
nb_epoch = epochs,
validation_data = validation_generator,
nb_val_samples = nb_validation_samples,
callbacks = [checkpoint, early])<br/><br/>Epoch 1/50
288/300 [===========================&gt;..] - ETA: 2s - loss: 0.7809 - acc: 0.5000

/usr/local/lib/python3.5/dist-packages/Keras-1.2.2-py3.5.egg/keras/engine/training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.
  warnings.warn('Epoch comprised more than '


Epoch 00000: val_acc improved from -inf to 0.63393, saving model to vgg16_1.h5
304/300 [==============================] - 59s - loss: 0.7802 - acc: 0.4934 - val_loss: 0.6314 - val_acc: 0.6339
Epoch 2/50
296/300 [============================&gt;.] - ETA: 0s - loss: 0.6133 - acc: 0.6385Epoch 00001: val_acc improved from 0.63393 to 0.80833, saving model to vgg16_1.h5
312/300 [===============================] - 45s - loss: 0.6114 - acc: 0.6378 - val_loss: 0.5351 - val_acc: 0.8083
Epoch 3/50
288/300 [===========================&gt;..] - ETA: 0s - loss: 0.4862 - acc: 0.7986Epoch 00002: val_acc improved from 0.80833 to 0.85833, saving model to vgg16_1.h5
304/300 [==============================] - 50s - loss: 0.4825 - acc: 0.8059 - val_loss: 0.4359 - val_acc: 0.8583
Epoch 4/50
296/300 [============================&gt;.] - ETA: 0s - loss: 0.3524 - acc: 0.8581Epoch 00003: val_acc improved from 0.85833 to 0.86667, saving model to vgg16_1.h5
312/300 [===============================] - 48s - loss: 0.3523 - acc: 0.8590 - val_loss: 0.3194 - val_acc: 0.8667
Epoch 5/50
288/300 [===========================&gt;..] - ETA: 0s - loss: 0.2056 - acc: 0.9549Epoch 00004: val_acc improved from 0.86667 to 0.89167, saving model to vgg16_1.h5
304/300 [==============================] - 45s - loss: 0.2014 - acc: 0.9539 - val_loss: 0.2488 - val_acc: 0.8917
Epoch 6/50
296/300 [============================&gt;.] - ETA: 0s - loss: 0.1832 - acc: 0.9561Epoch 00005: val_acc did not improve
312/300 [===============================] - 17s - loss: 0.1821 - acc: 0.9551 - val_loss: 0.2537 - val_acc: 0.8917
Epoch 7/50
288/300 [===========================&gt;..] - ETA: 0s - loss: 0.0853 - acc: 0.9792Epoch 00006: val_acc improved from 0.89167 to 0.94167, saving model to vgg16_1.h5
304/300 [==============================] - 48s - loss: 0.0840 - acc: 0.9803 - val_loss: 0.1537 - val_acc: 0.9417
Epoch 8/50
296/300 [============================&gt;.] - ETA: 0s - loss: 0.0776 - acc: 0.9764Epoch 00007: val_acc did not improve
312/300 [===============================] - 17s - loss: 0.0770 - acc: 0.9776 - val_loss: 0.1354 - val_acc: 0.9417
Epoch 9/50
296/300 [============================&gt;.] - ETA: 0s - loss: 0.0751 - acc: 0.9865Epoch 00008: val_acc did not improve
312/300 [===============================] - 17s - loss: 0.0719 - acc: 0.9872 - val_loss: 0.1565 - val_acc: 0.9250
Epoch 10/50
288/300 [===========================&gt;..] - ETA: 0s - loss: 0.0465 - acc: 0.9931Epoch 00009: val_acc did not improve
304/300 [==============================] - 16s - loss: 0.0484 - acc: 0.9901 - val_loss: 0.2148 - val_acc: 0.9167
Epoch 11/50
296/300 [============================&gt;.] - ETA: 0s - loss: 0.0602 - acc: 0.9764Epoch 00010: val_acc did not improve
312/300 [===============================] - 17s - loss: 0.0634 - acc: 0.9744 - val_loss: 0.1759 - val_acc: 0.9333
Epoch 12/50
288/300 [===========================&gt;..] - ETA: 0s - loss: 0.0305 - acc: 0.9931</pre></div>
<p>现在让我们用水仙花的图像来尝试一下。让我们测试分类器的输出，它应该输出一个接近<kbd>[1.,0.]</kbd>的数组，表明第一个选项的概率非常高:</p>
<div><pre>im = cv2.resize(cv2.imread('test/gaff2.jpg'), (img_width, img_height))
im = np.expand_dims(im, axis=0).astype(np.float32)
im=preprocess_input(im)


out = model_final.predict(im)

print (out)
print (np.argmax(out))<br/><br/>[[  1.00000000e+00   1.35796010e-13]]
0</pre></div>
<p>所以，对于这种花，我们有了一个非常确定的答案。您可以使用新的图像，并使用剪切或扭曲的图像(甚至是相关的类)来测试模型，以测试精确度。</p>


            

            
        
    






    
        <title>References</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">参考</h1>
                
            
            
                
<ul>
<li>Fukushima，Kunihiko和Sei Miyake，<em> Neocognitron:视觉模式识别机制的自组织神经网络模型。</em>神经网络中的竞争与合作。施普林格，柏林，海德堡，1982。267-285.</li>
<li><em>基于梯度的学习在文档识别中的应用。IEEE 86.11会议录(1998): 2278-2324。</em></li>
<li>Krizhevsky、Alex、Ilya Sutskever和Geoffrey E. Hinton，<em>使用深度卷积神经网络的ImageNet分类。</em>神经信息处理系统的进展。2012.</li>
<li>Hinton，Geoffrey E .等人<em>通过防止特征检测器的共同适应来改进神经网络。arXiv预印本arXiv:1207.0580 (2012)。</em></li>
<li>Simonyan，Karen和Andrew Zisserman，<em>用于大规模图像识别的极深度卷积网络</em>。arXiv预印本arXiv:1409.1556 (2014)。</li>
<li>Srivastava，Nitish等人<em>辍学:防止神经网络过拟合的简单方法。</em>机器学习研究杂志15.1 (2014): 1929-1958。</li>
<li>重新思考计算机视觉的盗梦空间架构。IEEE计算机视觉和模式识别会议论文集。2016.</li>
<li>何，，等，<em>深度残差学习用于图像识别。</em>IEEE计算机视觉和模式识别会议论文集。2016.</li>
<li>Chollet，Franç ois，<em>例外:具有深度方向可分离卷积的深度学习。</em> arXiv预印本arXiv:1610.02357 (2016)。</li>
</ul>


            

            
        
    






    
        <title>Summary</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">摘要</h1>
                
            
            
                
<p>这一章提供了对你每天在媒体上看到的令人惊奇的新应用的技术之一的重要见解。此外，通过提供的实际示例，您甚至能够创建新的定制解决方案。</p>
<p>由于我们的模型不足以解决非常复杂的问题，在下一章中，我们的范围将进一步扩大，将重要的时间维度添加到我们的概括中。</p>
<p class="mce-root"/>


            

            
        
    


</body></html>