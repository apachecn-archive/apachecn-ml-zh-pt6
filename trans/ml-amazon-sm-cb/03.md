# *第三章*:用亚马逊 SageMaker 使用机器学习和深度学习框架

**神经网络**和**深度学习**是目前科技行业最热门的话题。如果这是你第一次听说**人工神经网络**，它只是一个由被称为神经元的互连单元组成的网络，用于解决特定的机器学习问题。这些神经网络模型已经被用于解决不同的实际生活问题，包括图像分类，时间序列预测，甚至语言翻译。神经网络的特性之一是这些网络具有的节点层数。通常，拥有更多层有助于在一定程度上提高模型的性能。当神经网络具有大约三层或更多层时，我们认为人工神经网络是深度神经网络。当处理更大的数据集时，深度学习模型可以实现更好的性能，因为这些模型可以有效地扩展数据。在过去的几年里，随着越来越多的专业人士了解深度神经网络的力量，深度学习框架已经被广泛采用。

在本章中，我们将使用几个机器学习和深度学习框架以及 **SageMaker Python SDK** 来定义、训练和部署我们自己的模型。这将允许我们使用我们使用库和框架准备的任何定制模型，例如 **TensorFlow** 、 **Keras** 、 **scikit-learn** 和 **PyTorch** ，并将这些模型移植到 **SageMaker** 。这将使我们能够使用 **SageMaker** 特性和基础设施抽象能力，以及我们将使用上述框架准备的定制算法。我们将把我们定制的深度学习网络代码移植到 **SageMaker** ，并使用 **SageMaker Python SDK** 进行训练、部署和推理。我们将在定制神经网络的训练期间生成并使用合成训练数据集。训练步骤完成后，生成的模型将部署在**本地模式**中。

对于我们刚刚提到的每一个机器学习和深度学习库和框架，我们将准备 entrypoint 脚本，它将在训练步骤中使用。然后，entrypoint 脚本被用作来自 **SageMaker Python SDK** 的相应框架估算器的参数。之后，使用 SDK 中的 fit()、deploy()和 predict()函数，训练、部署和推理步骤照常进行。请注意，在本章的菜谱中，将这些框架与 **SageMaker Python SDK** 一起使用时，有一些细微但重要的区别。我们将在本章中介绍它们。最后，在调用 fit()和 deploy()函数后，我们将使用两种方法来帮助我们调试和修复一些最常见的错误。

我们将在本章中介绍以下配方:

*   为多个深度学习本地实验准备 **SageMaker 笔记本实例**
*   为深度学习实验生成合成数据集
*   准备入口点 **TensorFlow** 和 Keras 培训脚本
*   训练和部署一个 **TensorFlow** 和 **Keras** 模型，带有**亚马逊 SageMaker 本地模式**
*   准备入口点 **PyTorch** 培训脚本
*   准备入口点 **PyTorch** 推理脚本
*   训练和部署一个 **PyTorch** 模型，带有 **Amazon SageMaker 本地模式**
*   准备切入点 **scikit-learn** 培训脚本
*   使用 **Amazon SageMaker 本地模式**培训和部署 **scikit-learn** 模型
*   使用**本地模式**时调试磁盘空间问题
*   使用**本地模式**时调试容器执行问题

一旦我们完成了本章中的食谱，我们将能够轻松地处理更具体和更复杂的例子，包括稍后的**图像分类**、**时间序列预测**和**自然语言处理** ( **NLP** )需求。

我们开始吧！

# 技术要求

要执行本章中的配方，请确保您已准备好以下内容:

*   一个正在运行的 **Amazon SageMaker** 笔记本实例(例如 ml.t2.large)
*   一个亚马逊 S3 桶

如果您还没有准备好这些先决条件，请随时查看第一章 *中的*启动亚马逊 SageMaker 笔记本实例*和*准备亚马逊 S3 桶和线性回归实验的训练数据集*菜谱，使用亚马逊 SageMaker* 开始机器学习。

由于本章中的食谱涉及到一些代码，我们已经在这个资源库中提供了这些脚本和笔记本:[https://github . com/packt publishing/Machine-Learning-with-Amazon-sage maker-Cookbook/tree/master/chapter 03](https://github.com/PacktPublishing/Machine-Learning-with-Amazon-SageMaker-Cookbook/tree/master/Chapter03)。

![Figure 3.1 – Machine-Learning-with-Amazon-SageMaker-Cookbook GitHub repository
](img/B16850_03_01.jpg)

图 3.1-使用 Amazon-SageMaker-Cookbook 的机器学习 GitHub 知识库

如*图 3.1* 所示，我们将本章食谱的源代码组织在*Machine-Learning-with-Amazon-sage maker-Cookbook*GitHub 知识库的 Chapter03 目录中。

请点击以下链接查看动作视频中的相关代码:

[https://bit.ly/38QZnmn](https://bit.ly/38QZnmn)

# 为多个深度学习本地实验准备 SageMaker 笔记本实例

当在**亚马逊 SageMaker** 中进行深度学习实验时，需要注意的是，开发并用于训练和部署我们模型的定制脚本可以在运行的深度学习容器中使用**本地模式**进行测试。这使得我们可以立即修复定制脚本中的任何问题，而不必使用专用的 ML 训练实例。然而，使用深度学习容器涉及提取容器图像，这可能会导致磁盘空间问题。也就是说，我们首先准备好 **SageMaker 笔记本实例**并对其进行配置，以防止以后出现任何磁盘空间问题，这一点至关重要。

在这个菜谱中，我们将(1)修改笔记本实例的卷大小，(2)创建我们将在本章中存储笔记本和脚本的目录，以及(3)配置 Docker 服务，以帮助我们在提取容器映像和运行深度学习容器时防止潜在的磁盘空间问题。

重要说明

如果你想知道为什么在本章中我们不使用 **SageMaker Studio** ，重要的是要注意 SageMaker **本地模式**在 **SageMaker Studio** 中不受支持。如果没有**本地模式**，调试和修复我们定制的深度学习脚本会更加困难，也需要花费更多的时间。

## 正在准备中

我们需要做的只是一个现有的 **SageMaker 笔记本实例**。

## 怎么做

第一组步骤主要是更新 SageMaker 笔记本实例的设置:

1.  使用侧边栏导航到**笔记本实例**页面。
2.  Select the notebook instance we wish to modify. Under the **Actions** dropdown, click **Update settings**. If the selected notebook instance is still running, click **Stop** first and wait for about a minute or two before clicking **Update settings**. Make sure to save your progress first (if any) before stopping the notebook instance.![Figure 3.2 – Update settings option in the Actions dropdown
    ](img/B16850_03_02.jpg)

    图 3.2–操作下拉列表中的更新设置选项

    在*图 3.2* 中，我们可以在屏幕右上角的**动作**下拉列表中看到**更新设置**选项。

3.  Change the **Volume size in GB - optional** field to 200.![Figure 3.3 – Modifying the Notebook instance volume size
    ](img/B16850_03_03.jpg)

    图 3.3–修改笔记本实例卷大小

    我们在*图 3.3* 中有**笔记本实例设置**表单，我们可以在其中修改笔记本实例的卷大小。请注意，修改 **SageMaker 笔记本实例**的卷大小配置被限制为最多每 6 小时修改一次。

4.  Scroll down towards the end of the page and click **Update notebook instance**.![Figure 3.4 – Update notebook instance button
    ](img/B16850_03_04.jpg)

    图 3.4–更新笔记本实例按钮

    当我们向下滚动到页面末尾时，我们会在页面的右下角看到**更新笔记本实例**按钮。

5.  等待大约一两分钟，让笔记本实例更新，然后单击**开始**。一旦笔记本实例处于 **InService** 状态，点击 **Open Jupyter** 。
6.  A new tab similar to what is shown in *Figure 3.5* will open. After that, navigate to the / directory by clicking the folder icon.![Figure 3.5 – Root working directory
    ](img/B16850_03_05.jpg)

    图 3.5-根工作目录

    我们可以在*图 3.5* 中看到两个目录——包含克隆 GitHub 存储库的参考脚本和笔记本的机器学习-Amazon-SageMaker-Cookbook 目录和 my-experiments 目录，我们在学习本章的食谱时会将脚本和笔记本放在那里。

    下一组步骤集中于在 my-experiments 目录中准备首选的目录结构。这将使组织本章将要介绍的脚本和笔记本变得更加容易。

7.  Navigate to the /my-experiments directory. Click **New** and then **Folder**.![Figure 3.6 – Creating a new folder inside the my-experiments directory
    ](img/B16850_03_06.jpg)

    图 3.6–在 my-experiments 目录中创建新文件夹

    在*图 3.6* 中，我们可以看到**文件夹**选项位于下拉列表的末尾附近。请注意，一些其他内核选项已经从图像中移除，以保持图像更小。

8.  Select the **Untitled Folder** directory. After that, click the **Rename** button.![Figure 3.7 – Renaming the Untitled Folder
    ](img/B16850_03_07.jpg)

    图 3.7–重命名无标题文件夹

    重命名文件或目录应该很简单，如*图 3.7* 所示。

9.  A popup will appear. Rename the directory to chapter03\. After that, click the **Rename** button.![Figure 3.8 – Rename directory popup
    ](img/B16850_03_08.jpg)

    图 3.8-重命名目录弹出窗口

    在*图 3.8* 中，我们可以看到弹出窗口，在这里我们可以将目录名称更改为 chapter03。

10.  Navigate to the /my-experiments/chapter03 directory. Click the **New** button, which opens a drop-down list of options, and then select **Folder** from the list.![Figure 3.9 – Creating a new directory inside the chapter03 directory
    ](img/B16850_03_09.jpg)

    图 3.9–在 chapter03 目录中创建新目录

    我们可以在图 3.9 中看到，chapter03 目录中仍然没有文件和目录。一旦我们完成了这本书里的食谱，这个目录将包含(或多或少)GitHub 知识库*Machine-Learning-with-Amazon-sage maker-Cookbook***的第 03 章目录中的内容。请随意查看我们官方 GitHub 知识库中的源代码:[https://GitHub . com/packt publishing/Machine-Learning-with-Amazon-sage maker-Cookbook/tree/master/chapter 03](https://github.com/PacktPublishing/Machine-Learning-with-Amazon-SageMaker-Cookbook/tree/master/Chapter03)。**

***   Using a similar set of steps, create three directories inside the /my-experiments/chapter03 directory — PyTorch, SKLearn, and TensorFlow.![Figure 3.10 – Preferred directory structure
    ](img/B16850_03_10.jpg)

    图 3.10–首选目录结构

    在*图 3.10* 中，我们有 my-experiments/chapter03 目录中的首选目录结构。在每个目录中，我们将拥有入口点脚本文件(例如 tensorflow_script.py)和 Jupyter 笔记本，该笔记本使用 **SageMaker Python SDK** 来训练和部署定制神经网络模型。我们将在本章后面的每个食谱中介绍这些文件。

    注意

    什么是入口点文件？entrypoint 或 entry_point 文件是一个脚本，包含深度学习容器在训练和部署期间使用的自定义函数。当使用来自 **SageMaker Python SDK** 的深度学习框架估计器类时，初始化估计器对象时，entrypoint 脚本文件的路径被指定为 entry_point 参数的值。

    *   Click the **New** button to open the drop-down options. Select **Terminal** from the list of drop-down options, as shown in *Figure 3.11*.![Figure 3.11 – Creating a new Terminal
    ](img/B16850_03_11.jpg)

    图 3.11–创建新的终端

    一个我们可以运行终端命令的新标签将会打开。

    ![Figure 3.12 – New Terminal tab
    ](img/B16850_03_12.jpg)

    图 3.12–新端子选项卡

    该配方中的后续步骤将在该终端内执行。

    下一组步骤集中在为 **Docker** 服务更改数据目录。假设当前的 **Docker** 数据目录存储在一个存储容量相对有限的分区内，我们的目标是将这个数据目录更改为一个可以存储更多文件的分区上的新目录。

    *   运行以下命令创建一个新目录:

    ```
    sudo mkdir -p /home/ec2-user/SageMaker/docker-stuff
    ```

    *   Copy the contents of the /var/lib/docker directory to the new directory created:

    ```
    sudo cp -r /var/lib/docker/. /home/ec2-user/SageMaker/docker-stuff/ --verbose
    ```

    我们应该得到一组相似的日志，如图*图 3.13* 所示。

    ![Figure 3.13 – Verbose log messages while copying the contents of the 
    /var/lib/docker directory to the docker-stuff directory
    ](img/B16850_03_13.jpg)

    图 3.13–将/var/lib/docker 目录的内容复制到 docker-stuff 目录时的详细日志消息

    这里发生了什么事？由于我们在 cp 命令中使用了- verbose 选项，我们将看到每个被复制文件的日志条目。

    *   Create a backup copy of the docker configuration file:

    ```
    sudo cp /etc/sysconfig/docker /etc/sysconfig/docker.bak
    ```

    如果我们在更新配置文件时出错，这将允许我们使用备份副本将事情恢复到原始配置。

    *   Run the following command to open the docker configuration file using Vim. Vim is a text editor that can be used to edit files inside a Terminal:

    ```
    sudo vim /etc/sysconfig/docker
    ```

    我们应该看到在 Vim 中打开的 docker 配置文件，类似于图 3.14 中的所示。

    ![Figure 3.14 – The docker configuration file in Vim
    ](img/B16850_03_14.jpg)

    图 3.14–Vim 中的 docker 配置文件

    如果这是你第一次使用 Vim，一开始可能会有点吓人。如果您犯了错误，并且意外地保存了一个配置不正确的 docker 配置文件，您可以随意运行 sudo CP/etc/sys config/docker . bak/etc/sys config/docker 来将配置文件恢复到其原始配置。重新启动笔记本实例也会重置配置设置。

    *   类型:。*   类型设置 nu。*   Press the *Enter* key.![Figure 3.15 – Using line numbers in Vim
    ](img/B16850_03_15.jpg)

    图 3.15–在 Vim 中使用行号

    如*图 3.15* 中的所示，线号出现在端子的左侧。键入:set nu 只是在命令模式下设置行号。

    *   使用箭头键将光标放在第 9 行的末尾，就在双引号之前。之后，按下 *i* 。*   add-g/home/ec2-user/SageMaker/docker-stuff 紧跟在-default-ulimit nofile = 1024:4096 之后。执行完这一步后，第 9 行应该类似于下面的代码行:

    ```
    OPTIONS="--default-ulimit nofile=1024:4096 -g /home/ec2-user/SageMaker/docker-stuff"
    ```

    *   Press *Esc*. Type :. Then, type wq!. Press *Enter* afterward. This will save and exit Vim in the Terminal.

    小费

    只要运行 sudo wget https://raw . githubusercontent . com/packt publishing/Machine-Learning-with-Amazon-sage maker-Cookbook/master/chapter 03/docker-O/etc/sys config/docker，您也可以跳过前面涉及使用 Vim 命令的六个步骤。注意，前面的代码块包含一行语法为 sudo wget  <destination>的命令，不带点(。)在 docker 之后。该命令的作用是下载现有的 docker 配置文件，并将其替换为 GitHub 存储库中已有的配置。试着打开这个文件，看看我们这么说是什么意思:[https://raw . githubusercontent . com/packt publishing/Machine-Learning-with-Amazon-sage maker-Cookbook/master/chapter 03/docker](https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-with-Amazon-SageMaker-Cookbook/master/Chapter03/docker)。请注意，使用 Vim 仍然是首选方法，因为如果 **docker** 的后续版本有重大更改，GitHub 存储库中的 Docker 配置文件可能无法工作。</destination>

    *   Run the following command to check whether we have updated the file successfully:

    ```
    cat /etc/sysconfig/docker
    ```

    我们应该看到类似于图 3.16 中所示的 docker 配置文件。

    ![Figure 3.16 – The updated docker configuration file
    ](img/B16850_03_16.jpg)

    图 3.16-更新的 docker 配置文件

    在*图 3.16* 中，我们可以在/etc/sysconfig/docker 中看到想要的配置。

    *   Run the following command to restart the docker service:

    ```
    sudo service docker restart
    ```

    运行前面的命令将重启 Docker 服务，如图*图 3.17* 所示。** 

**![Figure 3.17 – Restarting the Docker service
](img/B16850_03_17.jpg)

图 3.17–重新启动 Docker 服务

现在我们已经重启了服务，我们在/etc/sysconfig/docker 配置文件中所做的修改现在应该生效了。

重要说明

注意，当重启 **SageMaker 笔记本实例**时，/etc/sysconfig/docker 文件恢复到 AWS 最初配置的状态。如果 SageMaker notebook 实例已经被重启，那么可以对 docker 配置文件随意重复最后一组步骤。另一个选择是使用**生命周期配置脚本**来自动化这个过程，我们将在这个配方的*还有更多……*部分讨论。

## 它是如何工作的...

该配方由三部分组成——卷大小修改、本章中即将出现的配方的 Jupyter 笔记本目录设置和 Docker 配置更新。

这个方法的第一部分包括增加笔记本实例的卷大小。鉴于我们将在本章的后续食谱中下载几个 Docker 容器映像，我们肯定需要比我们在第 1 章 *【使用 Amazon SageMaker* 开始机器学习】中最初配置的更多的磁盘空间。容器映像的大小很可能在 1 GB 左右或更大。fit()函数下载并使用容器映像进行训练。同样，deploy()函数下载并利用容器映像进行推理。

该配方的第二部分更侧重于本章的目录设置。由于我们将运行涉及不同深度学习框架的不同实验，因此这些实验中使用的源代码和文件彼此不冲突非常重要。这也将使事情更容易管理，因为我们将使用不同的笔记本和入口点脚本文件。

该方法的最后一部分更侧重于更新 **Docker** 配置文件。这一点非常重要，因为修改 SageMaker notebook 实例的卷大小不足以防止磁盘空间问题的发生。当我们使用 docker pull 命令时，容器图像被提取到/var/lib/docker 数据目录中。该目录位于可用存储空间有限的卷中。增加卷大小的修改实际上影响了安装在/home/ec2-user/SageMaker 上的另一个卷。如果我们运行 df -h /var/lib/docker，我们将得到类似于图 3.18 所示的结果。

![Figure 3.18 – Results of running df -h /var/lib/docker
](img/B16850_03_18.jpg)

图 3.18–运行 df -h /var/lib/docker 的结果

正如我们在*图 3.18* 中看到的，我们可以看到/中还有大约 9.7 GB 的剩余存储空间。这意味着，如果我们执行一些实验，并使用**本地模式**进行训练和部署，我们可能会很快遇到磁盘空间问题，因为我们会在这个过程中提取一定数量的大约 1 GB 的容器映像。如果我们运行 df-h/home/ec2-user/sage maker/docker-stuff，我们会得到类似于*图 3.19* 所示的结果。

![Figure 3.19 – Results of running df -h /home/ec2-user/SageMaker/docker-stuff
](img/B16850_03_19.jpg)

图 3.19–运行 df-h/home/ec2-user/SageMaker/docker-stuff 的结果

正如我们在*图 3.19* 中看到的，我们可以看到，我们只使用了大约 11%的总大小，我们可以存储在/home/ec2-user/SageMaker 中。一旦我们将/home/ec2-user/sage maker/docker-stuff 目录设置为新的数据目录，容器图像将自动提取到这里。这意味着我们可以用 **SageMaker 本地模式**进行多个实验，而不必太担心磁盘空间问题。

## 还有更多...

假设在重启 SageMaker notebook 实例时重新设置了这个方法中引入的配置更改，那么每次启动 SageMaker notebook 实例时都要手动执行这些步骤，这既耗时又容易出错。

我们可以通过使用**生命周期配置脚本**中的来自动完成这个配方中的一些步骤，而不是手动完成。使用生命周期配置脚本，当创建或启动 **SageMaker 笔记本实例**时，将执行指定脚本中的命令。

这里有一个示例脚本，我们可以用它来自动执行本食谱的*如何做…* 部分中讨论的步骤:

```
sudo mkdir -p /home/ec2-user/SageMaker/docker-stuff
sudo cp -r /var/lib/docker/. /home/ec2-user/SageMaker/docker-stuff/ --verbose
sudo cp /etc/sysconfig/docker /etc/sysconfig/docker.bak
sudo wget https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-with-Amazon-SageMaker-Cookbook/master/Chapter03/docker -O /etc/sysconfig/docker
sudo service docker restart
```

我们把这些命令放在哪里？我们可以通过使用侧栏(**笔记本** > **生命周期配置**)导航到**生命周期配置**页面，并单击**创建配置**按钮来创建生命周期配置。

![Figure 3.20 – Create lifecycle configuration form
](img/B16850_03_20.jpg)

图 3.20–创建生命周期配置表单

在*图 3.20* 表单中，我们可以指定生命周期配置的名称以及启动笔记本实例时要执行的脚本。创建生命周期配置后，我们可以简单地停止并修改我们正在使用的 **SageMaker 笔记本实例**，并将我们刚刚创建的生命周期配置链接到该笔记本实例。

![Figure 3.21 – Customizing the notebook environment with a lifecycle configuration 
](img/B16850_03_21.jpg)

图 3.21–使用生命周期配置定制笔记本电脑环境

当 SageMaker notebook 实例启动时，我们可以在 **CloudWatch Logs** 中查看生命周期配置脚本产生的信息和错误日志。

![Figure 3.22 – CloudWatch log events generated by the lifecycle configuration script
](img/B16850_03_22.jpg)

图 3.22–生命周期配置脚本生成的 CloudWatch 日志事件

在*图 3.22* 中，我们可以看到**生命周期配置脚本**已经生成了一组日志，这些日志与我们手动执行脚本中的语句时的日志相似。请随意使用生命周期配置脚本，因为这些脚本将帮助您自动化许多任务，并在这种情况下派上用场。通过使用生命周期配置脚本，我们可以执行更多实际的自动化工作。更多信息请参考[https://docs . AWS . Amazon . com/sage maker/latest/DG/notebook-life cycle-config . html](https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-lifecycle-config.html)。

# 为深度学习实验生成合成数据集

**合成数据生成**是以编程方式生成人工数据的过程，目的是帮助数据科学家和机器学习工程师测试不同的算法，并在不使用真实收集的数据的情况下执行机器学习实验。由于我们将使用神经网络和深度学习框架，我们将需要一个可接受的大数据集。我们在 [*第 1 章*](B16850_01_Final_ASB_ePub.xhtml#_idTextAnchor020) *，* *使用亚马逊 SageMaker 的机器学习入门*中的数据集只有 20 条记录，肯定不适合本章的食谱。在这个菜谱中，我们将使用定制的合成数据生成器生成训练、验证和测试虚拟数据，并将这些数据集存储在**亚马逊 S3** 中。

重要说明

为什么要生成和使用合成数据集？使用合成数据集将使我们能够更专注于我们正在处理的任务，因为我们可以简单地生成一个最少的合成数据集来帮助我们演示一个概念或技术。有时，当处理**概念验证** ( **PoC** )代码时，使用真实数据集可能会使事情变得有点复杂，因为使用这些可能需要一些关于所使用的数据集的先决知识，尤其是数据集中的列和记录。使用合成数据集将帮助我们避免任何额外的数据清理和数据处理步骤，以便我们可以直接使用本书中的不同解决方案。不要担心，因为我们会分享一些笔记和例子，说明本书中的某些方法和解决方案是如何在实际数据集上使用的。

在完成这个配方后，我们将能够继续使用我们准备的合成数据 ts 执行几个涉及不同机器学习和深度学习框架的机器学习实验。

## 准备就绪

该配方上接配方*为多次深度学习准备 SageMaker 笔记本实例 g 本地实验*。

## 怎么做

我们将在运行于 **Amazon SageMaker 笔记本实例**中的 Jupyter 笔记本中运行以下步骤。我们将从定义 formula()函数开始，它将作为我们的示例合成数据生成器:

1.  Navigate to the /my-experiments/chapter03 directory in the SageMaker notebook instance. Create a new notebook using the conda_python3 kernel by clicking on **New** and then choosing **conda_python3** from the drop-down options.![Figure 3.23 – Creating a new Jupyter notebook using the conda_python3 kernel
    ](img/B16850_03_23.jpg)

    图 3.23–使用 conda_python3 内核创建一个新的 Jupyter 笔记本

2.  Import numpy:

    ```
    import numpy as np
    ```

    **NumPy** 是一个库，它有一个函数和实用程序的集合，帮助机器学习实践者处理数字数据和数组。在这个方法中，我们将利用 NumPy 中的 random.randint()、vectorize()和 random.normal()函数来生成合成数据集。

3.  定义 formula()函数，如下面的代码块所示:

    ```
    def formula(x):     if x >= -2000:         return x     else:         return -x – 4000
    ```

4.  Test whether the function formula() works by running the following line of code:

    ```
    formula(100)
    ```

    接下来的几个步骤集中在使用前面步骤中准备的公式函数从 x 生成 y 的值。

    ![Figure 3.24 – Generating a synthetic dataset using a formula and noise functions   
    ](img/B16850_03_24.jpg)

    图 3.24–使用公式和噪声函数生成合成数据集

    *图 3.24* 向展示了 y 值是通过将 x 值作为输入传递给 formula()函数并向其添加噪声而获得的。

5.  Prepare the data generation function called generate_synthetic_data(). Internally, it uses the formula() function previously defined and adds a bit of noise using np.random.normal():

    ```
    def generate_synthetic_data(n_samples=1000, 
                                start=-5000, 
                                end=5000):
        np.random.seed(42)
        x = np.random.randint(
            low=start, 
            high=end,                            
            size=(n_samples,)).astype(int)
        y = np.vectorize(formula)(x) + \
            np.random.normal(150, 150, n_samples) 
        return (x, y)     
    ```

    函数接受一个函数，并使它接受和返回 numpy 数组。也就是说，我们的接受单个值并返回单个值的 formula()函数将被矢量化，然后能够对更多的值进行运算。

    小费

    有关 np.vectorize()的更多信息，请随意查看此链接:[https://numpy . org/doc/stable/reference/generated/numpy . vectorize . html](https://numpy.org/doc/stable/reference/generated/numpy.vectorize.html)

6.  接下来，我们使用之前准备的 generate _ synthethic _ data()函数，将数组结果分别赋给 X 和 y:

    ```
    X, y = generate_synthetic_data()
    ```

7.  Render the scatterplot using matplotlib:

    ```
    from matplotlib import pyplot
    pyplot.rcParams["figure.figsize"] = (10,8)
    pyplot.scatter(X,y,s=1)
    pyplot.show()
    ```

    在*图 3.25* 中，我们有一个合成数据集的散点图。在接下来的几个食谱中，我们的目标是定义和训练我们的定制神经网络，看看它如何适应我们在这个食谱中生成的数据:

    ![Figure 3.25 – Scatterplot of the synthetic dataset
    ](img/B16850_03_25.jpg)

    图 3.25–合成数据集的散点图

    接下来的两个步骤集中在对生成的数据集执行训练测试分割，以准备训练、验证和测试集。

8.  Use the train_test_split() function from sklearn to split the generated dataset into training, validation, and test sets:

    ```
    from sklearn.model_selection import train_test_split

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=0)

    X_train, X_validation, y_train, y_validation = train_test_split(
        X_train, y_train, test_size=0.2, random_state=0)

    print(X_train.shape)
    print(X_validation.shape)
    print(X_test.shape)
    ```

    这应该会给我们输出(640)，(160)，(200)。

    ![Figure 3.26 – Dividing the generated synthetic dataset into train, validation, and test datasets
    ](img/B16850_03_26.jpg)

    图 3.26–将生成的合成数据集分为训练、验证和测试数据集

    *图 3.26* 向我们快速展示了数据集如何包含由 generate _ synthethic _ data()函数生成的所有记录，并将其划分为训练集、验证集和测试集。

9.  如果 tmp 目录还不存在，使用 mkdir 命令创建它:

    ```
    !mkdir -p tmp
    ```

10.  Import the pandas library:

    ```
    import pandas as pd
    ```

    **pandas** (面板数据)库是 Python 中用于数据分析的库。它提供了 DataFrame 和 Series 类，帮助机器学习从业者分析和操作数据。

11.  Use the to_csv() function to generate a CSV file with the first column containing y values and the second column containing X values:

    ```
    df_all_data = pd.DataFrame({ 'y': y, 'x': X})
    df_all_data.to_csv('all_data.csv', header=False, index=False)
    ```

    请注意，header 和 index 参数被设置为 False，这意味着生成的 CSV 文件将只包含用逗号分隔的 y 和 x 值。

    ![Figure 3.27 – The NumPy arrays containing the x and y values are first combined into a single DataFrame object before being saved into a CSV file with the y values in the first column.
    ](img/B16850_03_27.jpg)

    图 3.27–包含 x 和 y 值的 NumPy 数组首先组合成一个 DataFrame 对象，然后保存到第一列中包含 y 值的 CSV 文件中。

    *图 3.27* 向展示了 x 和 y **NumPy** 数组如何在导出到 CSV 文件之前组合成一个 DataFrame 对象。注意 y 值在第一列。类似于本书中的一个早期食谱，标题和索引被从 CSV 文件中删除。

12.  We perform the same set of steps on the training, validation, and test datasets:

    ```
    df_training_data = pd.DataFrame({ 'y': y_train, 'x': X_train})
    df_training_data.to_csv('training_data.csv', 
        header=False, index=False)
    df_validation_data = pd.DataFrame({ 'y': y_validation, 'x': X_validation})
    df_validation_data.to_csv('validation_data.csv', 
        header=False, index=False)
    df_test_data = pd.DataFrame({ 'y': y_test, 'x': X_test})
    df_test_data.to_csv('test_data.csv', 
        header=False, index=False)
    ```

    接下来的几个步骤主要是将生成的 CSV 文件上传到 S3 存储桶。将这些文件存储在 S3 存储桶中，将允许我们在训练和评估模型时在后续的配方中使用它们。

13.  将 **S3** 桶值设置为您的 AWS 帐户中存在的桶。使用配方*中创建的桶名准备亚马逊 S3 桶和线性回归实验的训练数据集*来自 [*第 1 章*](B16850_01_Final_ASB_ePub.xhtml#_idTextAnchor020) 、*使用亚马逊 SageMaker 开始机器学习* :

    ```
    s3_bucket = "<insert s3 bucket name here>" prefix = "chapter03"
    ```

14.  我们使用 **AWS CLI** 将生成的包含所有生成记录的数据 CSV 文件保存到 S3:

    ```
    !aws s3 cp tmp/all_data.csv \ s3://{s3_bucket}/{prefix}/synthetic/all_data.csv
    ```

15.  Next, we use the same approach to upload the training, validation, and test datasets to **S3**:

    ```
    !aws s3 cp tmp/training_data.csv \
    s3://{s3_bucket}/{prefix}/synthetic/training_data.csv
    !aws s3 cp tmp/validation_data.csv \
    s3://{s3_bucket}/{prefix}/synthetic/validation_data.csv
    !aws s3 cp tmp/test_data.csv \
    s3://{s3_bucket}/{prefix}/synthetic/test_data.csv
    ```

    在这一点上，我们的 CSV 文件应该上传到 S3 桶。

现在让我们看看这个食谱是如何运作的！

## 它是如何工作的...

在这个配方中，我们使用定制的合成生成器函数生成了 x 和 y 的样本值。我们使用 np.random.normal()添加了一些噪声。如果没有噪声，合成数据生成函数将为每个 x 值生成相同的 y 值。为了确保我们稍后将训练和部署的模型的稳健性，我们添加了一些噪声。然后，我们执行了两次训练和测试分割，这样我们将有三个数据集:训练集、验证集和测试集。我们将 x 和 y 值保存在相应的 CSV 文件中，并上传到 S3。上传到 S3 的文件将用于后续的食谱。

我们在这个配方中定义的公式()函数基本上是一个分段函数，由几个不同间隔应用的子函数定义。这个分段函数有以下几个部分:

*   f(x) = x 如果 x >= -2000
*   f(x) = - x - 4000 否则

如本配方*如何做…* 部分的散点图所示，使用简单的线性回归模型可能不容易解决该数据集。在本章中，我们将使用定制的神经网络，使用不同的机器学习和深度学习框架与**亚马逊 SageMaker** 一起训练和部署一个模型，该模型概括和表示生成的数据。

# 准备 entrypoint TensorFlow 和 Keras 培训脚本

**TensorFlow** 是一个流行的用于机器学习的开源软件库。**另一方面，Keras**是一个用户友好的高级神经网络库，有助于更快地建立和训练模型。

在这个菜谱中，我们将定义一个自定义的 **TensorFlow** 和 **Keras** 神经网络模型，并准备 entrypoint 训练脚本。在下一个配方中，我们将使用来自 **SageMaker Python SDK** 的 TensorFlow 估计器类，并将该脚本作为训练和部署的入口点参数。如果您计划从本地机器中迁移您的自定义 **TensorFlow** 和 **Keras** 神经网络代码，并使用 **SageMaker** 平台执行培训和部署，那么这的方法(以及下一个)适合您！

## 准备就绪

这种方法延续了*生成用于深度学习实验*的合成数据 aset。

## 怎么做

该配方中的指令主要用于准备入口点脚本。让我们首先在 Jupyter notebook 实例中创建一个名为 tensorflow_script.py 的空文件，然后继续下一组步骤:

1.  Navigate to the my-experiments/chapter03/TensorFlow directory.![Figure 3.28 – Navigating to the my-experiments/chapter03/TensorFlow directory
    ](img/B16850_03_28.jpg)

    图 3.28–导航至 my-experiments/chapter 03/tensor flow 目录

    我们可以在*图 3.28* 中看到 **TensorFlow** 目录。请记住，我们在配方*中创建了这个目录，为多个深度学习本地实验*准备 SageMaker 笔记本实例。

2.  Create a new file – tensorflow_script.py. Open the file using the editor provided in Jupyter.![Figure 3.29 – Creating a new file
    ](img/B16850_03_29.jpg)

    图 3.29–创建新文件

    在我们开始在 tensorflow_script.py 中编写代码之前，让我们快速检查一下 entryscript 文件的外观。如果您想知道它为什么被称为 entryscript 文件，我们将在初始化 TensorFlow estimator 对象时将 tensorflow_script.py 文件名作为值传递给参数 entry_script。

    ![Figure 3.30 – TensorFlow entrypoint script code structure
    ](img/B16850_03_30.jpg)

    图 3.30–tensor flow 入口点脚本代码结构

    *图 3.30* 向我们展示了在我们完成该食谱中的说明后，我们的训练脚本与我们的定制 **TensorFlow** 模型的外观。

3.  使用 shebang 行启动 Python 脚本:

    ```
    #!/usr/bin/env python
    ```

4.  添加以下代码行以导入 tensorflow、numpy 和 os Python 模块:

    ```
    import os import tensorflow as tf import numpy as np
    ```

5.  接下来，添加下面的代码行来导入顺序的和密集的:

    ```
    from tensorflow.keras import datasets, layers, models from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, BatchNormalization from numpy.random import seed
    ```

6.  定义 set_seed()函数，它将帮助我们在每次运行实验时生成相同的一组结果:

    ```
    def set_seed():     seed(42)     tf.random.set_seed(42)
    ```

7.  定义 load_data()函数，该函数加载一个目标 CSV 文件，提取 x 和 y 列，并将这些值作为函数输出返回:

    ```
    def load_data(training_data_location):     result = np.loadtxt(         open(training_data_location, "rb"),          delimiter=",")          y = result[:, 0]     x = result[:, 1]     return (x, y)
    ```

8.  Define the prepare_model() function that returns the model's architecture. In this example, we have prepared a model with an arbitrary network architecture using the **Keras** Sequential class:

    ```
    def prepare_model():
        model = Sequential([
            Dense(100, activation=tf.nn.leaky_relu, 
                       input_shape=[1]),
            Dense(100, activation=tf.nn.leaky_relu),
            Dense(100, activation=tf.nn.leaky_relu),
            Dense(100, activation=tf.nn.leaky_relu),
            Dense(100, activation=tf.nn.leaky_relu),
            Dense(100, activation=tf.nn.leaky_relu),
            Dense(100, activation=tf.nn.leaky_relu),
            Dense(1)
          ])

        model.compile(loss='mean_squared_error', 
                      optimizer='adam')
        return model
    ```

    顺序类有助于使用线性层堆栈准备模型。Dense 类帮助准备一个**密集层**(一个接受来自神经网络中前一层的所有神经元的输入的层)。要了解更多信息，请随时查看 https://keras.io/guides/sequential_model/的 **Keras** 开发者指南:[。](https://keras.io/guides/sequential_model/)

    请注意，我们只是使用一个虚拟神经网络，我们可以很容易地用其他 **TensorFlow** 和 **Keras** 模型来代替它。我们甚至可以加载预训练的 **TensorFlow** 模型，并使用**迁移学习**来构建新模型。

    重要说明

    注意prepare _ model()函数返回模型的初始版本，它还没有经过训练步骤。

9.  Prepare the main() function. This main() function will be called at the end of the script:

    ```
    def main(model_dir, train_path, val_path, 
             batch_size=200, epochs=2000):
        set_seed()

        model = prepare_model()
        model.summary()

        x, y = load_data(train_path)
        print("x.shape:", x.shape)
        print("y.shape:", y.shape)

        x_val, y_val = load_data(val_path)
        print("x_val.shape:", x_val.shape)
        print("y_val.shape:", y_val.shape)

        model.fit(x=x, 
                  y=y, 
                  batch_size=batch_size, 
                  epochs=epochs, 
                  validation_data=(x_val, y_val))    

        tf.saved_model.save(
            model, 
            os.path.join(model_dir, '000000001'))
    ```

    *图 3.31* 显示在 main()函数中执行的各组动作:

    ![Figure 3.31 – High-level set of actions performed inside the main() function
    ](img/B16850_03_31.jpg)

    图 3.31–在 main()函数中执行的高级动作集

10.  The model files saved using the tf.saved_mode.save() function will be loaded inside the container using **TensorFlow Serving**. Compared to the implementation using other frameworks in this chapter, there is no need to define a model_fn() function to load the architecture and weights of the serialized model. Execute the main() function if the script is run directly and not imported:

    ```
    if __name__ == "__main__":
        data_path = "/opt/ml/input/data"
        model_dir = "/opt/ml/model"
        train_csv = "train/training_data.csv"
        train_path = f"{data_path}/{train_csv}"
        val_csv = "validation/validation_data.csv"
        val_path = f"{data_path}/{val_csv}"

        main(model_dir=model_dir,
             train_path=train_path,
             val_path=val_path,
             batch_size=200,
             epochs=1000)
    ```

    小费

    你可以访问亚马逊 SageMaker 官方机器学习 Cookbook 中 tensorflow_script.py 文件的工作副本:[https://github . com/packt publishing/Machine-Learning-with-Amazon-SageMaker-Cookbook/blob/master/chapter 03/tensor flow/tensor flow _ script . py](https://github.com/PacktPublishing/Machine-Learning-with-Amazon-SageMaker-Cookbook/blob/master/Chapter03/TensorFlow/tensorflow_script.py)。

现在让我们看看这个食谱是如何工作的！

## 工作原理...

在下一个配方中，当从 **SageMaker Python SDK** 初始化 TensorFlow estimator 对象时，我们在该配方中准备的脚本的路径将是 entry_point 参数的值。入口点脚本应该执行以下关键步骤:

*   定义模型(架构)
*   从/opt/ml/input/data 目录加载训练数据
*   执行模型训练，并使用*超参数*配置训练作业
*   将模型(例如，参数和权重)保存在/opt/ml/model 目录中

在这个配方中，为了便于演示，我们使用了一个任意的神经网络架构，该架构使用了几个密集层。随意修改或替换该脚本中使用的模型。

entrypoint 训练脚本遵循与第二章 *中的训练脚本相同的假设，构建并使用您自己的算法容器映像，*在脚本执行之前已经有了几个环境变量、文件夹结构和文件(例如，训练输入文件)。请注意，一旦我们使用了 **SageMaker Python SDK** ，这个 entrypoint 脚本将在一个容器中执行，并且不会看到您在 SageMaker notebook 实例中准备的相同文件和目录。

如果我们需要在训练脚本中使用其他包，有几种方法可以使用。

第一个是在训练脚本中安装附加的依赖项。请参考以下实用程序函数的代码，它有助于我们安装和导入附加的依赖项:

```
def install_and_load(target):
    sequence = [executable, "-m", 
                "pip", "install", target]
    subprocess.call(sequence)
    return importlib.import_module(target)
```

这个函数利用 subprocess.call()函数来执行 Bash 命令，并使用 pip install 命令安装软件包。我们将在下一章看到这一点。

第二种方法包括使用 requirements.txt 文件，将其放在与 entrypoint 训练脚本相同的目录中，并在从 **SageMaker Python SDK** 初始化 TensorFlow 估计器时指定 source_dir 参数。拿注意，这只对 **TensorFlow** 版本 *1.15.2+* 使用 *Python 3.7* 有效。

## 还有更多...

我们已经从*如何做…* 一节中排除了加载环境变量，但是这里有一些环境变量， **SageMaker** 将在脚本的容器训练环境中自动设置:

*   SM _ MODEL _ DIR–应该保存模型工件的目标路径。设置为/opt/ml/model。
*   SM _ NUM _ GPU–反映可用的 GPU 数量。如果没有可用的 GPU，则设置为 0。
*   SM _ INPUT _ DIR–加载输入文件和配置数据的目标路径。设置为/opt/ml/input。
*   SM _ INPUT _ DATA _ CONFIG–包含来自 inputdataconfig.json 的输入数据配置 JSON 值。在这里我们可以检查输入数据通道的模式是管道还是文件。
*   SM _ INPUT _ CONFIG _ DIR–加载配置数据的目标路径。设置为/opt/ml/input/config。
*   SM _ OUTPUT _ DATA _ DIR–保存输出文件的目标路径，不包括模型工件。
*   SM _ CHANNEL _<channel name="">–包含 fit()函数中指定的输入数据的目录的目标路径。如果我们使用 fit({"a": s3_input(…)，" b": s3_input(…)}，那么我们有两个通道，A 和 B。这意味着我们将有包含/opt/ml/input/data/a 和/opt/ml/input/data/b 值的 SM_CHANNEL_A 和 SM_CHANNEL_B 环境变量。</channel>

如果要加载存储在这些环境变量中的值，可以使用下面的代码块:

```
import os
model_directory = os.environ["SM_MODEL_DIR"]
print(model_directory)
```

现在让我们检查下一个关于训练和部署 **TensorFlow** entrypoint 脚本的配方，我们已经在这个配方中准备好了。

# 使用 SageMaker Python SDK 训练和部署 TensorFlow 和 Keras 模型

使用 **SageMaker** 执行定制 **TensorFlow** 和 **Keras** 模型的训练和部署相当简单。第 1 步涉及创建 entrypoint 脚本，其中定义和编码了我们的自定义神经网络和训练逻辑。第 2 步涉及使用该脚本作为来自 **SageMaker Python SDK** 的 TensorFlow 估计器的参数，以继续进行训练和部署步骤。

在本食谱中，我们将关注*步骤 2* ，并继续在 **SageMaker** 中训练和部署我们的自定义 **TensorFlow** 和 **Keras** 神经网络模型。如果您正在寻找*步骤 1* ，请随意检查之前的配方、*准备entry point tensor flow 和 Keras 培训脚本*。

## 准备就绪

这份食谱上接*准备入口点张量流和 Keras 训练脚本*。

## 怎么做

当初始化张量流估计器时，本配方中的说明集中于使用来自先前配方的自定义入口点训练脚本。初始化tensor flow 估计器后，我们将使用 fit()和 deploy()函数，然后使用 predict()函数来使用我们的定制神经网络进行推理:

1.  Navigate to the /my-experiments/chapter03/TensorFlow directory.![Figure 3.32 – Navigating to the TensorFlow directory
    ](img/B16850_03_32.jpg)

    图 3.32–导航至张量流目录

    正如我们在*图 3.32* 中看到的，这个目录中已经有一个名为 tensorflow_script.py 的文件，在后面的步骤中初始化 TensorFlow estimator 对象时，我们将使用 tensorflow_script.py 的路径作为 entry_point 参数的值。

2.  Click the **New** button to open a dropdown of notebook kernel options. Select conda_tensorflow_p36\.![Figure 3.33 – Creating a new Jupyter notebook using the conda_tensorflow_p36 kernel
    ](img/B16850_03_33.jpg)

    图 3.33–使用 conda_tensorflow_p36 内核创建新的 Jupyter 笔记本

    如图*3.33*中的所示，新 Jupyter 笔记本必须与 tensorflow_script.py 文件在同一个目录下。

3.  更新安装 **SageMaker 本地模式** :

    ```
    !pip install 'sagemaker[local]' --upgrade
    ```

4.  重新启动 Docker 服务以确保我们不会因为之前运行的容器而遇到任何问题:

    ```
    !sudo service docker restart
    ```

5.  Delete all Docker container images to free up a bit of space:

    ```
    !docker rmi -f $(docker images -a -q)
    ```

    随着一些准备和安装工作的完成，我们将继续关注准备 **SageMaker** 培训工作的先决条件和参数的步骤:

6.  Set the s3_bucket variable containing the S3 bucket path. Use the bucket created in the recipe *Preparing the Amazon S3 bucket and the training dataset for the linear regression experiment* from [*Chapter 1*](B16850_01_Final_ASB_ePub.xhtml#_idTextAnchor020)*, Getting Started with Machine Learning Using Amazon SageMaker*:

    ```
    s3_bucket = '<insert bucket name here>'
    prefix = 'chapter03'
    ```

    注意我们的training _ data . CSV 文件应该已经存在于 S3 存储桶中，其路径应该是:

    ```
    s3://<S3 BUCKET NAME>/<PREFIX>/synthetic/training_data.csv
    ```

    validation_data.csv 文件也是如此。

7.  将训练和验证数据集的 S3 路径分别存储到 train_s3 和 val_s3 变量:

    ```
    train_s3 = \ f"s3://{s3_bucket}/{prefix}/synthetic/training_data.csv"  val_s3 = \ f"s3://{s3_bucket}/{prefix}/synthetic/validation_data.csv"  s3_output_location = \ f"s3://{s3_bucket}/{prefix}/output/tensorflow/"
    ```

8.  使用 TrainingInput 类准备 content_type 设置为“text/csv”的输入参数:

    ```
    from sagemaker.inputs import TrainingInput train_input = TrainingInput(train_s3, content_type="text/csv") val_input = TrainingInput(val_s3, content_type="text/csv")
    ```

9.  Import a few prerequisites to run the training job. In addition to this, we will use LocalSession to initialize the sagemaker_session object. The LocalSession class allows us to use **local mode** in the training and deployment steps. As we will see in a later step, we will specify 'local' as the parameter value for instance_type when initializing the estimator to make us perform the training job locally in the SageMaker notebook instance:

    ```
    import sagemaker
    from sagemaker import get_execution_role
    from sagemaker.local import LocalSession
    sagemaker_session = LocalSession()
    sagemaker_session.config = {'local': {'local_code': True}}
    role = get_execution_role()
    ```

    接下来的几个步骤遵循标准的 SageMaker 培训和部署步骤。

10.  Initialize the TensorFlow estimator from the **SageMaker Python SDK**. Note that we set the instance_type parameter value to 'local':

    ```
    from sagemaker.tensorflow.estimator import TensorFlow
    estimator = TensorFlow(
        entry_point='tensorflow_script.py',
        output_path=s3_output_location,
        role=role,
        sesion=sagemaker_session,
        instance_count=1,
        instance_type='local',
        framework_version='2.1.0',
        py_version='py3')
    ```

    正如您可以看到的，我们没有指定脚本文件将在其中运行的容器映像。TensorFlow 估计器利用了 **SageMaker** 中一个预建的 Docker 深度学习容器映像。这意味着我们不必担心准备容器映像，因为 **SageMaker** 已经为我们准备好了。如果这还不够，我们还可以选择扩展现有的**预建 SageMaker Docker 映像**。

    注意

    另一种选择是从头开始构建和使用我们的自定义容器映像。可以随意查看 [*第二章*](B16850_02_Final_ASB_ePub.xhtml#_idTextAnchor061) 、*构建和使用自己的算法容器镜像*中的菜谱。

11.  Start the training job by running the fit() method:

    ```
    estimator.fit({'train': train_input, 'validation': val_input})
    ```

    *图 3.34* 向我们展示了使用 TensorFlow 估计器的 fit()函数几分钟后训练日志的样子。

    ![Figure 3.34 – Training logs after running the fit() function after initializing the TensorFlow estimator
    ](img/B16850_03_34.jpg)

    图 3.34–初始化张量流估计器后运行 fit()函数后的训练日志

    如果您在运行 fit()函数时遇到问题，您可以查看本章末尾的*调试使用本地模式时的磁盘空间问题*和*调试使用本地模式时的容器执行问题*。

    注意

    请注意，在使用本地模式和使用专用 ML 训练实例时，有一些关键区别。一个显著的区别是，当不使用**本地模式**时，训练作业名称被验证(例如，模式= ^[a-za-z0-9](-*[a-za-z0-9]){0,62})。考虑到训练作业名称的前缀有时可能源自 ECR 容器映像存储库名称，如果 ECR 存储库名称包含下划线(_)，则在使用专用 ML 实例时可能会遇到验证错误。这意味着当使用 ML 训练实例时，由于这种潜在的验证阻塞，在本地模式下工作的训练实验可能不一定马上工作。要解决此问题，只需指定一个有效的 base_job_name 值来替换默认的培训作业名称前缀。

12.  After the training job completes, deploy the model using the deploy() function:

    ```
    predictor = estimator.deploy(
        initial_instance_count=1, 
        instance_type='local'
    )
    ```

    注意在使用 deploy()函数时，我们将‘local’指定为【instance _ type 的参数值。当调用 deploy()函数时，一个为 **TensorFlow 服务**而优化的容器(一个面向生产环境的灵活、高性能的 ML 模型服务系统)被启动。之后，配置并启动一个 **TensorFlow 服务**进程来运行模型，并在容器内部启动一个 HTTP 服务器。这个 HTTP 服务器用 SageMaker API 桥接了 **TensorFlow 服务器**进程，尤其是在调用 InvokeEndpoint API 时。有关 **TensorFlow 服务**的更多信息，请随时查看[https://www.tensorflow.org/tfx/guide/serving](https://www.tensorflow.org/tfx/guide/serving)。

    小费

    如果您想知道在运行本地预测端点时使用哪个容器映像，只需运行！码头集装箱显示运行中的集装箱。我们应该得到一个等于或类似于 763104351884 . dkr . ECR . us-east-1 . Amazon AWS . com/tensor flow-inference:2 . 1 . 0-CPU 的值。有关更多信息，请查看位于[https://github . com/AWS/deep-learning-containers/blob/master/available _ images . MD](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)的可用图片。

13.  Perform a few test predictions using the predict() function:

    ```
    input = {
      'instances': [[100], [200]]
    }
    result = predictor.predict(input)
    result
    ```

    我们应该得到一组类似于*图 3.35* 中的日志和输出值:

    ![Figure 3.35 – Logs and output after using the predict() function in local mode
    ](img/B16850_03_35.jpg)

    图 3.35–在本地模式下使用 predict()函数后的日志和输出

    正如在*图 3.35* 中看到的，当在本地模式下调用predict()函数时，一个正在运行的容器内的/invocations 端点被 POST 请求触发。处理完请求后，predict()函数将响应返回给用户。

14.  创建一个名为 tmp:

    ```
    !mkdir -p tmp
    ```

    的新目录
15.  Download the data CSV file to the tmp directory:

    ```
    all_s3 = f"s3://{s3_bucket}/{prefix}/synthetic/all_data.csv" 
    !aws s3 cp {all_s3} tmp/all_data.csv
    ```

    接下来的几个步骤集中在检查代表“预测线”的一组 **X** 值的 **Y** 预测值。

    ![Figure 3.36 – The predict() function using the custom neural network deployed in the inference endpoint to predict the y values from the x input values
    ](img/B16850_03_36.jpg)

    图 3.36–predict()函数使用部署在推理端点中的自定义神经网络从 x 输入值预测 y 值

    在*图 3.36* 中，我们使用自定义**张量流**和 **Keras** 神经网络从 **X** 值中生成预测的 **Y** 值，我们对其进行了训练并部署到推理端点。

16.  使用 read_csv()函数将 CSV 文件的内容从 pandas 加载到 x 和 y 变量:

    ```
    import pandas as pd all_data = pd.read_csv("tmp/all_data.csv", header=None) x = all_data[[1]].values y = all_data[[0]].values
    ```

17.  Prepare the line_x array variable that contains the x values between -5000 and 5000:

    ```
    from numpy import arange
    line_x = arange(-5000, 5000, 10)
    ```

    来自 **NumPy** 的 arange()函数在指定的间隔内(例如，在-5000 和 5000 之间)生成一个均匀间隔的值的数组。

18.  Perform predictions using the deployed model using the predict() function with the x array values (reshaped into a 1D array) from the previous step:

    ```
    input = { 'instances': line_x.reshape(-1, 1) }
    result = predictor.predict(input)
    result
    ```

    我们应该得到一组类似于图 3.37 中的值:

    ![Figure 3.37 – Results after using the predict() function on a range of x values
    ](img/B16850_03_37.jpg)

    图 3.37–对 x 值范围使用 predict()函数后的结果

    在*图 3.37* 中，我们可以看到预测的一个数组内部的 y 值。这个数组嵌套在一个字典中，预测作为键，数组作为值。

19.  Store the predicted values inside line_y:

    ```
    import numpy as np
    line_y = np.array(result['predictions']).flatten()
    line_y
    ```

    我们应该得到一组类似于*图 3.38* 中的日志和输出值:

    ![Figure 3.38 – Array of values inside line_y after using the flatten() function
    ](img/B16850_03_38.jpg)

    图 3.38–使用 flatten()函数后 line_y 内部的值数组

    在*图 3.38* 中，我们有一个扁平的值数组，存储在 line_y 变量中。

20.  Finally, we render the line on top of the scatterplot of the original dataset using matplotlib:

    ```
    from matplotlib import pyplot
    pyplot.plot(line_x, line_y, 'r')
    pyplot.scatter(x,y,s=1)
    pyplot.show()
    ```

    在*图 3.39* 中，我们可以看到预测线以及数据集的散点图，以便快速查看预测值与实际值的匹配情况。

    ![Figure 3.39 – Prediction line using the TensorFlow and Keras custom neural network over the scatterplot showing the actual values from the generated synthetic dataset.
    ](img/B16850_03_39.jpg)

    图 3.39–散点图上使用 TensorFlow 和 Keras 自定义神经网络的预测线，显示生成的合成数据集的实际值。

    正如我们在*图 3.39* 中看到的，我们定制的 TensorFlow 模型似乎在数据集的概化方面做得很好。我们可以执行额外的模型评估步骤来恰当地测量 m 模型的性能。我们将跳过这一步，以防止这个食谱太长。

21.  Delete the endpoint after running the experiment:

    ```
    predictor.delete_endpoint()
    ```

    这个应该用本地推理端点停止正在运行的容器。

让我们看看这是如何工作的！

## 工作原理...

使用 **SageMaker Python SDK** 的 **TensorFlow** 支持，我们能够直接使用 **TensorFlow** entrypoint 脚本，而不必构建实验所用的容器映像和容器，因为 SDK 为我们抽象了这些。

让我们讨论使用张量流估计量时的不同选项和参数。在这个配方的示例中，我们指定了以下参数——entry _ point、role、instance_count、instance_type、framework_version 和 py_version。这些选项中的大多数直接映射到估计器类的可用选项。使用相同的 TensorFlow 估计器类，我们可以指定分布、input_mode 和其他选项，我们稍后将讨论这些选项。

小费

如果我们想在 entrypoint 脚本中使用其他 Python 包呢？对于使用 Python 3.7 或更高版本的 TensorFlow 版本 1.15.2，以及 TensorFlow 版本 2.2 或更高版本，我们只需在存储 entrypoint 脚本的同一目录中创建并包含 requirements.txt 文件。之后，在初始化 TensorFlow 估计器时，我们将目录的路径指定为 source_dir 参数的值。有关更多信息，请随意查看[https://sagemaker . readthe docs . io/en/stable/frameworks/tensor flow/using _ TF . html # use-third-party-libraries](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#use-third-party-libraries)。

**使用扩展的预构建容器映像**:如果我们已经扩展了一个预构建框架容器映像，并且我们想要使用该容器映像而不是 **SageMaker Python SDK** 自动为我们设置的预构建容器映像，我们可以指定 image_name 参数并指定 **Amazon ECR URI** ，类似于我们初始化一个评估器对象的方式。使用这种方法时，不要忘记将 script_mode 设置为 True。

**本地模式**:在训练和部署步骤中，我们可以选择通过将 instance_type 设置为 Local 来“本地”执行这些步骤。默认情况下，培训和部署包括在 Jupyter notebook 实例之外构建新的 ML 实例，每个实验需要几分钟才能完成。当使用**本地模式**选项时，整个过程可以在更短的时间内完成，因为不需要在后台启动和配置任何实例和硬件。这允许我们以更快的速度测试、调试和修改 **TensorFlow** entrypoint 脚本。

重要说明

使用**本地模式**有其自身的局限性。这包括只处理相对较轻的培训和部署工作负载，因为只有正在运行的 SageMaker notebook 实例支持的工作负载才能在本地模式下执行。例如，在一个 ml.t2.medium SageMaker 笔记本实例中，很难运行一个涉及微调 **DistilBERT** 模型的培训作业，因为我们需要一个更大的笔记本实例来本地支持这个培训作业。

**部署现有模型**:在这个方法中，我们已经在培训步骤之后执行了部署步骤。如果我们的 S3 存储桶中已经存在序列化的模型文件，我们可以直接执行部署步骤。我们可以使用来自 **SageMaker Python SDK** 的 TensorFlowModel，如下面的代码块所示:

```
from sagemaker.tensorflow import TensorFlowModel
model = TensorFlowModel(model_data='s3://<insert S3 bucket name + prefix>/model.tar.gz', ...)
predictor = model.deploy(...)
```

TensrFlowModel 允许我们使用可选的自定义入口点推理脚本。使用定制的 entrypoint 推理脚本，我们将能够实现和修改默认的预处理和后处理处理程序，以便改变输入和输出数据的处理方式。

请注意，即使我们在**本地模式**下执行了训练步骤，模型文件仍然会上传到亚马逊 S3 目标输出位置。类似于我们在 [*第一章*](B16850_01_Final_ASB_ePub.xhtml#_idTextAnchor020) ，*使用亚马逊 SageMaker 开始机器学习，*我们可以在运行 fit()函数后，使用 estimator.model_data 中的 S3 路径值定位并下载 model.tar.gz 文件。一旦我们下载了model.tar.gz 文件并且提取了它的内容，我们就可以使用 **TensorFlow** 和**Keras**Python API 来加载和分析模型。要了解更多信息，请点击这个链接，它包含了关于这个主题的更多信息:[https://www.tensorflow.org/tutorials/keras/save_and_load](https://www.tensorflow.org/tutorials/keras/save_and_load)。

## 还有更多...

使用来自 **SageMaker Python SDK** 的 TensorFlow 估算器，我们可以设置几个额外的选项。

**分布式训练**:如果我们计划使用**分布式训练**并且 instance_count 大于 1，我们可以使用参数服务器进行训练，也可以使用名为 **Horovod** 的分布式训练框架进行训练。

当使用参数服务器进行训练时，我们会在初始化 TensorFlow 估计器时添加以下代码:

```
distribution={
    "parameter_server": {"enabled": True}
}
```

否则，如果我们想使用 **Horovod** 替代，我们将以下代码添加到 TensorFlow 估算器初始化中:

```
distribution={
    "mpi": {
        "custom_mpi_options": "--NCCL_DEBUG INFO",
        "processes_per_host": <Insert Number Here>,
        "enabled": True
    }
}
```

**管道模式**:对于更大的数据集，我们可以利用**管道模式**来进一步加速。在初始化我们的 TensorFlow 估计器时，我们可以设置 input_mode="Pipe "并更新 entrypoint 训练脚本，以使用 sagemaker_tensorflow 包中的 PipeModeDataset。我们不会在本章中讨论使用管道模式的不同步骤，因此请在此处查看 **SageMaker Python SDK** 文档页面了解更多信息:[https://SageMaker . readthe docs . io/en/stable/frameworks/tensor flow/using _ TF . html # training-with-Pipe-mode-using-Pipe mode dataset](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#training-with-pipe-mode-using-pipemodedataset)。

## 参见

如果您正在寻找使用真实数据集在 SageMaker 中训练和部署 **TensorFlow** 模型的示例，请随意查看**AWS/Amazon-SageMaker-examples**GitHub 资源库中的一些笔记本:

*   在 MNIST 数据集上训练一个分类模型:[https://github . com/AWS/Amazon-sagemaker-examples/blob/master/sagemaker-python-SDK/tensor flow _ script _ mode _ training _ and _ serving/tensor flow _ script _ mode _ training _ and _ serving . ipynb](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_script_mode_training_and_serving/tensorflow_script_mode_training_and_serving.ipynb)
*   Horovod 分布式培训:[https://github . com/AWS/Amazon-sage maker-examples/blob/master/sage maker-python-SDK/tensor flow _ script _ mode _ horo VOD/tensor flow _ script _ mode _ horo VOD . ipynb](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_script_mode_horovod/tensorflow_script_mode_horovod.ipynb)

有关这个主题的更多信息，请随意查看[https://sagemaker . readthe docs . io/en/stable/frameworks/tensor flow/using _ TF . html](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html)。

# 准备入口点 PyTorch 培训脚本

**PyTorch** 是流行的用于机器学习的开源软件库。在这个菜谱中，我们将定义一个定制的 **PyTorch** 神经网络模型，并准备入口点训练脚本。在随后的一个配方中，我们将使用来自 **SageMaker Python SDK** 的 PyTorch estimator 类，并在初始化 estimator 对象时将该脚本作为入口点参数。请注意，我们将有一个单独的入口点推理脚本来部署一个带有 **SageMaker** 的 **PyTorch** 模型，我们将在后面的菜谱中看到。

如果您计划迁移您的定制 **PyTorch** 神经网络代码 a 并使用 **SageMaker** 平台执行训练和部署，那么这个方法和接下来的方法都适合您！

## 准备就绪

这个配方延续了*为深度学习实验生成合成数据集*。

## 怎么做

本菜谱中的说明侧重于准备培训入口点脚本。让我们首先在 Jupyter notebook 实例中创建一个名为 pytorch_training.py 的空文件，然后继续下一组步骤:

1.  Navigate to the /ml-experients/chapter03/PyTorch directory.![Figure 3.40 – Navigating to the my-experiments/chapter03/PyTorch directory
    ](img/B16850_03_40.jpg)

    图 3.40-导航到 my-experiments/chapter 03/py torch 目录

    我们可以在*图 3.40* 中看到 **PyTorch** 目录。请记住，我们在配方*中创建了这个目录，为多个深度学习本地实验*准备 SageMaker 笔记本实例。

2.  Create a new file and rename it to pytorch_training.py. After that, open the empty file as we will be adding a few lines of code in the next set of steps.

    在开始编写 pytorch_training.py 中的代码之前，让我们快速检查一下 entryscript 文件的外观。如果您想知道它为什么被称为 entryscript 文件，我们将在初始化 pytorch 估计器时将 pytorch_training.py 文件名作为值传递给参数 entry_script。

    ![Figure 3.41 – PyTorch training entrypoint script code structure
    ](img/B16850_03_30.jpg)

    图 3.41–py torch 培训入口点脚本代码结构

    接下来的几个步骤集中在准备入口点脚本中的代码。*图 3.41* 向我们展示了在我们完成食谱中的说明后，我们的定制 **PyTorch** 模型的培训脚本将会是什么样子。

3.  用 shebang 行开始脚本:

    ```
    #!/usr/bin/env python
    ```

4.  导入培训脚本的先决条件:

    ```
    import os import numpy as np import torch import torch.utils.data as Data import random
    ```

5.  定义 set_seed()函数:

    ```
    def set_seed():     torch.manual_seed(0)     random.seed(0)     np.random.seed(0)
    ```

6.  准备一个名为 load_data()的函数，该函数加载一个 CSV 文件并返回 x 和 y 值的张量:

    ```
    def load_data(training_data_location):     result = np.loadtxt(         open(training_data_location, "rb"),          delimiter=","     )     x = result[:, 1]     xt = torch.Tensor(x.reshape(-1, 1))     y = result[:, 0]     yt = torch.Tensor(y.reshape(-1, 1))     return (xt, yt)
    ```

7.  Define a function that prepares the model:

    ```
    def prepare_model():
        model = torch.nn.Sequential(
            torch.nn.Linear(1, 50),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 50),
            torch.nn.Dropout(0.01),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 50),
            torch.nn.Dropout(0.01),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 50),
            torch.nn.Dropout(0.01),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 50),
            torch.nn.Dropout(0.01),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 50),
            torch.nn.Dropout(0.01),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 50),
            torch.nn.Dropout(0.01),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 50),
            torch.nn.Dropout(0.01),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 1),
        )

        return model
    ```

    注意我们只是使用了一个虚拟神经网络，我们可以很容易地用其他 **PyTorch** 模型来代替它。我们甚至可以加载预训练的 **PyTorch** 模型，并使用**迁移学习**来构建新模型。

    重要说明

    注意，prepare_model()函数返回模型的初始版本，它还没有经过训练步骤。

8.  定义一个接受 x 和 y 值并返回数据加载器的函数，该函数将在后面的步骤中使用。我们将把这个函数命名为 prepare_data_loader():

    ```
    def prepare_data_loader(x, y, batch_size):     dataset = Data.TensorDataset(x, y)     data_loader = Data.DataLoader(         dataset=dataset,          batch_size=batch_size,          shuffle=False, num_workers=2)     return data_loader
    ```

9.  Prepare the train() model function:

    ```
    def train(model, x, y, epochs=200, 
              learning_rate = 0.001, batch_size=100):

        data_loader = prepare_data_loader(
            x=x, y=y, batch_size=batch_size
        )
        loss_fn = torch.nn.MSELoss(reduction='sum')
        optimizer = torch.optim.Adam(
            model.parameters(), 
            lr=learning_rate
        )
        for e in range(epochs):
            for step, (batch_x, batch_y) in \ 
                enumerate(data_loader): 
                prediction = model(batch_x)     

                loss = loss_fn(prediction, batch_y)     

                optimizer.zero_grad()   
                loss.backward()         
                optimizer.step()        

            if (e % 10 == 0):
                print("Iteration:", e, 
                      "\t| Loss:", loss.item())

        return model
    ```

    在内部 for 循环中，训练数据集(存储在 batch_x 中)作为模型预测步骤的有效载荷传递，然后，给定前一步骤计算的预测误差，执行反向传播以调整所述模型的参数。

    小费

    更多关于如何使用 **PyTorch** 的信息，可以随意查看以下链接:[https://py torch . org/tutorials/beginner/blitz/neural _ networks _ tutorial . html](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)。

10.  Define the main() function that makes use of the functions we have defined in the previous steps to prepare, train, and save the model:

    ```
    def main(model_dir, train_path, epochs=200, 
             learning_rate=0.001, batch_size=100):                                  

        set_seed()
        model = prepare_model()

        x, y = load_data(train_path)
        print("x.shape:", x.shape)
        print("y.shape:", y.shape)

        model = train(model=model, 
                      x=x, 
                      y=y, 
                      epochs=epochs, 
                      learning_rate=learning_rate,
                      batch_size=batch_size)

        print(model)

        torch.save(model.state_dict(), 
                   os.path.join(model_dir, "model.pth"))
    ```

    在 main()函数中，我们执行以下一组操作:(1)使用 prepare_model()函数准备模型，(2)使用 load_data()函数加载数据，(3)使用 train()函数训练模型，最后，(4)使用 torch.save()函数保存模型。请注意，前面代码块中的 torch.save()函数只保存 model.pth 文件中的模型参数。

    注意

    当模型被再次加载时， **SageMaker** 如何知道模型架构？在部署和推理步骤中， **SageMaker** 在推理入口点脚本文件中寻找 model_fn()函数，我们将在下一个配方中看到。这个 model_fn()函数使用了相同的 prepare_model()函数，该函数在从 model.pth 文件加载参数和权重之前定义了我们的定制模型架构。

11.  Finally, add the lines of code to trigger the main() function:

    ```
    if __name__ == "__main__":
        model_dir = "/opt/ml/model"
        train_csv = "train/training_data.csv"
        train_path = f"/opt/ml/input/data/{train_csv}"

        main(model_dir=model_dir,
             train_path=train_path,
             epochs=1000,
             learning_rate=0.001,
             batch_size=100)
    ```

    小费

    你可以在*亚马逊 SageMaker Cookbook 的机器学习* GitHub 资源库:[https://GitHub . com/packt publishing/Machine-Learning-with-Amazon-sage maker-Cookbook/blob/master/chapter 03/py torch/pytorch_training.py](https://github.com/PacktPublishing/Machine-Learning-with-Amazon-SageMaker-Cookbook/blob/master/Chapter03/PyTorch/pytorch_training.py)中访问 py torch _ training . py 文件的工作副本。

现在让我们看看这个食谱是如何工作的。

## 工作原理...

我们在这个配方中准备的脚本的路径将是 entry_point 参数的输入参数，用于在下一个配方中初始化来自 **SageMaker Python SDK** 的 **PyTorch** 类。当保存 **PyTorch** 模型时，你必须意识到我们只是在使用这个配方中的一种方法。如果我们正在使用**弹性推理** ( **EI** )，那么我们必须做以下事情:

1.  将训练好的模型转换成 **TorchScript** 格式。
2.  在 entrypoint 脚本中保存模型时，使用 torch.jit.save()函数，而不是 torch.save()。

为了在部署步骤中将 EI 加速器附加到端点，我们在使用 deploy()函数时指定 accelerator_type 参数。关于使用亚马逊弹性推断的更多信息，请随意查看 AWS 文档网站的链接:[https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html](https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html)。

注意

请随意看看*是如何工作的...还有更多...配方的*部分*准备 entrypoint TensorFlow 和 Keras 培训脚本*了解关于 entrypoint 脚本的更多信息。为了避免重复的内容，我们在本节中不讨论它。

与 **TensorFlow** entrypoint 训练脚本类似， **PyTorch** entrypoint 训练脚本可以利用 SageMaker 设置的环境变量。其中包括 SM_MODEL_DIR、SM_INPUT_DATA_CONFIG 和 SM_OUTPUT_DATA_DIR。有关这些环境变量的更多信息，请参考*还有更多...*准备 entrypoint TensorFlow 和 Keras 训练脚本*配方的*部分。

# 准备入口点 PyTorch 推理脚本

在之前的配方中，我们准备了 entrypoint 训练脚本来训练一个 **PyTorch** 模型。在这个菜谱中，我们将准备入口点推理脚本来部署一个带有 **SageMaker** 的 **PyTorch** 模型，我们将在下一个菜谱中看到。

如果您打算用 **SageMaker** 平台迁移您的自定义 **PyTorch** 神经网络代码和 perf orm 培训和部署，那么这个配方、上一个和下一个都适合您！

## 准备就绪

本食谱上接*准备 entrypoint PyTorch 培训脚本*。

## 怎么做

本菜谱中的说明集中于准备推理入口点脚本。让我们首先在 Jupyter notebook 实例中创建一个名为 pytorch_inference.py 的空文件，然后继续下一组步骤:

1.  Navigate to the /ml-experients/chapter03/PyTorch directory.![Figure 3.42 – Navigating to the my-experiments/chapter03/SKLearn directory
    ](img/B16850_03_42.jpg)

    图 3.42–导航至 my-experiments/chapter 03/sk learn 目录

    我们可以在*图 3.42* 中看到 PyTorch 目录。它已经包含 pytorch_training.py，这是我们在准备 entrypoint PyTorch 训练脚本的菜谱*中创建的。*

2.  Create a new file and name it pytorch_inference.py. After that, open the empty file as we will be adding a few lines of code in the next set of steps.![Figure 3.43 – Creating a new file
    ](img/B16850_03_43.jpg)

    图 3.43–创建新文件

    在*图 3.43* 中，我们在**新建**下拉菜单下的选项列表中选择**文本文件**。完成此步骤后，PyTorch 目录中应该有两个文件——py torch _ training . py 和 pytorch_inference.py。在继续下一组步骤之前，请确保打开 pytorch_inference.py 文件。

3.  用 shebang 行开始脚本:

    ```
    #!/usr/bin/env python
    ```

4.  导入先决条件:

    ```
    import os import torch import numpy as np
    ```

5.  Define the function that prepares the model. We'll call this function prepare_model():

    ```
    def prepare_model():
        model = torch.nn.Sequential(
            torch.nn.Linear(1, 50),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 50),
            torch.nn.Dropout(0.01),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 50),
            torch.nn.Dropout(0.01),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 50),
            torch.nn.Dropout(0.01),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 50),
            torch.nn.Dropout(0.01),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 50),
            torch.nn.Dropout(0.01),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 50),
            torch.nn.Dropout(0.01),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 50),
            torch.nn.Dropout(0.01),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 1),
        )

        return model
    ```

    请注意，这需要与配方*准备入口点 PyTorch 训练脚本*中入口点训练脚本中定义的 prepare_model()函数具有相同的模型架构。

6.  Prepare the function model_fn() that loads and returns the model:

    ```
    def model_fn(model_dir):
        model = prepare_model()
        path = os.path.join(model_dir, 'model.pth')
        model.load_state_dict(torch.load(path))
        model.eval()
        return model
    ```

    确保保存文件并使用 pytorch_inference.py 作为文件名。这个食谱的下一步也是最后一步是运行准备好的脚本。

    小费

    你可以在这里访问 pytorch_inference.py 文件的工作副本，在*Machine Learning with Amazon sage maker Cookbook*Github 资源库:[https://Github . com/packt publishing/Machine-Learning-with-Amazon-sage maker-Cookbook/blob/master/chapter 03/py torch/py torch _ inference . py](https://github.com/PacktPublishing/Machine-Learning-with-Amazon-SageMaker-Cookbook/blob/master/Chapter03/PyTorch/pytorch_inference.py)。

现在让我们看看这个食谱是如何工作的。

## 工作原理...

我们准备的脚本将是 entrypoint 参数的输入参数，用于初始化来自 **SageMaker Python SDK** 的 PyTorchModel 对象，我们将在下一个菜谱中看到。

在推理入口点脚本中，您需要执行以下操作:

*   替换默认的 model_fn()实现(必需的)。
*   替换 input_fn()实现(可选)–该函数接受请求数据并执行反序列化步骤，该步骤转换预测步骤之前使用的数据。
*   替换 output_fn()实现(可选)–该函数接受预测结果并执行序列化步骤，该步骤根据响应内容类型转换结果。
*   替换 predict_fn()实现(可选)——在 input_fn()函数完成反序列化步骤后，该函数接受反序列化的请求对象。加载的模型用于对反序列化的请求对象执行预测，并返回预测结果。

请注意，训练脚本(pytorch_script.py)中使用的模型架构需要与推理脚本(pytorch_inference.py)相同。因为我们只在模型文件中保存状态，所以在使用 load_state_dict()函数加载状态数据之前，我们需要通过代码定义模型架构。也就是说，在 pytorch_script.py 中保存模型和在 pytorch_inference.py 中加载模型时，这个模型架构需要是相同的。

# 使用 SageMaker Python SDK 训练和部署 PyTorch 模型

使用 **SageMaker** 执行定制 **PyTorch** 模型的培训和部署相当简单。第 1 步涉及创建 entrypoint 脚本，其中定义和编码了我们的自定义神经网络和训练逻辑。第 2 步涉及创建推理入口点脚本，它帮助我们加载训练好的模型。第 3 步涉及分别在初始化 PyTorch 和 PyTorchModel 对象时使用这些脚本作为参数。

在本食谱中，我们将关注*步骤 3* ，并继续在 **SageMaker** 中训练和部署我们定制的 **PyTorch** 神经网络模型。如果你正在寻找*步骤 1* ，请随意查看配方*准备入点 PyTorch trai ning 脚本*。如果您正在寻找*步骤 2* ，请检查配方*准备 en trypoint PyTorch 推理脚本*。

## 准备就绪

这个配方延续了准备入口点 PyTorch 推理脚本的*。*

## 怎么做

在初始化 PyTorch 估算器时，本配方中的说明重点在于使用*准备入口点 PyTorch 训练脚本*配方中的自定义入口点训练脚本。初始化 PyTorch 估计器后，我们将使用 fit()函数，然后使用 PyTorchModel 进行部署，然后使用 predict()函数使用我们的自定义神经网络进行推理:

1.  Navigate to the /my-experiments/chapter03/PyTorch directory. If you can still remember, we created this directory in the recipe *Preparing the SageMaker notebook instance for multiple deep learning local experiments*.![Figure 3.44 – Navigating to the PyTorch directory
    ](img/B16850_03_44.jpg)

    图 3.44–导航到 PyTorch 目录

    正如我们在*图 3.44* 中看到的，这个目录中已经有了名为 pytorch_inference.py 和 pytorch_training.py 的文件。在后面的步骤中初始化 pytorch 估计器和 PyTorchModel 时，我们将使用这些文件作为 entry_point 参数的值。

2.  Click the **New** button to open a dropdown of notebook kernel options. Select conda_pytorch_p36.![Figure 3.45 – Creating a new Jupyter notebook using the conda_pytorch_p36 kernel
    ](img/B16850_03_45.jpg)

    图 3.45–使用 conda_pytorch_p36 内核创建新的 Jupyter 笔记本

    如*图 3.45* 所示，新的 Jupyter 笔记本必须与 pytorch_inference.py 和 pytorch_training.py 文件在同一个目录下。

3.  更新安装 **SageMaker 本地模式** :

    ```
    !pip install 'sagemaker[local]' --upgrade
    ```

4.  重新启动 Docker 服务以确保我们不会因为之前运行的容器而遇到任何问题:

    ```
    !sudo service docker restart
    ```

5.  Delete all Docker container images to free up a bit of space:

    ```
    !docker rmi -f $(docker images -a -q)
    ```

    随着一些准备和安装工作的完成，我们将继续重点准备 **SageMaker** 培训工作的先决条件和论证的步骤:

6.  Set the s3_bucket variable containing the S3 bucket path. Use the bucket created in the recipe *Preparing the Amazon S3 bucket and the training dataset for the linear regression experiment* from [*Chapter 1*](B16850_01_Final_ASB_ePub.xhtml#_idTextAnchor020)*, Getting Started with Machine Learning Using Amazon SageMaker*:

    ```
    s3_bucket = '<insert bucket name here>'
    prefix = 'chapter03'
    ```

    请注意，我们的 training_data.csv 文件应该已经存在于 S3 存储桶中，路径应该是 S3://<s3 bucket="" name="">/<prefix>/synthetic/training _ data . CSV</prefix></s3>

7.  设置变量 train_s3 的值，该变量包含我们在之前的配方中上传的训练数据 CSV 文件的 s3 路径:

    ```
    train_s3 = \ f"s3://{s3_bucket}/{prefix}/synthetic/training_data.csv"  
    ```

8.  使用 TrainingInput 类准备 content_type 设置为“text/csv”的训练输入参数:

    ```
    from sagemaker.inputs import TrainingInput train_input = TrainingInput(train_s3, content_type="text/csv")
    ```

9.  导入一些运行培训作业的先决条件。除此之外，我们将使用 LocalSession 来初始化 sagemaker_session 对象。LocalSession 类允许我们在训练和部署步骤中使用**本地模式**。正如我们将在后面的步骤中看到的，在初始化估计器时，我们将指定“local”作为 instance_type 的参数值，以使我们在 SageMaker 笔记本实例:

    ```
    import sagemaker from sagemaker import get_execution_role from sagemaker.local import LocalSession sagemaker_session = LocalSession() sagemaker_session.config = {'local': {'local_code': True}} role = get_execution_role()
    ```

    中本地执行训练作业
10.  Initialize the PyTorch estimator from the **SageMaker Python SDK**:

    ```
    from sagemaker.pytorch import PyTorch
    estimator = PyTorch(
        entry_point='pytorch_training.py',
        session=sagemaker_session,
        role=role,
        instance_count=1,
        instance_type='local',
        framework_version='1.5.0',
        py_version='py3')
    ```

    如您所见，我们没有指定脚本文件运行的容器。PyTorch 估计器利用了 **SageMaker** 中的一个预建 Docker 容器映像。这意味着我们不必担心准备容器，因为 SageMaker 已经为我们准备好了。如果这还不够，我们还可以选择扩展现有的**预构建 SageMaker Docker 映像**。

    注意

    另一种选择是从头开始构建和使用我们的自定义容器映像。请随意查看 [*第二章*](B16850_02_Final_ASB_ePub.xhtml#_idTextAnchor061) 、*中的菜谱，构建并使用自己的算法容器映像*。

11.  Execute the training job using the fit() function:

    ```
    estimator.fit({'train': train_input})
    ```

    *图 3.46* 显示了使用 PyTorch 估计器的 fit()函数几分钟后训练日志的样子:

    ![Figure 3.46 – Training logs after running the fit() function after initializing the PyTorch estimator
    ](img/B16850_03_46.jpg)

    图 3.46–初始化 PyTorch 估计器后运行 fit()函数后的训练日志

    如果您在运行 fit()函数时遇到问题，您可以查看本章末尾的方法*使用本地模式时调试磁盘空间问题*和*使用本地模式时调试容器执行问题*。

12.  Initialize a PyTorchModel object the value of estimator.model_data:

    ```
    from sagemaker.pytorch.model import PyTorchModel
    pytorch_model = PyTorchModel(
        model_data=estimator.model_data, 
        role=role,                               
        entry_point='pytorch_inference.py', 
        framework_version='1.5.0',
        py_version="py3")
    ```

    这就是 **PyTorch** 与其他框架估算器的用法稍有不同的地方。正如您在前面的代码块中看到的，我们使用 PyTorch estimator 实例中的 model_data 初始化了 PyTorchModel 实例。我们还为部署和推理步骤使用了单独的推理入口点脚本。请注意，我们在配方*准备入口点 PyTorch 推理脚本*中准备了推理入口点脚本。

13.  Deploy the model using the deploy() function:

    ```
    predictor = pytorch_model.deploy(instance_type='local', 
                                     initial_instance_count=1)
    ```

    我们应该得到一组类似于*图 3.47* 中的日志和输出值:

    ![Figure 3.47 – Logs and output after calling the deploy() function
    ](img/B16850_03_47.jpg)

    图 3.47–调用 deploy()函数后的日志和输出

    这将使用 **PyTorch** 推理容器映像在本地运行带有推理端点的容器。

    注意

    如果您想知道在运行本地预测端点时使用哪个容器映像，只需运行！码头集装箱显示运行中的集装箱。我们应该会得到一个等于或类似于 763104351884 . dkr . ECR . us-east-1 . Amazon AWS . com/py torch-推论:1.5.0-cpu-py3 的值。有关更多信息，请查看图片，网址为[https://github . com/AWS/deep-learning-containers/blob/master/available _ images . MD](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)。

14.  Import **NumPy**. Perform a few test predictions using the predict() function:

    ```
    import numpy as np
    predictor.predict(np.array([[100], [200]], dtype=np.float32))
    ```

    我们应该得到一组类似于*图 3.48* 中的日志和输出值:

    ![Figure 3.48 – Logs and output after using the predict() function in local mode
    ](img/B16850_03_48.jpg)

    图 3.48–在本地模式下使用 predict()函数后的日志和输出

    如图*图 3.48* 所示，在本地模式下调用 predict()函数时，正在运行的容器内的/invocations 端点被 POST 请求触发。处理完请求后，predict()函数将响应返回给用户。

15.  创建一个 tmp 目录，在这里我们将下载并存储所有数据，以便在后面的步骤中快速可视化。请注意，这不同于/tmp 目录:

    ```
    !mkdir -p tmp
    ```

16.  Prepare a variable that contains the path where the CSV File with all the records are stored:

    ```
    all_s3 = \
    f"s3://{s3_bucket}/{prefix}/synthetic/all_data.csv" 
    !aws s3 cp {all_s3} tmp/all_data.csv
    ```

    接下来的几个步骤集中在检查一组代表“预测线”的 x 值的 y 预测值。

    ![Figure 3.49 – The predict() function using the custom neural network deployed in the inference endpoint to predict the y values from the x input values
    ](img/B16850_03_49.jpg)

    图 3.49–predict()函数使用部署在推理端点中的自定义神经网络从 x 输入值预测 y 值

    在*图 3.49* 中，我们让 predict()函数使用自定义的 **PyTorch** 神经网络从 x 值生成预测的 y 值，我们已经训练并部署到一个推理端点。

17.  加载包含所有记录的 CSV 文件，并将 x 和 y 值作为数组分别存储在的 x 和 y 变量中:

    ```
    import pandas as pd all_data = pd.read_csv("tmp/all_data.csv", header=None) x = all_data[[1]].values y = all_data[[0]].values
    ```

18.  使用 arange()函数，生成一个值在-5000 和 5000 之间的数组，并将这些值存储在 line_x:

    ```
    from numpy import arange line_x = arange(-5000, 5000, 10)
    ```

    中
19.  Execute the predict() function using the deployed model with the input values from the previous step:

    ```
    input_data = np.array(line_x.reshape(-1, 1), dtype=np.float32)
    result = predictor.predict(input_data)
    result
    ```

    我们应该得到一组类似于图 3.50 中的日志和输出值:

    ![Figure 3.50 – Results after using the predict() function on an array of values
    ](img/B16850_03_50.jpg)

    图 3.50–对数值数组使用 predict()函数后的结果

    在*图 3.50* 中，我们可以看到经过 x 值数组后的预测 y 值。

20.  将结果存储在名为 line_y:

    ```
    line_y = result
    ```

    的变量中
21.  Render the line on top of the scatterplot of the original dataset using **Matplotlib**:

    ```
    from matplotlib import pyplot
    pyplot.plot(line_x, line_y, 'r')
    pyplot.scatter(x,y,s=1)
    pyplot.show()
    ```

    这将呈现一个类似于图 3.51 中所示的图表。

    ![Figure 3.51 – Prediction line over the scatterplot of points
    ](img/B16850_03_51.jpg)

    图 3.51–散点图上的预测线

    正如我们可以在*图 3.51* 中看到的，我们的定制 **PyTorch** 模型似乎在对数据集进行概化方面做得很好。当然，我们需要一套更全面的模型评估步骤，但是我们将在本菜谱中跳过这一步。

22.  Stop the running container with the inference endpoint:

    ```
    predictor.delete_endpoint()
    ```

    这将停止带有本地推理端点的正在运行的容器。

让我们看看这是如何工作的！

## 它是如何工作的...

使用 **SageMaker Python SDK** 的 **PyTorch** 支持，我们能够直接使用 **PyTorch** entrypoint 脚本，而不必构建和管理用于实验的容器，因为 SDK 为我们抽象了这些。在这个配方中，我们初始化并使用了来自 **SageMaker Python SDK** 的 PyTorchModel 来让这个配方工作。这与其他框架有些不同，在其他框架中，我们可以使用估计器的 deploy()函数直接执行部署。下面是我们在使用 PyTorchModel 时可以指定的一些参数:

*   模型 _ 数据
*   作用
*   entry_point(需要推理入口点脚本)
*   框架 _ 版本
*   py 版本
*   图像 _uri
*   模型 _ 服务器 _ 工人

在 PyTorchModel 对象已经初始化之后，PyTorchModel 对象具有 deploy()函数，该函数接受与来自其他估算器的 deploy()函数相同的一组参数。

注意

有关更多信息，请随意查看配方*的*工作原理……*部分，使用 Amazon SageMaker 本地模式*培训和部署 TensorFlow 和 Keras 模型。为了避免重复的内容，我们将不讨论本节中的一些细节。

## 参见

如果您正在寻找使用真实数据集在 SageMaker 中训练和部署 **PyTorch** 模型的示例，请随意查看**AWS/Amazon-SageMaker-examples**GitHub 资源库中的一些笔记本:

*   部署预训练 **PyTorch** 模型:[https://github . com/AWS/Amazon-sage maker-examples/blob/master/sage maker _ neo _ compilation _ jobs/py torch _ torch vision/py torch _ torch vision _ neo . ipynb](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker_neo_compilation_jobs/pytorch_torchvision/pytorch_torchvision_neo.ipynb)
*   培训时使用 **SageMaker 调试器**py torch 型号:[https://github . com/AWS/Amazon-sage maker-examples/blob/master/sage maker-Debugger/py torch _ custom _ container/py torch _ byoc _ sm debug . ipynb](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-debugger/pytorch_custom_container/pytorch_byoc_smdebug.ipynb)

请注意，我们将在菜谱*中处理 **SageMaker 调试器**识别 SageMaker 调试器*的问题，以及*检查 SageMaker 调试器日志和结果*来自 [*第 5 章*](B16850_05_Final_ASB_ePub.xhtml#_idTextAnchor267) 、*有效管理机器学习实验*。

# 准备入口点 scikit-learn 培训脚本

**Scikit-learn** 是一个流行的用于机器学习的开源软件库。在这个菜谱中，我们将定义一个定制的 **scikit-learn** 神经网络模型，并准备入口点训练脚本。在下一个菜谱中，我们将使用来自 **SageMaker Python SDK** 的 SKLearn 估算器，并将这个脚本作为训练和部署的入口点参数。如果您计划迁移您的客户 om **scikit-learn** 神经网络代码，并使用 **SageMaker** 平台执行培训和部署，那么这个方法(以及下一个)就是为您准备的！

## 做好准备

这个配方延续了*生成深度学习实验的合成数据集*。

## 怎么做

本菜谱中的说明侧重于准备入口点脚本。让我们首先在 Jupyter notebook 实例中创建一个名为 sklearn_script.py 的空文件，然后继续下一组步骤:

1.  Navigate to the /ml-experients/chapter03/SKLearn directory.![Figure 3.52 – Navigating to the my-experiments/chapter03/SKLearn directory
    ](img/B16850_03_52.jpg)

    图 3.52–导航到 my-experiments/chapter 03/sk learn 目录

    我们可以在*图 3.52* 中看到 SKLearn 目录。请记住，我们在配方*中创建了这个目录，为多个深度学习本地实验*准备 SageMaker 笔记本实例。

2.  Create a new file and name it sklearn_script.py. After that, open the empty file as we will be adding a few lines of code in the next set of steps.![Figure 3.53 – Creating a new file
    ](img/B16850_03_53.jpg)

    图 3.53–创建新文件

    在开始编写 sklearn_script.py 中的代码之前，让我们快速检查一下 entryscript 文件的外观。如果您想知道它为什么被称为 entryscript 文件，我们将在初始化 sklearn 估计器时将 sklearn_script.py 文件名作为值传递给参数 entry_script。

    ![Figure 3.54 – SKLearn entrypoint script code structure
    ](img/B16850_03_30.jpg)

    图 3.54–sk learn 入口点脚本代码结构

    *图 3.54* 向我们展示了在我们完成该食谱中的说明后，我们的定制 **SKLearn** 模型的训练脚本将会是什么样子。

3.  Start by importing a few essential libraries and utilities such as numpy, json, and the os libraries and packages:

    ```
    #!/usr/bin/env python

    import os
    import numpy as np
    from sklearn.neural_network import MLPRegressor
    from sklearn.externals import joblib
    ```

    对于这个食谱，我们将使用 **scikit-learn** **多层感知器回归器**。我们通过使用 **scikit-learn** 包中的 MLPRegressor 来实现这一点。

    定义 set_seed()函数，它将帮助我们在每次运行实验时生成相同的结果集:

    ```
    def set_seed():
        np.random.seed(0)
    ```

4.  Prepare the model_fn function that loads an existing model given a path:

    ```
    def model_fn(model_dir):
        path = os.path.join(model_dir, "model.joblib")
        model = joblib.load(path)
        return model
    ```

    这个 model_fn()函数只是加载并返回存储在/opt/ml/model 中的 model.joblib 文件中的模型。

    重要说明

    在前面代码块中的 model_fn()函数中，我们使用 joblib.load()来加载一个经过训练的模型。使用 joblib.load()函数时要小心，因为来自不可信来源的机器学习模型可能包含导致安全问题的恶意指令(如任意代码执行)！关于这个话题的更多信息，请随时查看[https://joblib.readthedocs.io/en/latest/persistence.html](https://joblib.readthedocs.io/en/latest/persistence.html)。

5.  准备 load_data()函数，该函数加载 CSV 文件的内容并返回 x 和 y 值:

    ```
    def load_data(training_data_location):     result = np.loadtxt(         open(training_data_location, "rb"),          delimiter=","     )     y = result[:, 0]     x = result[:, 1]     return (x, y)
    ```

6.  Define the model inside the prepare_model() function:

    ```
    def prepare_model(epochs=1000):
        model = MLPRegressor(
            hidden_layer_sizes=(10,10,10,10,10), 
            activation='relu', 
            solver='adam', 
            max_iter=2000, 
            verbose=True,
            batch_size=100,
            learning_rate='adaptive',
            n_iter_no_change=2000,
            early_stopping=True,
            tol=0.01,
            random_state=0
        )

        return model
    ```

    注意我们只是使用了一个虚拟神经网络，我们可能会用其他 **SKLearn** 模型来代替它。想了解更多关于 MLPRegressor 的信息，请随时查看[https://sci kit-learn . org/stable/modules/neural _ networks _ supervised . html](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)。

7.  定义 train()函数:

    ```
    def train(model, x, y):     model.fit(x.reshape(-1, 1),y.reshape(-1, 1))     return model
    ```

8.  Define the main() function that prepares, trains, and saves the model when executed:

    ```
    def main(model_dir, train_path, epochs=2000):
        set_seed()
        model = prepare_model(epochs=epochs)
        x, y = load_data(train_path)
        print(x.shape)
        print(y.shape)

        model = train(model, x, y)
        print(model)

        path = os.path.join(model_dir, "model.joblib")
        joblib.dump(model, path)
    ```

    注意joblib . dump()函数将训练好的模型序列化并保存在 model.joblib 文件中。在部署和推理过程中， **SageMaker** 在同一个脚本文件中寻找 model_fn()函数，并使用加载的模型进行推理。我们在同一个文件中定义的 model_fn()函数利用了 joblib.load()函数，该函数从 model.joblib 文件中反序列化并加载模型。

9.  Finally, add the following lines of code to trigger the main() function when the script is used directly:

    ```
    if __name__ == "__main__":
        model_dir = "/opt/ml/model"    
        train_csv = "train/training_data.csv"
        train_path = f"/opt/ml/input/data/{train_csv}"

        main(model_dir=model_dir,
             train_path=train_path,
             epochs=1000)
    ```

    至此，sklearn_script.py 就完成了，我们可以开始下一部分了。

    小费

    你可以在*亚马逊 SageMaker Cookbook 的机器学习* GitHub 资源库:[https://GitHub . com/packt publishing/Machine-Learning-with-Amazon-sage maker-Cookbook/blob/master/chapter 03/sk learn/sklearn_script.py](https://github.com/PacktPublishing/Machine-Learning-with-Amazon-SageMaker-Cookbook/blob/master/Chapter03/SKLearn/sklearn_script.py)中访问 sk learn _ script . py 文件的工作副本。

现在让我们看看这个食谱是如何运作的！

## 工作原理...

在这个配方中，为了演示的目的，我们使用了一个任意的神经网络结构。您可以随意修改或替换该脚本中使用的模型。

与 **TensorFlow** entrypoint 脚本相比，**scikit-learn**entry point 脚本需要定义 model_fn()函数。当在后面的配方中使用 deploy()函数时， **SageMaker scikit-learn 模型服务器**通过调用入口点脚本中定义的 model_fn()函数来加载模型。

除了 model_fn()函数之外，我们还可以在 entrypoint 脚本中定义一些其他可选函数:

*   input _ fn()–该函数接受请求数据并执行反序列化步骤，该步骤转换预测步骤之前使用的数据。
*   output _ fn()–该函数接受预测结果并执行序列化步骤，该步骤根据响应内容类型转换结果。
*   predict _ fn()–在 input_fn()函数完成反序列化步骤后，该函数接受反序列化的请求对象。加载的模型用于对反序列化的请求对象执行预测，并返回预测结果。

请随意查看配方*准备入口点 TensorFlow 和 Keras 培训脚本*的*工作原理……*部分，了解更多关于入口点脚本的详细信息。为了避免重复的内容，我们将不讨论本节中的一些细节。

# 使用 SageMaker Python SDK 培训和部署 scikit-learn 模型

用 **SageMaker** 执行定制 **scikit-learn** 模型的训练和部署相当简单。*步骤 1* 包括创建入口点脚本，在这里定义和编码我们的定制神经网络和训练逻辑。*步骤 2* 包括使用这个脚本作为来自 **SageMaker Python SDK** 的 SKLearn 估计器的参数，以继续进行训练和部署步骤。

在本食谱中，我们将关注*步骤 2* 并继续在 **SageMaker** 中训练和部署我们的定制 **scikit-learn** 神经网络模型。如果您正在寻找*步骤 1* ，请随意查看之前的配方， *准备入口点 scikit-learn 培训脚本*。

## 做好准备

本食谱上接*准备入口点 scikit-learn 培训脚本*。

## 怎么做

在初始化 SKLearn 估计器时，本配方中的说明侧重于使用上一配方中的自定义入口点训练脚本。一旦我们初始化了 SKLearn 估计器，我们将使用 fit()和 deploy()函数，然后使用 predict()函数来使用我们的定制神经网络进行推理:

1.  Navigate to the /my-experiments/chapter03/SKLearn directory.![Figure 3.55 – Navigating to the SKLearn directory
    ](img/B16850_03_55.jpg)

    图 3.55-导航到 SKLearn 目录

    正如我们在*图 3.55* 中看到的，这个目录中已经有一个名为 sklearn_script.py 的文件，我们将在后面的步骤中初始化 sklearn 估算器时使用 sklearn_script.py 作为 entry_point 参数的值。

2.  Click the **New** button to open a dropdown of notebook kernel options. Select conda_python3.![Figure 3.56 – Creating a new Jupyter notebook using the conda_python3 kernel
    ](img/B16850_03_56.jpg)

    图 3.56–使用 conda_python3 内核创建一个新的 Jupyter 笔记本

    正如我们在*图 3.56* 中所拥有的，新的 Jupyter 笔记本必须和 sklearn_script.py 文件在同一个目录下。

3.  更新安装 **SageMaker 本地模式** :

    ```
    !pip install 'sagemaker[local]' --upgrade
    ```

4.  重新启动 Docker 服务以确保我们不会因为之前运行的容器而遇到任何问题:

    ```
    !sudo service docker restart
    ```

5.  Delete all Docker container images to free up a bit of space:

    ```
    !docker rmi -f $(docker images -a -q)
    ```

    随着一些准备和安装工作的完成，我们将继续关注准备 **SageMaker** 培训工作的先决条件和参数的步骤:

6.  Set the s3_bucket variable containing the S3 bucket path. Use the bucket created in the recipe *Preparing the Amazon S3 bucket and the training dataset for the linear regression experiment* from [*Chapter 1*](B16850_01_Final_ASB_ePub.xhtml#_idTextAnchor020)*, Getting Started with Machine Learning Using Amazon SageMaker*:

    ```
    s3_bucket = '<insert bucket name here>'
    prefix = 'chapter03'
    ```

    请注意，我们的 training_data.csv 文件应该(1)已经存在于 S3 存储桶中，并且(2)它应该具有路径 S3://<s3 bucket="" name="">/<prefix>/synthetic/training _ data . CSV</prefix></s3>

7.  设置train _ S3 变量的值:

    ```
    train_s3 = \ f"s3://{s3_bucket}/{prefix}/synthetic/training_data.csv"  
    ```

8.  使用 TrainingInput 类准备 content_type 设置为“text/csv”的训练输入参数:

    ```
    from sagemaker.inputs import TrainingInput train_input = TrainingInput(train_s3, content_type="text/csv")
    ```

9.  导入一些运行培训作业的先决条件。除此之外，我们将使用 LocalSession 来初始化 sagemaker_session 对象。LocalSession 类允许我们在训练和部署步骤中使用**本地模式**。正如我们将在后面的步骤中看到的，在初始化估计器时，我们将指定' local '作为 instance_type 的参数值，以使我们在 SageMaker 笔记本实例中本地执行训练工作:

    ```
    import sagemaker from sagemaker import get_execution_role from sagemaker.local import LocalSession      sagemaker_session = LocalSession() sagemaker_session.config = {'local': {'local_code': True}}      role = get_execution_role()
    ```

10.  Initialize the SKLearn estimator from the **SageMaker Python SDK**:

    ```
    from sagemaker.sklearn.estimator import SKLearn

    estimator = SKLearn(entry_point='sklearn_script.py',
                        session=sagemaker_session,
                        role=role,
                        instance_type='local',
                        instance_count=1,
                        py_version='py3',
                        framework_version='0.20.0')
    ```

    如您所见，我们没有指定脚本文件运行的容器。SKLearn 估计器利用了 **SageMaker** 中的一个预建 Docker 容器映像。这意味着我们不必担心准备容器，因为 SageMaker 已经为我们准备好了。如果这还不够，我们还可以选择扩展现有的**预构建 SageMaker Docker 映像**。

11.  Perform the training step using the fit() function:

    ```
    estimator.fit({'train': train_input})
    ```

    这将产生一组类似于图 3.57 中所示的日志。

    ![Figure 3.57 – Training logs after running the fit() function after initializing the SKlearn estimator
    ](img/B16850_03_57.jpg)

    图 3.57–初始化 SKlearn 估计器后运行 fit()函数后的训练日志

    如果您在运行 fit()函数时遇到问题，您可以查看本章末尾的配方*使用本地模式时调试磁盘空间问题*和*使用本地模式时调试容器执行问题*。

12.  Once the training step has been completed, deploy the model using the deploy() function:

    ```
    predictor = estimator.deploy(initial_instance_count=1, instance_type='local')
    ```

    我们应该得到一组类似于*图 3.58* 中的日志和输出值:

    ![Figure 3.58 – Logs and output after using the predict() function in local mode
    ](img/B16850_03_58.jpg)

    图 3.58–在本地模式下使用 predict()函数后的日志和输出

    如图*图 3.58* 所示，当在**本地模式**中调用 predict()函数时，运行容器内的/invocations 端点被 POST 请求触发。处理完请求后，predict()函数将响应返回给用户。

    注意

    如果您想知道在运行本地预测端点时使用哪个容器映像，只需 run！停靠集装箱，向展示正在运行的集装箱。我们应该得到一个等于或类似于 683313688378 . dkr . ECR . us-east-1 . Amazon AWS . com/sage maker-sci kit-learn:0 . 20 . 0-CPU-py3 的值。有关更多信息，请查看位于[https://github . com/AWS/deep-learning-containers/blob/master/available _ images . MD](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)的图片。

13.  Perform a test prediction using the predict() function:

    ```
    import numpy as np

    input_data = np.array([[100], [200]], dtype=np.float32)
    result = predictor.predict(input_data)

    result
    ```

    我们应该得到一组类似于图 3.59 中所示的日志:

    ![Figure 3.59 – Logs and output after using the predict() function in local mode
    ](img/B16850_03_59.jpg)

    图 3.59–在本地模式下使用 predict()函数后的日志和输出

    如图*图 3.59* 所示，在本地模式下调用 predict()函数时，正在运行的容器内的/invocations 端点被 POST 请求触发。处理完请求后，predict()函数将响应返回给用户。

14.  创建一个名为 tmp:

    ```
    !mkdir -p tmp
    ```

    的空目录
15.  Download the all_data.csv file from the S3 bucket to the tmp directory:

    ```
    all_s3 = \
    f"s3://{s3_bucket}/{prefix}/synthetic/all_data.csv" 
    !aws s3 cp {all_s3} tmp/all_data.csv
    ```

    接下来的几个步骤集中在检查一组代表“预测线”的 x 值的 y 预测值。

    ![Figure 3.60 – The predict() function using the custom neural network deployed in the inference endpoint to predict the y values from the x input values
    ](img/B16850_03_60.jpg)

    图 3.60–predict()函数使用部署在推理端点中的自定义神经网络从 x 输入值预测 y 值

    在*图 3.60* 中，我们使用自定义的 **SKLearn** MLPRegressor 神经网络，使用 predict()函数从 x 值生成预测的 y 值，我们已经训练并部署到推理端点。

16.  加载 all_data.csv 文件:

    ```
    import pandas as pd all_data = pd.read_csv("tmp/all_data.csv", header=None) x = all_data[[1]].values y = all_data[[0]].values
    ```

17.  准备包含 x 值在-5000 和 5000 之间的 line_x 变量【T4:】
18.  使用部署的模型预测 line_x:

    ```
    input_data = np.array(line_x.reshape(-1, 1), dtype=np.float32) result = predictor.predict(input_data) result
    ```

    中每个 x 的 y 值
19.  将结果存储在 line_y 变量中:

    ```
    line_y = result
    ```

20.  Render the line on top of the scatterplot of the original dataset:

    ```
    from matplotlib import pyplot
    pyplot.plot(line_x, line_y, 'r')
    pyplot.scatter(x,y,s=1)
    pyplot.show()
    ```

    在*图 3.61* 中，我们可以看到预测线以及数据集的散点图，以便快速查看预测值如何与实际值相匹配:

    ![Figure 3.61 – Prediction line using the scikit-learn custom neural network over the scatterplot showing the actual values from the generated synthetic dataset
    ](img/B16850_03_61.jpg)

    图 3.61–在散点图上使用 scikit-learn 自定义神经网络的预测线，显示生成的合成数据集的实际值

    正如我们在*图 3.61* 中看到的，我们的定制 scikit-learn 模型似乎在数据集上做得很好。我们可以执行额外的模型评估步骤来恰当地度量模型的性能。我们将跳过这一步，以防止这个食谱太长。

21.  Finally, stop the running container with the inference endpoint using the delete_endpoint() function:

    ```
    predictor.delete_endpoint()
    ```

    这应该会阻止容器运行我们的本地预测端点。

让我们看看这是如何工作的！

## 工作原理...

使用 **SageMaker Python SDK** 的 **scikit-learn** 支持，我们能够使用来自 recipe *的**sci kit-learn**entry point 脚本直接准备 entrypoint scikit-learn 训练脚本*，而不必构建和管理用于实验的容器，因为 SDK 为我们抽象了这个脚本。

注意

对于更多的信息，随时查看*它是如何工作的...*配方部分*使用 Amazon SageMaker 本地模式*培训和部署 TensorFlow 和 Keras 模型。为了避免重复的内容，我们将不讨论本节中的一些细节。

## 亦见

如果您正在寻找使用真实数据集在 SageMaker 中培训和部署 **scikit-learn** 模型的示例，请随意查看**AWS/Amazon-SageMaker-examples**GitHub 资源库中的一些笔记本:

*   使用 **scikit-learn** 训练和部署分类器:[https://github . com/AWS/Amazon-sagemaker-examples/blob/master/sagemaker-python-SDK/scikit _ learn _ iris/scikit _ learn _ estimator _ example _ with _ batch _ transform . ipynb](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_iris/scikit_learn_estimator_example_with_batch_transform.ipynb)
*   构建自己的算法容器:[https://github . com/AWS/Amazon-sage maker-examples/blob/master/advanced _ functionality/scikit _ bring _ your _ own/scikit _ bring _ your _ own . ipynb](https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb)

除了 **TensorFlow、PyTorch 和 scikit-learn** 之外，我们还可以通过使用特定于框架的估计器类——MXNet、Chainer、Hugging Face 和 XGBoost,**SageMaker Python SDK**轻松使用其他框架和库。在本书中，我们不会深入研究使用 **Apache MXNet** 、 **Chainer** 和**抱脸**训练和部署模型。

# 使用本地模式时调试磁盘空间问题

有时，当在使用**本地模式**的实验中运行 estimator.fit()函数时，我们可能会遇到类似于*图 3.62* 所示的问题。

![Figure 3.62 – CalledProcessError potentially due to a disk space issue
](img/B16850_03_62.jpg)

图 3.62–调用 ProcessError 可能是由于磁盘空间问题

请注意，这可能是也可能不是由磁盘空间问题引起的，但根本原因很可能是没有剩余空间。该错误消息可能包括以下错误消息:

```
CalledProcessError: Command '['docker', 'pull', '763104351884.dkr.ecr.us-east-1.amazon.com/<image-uri>:<tag>'] ' returned non-zero exit status 1.
```

在这个食谱中，我们将看看如何调试这个问题。

小费

如果运行本章中的食谱时一切顺利，请随时查看*还有更多...*一节，了解如何重现此问题的说明。一旦复制了这个问题，您可以按照*如何做*部分中的说明继续操作。

## 准备就绪

这个配方的先决条件是使用 fit()函数时触发的 CalledProcessError。你可以试着通过检查*来触发这个错误...*部分在进行*如何做*部分的步骤之前。

## 怎么做

我们需要在出现问题的 Jupyter 笔记本中运行以下语句。

1.  Create a new cell after the Estimator object has been initialized. Run the following statement so that we can check which Docker image was being downloaded when the error occurred:

    ```
    estimator.training_image_uri()
    ```

    我们期望前面语句的输出等于或类似于' 763104351884 . dkr . ECR . us-east-1 . Amazon AWS . com/tensor flow-training:2 . 1 . 0-CPU-py3 '。

2.  Run the following command to see the actual error message when pulling the container image:

    ```
    !docker pull {estimator.training_image_uri()}
    ```

    我们应该会看到一组类似于图 3.63 中的日志。

    ![Figure 3.63 – The "no space left on deviceA" issue
    ](img/B16850_03_63.jpg)

    图 3.63–设备 a 上没有剩余空间的问题

    如*图 3.63* 所示，我们在运行 docker pull 命令后遇到了磁盘空间问题。

3.  Next, use the df -h command to check the available disk space:

    ```
    !df -h
    ```

    这将为我们提供类似于图 3.64 所示的磁盘空间统计数据。

    ![Figure 3.64 – /dev/xvda1 has 1.9 GB of available disk space left
    ](img/B16850_03_64.jpg)

    图 3.64-/dev/xvda 1 还有 1.9 GB 的可用磁盘空间

    在*图 3.64* 中，我们可以看到我们还有 20 MB 的可用磁盘空间。当我们执行 docker pull 命令时，它拉出了一个大到足以触发磁盘空间问题的容器。

4.  You may initially try running the following command to clear up a bit of space as a quick fix:

    ```
    !docker rmi -f $(docker images -a -q)
    ```

    运行此命令将删除系统中的所有 Docker 容器映像。我们应该得到类似于图 3.65 所示的输出。

    ![Figure 3.65 – Docker container images deleted
    ](img/B16850_03_65.jpg)

    图 3.65–Docker 容器图像被删除

    我们可以看到使用 docker rmi 已经清理了一些空间。

    小费

    要查看下载的深度学习容器使用的图像空间，可以使用命令！Jupyter 笔记本电脑单元中的 docker 系统 df -v。

5.  Run the following command to see how much space we have freed up:

    ```
    !df -h
    ```

    这将为我们提供类似于图 3.66 所示的磁盘空间统计数据。

    ![Figure 3.66 – /dev/xvda1 has 9.6 GB of available disk space left 
    ](img/B16850_03_66.jpg)

    图 3.66-/dev/xvda 1 还有 9.6 GB 的可用磁盘空间

    我们可以在*图 3.66* 中看到，在运行 docker rmi 命令后，我们已经恢复了 14 GB 的磁盘空间。以前我们只有 20 MB 的磁盘空间。现在我们有 14 GB 了！

6.  Run the cell again with the fit() or predict() function call. If that succeeds, then that's great! This means that we freed up just enough space to pull the container image. Otherwise, follow the steps in the recipe *Preparing the SageMaker notebook instance for multiple deep learning local experiments* at the start of this chapter to give us the extra disk space we need to run our experiments.

    小费

    如果所有其他方法都失败了，重新启动 notebook 实例，看看这是否能解决您的问题。重启 notebook 实例应该会重置正在运行的服务(例如 docker)的配置。

现在，让我们看看这是如何工作的！

## 它是如何工作的...

在这个方法中，我们执行了几个步骤来调试和修复调用 fit()或 predict()函数时的潜在磁盘空间问题。当在**本地模式**下调用 fit()函数时，从 ECR 储存库中提取训练容器图像，然后使用适当的参数运行。当存在磁盘空间问题时，我们可能会收到如下所示的错误消息:

```
CalledProcessError: Command '['docker', 'pull', '763104351884.dkr.ecr.us-east-1.amazon.com/<image-uri>:<tag>'] ' returned non-zero exit status 1.
```

也有可能出现类似这样的错误:

```
RuntimeError: Failed to run: ['docker-compose', '-f', '/tmp/abcdefghij12345/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1
```

错误消息取决于磁盘空间问题发生的时间。在第一个场景中，由于缺少可用空间，磁盘空间问题阻止了 docker pull 命令的成功。在第二个场景中，容器已经成功下载，磁盘空间问题导致容器中的脚本失败。在下一个菜谱中，我们将看到 fit()函数失败的其他可能原因。

最后，我们还可能会收到错误消息，这些消息看起来完全是另一个问题，但却是磁盘空间问题的副作用。正如在本食谱的*如何做*部分中所看到的，一个好的第一步是首先检查我们是否有磁盘空间问题。如果是磁盘空间问题，我们可以使用以下一种或两种方法来解决问题:

*   例如，一个短期的解决方案，为实验腾出足够的空间
*   一个长期的解决方案，例如，增加 SageMaker 笔记本实例的卷大小，这样我们就不需要每次遇到磁盘空间问题拦截器时都删除大文件

对于长期解决方案，请查看本章中的配方*为多个深度学习本地实验准备 SageMaker 笔记本实例*。

## 还有更多...

您想触发此错误消息吗？很简单！打开一个新的终端并运行以下命令来下载一些容器映像，这会占用大量空间:

```
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com
docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.5.0-cpu-py3
docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.5.0-cpu-py3
docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com/mxnet-inference:1.7.0-cpu-py36-ubuntu16.04
docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com/mxnet-inference-eia:1.4.1-cpu-py36-ubuntu16.04
```

当我们使用 fit()函数时，继续在不同的容器图像上使用 docker pull，而不是我们想要拉的容器图像。如果你需要一个容器图片列表和它们的容器图片 URI 值，可以参考这里的可用图片列表:[https://github . com/AWS/deep-learning-containers/blob/master/available _ images . MD](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)。

一旦收到类似如下的错误消息，您可以停止提取图像容器:

无法注册层:处理 tar 文件时出错(退出状态 1):write/usr/local/lib/python 3.6/site-packages/...:设备上没有剩余空间

此时，您可以返回到使用 SageMaker 本地模式运行实验的 Jupyter 笔记本。再次运行包含 fit()函数调用的单元格，看看是否会触发错误。

![Figure 3.67 – CalledProcessError due to a disk space issue
](img/B16850_03_67.jpg)

图 3.67–由于磁盘空间问题调用了 ProcessError

在*图 3.67* 中，我们可以看到我们能够通过提取大量容器映像来触发磁盘空间问题错误。当然，有不同的方法来填满可用空间，但这一个应该工作得很好！

重要说明

如果您没有收到涉及 docker pull 的错误，而是收到了 docker compose 错误，这意味着已经下载了训练映像。解决方法是在初始化估计器对象时指定一个不同的 framework_version。这将强制提取一个新的 Docker 容器映像，从而触发所需的问题和错误消息。

既然您已经触发了这个错误消息，那么您现在可以继续执行这个方法中*如何做*部分的指令来调试和解决这个磁盘空间问题。

# 调试使用本地模式时的容器执行问题

如果您在使用 **SageMaker 本地模式**调用 fit()函数时遇到了类似于图 3.68 所示的问题，这就是您的解决方法！

![Figure 3.68 – Error running the fit() function
](img/B16850_03_68.jpg)

图 3.68–运行 fit()函数时出错

在*图 3.68* 中，我们可以看到我们在执行 fit()函数时遇到了问题。有时，错误消息会在调试信息的末尾包含以下日志消息:

```
RuntimeError: Failed to run: ['docker-compose', '-f', '/tmp/abcdefghij12345/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1
```

在某些情况下，错误的根本原因并没有真正显示给用户，这使得这个问题对于一些机器学习实践者来说很难调试。不要担心，因为这种方法将证明在调试这些类型的问题有用！

小费

如果运行本章中的食谱时一切顺利，请随时查看*还有更多...*一节，了解如何重现此问题的说明。一旦复制了此问题，您可以按照*如何做*部分中的说明继续操作。

## 准备就绪

该配方的先决条件是使用 fit()函数时触发的 RunTimeError。你可以试着通过检查*来触发这个错误...*一节继续进行*如何做*一节中的步骤。

## 怎么做...

这个菜谱中的步骤主要是识别容器 ID，并使用 docker logs 命令来帮助我们调试容器内部的错误:

使用 docker container ls -a 命令列出所有容器。记下容器 ID 值:

```
!docker container ls -a
```

我们应该得到一组类似于图 3.69 所示的结果。

![Figure 3.69 Result of running docker container ls -a
](img/B16850_03_69.jpg)

图 3.69 运行 docker 容器 ls -a 的结果

或者，我们可以通过调用 fit()函数后检查日志来获得容器 ID。

![Figure 3.70 – Getting the container ID 
](img/B16850_03_70.jpg)

图 3.70–获取容器 ID

在获取容器 ID 时，您可以选择前面两个选项中的任何一个。在*图 3.70* 的示例中，集装箱 ID 为 ss25108nrl-algo-1-lo166。当然，在您的情况下，您将获得一个不同的容器 ID，因为这是在每次运行容器时随机生成的。

使用 docker logs 命令检查目标容器的日志。如果我们要使用上一步示例中的容器 ID，我们将用“ss25108nrl-algo-1-lo166”替换“<在此处插入容器 ID>”:

```
CONTAINER_ID="<insert container id here>"
docker logs $CONTAINER_ID
```

请注意，每次执行 fit()函数时，容器 ID 都会发生变化，因此请确保从 Jupyter 笔记本的日志中获取它的最新值。

您还可以运行以下代码块来获取上次容器运行的日志:

```
%%bash
LAST_CONTAINER_ID=$(docker container ls -a -q | head -n 1)
docker logs $LAST_CONTAINER_ID
```

这里发生了什么？docker container ls -a -q 命令将列出所有的容器 id，而 head -n 1 命令将返回它得到的第一个条目。由于 docker 容器的结果是从最新到最早执行的容器排序的，这些命令将返回最后执行的容器的 ID。当然，这段代码需要在 fit()函数失败后立即执行，因为我们正在检查最后执行的容器的日志。我们应该得到类似于图 3.71 所示的输出。

![Figure 3.71 – Logs of the container
](img/B16850_03_71.jpg)

图 3.71–集装箱日志

我们可以在*图 3.71* 中看到我们在 numpy.random import seeds 的行中不小心把 seed 拼错成了 seeds。当然，这不是意外！检查一下*还有更多...*关于如何复制和触发此问题的章节。

小费

有时候，问题可以通过跑步来解决！sudo 服务 docker 重新启动。在进行其他调试步骤之前，您可以先尝试一下。

现在，让我们看看这是如何工作的！

## 它是如何工作的...

在这个菜谱中，我们使用 docker logs 命令来帮助我们调试失败的容器运行。当在**本地模式**下调用 fit()函数时，从 ECR 存储库中提取训练容器映像，然后使用适当的参数运行。

这将调用容器中的训练脚本，并运行训练脚本中的代码。脚本执行完毕后，一些信息和错误日志会返回给用户。在大多数情况下，这些错误日志是有帮助的，但在某些情况下，Jupyter 笔记本单元格中打印的错误消息可能对我们没有任何帮助。

fit()函数可能失败的原因有很多。以下是一些可能的原因:

*   磁盘空间问题
*   权限问题
*   自定义脚本错误和问题
*   导入的库或包问题
*   容器错误和问题

当处理磁盘空间问题时，我们可以简单地按照菜谱*中的步骤使用本地模式*调试磁盘空间问题。对于其他问题，这将涉及打开一个终端选项卡并运行几个 bash 命令，以便我们可以检查上次执行的日志。由于 entry_point 脚本在容器内部运行，这意味着我们可以使用 docker logs 命令来检查正在运行和以前运行的容器的日志。

最后，有时问题的根本原因是深度学习框架或容器映像中引入的错误。这方面的一个例子是，一项培训工作在使用 1.5.0 版本的 **PyTorch** 深度学习容器时工作正常，然后在使用 1.6.0 版本时突然失败。当然，这取决于问题的背景，但了解这一点将有助于您了解要更改哪些配置选项，以便更快地调试问题。

## 还有更多...

您想触发此错误消息吗？很简单！

第一步包括复制在训练和推断过程中使用的原始工作入口点脚本文件。

![Figure 3.72 – Copying the entrypoint script file to preserve the working version
](img/B16850_03_72.jpg)

图 3.72–复制入口点脚本文件以保留工作版本

第二步是故意在新入口点文件的源代码中造成一个错误。在这种情况下，我们可以简单地将 from numpy.random 导入种子替换为 from numpy.random 导入种子。假设 numpy.random 中没有种子函数，那么这会导致一个错误。

第三步是在初始化评估器时更新 Jupyter 笔记本中的相关代码块。我们只需要更新 entry_point 参数的值来指向新文件:

```
estimator = TensorFlow(entry_point='tensorflow_script_broken.py',
                       output_path=s3_output_location,
                       role=role,
                       sesion=sagemaker_session,
                       instance_count=1,
                       instance_type='local',
                       framework_version='2.1.0',
                       py_version='py3')
```

最后，当我们调用 fit()函数时，应该会得到一个类似于*图 3.73* 所示的错误消息。

![Figure 3.73 – Failed to run command (RuntimeError)
](img/B16850_03_68.jpg)

图 3.73–无法运行命令(RuntimeError)

既然您已经触发了该错误信息，您现在可以继续执行*如何操作中的指令...本配方的*部分用于调试和解决磁盘空间问题。**