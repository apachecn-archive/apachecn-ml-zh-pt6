<html><head/><body>





<style type="text/css">body{margin:1em;background-color:transparent!important;}#sbo-rt-content *{text-indent:0pt!important;}#sbo-rt-content .bq{margin-right:1em!important;}</style>
<div><div><h1 id="_idParaDest-169">第四章:准备、处理和分析数据</h1>
			<p>在我们开始训练我们的机器学习模型之前，我们必须准备、处理并转换我们的数据为算法可以处理的结构和格式。我们可以使用不同的技术和服务来处理不同的数据处理和分析需求。本章中的配方集中在执行这些任务时的关键SageMaker功能、算法和特性。其中包括针对我们的托管数据处理和转换需求的<strong class="bold"> SageMaker处理</strong>，支持使用<strong class="bold"> Amazon Athena </strong>调用已部署的SageMaker机器学习模型以使用SQL语句分析我们的数据，内置的<strong class="bold">主成分分析</strong> ( <strong class="bold"> PCA </strong>)算法用于执行维度缩减，内置的<strong class="bold"> KMeans </strong>算法用于执行聚类分析。</p>
			<p>我们将从对Amazon Athena的简单介绍开始，我们将使用它来帮助我们使用SQL语法处理和分析我们在S3的大型数据集和文件。我们还将调用一个已部署的<strong class="bold">随机砍伐森林</strong> ( <strong class="bold"> RCF </strong>)模型，使用SQL语句来检测我们的合成数据集中的异常。如果这是你第一次听说Amazon Athena，这是一个完全托管的服务，帮助我们使用SQL语法分析亚马逊S3中的数据。我们还将展示如何将CSV格式的数据转换为protobuf recordIO格式。大多数SageMaker算法使用这种格式效果最好，因为它允许我们利用<strong class="bold">管道模式</strong>。我们将用生成的protobuf recordIO文件训练一个<strong class="bold"> K近邻</strong> ( <strong class="bold"> KNN </strong>)模型。最后，我们将使用SageMaker处理来帮助我们从基础设施中自动化和抽象处理作业。SageMaker处理可以用来帮助我们在我们的<strong class="bold"> SageMaker笔记本实例</strong>之外的托管基础设施内运行脚本。这些脚本可能包含使用小型或大型数据集处理数据和评估模型的任务。</p>
			<p>我们将在本章中介绍以下配方:</p>
			<ul>
				<li>生成用于异常检测实验的合成数据集</li>
				<li>培训和部署RCF模型</li>
				<li>使用SQL查询调用Amazon Athena的机器学习模型</li>
				<li>使用Python中的Amazon Athena分析数据</li>
				<li>生成用于分析和转换的合成数据集</li>
				<li>使用内置的PCA算法执行降维</li>
				<li>使用内置的KMeans算法执行聚类分析</li>
				<li>将CSV数据转换为protobuf recordIO格式</li>
				<li>使用protobuf recordIO训练输入类型训练KNN模型</li>
				<li>使用AWS CLI准备SageMaker处理先决条件</li>
				<li>Python中使用SageMaker处理的托管数据处理</li>
				<li>R中使用SageMaker处理的托管数据处理</li>
			</ul>
			<p>这些配方将被证明对数据科学家和机器学习实践者有用，因为实验中完成的工作的主要部分涉及数据处理、转换和分析。如果您正在寻找使用<strong class="bold"> SageMaker Clarify </strong>进行培训前和培训后偏差检测的方法，请随时查看<a href="B16850_07_Final_ASB_ePub.xhtml#_idTextAnchor602"> <em class="italic">第7章</em> </a>、<em class="italic">使用SageMaker特征库、SageMaker Clarify和SageMaker模型监视器</em>。</p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor201"/>技术要求</h1>
			<p>要执行本章中的配方，请确保您具备以下条件:</p>
			<ul>
				<li>正在运行的Amazon SageMaker笔记本实例(例如ml.t2.large)</li>
				<li>一个亚马逊S3桶</li>
			</ul>
			<p>如果您还没有准备好这些先决条件，请随时查看第一章  <em class="italic">中的<em class="italic">启动亚马逊SageMaker笔记本实例</em>和<em class="italic">准备亚马逊S3桶和线性回归实验的训练数据集</em>食谱，使用亚马逊SageMaker </em>开始机器学习。</p>
			<p>由于本章中的食谱涉及到一些代码，我们在这个资源库中提供了脚本和笔记本:<a href="https://github.com/PacktPublishing/Machine-Learning-with-Amazon-SageMaker-Cookbook/tree/master/Chapter04">https://github . com/packt publishing/Machine-Learning-with-Amazon-sage maker-Cookbook/tree/master/chapter 04</a>。</p>
			<div><div><img src="img/B16850_04_01.jpg" alt="Figure 4.1 – Machine-Learning-with-Amazon-SageMaker-Cookbook GitHub repository&#13;&#10;" width="1650" height="954"/>
				</div>
			</div>
			<p class="figure-caption">图4.1-使用Amazon-SageMaker-Cookbook的机器学习GitHub知识库</p>
			<p>如<em class="italic">图4.1 </em>所示，我们在第04章目录中组织了本章食谱的脚本和笔记本的源代码。</p>
			<p>请点击以下链接查看动作视频中的相关代码:</p>
			<p><a href="https://bit.ly/3he32PO">https://bit.ly/3he32PO</a></p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor202"/>生成用于异常检测实验的合成数据集</h1>
			<p>在这个配方中，我们<a id="_idIndexMarker688"/>将<a id="_idIndexMarker689"/>生成一个包含异常值或异常值的合成数据集。这将使我们能够使用诸如<strong class="bold">随机砍伐森林</strong> (RCF)等算法来执行异常检测实验。如果这是您第一次听说异常检测，这是异常值或与数据集的其余记录显著不同的记录的识别。什么是RCF算法？RCF算法是一种无监督算法，用于检测数据集中的这些异常。</p>
			<p>在我们在该配方中生成合成数据集后，我们将使用生成的数据集来训练和部署RCF模型，并在<em class="italic">使用SQL查询调用Amazon Athena机器学习模型</em>配方中的Amazon Athena查询内触发该模型。这将使我们能够在数据准备和分析阶段标记数据集中的异常。</p>
			<p class="callout-heading">小费</p>
			<p class="callout">由于我们将展示如何在该配方中生成合成数据集的步骤，您将有机会在以后调整该配方以满足您的需求。您可以决定生成更多记录来测试Amazon Athena的扩展能力，并查看您的查询在处理大型数据集时的表现。</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor203"/>做好准备</h2>
			<p>以下<a id="_idIndexMarker690"/>是该配方的先决条件<a id="_idIndexMarker691"/>:</p>
			<ul>
				<li>在SageMaker笔记本实例中使用conda_python3内核的新Jupyter笔记本</li>
			</ul>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor204"/>怎么做……</h2>
			<p>该配方中的第一组步骤涉及在生成合成数据集之前设置和准备一些先决条件:</p>
			<ol>
				<li>导航到SageMaker笔记本实例中的my-experiments/chapter04目录。如果这个目录还不存在，请随意创建。</li>
				<li>在my-experiments/chapter04目录下使用conda_python3内核创建一个新的笔记本，并以这个配方的名称命名(即生成一个用于异常检测实验的合成数据集)。打开此笔记本进行编辑，因为我们将在接下来的几个步骤中使用代码更新此文件。</li>
				<li>通过导入一些先决条件来生成随机数和字符串值，启动笔记本:<pre>import random from string import ascii_uppercase from random import randint, choice</pre></li>
				<li>定义<a id="_idIndexMarker692"/>三个<a id="_idIndexMarker693"/>函数，分别叫做generate_normal_point()，generate _ anomal _ point()，generate_random_string(): <pre>def generate_normal_point():     return randint(0, 10)       def generate_abnormal_point():     return randint(70, 80)       def generate_random_string():     letters = ascii_uppercase     return ''.join(         choice(letters) for i in range(10)     )</pre></li>
				<li>Define the normal_or_abnormal() function:<pre>def normal_or_abnormal():
    tmp = randint(0, 20)
    
    if tmp == 20:
        return "abnormal"
    else:
        return "normal"</pre><p>现在我们已经有了所有需要的先决条件和函数，我们将运行几个代码块来生成数据集。</p></li>
				<li>Generate 1,000 points<a id="_idIndexMarker694"/> with <a id="_idIndexMarker695"/>a small set of numbers tagged as abnormal:<pre>list_of_points = []
     
for _ in range (0,1000):
    point_type = <strong class="bold">normal_or_abnormal()</strong>
    
    point_value = 0
    string_value = <strong class="bold">generate_random_string()</strong>
    
    if point_type == "normal":
        point_value = <strong class="bold">generate_normal_point()</strong>
    else:
        point_value = <strong class="bold">generate_abnormal_point()</strong>
    
    point = {
        "label": string_value,
        "value": point_value
    }
    list_of_points.append(point)</pre><p>前面的代码块使用for循环简单地生成了1，000次记录。在这1，000个记录中，一定百分比(例如5%)将是值明显大于正常点的异常点。</p></li>
				<li>Inspect the list_of_points list. All values between 70 and 80 are tagged as abnormal:<pre>list_of_points</pre><p>这个<a id="_idIndexMarker696"/>应该<a id="_idIndexMarker697"/>产生一组类似于<em class="italic">图4.2 </em>所示的结果:</p><div><img src="img/B16850_04_02.jpg" alt="Figure 4.2 – Label and value pairs inside the list_of_points list&#13;&#10;" width="328" height="324"/></div><p class="figure-caption">图4.2–点列表中的标签和值对</p><p>在图4.2 中，我们可以看到list_of_points列表中的字符串标签和整数值对。0到10分之间的正常分数应该比70到80分之间的异常分数多得多。</p></li>
				<li>Render a quick plot of the list of points generated:<pre>import pandas as pd
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [15, 5]
pd.DataFrame(list_of_points).plot()</pre><p>这将呈现类似于图4.3 中所示的图:</p><div><img src="img/B16850_04_03.jpg" alt="Figure 4.3 – Plot of the synthetic dataset containing outliers&#13;&#10;" width="857" height="301"/></div><p class="figure-caption">图4.3–包含异常值的合成数据集图</p><p>在<em class="italic">图4.3 </em>中，我们<a id="_idIndexMarker698"/>有<a id="_idIndexMarker699"/>一个显示正常数据点和异常值的合成数据集图。长垂直线表示异常值的存在，因为这些异常数据点的值在70和80之间。请注意，正常数据点的值在0到10之间。</p><p>在最后一组步骤中，我们集中精力将这个数据集上传到亚马逊S3桶。</p></li>
				<li>准备名为s3_files的目录，我们将使用它以JSON格式存储生成的点:<pre>!rm -rf s3_files !mkdir -p s3_files</pre></li>
				<li>Define the save_json_file() function. This function accepts an element of list_of_points and creates a JSON file inside the directory:<pre>import json
     
def save_json_file(point):
    label = point['label']
    filename = "s3_files/" + label + '.json'
    with open(filename, 'w') as file:
        json.dump(point, file)
        print(f"Saved {label}!")</pre><p class="callout-heading">注意</p><p class="callout">当<a id="_idIndexMarker700"/>使用<a id="_idIndexMarker701"/>处理文件路径时，建议使用os.path.join()连接路径。请注意，我们在这里采取了一种简化这个食谱的捷径。</p></li>
				<li>Use the save_json_file() function for each of the points generated inside list_of_points:<pre>for point in <strong class="bold">list_of_points</strong>:
    save_json_file(point)</pre><p>此时，s3_files目录中应该总共有1，000个文件。</p></li>
				<li>Create a new S3 bucket to store the Athena results. Note that this should be different from the S3 bucket we created in <a href="B16850_01_Final_ASB_ePub.xhtml#_idTextAnchor020"><em class="italic">Chapter 1</em></a><em class="italic">,  Getting Started with Machine Learning Using Amazon SageMaker</em>:<pre>bucket_name = "<strong class="bold">&lt;insert S3 bucket name here&gt;</strong>"
!aws s3 mb s3://{bucket_name}</pre><p>这应该会产生类似于make_bucket: <bucket name="">的日志消息。</bucket></p><p class="callout-heading">重要说明</p><p class="callout">请注意这里的S3存储桶名称，因为我们将在使用SQL查询的亚马逊Athena配方的<em class="italic">调用机器学习模型中使用该存储桶名称。</em></p></li>
				<li>Copy and upload the files inside the s3_files directory to the S3 bucket we just created:<pre>!aws s3 cp s3_files/ s3://{bucket_name}/ --recursive</pre><p>这应该<a id="_idIndexMarker702"/>产生<a id="_idIndexMarker703"/>一些类似于<em class="italic">图4.4 </em>所示的日志消息:</p><div><img src="img/B16850_04_04.jpg" alt="Figure 4.4 – Uploading the files to the S3 bucket&#13;&#10;" width="894" height="224"/></div><p class="figure-caption">图4.4–将文件上传到S3存储桶</p><p>我们可以在<em class="italic">图4.4 </em>中看到，存储在s3_files目录中的文件正在一次一个地上传到我们刚刚创建的s3存储桶中。</p></li>
				<li>Finally, use the %store magic to store the list_of_points variable for later use:<pre>%store list_of_points</pre><p>这将产生一条类似于存储的“点列表”(list)的日志消息。</p></li>
			</ol>
			<p>现在，让我们看看这是如何工作的！</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor205"/>工作原理……</h2>
			<p>在这个配方中，我们生成了一个包含简单标签-值对的合成数据集。这个合成数据集大约有5%是异常值，我们有意使异常值<a id="_idIndexMarker704"/>比“正常”值<a id="_idIndexMarker705"/>高得多。</p>
			<div><div><img src="img/B16850_04_05.jpg" alt="Figure 4.5 – Synthetic dataset with outlier values&#13;&#10;" width="776" height="272"/>
				</div>
			</div>
			<p class="figure-caption">图4.5–带有异常值的合成数据集</p>
			<p>我们可以在<em class="italic">图4.5 </em>中看到，异常值的值在70到80范围内，而“正常”点的值在0到10范围内。这是意料之中的，因为generate_normal_point()和generate _ anomal _ point()函数规定了这些随机生成的数字的最高和最低可能值。</p>
			<p>我们为什么要这么做？我们将使用这个数据集来训练和部署一个RCF模型，并演示如何在Amazon Athena查询中触发一个已部署的模型。我们将在稍后的<em class="italic">使用SQL查询</em>方法调用Amazon Athena的机器学习模型中完成这项工作。这将使我们能够在数据准备和分析阶段标记数据集中的异常。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">Amazon Athena的这一功能已经在预览模式下使用了一段时间，现在AWS最近宣布了这一功能的普遍可用性，我们决定在这里包含这些配方！请随意查看此页面了解更多信息:<a href="https://aws.amazon.com/about-aws/whats-new/2021/04/announcing-general-availability-of-amazon-athena-ml-powered-by-amazon-sagemaker/">https://AWS . Amazon . com/about-AWS/whats-new/2021/04/announding-general-avail-of-Amazon-Athena-ml-powered-by-Amazon-sage maker/</a>。</p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor206"/>培训和部署RCF模型</h1>
			<p>在这个菜谱中，我们<a id="_idIndexMarker706"/>将<a id="_idIndexMarker707"/>使用<strong class="bold"> SageMaker Python SDK </strong>训练和部署一个RCF模型。RCF算法是一种无监督算法，用于检测数据集中的异常。它将每个记录与异常分数值相关联，较高的异常分数值与可能被标记为异常值或异常的记录相关联。</p>
			<p>在我们已经在这个配方中训练和部署了RCF模型之后，我们将在<em class="italic">使用SQL查询调用机器学习模型的Amazon Athena配方中的Amazon Athena SQL查询中触发这个模型。这将使我们能够在数据准备和分析阶段标记数据集中的异常。</em></p>
			<h2 id="_idParaDest-176">正在准备…</h2>
			<p>该配方延续了<em class="italic">生成异常检测实验的合成数据集</em>。</p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor208"/>怎么做……</h2>
			<p>下一组步骤集中在使用我们在上一个配方中生成的数据集来准备RCF模型:</p>
			<ol>
				<li value="1">导航到SageMaker笔记本实例中的my-experiments/chapter04目录。如果这个目录还不存在，请随意创建。</li>
				<li>在my-experiments/chapter04目录下使用conda_python3内核创建一个新的笔记本，用这个菜谱的名字命名(即训练和部署一个随机采伐森林模型)。打开此笔记本进行编辑，因为我们将在接下来的几个步骤中使用代码更新此文件。</li>
				<li>Start the notebook by using the %store magic command to load the stored list of values for list_of_points:<pre>%store -r <strong class="bold">list_of_points</strong>
list_of_points</pre><p>我们应该会得到一个类似于<em class="italic">图4.6 </em>所示的字典列表:</p><div><img src="img/B16850_04_06.jpg" alt="Figure 4.6 – list_of_points&#13;&#10;" width="325" height="276"/></div><p class="figure-caption">图4.6–点列表</p><p>正如我们<a id="_idIndexMarker708"/>在<em class="italic">图4.6 </em>中可以<a id="_idIndexMarker709"/>看到的那样，它应该包含与<em class="italic">中的list_of_points列表完全相同的一组值，为异常检测实验</em>配方生成合成数据集。</p></li>
				<li>Extract the values using the map() function and store them in a list:<pre>point_values = list(map(lambda x: x["value"], list_of_points))
point_values</pre><p>我们应该得到类似于[10，1，0，2，3，8，3，… ]的结果列表。</p></li>
				<li>Use numpy to reshape the list:<pre>import numpy as np
np_array = np.array(point_values)
np_array = np_array.reshape(-1,1)
np_array</pre><p>我们应该得到一组结构类似array([[10]，[1]，[0]，[2]，…])的结果。</p></li>
				<li>为<a id="_idIndexMarker710"/> SageMaker <a id="_idIndexMarker711"/>实验初始化并准备一些先决条件，如role_arn和session:<pre>import sagemaker  from sagemaker import get_execution_role  role_arn = get_execution_role() session = sagemaker.Session()</pre></li>
				<li>初始化RandomCutForest估计器:<pre>from sagemaker import RandomCutForest estimator = <strong class="bold">RandomCutForest</strong>(     role_arn,      instance_count=1,      instance_type='ml.m5.xlarge',     sagemaker_session=session)</pre></li>
				<li>Run the fit() function to start preparing the model:<pre>record_set_input = estimator.record_set(np_array)
estimator.fit(record_set_input)</pre><p>这将产生一组类似于图4.7 中所示的日志:</p><div><img src="img/B16850_04_07.jpg" alt="Figure 4.7 – Logs after using fit() to prepare the RCF model&#13;&#10;" width="994" height="308"/></div><p class="figure-caption">图4.7–使用fit()准备RCF模型后的日志</p><p>在我们做好模型之前，应该要花几分钟的时间(T4)。注意，由于RCF算法是无监督的，因此在此步骤中我们不使用标记数据。</p></li>
				<li>使用deploy()函数将模型部署到推理端点。这里我们指定endpoint_name值，因为我们稍后将在触发该机器学习端点的SQL查询中使用该端点名称:<pre>predictor = estimator.<strong class="bold">deploy</strong>(     initial_instance_count=1,      instance_type="ml.m5.xlarge",      endpoint_name="<strong class="bold">sagemaker-cookbook-rcf</strong>")</pre></li>
				<li>Call the predict() function to get the anomaly scores of each of the points passed to the inference endpoint:<pre>results = predictor.<strong class="bold">predict</strong>(np_array)
results</pre><p>这将产生一组类似于图4.8 中<em class="italic">所示的结果:</em></p></li>
			</ol>
			<div><div><img src="img/B16850_04_08.jpg" alt="Figure 4.8 – Anomaly score results&#13;&#10;" width="402" height="420"/>
				</div>
			</div>
			<p class="figure-caption">图4.8-异常得分结果</p>
			<p>如<em class="italic">图4.8 </em>所示，<a id="_idIndexMarker714"/>推理端点将相应的<a id="_idIndexMarker715"/>异常分数返回给作为有效载荷传递给predict()函数的每个数据点。该值越高，数据点成为异常值或异常值的可能性就越大。</p>
			<p class="callout-heading">重要说明</p>
			<p class="callout">在使用Python中的Amazon Athena完成<em class="italic">数据分析之前，不要删除推理端点</em>。</p>
			<p>现在，让我们看看这是如何工作的！</p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor209"/>工作原理……</h2>
			<p>在这个配方中，我们使用了来自<em class="italic">的合成数据集生成用于异常检测实验的合成数据集</em>来准备和部署RCF模型。当使用RCF算法时，数据点与异常分数相关联，异常与更高的分数相关联。</p>
			<p>使用RCF算法非常简单。我们所需要做的就是将整形后的列表中的一组数值作为有效载荷传递给fit()函数，以便为模型准备几分钟。在我们使用deploy()函数将模型部署到推理端点之后，我们可以将不同的值作为有效负载传递给predict()函数<a id="_idIndexMarker716"/>以获得相应的异常<a id="_idIndexMarker717"/>分值。</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor210"/>参见</h2>
			<p>如果您正在寻找使用RCF模型检测真实数据集异常的示例和更复杂的示例，请随意查看aws/Amazon-sage maker-examples GitHub存储库中的一些笔记本，以及AWS网络研讨会视频中共享的笔记本:</p>
			<ul>
				<li>使用<a id="_idIndexMarker718"/> RCF模型检测Numenta异常基准NYC Taxi数据集中的异常:<a href="https://github.com/aws/amazon-sagemaker-examples/blob/35e2faf7d1cc48ccedf0b2ede1da9987a18727a5/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.ipynb">https://github . com/AWS/Amazon-sage maker-examples/blob/35 e 2 faf 7 D1 cc 48 cced f 0 B2 ede 1 da 987 a 18727 a5/introduction _ to _ Amazon _ algorithms/random _ cut _ forest/random _ cut _ forest . ipynb</a></li>
				<li>培训和<a id="_idIndexMarker719"/>部署RCF欺诈检测模型:<a href="https://github.com/awslabs/fraud-detection-using-machine-learning/blob/master/source/notebooks/sagemaker_fraud_detection.ipynb">https://github . com/aw slabs/fraud-detection-using-machine-learning/blob/master/source/notebooks/sage maker _ fraud _ detection . ipynb</a></li>
			</ul>
			<p>现在，让我们仔细看看如何在下一个菜谱中用SQL语句调用我们在Amazon Athena的这个菜谱中部署的RCF模型！</p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor211"/>使用SQL查询通过Amazon Athena调用机器学习模型</h1>
			<p>Amazon <a id="_idIndexMarker720"/> Athena <a id="_idIndexMarker721"/>是一个<strong class="bold">无服务器</strong>交互式查询服务，帮助我们使用SQL语法分析亚马逊S3的数据。由于它是一种无服务器的服务，机器学习从业者不再需要管理任何基础设施，因此我们可以专注于需要完成的工作。如果您以前使用过或听说过Amazon Athena，您一定知道该解决方案可以轻松扩展并支持<strong class="bold">大数据</strong>需求。Amazon Athena还支持多种数据格式(如CSV和文本文件)、分栏格式(如Parquet和ORC)和压缩数据格式(如Snappy和GZIP)。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">当然，这只是对无服务器的简单描述。如需更多信息，请随时查看https://aws.amazon.com/serverless/的<a href="https://aws.amazon.com/serverless/"/>。</p>
			<p>在这个菜谱中，我们将使用Amazon Athena来分析我们使用SQL <a id="_idIndexMarker722"/>语句存储在Amazon S3中的数据集。我们<a id="_idIndexMarker723"/>将在查询中使用部署的机器学习模型来检测数据中的异常。</p>
			<h2 id="_idParaDest-181">准备就绪</h2>
			<p>这个配方是从<em class="italic">训练和部署RCF模型</em>延续下来的。</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor213"/>怎么做……</h2>
			<p>第一组步骤主要是确保我们使用的是Athena引擎版本2:</p>
			<ol>
				<li value="1">Navigate to the Athena console. If you see a notification similar to what is shown in <em class="italic">Figure 4.9</em>, click the <strong class="bold">Edit workgroup page</strong> link. <div><img src="img/B16850_04_09.jpg" alt="Figure 4.9 – Athena console showing the New Athena query engine available notification&#13;&#10;" width="713" height="470"/></div><p class="figure-caption">图4.9–Athena控制台显示了新的Athena查询引擎可用通知</p><p>如果您<a id="_idIndexMarker724"/>没有<a id="_idIndexMarker725"/>看到<em class="italic">图4.9 </em>中的通知消息，您应该可以通过点击导航栏中的<strong class="bold">工作组:主</strong>选项卡导航到工作组列表，如下图所示:</p><div><img src="img/B16850_04_10.jpg" alt="Figure 4.10 – Navigation menu&#13;&#10;" width="720" height="93"/></div><p class="figure-caption">图4.10–导航菜单</p><p>这应该会将我们重定向到显示现有工作组的工作组列表页面。接下来，点击<strong class="bold">查看详情</strong>按钮，如图<em class="italic">图4.11 </em>所示:</p><div><img src="img/B16850_04_11.jpg" alt="Figure 4.11 – List of existing Athena workgroups&#13;&#10;" width="357" height="183"/></div><p class="figure-caption">图4.11–现有Athena工作组列表</p><p>最后点击<strong class="bold">编辑工作组</strong>按钮，如图<em class="italic">图4.12 </em>所示:</p><div><img src="img/B16850_04_12.jpg" alt="Figure 4.12 – Location of the Edit workgroup button on the workgroup details page&#13;&#10;" width="621" height="205"/></div><p class="figure-caption">图4.12–编辑工作组按钮在工作组详细信息页面上的位置</p><p>正如我们<a id="_idIndexMarker726"/>在<em class="italic">图4.12 </em>中<a id="_idIndexMarker727"/>所看到的，我们在工作组详细信息页面上有特定工作组的详细信息。我们还可以选择编辑、禁用或删除工作组。</p><div><img src="img/B16850_04_13.jpg" alt="Figure 4.13 – Edit workgroup page&#13;&#10;" width="877" height="481"/></div><p class="figure-caption">图4.13–编辑工作组页面</p><p>一旦我们进入<strong class="bold">编辑工作组</strong>页面，我们就可以继续下一步。</p></li>
				<li>On the <strong class="bold">Edit workgroup</strong> page, specify the <strong class="bold">Query result location</strong> path where we will store the query results.<div><img src="img/B16850_04_14.jpg" alt="Figure 4.14 – Selecting a query result location&#13;&#10;" width="865" height="556"/></div><p class="figure-caption">图4.14–选择查询结果位置</p><p>参见<em class="italic">图4.14 </em>至<a id="_idIndexMarker728"/>见<a id="_idIndexMarker729"/>该步骤更新哪个字段。点击<strong class="bold">选择</strong>按钮将打开一个弹出窗口，我们可以在其中选择目标S3桶和文件夹。</p></li>
				<li>Next, scroll down to the <strong class="bold">Query engine version</strong> section and make sure that the <strong class="bold">Athena engine version 2 (recommended)</strong> option is selected.<div><img src="img/B16850_04_15.jpg" alt="Figure 4.15 – Using Athena engine version 2&#13;&#10;" width="1296" height="520"/></div><p class="figure-caption">图4.15–使用Athena引擎版本2</p><p>如<em class="italic">图4.15 </em>所示，确保<a id="_idIndexMarker730"/>已选择<strong class="bold">的<a id="_idIndexMarker731"/>也手动选择一个发动机版本</strong>。</p></li>
				<li>Scroll down to the end of the page, and then click the <strong class="bold">Save</strong> button.<p class="callout-heading">小费</p><p class="callout">如果您在使用工作组时遇到问题，请随时查看该页面上共享的提示:<a href="https://docs.aws.amazon.com/athena/latest/ug/workgroups-troubleshooting.html">https://docs . AWS . Amazon . com/Athena/latest/ug/work groups-troubleshoot . html</a>。</p></li>
				<li>After we have been redirected back to the workgroup details page, click <strong class="bold">Query editor</strong> in the navigation bar. <div><img src="img/B16850_04_16.jpg" alt="Figure 4.16 – Workgroup details page&#13;&#10;" width="714" height="544"/></div><p class="figure-caption">图4.16–工作组详细信息页面</p><p>从<em class="italic">图4.16 </em>中<a id="_idIndexMarker732"/>可以看到，<strong class="bold">查询编辑器</strong>标签页应该位于页面的左上角(在<strong class="bold">保存的查询</strong>旁边)。</p></li>
				<li>On the left-hand side of the <strong class="bold">Query editor</strong> page, look for the <strong class="bold">Data source</strong> pane and click the <strong class="bold">Connect data source</strong> link.<div><img src="img/B16850_04_17.jpg" alt="Figure 4.17 – Connect data source&#13;&#10;" width="311" height="464"/></div><p class="figure-caption">图4.17–连接数据源</p><p>注意<em class="italic">图4.17 </em>中的<strong class="bold">数据源</strong>窗格位于<strong class="bold">查询编辑器</strong>页面的左侧角落。我们应该在刷新图标下面看到<strong class="bold">连接数据源</strong>链接。</p></li>
				<li>Make sure that the <strong class="bold">Query data in Amazon S3</strong> and <strong class="bold">AWS Glue Data Catalog</strong> options<a id="_idIndexMarker734"/> are <a id="_idIndexMarker735"/>selected. Click the <strong class="bold">Next</strong> button. <div><img src="img/B16850_04_18.jpg" alt="Figure 4.18 – Choose a data source&#13;&#10;" width="665" height="584"/></div><p class="figure-caption">图4.18–选择数据源</p><p>在<em class="italic">图4.18 </em>中，我们选择了<strong class="bold">亚马逊S3 </strong>中的查询数据，因为我们的数据集位于S3存储桶内。</p></li>
				<li>Choose <strong class="bold">AWS Glue Data Catalog in this account</strong>. Next, choose<strong class="bold"> Create a table using the Athena table wizard</strong>. Click the <strong class="bold">Continue to add table</strong> button.<div><img src="img/B16850_04_19.jpg" alt="Figure 4.19 – Connection details&#13;&#10;" width="1193" height="757"/></div><p class="figure-caption">图4.19–连接细节</p><p>点击<strong class="bold">继续添加表</strong>按钮，如图<em class="italic">图4.19 </em>所示，然后<a id="_idIndexMarker736"/>会让<a id="_idIndexMarker737"/>继续创建一个新的数据库，并在下一组步骤中在该数据库中添加一个表。</p></li>
				<li>Choose <strong class="bold">Create a new database</strong> for <strong class="bold">Database</strong>. Specify cookbook_athena_db for the database name. For the <strong class="bold">Table Name</strong> field, specify athena_table. Specify the S3 bucket name (that is, s3://&lt;bucket name&gt;) where the JSON files in the <em class="italic">Generating a synthetic dataset for anomaly detection experiments</em> recipe were uploaded in the <strong class="bold">Location of Input Data Set</strong> field. Click the <strong class="bold">Next</strong> button afterward.<div><img src="img/B16850_04_20.jpg" alt="Figure 4.20 – Add table form&#13;&#10;" width="1046" height="730"/></div><p class="figure-caption">图4.20–添加表格表单</p><p>确保<a id="_idIndexMarker738"/>检查<a id="_idIndexMarker739"/>指定的字段值与<em class="italic">图4.20 </em>中的值相同。</p></li>
				<li>Under <strong class="bold">Step 2: Data Format</strong>, select <strong class="bold">JSON</strong> and then click <strong class="bold">Next</strong>.<div><img src="img/B16850_04_21.jpg" alt="Figure 4.21 – Choosing the data format&#13;&#10;" width="740" height="465"/></div><p class="figure-caption">图4.21–选择数据格式</p><p>在<em class="italic">图4.21 </em>中，我们<a id="_idIndexMarker740"/>可以<a id="_idIndexMarker741"/>看到支持多种数据格式。这些包括阿帕奇网络日志，CSV和TSV。由于我们已经在S3桶中上传了JSON文件，我们将从这个选项组中选择<strong class="bold"> JSON </strong>。</p></li>
				<li>Under <strong class="bold">Step 3: Columns</strong>, specify two columns—<strong class="bold">Column Name</strong>: label and <strong class="bold">Column type</strong>: string and <strong class="bold">Column Name</strong>: value, and <strong class="bold">Column type</strong>: int. Click <strong class="bold">Next</strong>.<div><img src="img/B16850_04_22.jpg" alt="Figure 4.22 – Specifying the table columns&#13;&#10;" width="1052" height="710"/></div><p class="figure-caption">图4.22–指定表格列</p><p>确保<a id="_idIndexMarker742"/>至<a id="_idIndexMarker743"/>检查指定的字段值与我们在<em class="italic">图4.22 </em>中的值相同。</p></li>
				<li>Under <strong class="bold">Step 4: Partitions</strong>, click the <strong class="bold">Create table</strong> button.<div><img src="img/B16850_04_23.jpg" alt="Figure 4.23 – Adding a partition&#13;&#10;" width="1011" height="548"/></div><p class="figure-caption">图4.23–添加分区</p><p>注意<a id="_idIndexMarker744"/>我们<a id="_idIndexMarker745"/>在这一步不会添加任何分区，如图<em class="italic">图4.23 </em>所示。我们只需点击<strong class="bold">创建表格</strong>按钮来完成表格创建过程。</p><p class="callout-heading">小费</p><p class="callout">有关如何使用分区来提高查询性能和降低成本的更多信息，请随时查看<a href="https://docs.aws.amazon.com/athena/latest/ug/partitions.html">https://docs . AWS . Amazon . com/Athena/latest/ug/partitions . html</a>。</p></li>
				<li>Beside the <strong class="bold">New query 1</strong> or <strong class="bold">New query 2</strong> tab, click the <strong class="bold">+</strong> icon to create a new tab where we can run a new query.<div><img src="img/B16850_04_24.jpg" alt="Figure 4.24 – Creating a new tab&#13;&#10;" width="907" height="383"/></div><p class="figure-caption">图4.24–创建新选项卡</p><p>这个<a id="_idIndexMarker746"/>应该<a id="_idIndexMarker747"/>打开一个新的标签页，如图<em class="italic">图4.25 </em>所示:</p><div><img src="img/B16850_04_25.jpg" alt="Figure 4.25 – New tab&#13;&#10;" width="633" height="212"/></div><p class="figure-caption">图4.25–新选项卡</p><p>点击新选项卡内的文本区域，如<em class="italic">图4.25 </em>所示。</p></li>
				<li>Type the following SQL statement in the query editor pane. Click the <strong class="bold">Run query</strong> button to execute the query:<pre>SELECT label, value FROM <strong class="bold">cookbook_athena_db.athena_table</strong> LIMIT 10;</pre><p>如果一切都配置和设置正确，我们应该得到一组类似于图4.26 所示的结果:</p><div><img src="img/B16850_04_26.jpg" alt="Figure 4.26 – Query results&#13;&#10;" width="1124" height="741"/></div><p class="figure-caption">图4.26–查询结果</p><p>在<em class="italic">图4.26 </em>中，我们<a id="_idIndexMarker748"/>可以<a id="_idIndexMarker749"/>看到显示在查询编辑器窗格底部的查询结果。注意，查询结果中的每一项都对应于我们之前准备的S3存储桶中上传的一个JSON文件。</p></li>
				<li>Next, update the query in the text area with the following SQL query. Click the <strong class="bold">Run query</strong> button to execute the query. Note that the SageMaker inference endpoint with the name sagemaker-cookbook-rcf must be running for this query to execute:<pre>USING EXTERNAL FUNCTION detect_anomaly(value INT)
    RETURNS DOUBLE
    SAGEMAKER <strong class="bold">'sagemaker-cookbook-rcf</strong>'
SELECT label, value, detect_anomaly(value) AS anomaly_score
    FROM cookbook_athena_db.athena_table</pre><p>我们应该<a id="_idIndexMarker750"/>得到类似于<em class="italic">图4.27 </em>所示的<a id="_idIndexMarker751"/>结果:</p></li>
			</ol>
			<div><div><img src="img/B16850_04_27.jpg" alt="Figure 4.27 – Query results returning an anomaly_score value&#13;&#10;" width="1083" height="815"/>
				</div>
			</div>
			<p class="figure-caption">图4.27–返回异常分数值的查询结果</p>
			<p>在<em class="italic">图4.27 </em>中，我们可以看到我们能够在SQL语句中使用部署的RCF模型。请注意，我们不仅限于在Athena SQL查询中使用已部署的RCF模型，我们还可以在这里使用其他已部署的模型。如果遇到ThrottlingException，可以在SQL语句的末尾添加“LIMIT 1000”。</p>
			<p>让我们看看这是如何工作的！</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor214"/>工作原理……</h2>
			<p>在这个菜谱中，我们已经为我们的虚拟数据源在我们的S3存储桶中生成并存储了样本文件。然后，我们使用SQL语句通过Amazon Athena查询存储在S3存储桶中的数据。</p>
			<div><div><img src="img/B16850_04_28.jpg" alt="Figure 4.28 – Analyzing data stored in S3 with SQL statements using Amazon Athena&#13;&#10;" width="1130" height="719"/>
				</div>
			</div>
			<p class="figure-caption">图4.28-使用Amazon Athena使用SQL语句分析存储在S3的数据</p>
			<p>在<em class="italic">图4.28 </em>中，我们<a id="_idIndexMarker752"/>可以<a id="_idIndexMarker753"/>看到Amazon Athena使用SQL语句帮助我们查询和分析存储在我们的S3存储桶中的文件内部的数据。请注意，使用Amazon Athena的优势之一是它是一种无服务器查询服务——不需要管理任何服务器。除此之外，定价也与其使用量成正比。这使得它成为最实用的解决方案之一，尤其是在处理大数据需求时。除了不必担心必须自己管理基础架构之外，您也不必担心未充分利用或闲置资源的成本。使用与本食谱类似的设置，Amazon Athena可以轻松扩展和处理更大的数据集，并且仍然可以在几秒钟内做出响应。当然，我们可以执行一些调整、配置和最佳实践(例如，分区和压缩)来优化这个设置的性能。</p>
			<p>我们还在SQL查询中使用了部署的RCF模型来检测数据集中的异常。请注意，我们不仅限于在这里执行异常检测<a id="_idIndexMarker754"/>，我们<a id="_idIndexMarker755"/>可以使用其他不同类型的部署模型，例如使用<strong class="bold"> XGBoost </strong>或<strong class="bold">因式分解机器</strong>。</p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor215"/>用Python中的Amazon Athena分析数据</h1>
			<p>Amazon <a id="_idIndexMarker756"/> Athena <a id="_idIndexMarker757"/>是AWS的一个<a id="_idIndexMarker758"/>查询服务，使用SQL语法查询存储在亚马逊S3的数据。在前面的菜谱中，我们使用Athena控制台(UI)运行了几个SQL查询。当然，当处理机器学习和机器学习工程任务时，我们希望通过脚本来执行，以便我们有机会自动化该过程的某些步骤。</p>
			<p>在这个菜谱中，我们将使用<strong class="bold"> boto3 Python SDK </strong>以编程方式运行Amazon Athena SQL查询。一旦我们完成了这个食谱，我们将使用Amazon Athena和Python加载、查询和转换存储在我们的S3桶中的JSON数据为表格格式。我们将在这个菜谱中执行两个查询——一个简单的SELECT查询和一个调用Amazon SageMaker中已部署的机器学习模型的查询。</p>
			<h2 id="_idParaDest-185">准备就绪</h2>
			<p>这种方法继续于<em class="italic">使用SQL查询调用Amazon Athena的机器学习模型</em>。</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor217"/>怎么做……</h2>
			<p>接下来的几个步骤集中在使用Python SDK以编程方式运行Athena查询，而不使用UI。确保使用conda_python3内核的空笔记本运行下一组命令:</p>
			<ol>
				<li value="1">导航到SageMaker笔记本实例中的my-experiments/chapter04目录。如果这个目录还不存在，请随意创建。</li>
				<li>在<a id="_idTextAnchor218"/>my-experiments/chapter 04目录内使用conda_python3内核新建一个笔记本，用这个菜谱的名字命名(也就是用python中的Amazon Athena分析数据)。打开此笔记本进行编辑，因为我们将在接下来的几个步骤中使用代码更新此文件。</li>
				<li>通过导入boto3并初始化Athena客户端来启动笔记本:<pre>import boto3 athena = boto3.client('athena', region_name='us-east-1')</pre></li>
				<li>Specify the value for athena_results_bucket. Note that this is a different S3 bucket <a id="_idIndexMarker759"/>and <a id="_idIndexMarker760"/>we <a id="_idIndexMarker761"/>will use this bucket to store the results of the Athena queries:<pre>athena_results_bucket = "<strong class="bold">&lt;insert S3 bucket name here&gt;</strong>"
!aws s3 mb s3://{athena_results_bucket}</pre><p>这应该会产生类似于make_bucket: <bucket name="">的日志消息。</bucket></p></li>
				<li>Initialize the variable values for query, database, and results_bucket. Make sure to replace the database name, table name, and S3 target bucket in the following variables with the values used in the <em class="italic">Invoking machine learning models with Amazon Athena in SQL queries</em> recipe:<pre><strong class="bold">query</strong> = "SELECT label, value FROM cookbook_athena_db.athena_table;"
<strong class="bold">database</strong> = "cookbook_athena_db"
<strong class="bold">results_bucket</strong> = "s3://" + athena_results_bucket</pre><p>需要注意的是，前面的代码块仅由三行组成(以防由于语句长度的原因，代码块呈现为四行或更多行)。</p></li>
				<li>定义execute_athena_query()函数，该函数使用boto3 SDK中的start_query_execution()函数来运行参数:<pre>def <strong class="bold">execute_athena_query</strong>(<strong class="bold">query</strong>, <strong class="bold">database</strong>, <strong class="bold">results_bucket</strong>):     response = athena.start_query_execution(         QueryString = <strong class="bold">query</strong>,         QueryExecutionContext = {             'Database' : <strong class="bold">database</strong>         },         ResultConfiguration = {             'OutputLocation': <strong class="bold">results_bucket</strong>         }     )     return response['QueryExecutionId']</pre>中指定的查询</li>
				<li>定义<a id="_idIndexMarker762"/>get _ output _ path()函数，该函数<a id="_idIndexMarker763"/>使用<a id="_idIndexMarker764"/>get _ query _ execution()函数使用指定的执行ID加载Athena查询的细节，并返回存储Athena查询结果的输出位置:<pre>def <strong class="bold">get_output_path</strong>(<strong class="bold">execution_id</strong>):     query_details = athena.<strong class="bold">get_query_execution</strong>(         QueryExecutionId = <strong class="bold">execution_id</strong>     )     execution = query_details['QueryExecution']     configuration = execution['ResultConfiguration']     return configuration['OutputLocation']</pre></li>
				<li>Call the execute_athena_query() and get_output_path() functions. Note that the Athena query may run for a few seconds so we may need to wait for 3-5 seconds before the output CSV file becomes available in the S3 output path:<pre>execution_id = <strong class="bold">execute_athena_query</strong>(query, database, results_bucket)
output_path = <strong class="bold">get_output_path</strong>(execution_id)
output_path</pre><p>我们<a id="_idIndexMarker765"/>应该<a id="_idIndexMarker766"/>得到一个类似于‘S3://&lt;桶名&gt;/64957 FB b-b873-48ec-91aa-7377343 da 412 . CSV’的<a id="_idIndexMarker767"/>值。</p></li>
				<li>如果tmp目录尚不存在，则创建该目录:<pre>!mkdir -p <strong class="bold">tmp</strong></pre></li>
				<li>Download the CSV file containing the results of the query to the tmp directory created in the previous step:<pre>!<strong class="bold">aws s3 cp</strong> {output_path} tmp/output.csv</pre><p>这应该会生成一条类似于download:S3://<bucket name="">/97d 9da 6 f-6426-46a 1-b775-c 96580 FD 29 f 4 . CSV to tmp/output . CSV的日志消息。</bucket></p></li>
				<li>Load and inspect the contents of tmp/output.csv using the read_csv() function:<pre>import pandas as pd
pd.<strong class="bold">read_csv</strong>("tmp/output.csv")</pre><p>我们应该得到一组类似于图4.29 所示的结果:</p><div><img src="img/B16850_04_29.jpg" alt="Figure 4.29 – Contents of the output.csv file&#13;&#10;" width="180" height="346"/></div><p class="figure-caption">图4.29–输出. csv文件的内容</p><p>在<em class="italic">图4.29 </em>中，我们<a id="_idIndexMarker768"/>可以看到<a id="_idIndexMarker769"/>output . CSV文件中的<a id="_idIndexMarker770"/>标签和值对。这里的每条记录都应该正确地映射到我们之前在<em class="italic">为异常检测实验生成合成数据集</em>配方中生成的记录。</p></li>
				<li>将查询字符串值设置为触发现有SageMaker推理端点的Athena SQL查询。这是我们在控制台中使用的相同查询，在SQL查询中使用亚马逊Athena调用机器学习模型方法:<pre>query = """ USING EXTERNAL FUNCTION detect_anomaly(value INT)     RETURNS DOUBLE     SAGEMAKER '<strong class="bold">sagemaker-cookbook-rcf</strong>' SELECT label, value, detect_anomaly(value) AS anomaly_score     FROM cookbook_athena_db.athena_table """</pre></li>
				<li>Call the execute_athena_query() and get_output_path() functions again but this time using the query that triggers the machine learning endpoint. Note that the Athena query may run for a few seconds so we may need to wait for 3-5<a id="_idIndexMarker771"/> seconds <a id="_idIndexMarker772"/>before <a id="_idIndexMarker773"/>the output CSV file becomes available in the S3 output path:<pre>execution_id = <strong class="bold">execute_athena_query</strong>(query, 
    database, results_bucket)
output_path = get_output_path(execution_id)
output_path</pre><p>这将产生类似“S3://<bucket name="">/d 457328 e-b456-4d 11-a012-6 ea 26 a 22 CEB 9 . CSV”的输出。</bucket></p></li>
				<li>Download the CSV file containing the results of the query to the tmp directory:<pre>!aws s3 cp {output_path} tmp/output.csv</pre><p>这应该会生成一条类似于download:S3://<bucket name="">/d 457328 e-b456-4d 11-a012-6 ea 26 a 22 CEB 9 . CSV to tmp/output . CSV的日志消息。</bucket></p></li>
				<li>Load and inspect the contents of tmp/output.csv:<pre>df = pd.read_csv("tmp/output.csv")
df</pre><p>这将为我们提供一个结构类似于图4.30 所示的数据框架:</p><div><img src="img/B16850_04_30.jpg" alt="Figure 4.30 – Query results including anomaly_score&#13;&#10;" width="359" height="446"/></div><p class="figure-caption">图4.30–包括异常分数的查询结果</p><p>在<em class="italic">图4.30 </em>中，我们<a id="_idIndexMarker774"/>可以看到<a id="_idIndexMarker775"/>现在<a id="_idIndexMarker776"/>每条记录都有一个对应的异常_分值。</p></li>
				<li>Check the number of records with an anomaly_score value greater than 2:<pre>len(df[df.anomaly_score &gt; 2])</pre><p>我们应该得到一个等于或接近47的值。注意，我们只是选择了2作为一个任意的数字。鉴于较高的anomaly_score值表明数据点成为异常值的风险较高，选择2是一个好的开始。请注意，其他机器学习从业者和数据科学家可能会使用离群值的正式定义和公式，但我们将在本食谱中跳过这一点。</p></li>
			</ol>
			<p>此时，我们现在可以删除在<em class="italic">培训和部署RCF模型</em>配方中部署的端点。现在，让我们看看这个食谱是如何工作的！</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor219"/>工作原理……</h2>
			<p>在这个菜谱中，我们使用boto3在Python笔记本中以编程方式运行Amazon Athena SQL查询。正如在前面的食谱中所讨论的，Amazon Athena帮助我们分析存储在我们的S3存储桶中的文件内部的数据，而我们不必担心服务器和基础设施管理。</p>
			<p>该配方中执行的<a id="_idIndexMarker777"/>步骤<a id="_idIndexMarker778"/>可由下图总结:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/B16850_04_31.jpg" alt="Figure 4.31 – Using boto3 and pandas to perform a query on data stored in S3 using SQL statements&#13;&#10;" width="817" height="771"/>
				</div>
			</div>
			<p class="figure-caption">图4.31–使用boto3和pandas通过SQL语句对存储在S3的数据执行查询</p>
			<p>在<em class="italic">图4.31 </em>中，我们可以看到当使用boto3和Amazon Athena执行SQL查询以获得pandas数据框架中的结果时，涉及到几个步骤。一旦我们在pandas数据框架中有了结果，接下来的步骤可能包括在执行模型训练步骤之前处理、转换和分析这些结果。</p>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor220"/>生成用于分析和转换的合成数据集</h1>
			<p>在这个<a id="_idIndexMarker780"/>配方中，我们将生成一个合成数据集，该数据集将用于接下来的四个配方，包括降维、聚类分析和转换为protobuf recordIO格式。我们将生成数据集的一个标记版本和数据集的一个未标记版本。该数据集将有两个易于识别的聚类，如图4.32 中的<em class="italic">所示。它还将有六列用于数据集的已标记版本，五列用于数据集的未标记版本。</em></p>
			<div><div><img src="img/B16850_04_32.jpg" alt="Figure 4.32 – Synthetic dataset &#13;&#10;" width="355" height="242"/>
				</div>
			</div>
			<p class="figure-caption">图4.32–合成数据集</p>
			<p>在我们完成这个配方后，我们应该有一个类似于图4.32 所示的合成数据集。在<em class="italic">使用内置PCA算法</em>进行降维中，我们将使用PCA算法对这个合成数据集进行降维。在使用内置KMeans算法配方执行聚类分析的<em class="italic">中，我们将处理该数据集的未标记版本，并使用KMeans无监督学习算法来自动检测两个聚类，并将点分配给最近的聚类。我们还将在<em class="italic">将CSV数据转换为protobuf recordIO格式</em>方法中使用该数据集的标签版本展示如何将CSV数据转换为protobuf recordIO格式。</em></p>
			<p class="callout-heading">小费</p>
			<p class="callout">由于我们将在这个配方中展示如何生成合成数据集的步骤，我们将有机会在以后调整这个配方以满足我们的需要。我们将在本菜谱的<em class="italic">工作原理……</em>部分讨论一些不同的方法来调整它，以测试不同算法的性能和行为。</p>
			<h2 id="_idParaDest-189">准备就绪</h2>
			<p>以下是<a id="_idIndexMarker781"/>该配方的先决条件:</p>
			<ul>
				<li>在SageMaker笔记本实例中使用conda_python3内核的新Jupyter笔记本</li>
			</ul>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor222"/>怎么做……</h2>
			<p>第一组步骤侧重于生成随机的x和y整数值:</p>
			<ol>
				<li value="1">导航到SageMaker笔记本实例中的my-experiments/chapter04目录。如果这个目录还不存在，请随意创建。</li>
				<li>在my-experiments/chapter04目录中使用conda_python3内核创建一个新的笔记本，并用这个配方的名称命名(即生成一个用于分析和转换的合成数据集)。打开此笔记本进行编辑，因为我们将在接下来的几个步骤中使用代码更新此文件。</li>
				<li>使用numpy和random中的种子函数，这样我们可以生成相同的随机数集，使我们的实验具有可重复性:<pre>import random from numpy.random import seed as np_seed from random import randint np_seed(42) random.seed(42)</pre></li>
				<li>Define the generate_x_value() and generate_y_value() functions:<pre>def <strong class="bold">generate_x_value</strong>():
    return randint(500,2000)
     
def <strong class="bold">generate_y_value</strong>():
    return randint(20,50)</pre><p>有了这些<a id="_idIndexMarker782"/>函数，我们有望得到500到2000之间的x值，有望得到20到50之间的y值。</p></li>
				<li>Using the generate_x_value() function, we generate 100 random values and store them inside the x_values list:<pre><strong class="bold">x_values</strong> = []
     
for _ in range(0, 100):
    x_values.append(<strong class="bold">generate_x_value</strong>())
    
x_values[0:5]</pre><p>我们将得到类似于[1809，728，551，1063，1001]的值。</p></li>
				<li>Similarly, we generate 100 values using the generate_y_value() function and store these values inside the y_values list:<pre><strong class="bold">y_values</strong> = []
     
for _ in range(0, 100):
    y_values.append(<strong class="bold">generate_y_value</strong>())
    
y_values[0:5]</pre><p>我们将得到类似于[27，41，30，46，44]的值。</p><p>下一组步骤集中于从我们在前一组步骤中准备的原始x和y值<a id="_idIndexMarker783"/>生成衍生值。我们将使用list()、map()和lambda匿名函数来生成一个包含x和y的派生值的列表。</p></li>
				<li>We then generate a derived list of values from the x_values list and store them in x2_values:<pre>x2_values = list(map(<strong class="bold">lambda x: x * 2 + 7000</strong>, x_values))
x2_values[0:5]</pre><p>我们将得到类似于[10618，8456，8102，9126，9002]的值。</p></li>
				<li>In a similar fashion, we generate a derived list of values and store them in the x3_values list:<pre>x3_values = list(map(<strong class="bold">lambda x: x * 3 - 20</strong>, x_values))
x3_values[0:5]</pre><p>我们将得到类似于[5407，2164，1633，3169，2983]的值。</p></li>
				<li>We also generate a derived list of values from y_values and store them in the y2_values list:<pre>y2_values = list(map(<strong class="bold">lambda y: y * 2 + 1000</strong>, y_values))
y2_values[0:5]</pre><p>我们将得到类似于[1054，1082，1060，1092，1088]的值。</p></li>
				<li>现在我们已经有了列表中的所有列值，我们将它们组合到一个<a id="_idIndexMarker784"/>单一数据框架:<pre>import pandas as pd       df = pd.DataFrame({     "x": x_values,     "x2": x2_values,     "x3": x3_values,     "y": y_values,     "y2": y2_values })</pre></li>
				<li>Let's inspect what the DataFrame looks like:<pre>df</pre><p>我们应该得到一个类似于图4.33 所示的数据帧:</p><div><img src="img/B16850_04_33.jpg" alt="Figure 4.33 – DataFrame with the values from x_values, x2_values, x3_values, y_values, and y2_values&#13;&#10;" width="218" height="346"/></div><p class="figure-caption">图4.33–包含x值、x2值、x3值、y值和y2值的数据框</p><p>在<em class="italic">图4.33 </em>中，我们<a id="_idIndexMarker785"/>可以看到有五列的数据帧——x、x2、x3、y和y2。x2和x3列源自x，y2列源自y值。</p></li>
				<li>Next, generate the values for the label column. If the x values are greater than 1000 and the y values are greater than 35, we specify a value of 1 for the label column. Otherwise, we specify a value of 0:<pre>df["label"] = (df.x &gt; 1000) &amp; (df.y &gt; 35)
df['label'] = df['label'].apply(lambda x: 1 if x else 0)
df</pre><p>我们应该得到一个类似于图4.34 所示的数据帧:</p><div><img src="img/B16850_04_34.jpg" alt="Figure 4.34 – DataFrame with the label column values&#13;&#10;" width="339" height="463"/></div><p class="figure-caption">图4.34–带有标签列值的数据框</p><p>在<em class="italic">图4.34 </em>中，我们<a id="_idIndexMarker786"/>可以看到根据x和y值包含值1和0的新标签列。</p><p>现在，我们将集中精力删除数据集中的一些记录，这样我们将剩下两个不同的点聚类。</p></li>
				<li>We then generate the values for the keep column. As the name suggests, we will decide whether to keep the record or not based on the value in the keep column:<pre>df["keep"] = ((df.x &gt; 1000) &amp; (df.y &gt; 35)) | ((df.x &lt; 800) &amp; (df.y &lt; 30))
df</pre><p>我们应该得到一个类似于图4.35 所示的数据帧:</p><div><img src="img/B16850_04_35.jpg" alt="Figure 4.35 – DataFrame with the new keep column&#13;&#10;" width="375" height="445"/></div><p class="figure-caption">图4.35–带有新保留列的数据框</p><p>我们可以在图4.35中看到新的包含真值和假值的keep列。由于我们将对该数据集执行聚类分析，因此我们将有意删除“中间”的值，以便修整后的数据集将具有两个聚类，第一个聚类的x值大于1000，y值大于35，第二个聚类的x值小于800，y值小于30。</p></li>
				<li>Next, select and continue with the records with the keep column value equal to True:<pre>df = df[df.keep]
df.head()</pre><p>我们应该得到一个类似于图4.36 所示的数据帧:</p><div><img src="img/B16850_04_36.jpg" alt="Figure 4.36 – Trimmed dataset&#13;&#10;" width="288" height="164"/></div><p class="figure-caption">图4.36–修剪后的数据集</p><p>虽然<a id="_idIndexMarker788"/>我们无法在<em class="italic">图4.36 </em>中看到整个修剪后的数据集，但我们应该注意到，我们在使用head()函数的初始记录列表中遗漏了0、1、2和5个索引。这是因为具有这些索引的记录已经被过滤掉了，因为它们在keep列中的值等于False。</p></li>
				<li>现在我们已经修剪了数据集，让我们使用del关键字:<pre><strong class="bold">del</strong> df["keep"]</pre>删除keep列</li>
				<li>With everything ready, generate the scatterplot showing the labeled clusters using the following lines of code:<pre>import matplotlib.pyplot as plt
     
groups = df.groupby("label")
for name, group in groups:
    plt.plot(group["x"], 
             group["y"], 
             marker="o", 
             linestyle="", 
             label=name)
    
plt.legend()</pre><p>前面的代码块利用groupby()函数，通过使用标签列的值对点进行分组。这将生成一个类似于<em class="italic">图4.37 </em>所示的散点图:</p><div><img src="img/B16850_04_37.jpg" alt="" width="533" height="235"/></div><p class="figure-caption">图4.37–matplotlib散点图显示了标记的集群</p><p>在<em class="italic">图4.37 </em>中，我们可以看到有两个集群对应于每个标记组。标签为0的第一个聚类位于图的左下部分，标签为1的第二个聚类位于图的右上部分。</p></li>
				<li>如果tmp目录不存在，则生成该目录:<pre>!mkdir -p tmp</pre></li>
				<li>Use the to_csv() function to store the labeled DataFrame in a CSV file:<pre>df.to_csv("tmp/synthetic.all.labeled.csv")</pre><p>我们将在稍后的<em class="italic">将CSV数据转换成protobuf recordIO格式</em>配方中使用这个CSV文件。</p></li>
				<li>Store the labeled DataFrame using the %store magic command as well:<pre>labeled_df = df
%store labeled_df</pre><p>我们将得到一个与存储的“labelled _ df”(data frame)相同或相似的<a id="_idIndexMarker790"/>成功消息。</p><p>在最后一组步骤中，我们将准备和存储合成数据集的未标记版本。</p></li>
				<li>Delete the label column using the del keyword:<pre><strong class="bold">del</strong> df["label"]
df.head()</pre><p>我们应该得到一个类似于图4.38 所示的数据帧:</p><div><img src="img/B16850_04_38.jpg" alt="" width="513" height="155"/></div><p class="figure-caption">图4.38–没有标签列的数据框</p><p>我们可以在<em class="italic">图4.38 </em>中看到没有标签列的数据帧。我们将在哪里使用这个？在我们执行了缩放步骤后，我们将在<em class="italic">使用内置KMeans算法</em>配方执行聚类分析中使用这个未标记的数据帧执行无监督聚类分析。</p></li>
				<li>Use MinMaxScaler from scikit-learn to scale the values in the unlabeled<a id="_idIndexMarker791"/> DataFrame:<pre>from sklearn.preprocessing import <strong class="bold">MinMaxScaler</strong>
     
scaler = <strong class="bold">MinMaxScaler</strong>()
scaled_array = scaler.fit_transform(df.astype(float))
normalized_df = pd.DataFrame(scaled_array)
normalized_df.columns = df.columns
normalized_df.index = df.index
     
normalized_df.head()</pre><p>我们应该得到一个类似于<em class="italic">图4.39 </em>所示的数据帧:</p><div><img src="img/B16850_04_39.jpg" alt="Figure 4.39 – Scaled DataFrame&#13;&#10;" width="334" height="160"/></div><p class="figure-caption">图4.39-缩放数据框</p><p>如<em class="italic">图4.39 </em>所示，我们的值已被缩减为一组介于0和1之间的值。请注意，我们可以选择为MinMaxScaler的feature_range参数指定值，但我们将坚持使用默认值(0，1)。</p></li>
				<li>使用to_csv()函数将标准化无标签数据帧的值保存到一个csv文件:<pre>normalized_df.<strong class="bold">to_csv</strong>("tmp/synthetic.all_normalized.unlabeled.csv")</pre></li>
				<li>Finally, use <a id="_idIndexMarker792"/>the %store magic command to store the values of the unlabeled_normalized_df variable:<pre>unlabeled_normalized_df = normalized_df
%store unlabeled_normalized_df</pre><p>我们将得到一个等于或类似于存储的“unlabel _ normalized _ df”(data frame)的成功消息。</p></li>
			</ol>
			<p>让我们看看这是如何工作的！</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor223"/>工作原理…</h2>
			<p>在这个<a id="_idIndexMarker793"/>配方中，我们<a id="_idIndexMarker794"/>已经生成了一个合成数据集，该数据集将用于涉及<strong class="bold">降维</strong>、<strong class="bold">聚类分析</strong>以及序列化为protobuf recordIO格式的配方中。作为一名机器学习从业者，熟悉这些关于数据准备和分析的方法很重要，因为在将数据作为输入传递给模型训练步骤之前，数据很可能会被转换几次。</p>
			<p>这个数据集的属性是什么？这个数据集中的数据点被有意地分成两个聚类，以便我们更容易知道我们的聚类分析方法是否有效。我们还添加了几个派生列，帮助我们了解我们的降维方法是否也有效。</p>
			<p>我们可以对这个食谱进行哪些调整来产生这个数据集的变体？</p>
			<ul>
				<li>我们可以(显著地)生成更多的记录，并测试后续配方中使用的算法的性能。一些算法和解决方案适用于相对较小的数据集，但在处理较大的数据集时可能会有困难。</li>
				<li>我们可以向生成的数据集中添加更多的列。例如，我们可以生成100列，而不是只有5列。我们可以测试和比较一个算法在只有5列的数据集上的性能，以及同一算法在有100列的数据集上的性能。</li>
				<li>我们还可以将生成的集群数量从2个增加到10个，并看看<a id="_idIndexMarker795"/>集群算法在处理更多集群时的表现。</li>
			</ul>
			<p>记住这一点，让我们继续利用这个合成数据集的食谱！</p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor224"/>使用内置的PCA算法进行降维</h1>
			<p>在这个<a id="_idIndexMarker796"/>配方中，我们将<a id="_idIndexMarker797"/>演示如何使用内置的PCA算法对合成数据集进行降维。降维涉及将数据集的列数减少到更少的基本列数。如果你想知道为什么这很重要，那是因为一些算法在处理更少的维度时表现得更好更快！</p>
			<p>我们将对来自<em class="italic">的未标记数据集使用PCA算法，生成用于分析和转换</em>配方的合成数据集，并将该数据集的列数从五列减少到两列。通过使用PCA，我们还会注意到结果值不同于原始数据集中的任何行值。</p>
			<h2 id="_idParaDest-193">准备就绪</h2>
			<p>该配方延续了<em class="italic">生成用于分析和转换的合成数据集</em>。</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor226"/>怎么做……</h2>
			<p>下一组步骤集中于使用我们在前面的配方中生成的未标记数据集来准备我们将用于降维的PCA模型:</p>
			<ol>
				<li value="1">在SageMaker笔记本实例中，导航<a id="_idIndexMarker798"/>到<a id="_idIndexMarker799"/>my-experiments/chapter 04目录。如果这个目录还不存在，请随意创建。</li>
				<li>在my-experiments/chapter04目录下用conda_python3内核新建一个笔记本，用这个菜谱的名字命名(也就是用内置的PCA算法进行降维)。打开此笔记本进行编辑，因为我们将在接下来的几个步骤中使用代码更新此文件。</li>
				<li>Start the notebook by using the %store magic command to load the values of unlabeled_normalized_df:<pre>%store -r unlabeled_normalized_df</pre><p>如果您还记得，我们将该值存储在<em class="italic">生成用于分析和转换的合成数据集</em>配方中。我们将使用该数据帧中的值，并使用PCA模型对其进行降维。</p></li>
				<li>为我们的SageMaker实验导入并准备一些先决条件。这些包括会话和角色，我们将在后面的步骤中把它们作为参数值传递给PCA估计器:<pre>import sagemaker from sagemaker import get_execution_role       <strong class="bold">session</strong> = sagemaker.Session() <strong class="bold">role</strong> = get_execution_role()</pre></li>
				<li>We initialize a PCA estimator object using the parameter values prepared in the previous step<a id="_idIndexMarker800"/> as <a id="_idIndexMarker801"/>well as the values for the instance_count, instance_type, and num_components parameters:<pre>from sagemaker import PCA
     
estimator = <strong class="bold">PCA</strong>(
    role=role,
    instance_count=1,
    instance_type='ml.c5.xlarge',
    num_components=2,
    sagemaker_session=session)</pre><p>这个估计器中的num_components键映射到PCA算法的num_components超参数。该值对应于要计算的主成分的数量。这意味着，如果num_components的超参数值是2，那么如果我们输入一个具有5-6个组件或列的记录，那么我们将计算2个值，这将代表这个记录。</p></li>
				<li>使用record_set()函数创建一个记录集。record_set()函数接受一个NumPy ndarray对象，将其上传到S3，并返回一个record对象:<pre>data_np = unlabeled_normalized_df.values.astype('float32') record_set = estimator.<strong class="bold">record_set</strong>(data_np)</pre></li>
				<li>We pass the record set to the estimator's fit() function:<pre>estimator.<strong class="bold">fit</strong>(record_set)</pre><p>运行前面一行代码后，我们应该会得到一组类似的日志。</p><div><img src="img/B16850_04_40.jpg" alt="Figure 4.40 – Logs after calling the fit() function of the PCA estimator&#13;&#10;" width="794" height="245"/></div><p class="figure-caption">图4.40–调用PCA估计器的fit()函数后的日志</p><p>在<em class="italic">图4.40 </em>中，我们<a id="_idIndexMarker802"/>可以看到<a id="_idIndexMarker803"/>调用fit()函数后的日志。完成此步骤需要几分钟时间。</p></li>
				<li>使用deploy()函数将模型部署到一个推理端点:<pre>predictor = estimator.<strong class="bold">deploy</strong>(     initial_instance_count=1,     instance_type='ml.t2.medium')</pre></li>
				<li>Call the predict() function to perform dimension reduction with the PCA model:<pre>results = predictor.<strong class="bold">predict</strong>(data_np)
results</pre><p>这将产生类似于<em class="italic">图4.41 </em>所示的结果结构。我们应该在data_np数组和推理端点返回的列表中有相同数量的元素。</p><div><img src="img/B16850_04_41.jpg" alt="Figure 4.41 – Results after using the predict() function&#13;&#10;" width="383" height="386"/></div><pre>results[0].label['projection'].float32_tensor.values</pre><p>我们应该得到一组结构类似于[-0.48789161443710327，0.05951513672]的值。这意味着data_np中包含x、x2、x3、y和y2列值的第一条记录已经减少到只有两个值。输入数据的其他元素也是如此。</p></li>
				<li>定义extract_values()函数。该函数接受结果列表中的一个元素，并返回该元素的两个组成部分:<pre>def <strong class="bold">extract_values</strong>(item):     projection = item.label['projection']     pair = projection.float32_tensor.values     x = pair[0]     y = pair[1]          return {         "x": x,         "y": y     }</pre></li>
				<li>接下来，运行<a id="_idIndexMarker806"/>下面的<a id="_idIndexMarker807"/>代码块，它使用我们在上一步中定义的extract_values()函数从结果列表中提取组件值。运行这个之后，new_xs和new_ys列表应该包含两个新组件的值:<pre><strong class="bold">new_xs</strong> = [] <strong class="bold">new_ys</strong> = [] for result in results:     x_and_y = <strong class="bold">extract_values</strong>(result)     new_xs.append(x_and_y["x"])     new_ys.append(x_and_y["y"])</pre></li>
				<li>We check the first five values of new_xs:<pre>new_xs[0:5]</pre><p>我们应该得到一组结构类似于[-0.48789161443710327，-0.4503028988838196，0.31829965114593506，0.028231695294380188，0.0683558001279831]的值。</p></li>
				<li>Then, we check the first five values of new_ys:<pre>new_ys[0:5]</pre><p>我们应该得到一组结构类似于[0.011455059051513672，0.12702858448028564，0.9820671081542969，0.885252058506012，-0.72600531578064]的值。</p></li>
				<li>Create a pandas DataFrame <a id="_idIndexMarker808"/>using<a id="_idIndexMarker809"/> the following lines of code:<pre>import pandas as pd
     
new_df = pd.DataFrame({
    "new_x": new_xs,
    "new_y": new_ys
})
     
new_df</pre><p>我们应该得到一个有两列的数据帧，如图<em class="italic">图4.42 </em>所示:</p><div><img src="img/B16850_04_42.jpg" alt="Figure 4.41 – Results after using the predict() function&#13;&#10;" width="524" height="224"/></div><p class="figure-caption">图4.42-包含减少的组件值的数据框</p><p>如<em class="italic">图4.42 </em>所示，我们从缩放数据集的最初五个组件中获得了两个组件。请注意，这里的x和y列不同于原始数据集中的x和y列。在使用PCA执行降维之后，原始组件集与新组件集之间可能没有直接映射。</p></li>
				<li>Let's <a id="_idIndexMarker810"/>see a <a id="_idIndexMarker811"/>scatterplot of the points by running the following code:<pre>new_df.plot.scatter(x="new_x", y="new_y")</pre><p>我们应该得到一个类似于<em class="italic">图4.43 </em>中的散点图:</p></li>
			</ol>
			<div><div><img src="img/B16850_04_43.jpg" alt="Figure 4.43 – Scatterplot of points after performing dimension reduction&#13;&#10;" width="498" height="251"/>
				</div>
			</div>
			<p class="figure-caption">图4.43-降维后的散点图</p>
			<p>在<em class="italic">图4.43 </em>中，我们看到了从传递给PCA模型的原始点集生成的稍微倾斜的散点图。我们仍然可以看到有两个集群，其中一个集群明显大于另一个集群。</p>
			<p>此时，我们可以删除在这个配方中部署的端点。现在，让我们看看这是如何工作的！</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor227"/>工作原理……</h2>
			<p>PCA算法帮助我们在数据集中进行降维。为什么要进行降维？我们使用的功能越多，训练模型变得越复杂的可能性就越大，我们在训练过程中需要的样本就越多。随着更多的特征和更多的数据，训练时间将越长，并且训练的模型过度拟合的机会越高。也就是说，在将数据作为输入传递给另一个模型(例如XGBoost)的<a id="_idIndexMarker812"/>训练<a id="_idIndexMarker813"/>步骤之前，我们可以利用PCA算法在我们的数据集中执行降维。</p>
			<p class="callout-heading">小费</p>
			<p class="callout">有关PCA算法的更多信息，请随意查看以下链接:<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html">https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html</a>。</p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor228"/>参见</h2>
			<p>如果您正在寻找使用PCA模型对真实数据集和更复杂的示例执行降维的示例，请随意查看AWS/Amazon-sage maker-examples GitHub资源库中的一些笔记本:</p>
			<ul>
				<li>用PCA算法分析<a id="_idIndexMarker814"/>MNIST数据集:<a href="https://github.com/aws/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/pca_mnist/pca_mnist.ipynb">https://github . com/AWS/Amazon-sage maker-examples/blob/master/introduction _ to _ Amazon _ algorithms/PCA _ mnist/PCA _ mnist . ipynb</a></li>
				<li>使用PCA和KMeans聚类算法进行人口分割:<a href="https://github.com/aws/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/US-census_population_segmentation_PCA_Kmeans/sagemaker-countycensusclustering.ipynb">https://github . com/AWS/Amazon-sage maker-examples/blob/master/introduction _ to _ applying _ machine _ learning/US-census _ population _ segmentation _ PCA _ k means/sage maker-countysensusclustering . ipynb</a></li>
			</ul>
			<p>现在，让我们在下一个食谱中仔细看看KMeans算法！</p>
			<h1 id="_idParaDest-197"><a id="_idTextAnchor229"/>使用内置的KMeans算法进行聚类分析</h1>
			<p>在这个<a id="_idIndexMarker815"/>配方中，我们将演示<a id="_idIndexMarker816"/>如何使用KMeans算法对合成数据集执行聚类分析。<strong class="bold">聚类分析</strong>涉及识别数据集中展示相似属性的记录子组。这有助于解决与市场细分、欺诈检测和文档分析相关的不同问题和需求。</p>
			<h2 id="_idParaDest-198">做好准备</h2>
			<p>这个配方延续了<em class="italic">生成用于分析和转换的合成数据集</em>。</p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor231"/>怎么做……</h2>
			<p>下一组步骤集中在使用我们在<em class="italic">生成用于分析和转换的合成数据集</em>配方中生成的未标记数据集，以准备我们将用于聚类分析的KMeans模型:</p>
			<ol>
				<li value="1">导航到SageMaker笔记本实例中的my-experiments/chapter04目录。如果这个目录还不存在，请随意创建。</li>
				<li>在my-experiments/chapter04目录中使用conda_python3内核创建一个新的笔记本，并用这个菜谱的名称命名(即使用内置的KMeans算法执行聚类分析)。打开此笔记本进行编辑，因为我们将在接下来的几个步骤中使用代码更新此文件。</li>
				<li>Start the notebook by using the %store magic command to load the stored value for unlabeled_normalized_df:<pre>%store -r <strong class="bold">unlabeled_normalized_df</strong>
unlabeled_normalized_df.head()</pre><p>我们应该得到一个类似于图4.44 所示的数据帧:</p><div><img src="img/B16850_04_44.jpg" alt="Figure 4.44 – Unlabeled normalized DataFrame of values&#13;&#10;" width="334" height="161"/></div><p class="figure-caption">图4.44-未标记的标准化数据帧值</p><p>我们应该<a id="_idIndexMarker817"/>获得<a id="_idIndexMarker818"/>与我们在<em class="italic">生成用于分析和转换的合成数据集</em>配方中生成该数据帧时相同的一组值。</p></li>
				<li>为我们的SageMaker实验导入并准备一些先决条件。这些包括会话和角色，我们将在后面的步骤中把它们作为参数值传递给KMeans估计器:<pre>import sagemaker from sagemaker import get_execution_role       <strong class="bold">session</strong> = sagemaker.Session() <strong class="bold">role</strong> = get_execution_role()</pre></li>
				<li>We initialize a KMeans estimator object using the parameter values prepared in the previous step, as well as the values for the instance_count, instance_type, and k parameters:<pre>from sagemaker import KMeans
     
estimator = KMeans(
    role=role,
    instance_count=1,
    instance_type='ml.c4.xlarge',
    k=2)</pre><p>此<a id="_idIndexMarker820"/>估计器中的k键映射到KMeans算法的聚类数。这意味着，如果k的超参数值是2，那么我们将使用这种无监督的机器学习算法将数据集中的点分成两个聚类。</p></li>
				<li>Create a record set using the record_set() function on the unlabeled DataFrame values. The record_set() function accepts a NumPy ndarray object, uploads it to S3, and returns a Record object:<pre>data_np = unlabeled_normalized_df.values.astype('float32')
record_set = kmeans.record_set(data_np)
estimator.fit(record_set)</pre><p>这将产生一组类似于图4.45 中所示的日志:</p><div><img src="img/B16850_04_45.jpg" alt="Figure 4.45 – Logs after using the fit() function&#13;&#10;" width="795" height="243"/></div><p class="figure-caption">图4.45–使用fit()函数后的日志</p><p>这需要几分钟的时间来完成，所以请随意喝杯咖啡或茶吧！</p></li>
				<li>使用deploy()函数将KMeans模型部署到推理端点:<pre>predictor = estimator.<strong class="bold">deploy</strong>(     initial_instance_count=1,     instance_type='ml.t2.medium')</pre></li>
				<li>Call the predict() function <a id="_idIndexMarker821"/>to<a id="_idIndexMarker822"/> perform cluster analysis with the KMeans model:<pre>results = predictor.<strong class="bold">predict</strong>(data_np)
results</pre><p>这将产生一组类似于图4.46 中所示的结果:</p><div><img src="img/B16850_04_46.jpg" alt="Figure 4.46 – Results after using the predict() function on a deployed KMeans model&#13;&#10;" width="365" height="346"/></div><p class="figure-caption">图4.46–在已部署的KMeans模型上使用predict()函数后的结果</p><p>在<em class="italic">图4.46 </em>中，我们可以看到，对于我们在predict()函数中传递的每个数据点，我们都会得到两个值——最近聚类和距离聚类。</p></li>
				<li>定义extract_values()函数。该函数接受结果列表中的一个元素，并返回最近聚类和到聚类的距离值:<pre>def extract_values(item):     closest_cluster = item.label['closest_cluster']     cc_value = int(closest_cluster.float32_tensor.values[0])     distance_to_cluster = item.label['distance_to_cluster']     dtc_value = distance_to_cluster.float32_tensor.values[0]          return {         "closest_cluster": cc_value,         "distance_to_cluster": dtc_value     }</pre></li>
				<li>接下来，运行<a id="_idIndexMarker823"/>下面的<a id="_idIndexMarker824"/>代码块，它使用我们在上一步中定义的extract_values()函数从响应数据中提取最近的_cluster和距离_to_cluster值。运行此命令后，最近聚类列表和距离聚类列表应该包含适当的值:<pre><strong class="bold">closest_cluster_list</strong> = [] <strong class="bold">distance_to_cluster_list</strong> = [] for result in results:     cv = <strong class="bold">extract_values</strong>(result)     closest_cluster_list.append(cv["closest_cluster"])     distance_to_cluster_list.append(cv["distance_to_cluster"])</pre></li>
				<li>Inspect the first six elements of closest_cluster_list:<pre>closest_cluster_list[0:6]</pre><p>我们应该得到一组类似于[0，0，1，1，0，1]的结果。</p></li>
				<li>Update<a id="_idIndexMarker825"/> the unlabeled_normalized_df DataFrame <a id="_idIndexMarker826"/>with the corresponding values for closest_cluster and distance_to_cluster of each record:<pre>df = unlabeled_normalized_df
df = df.assign(closest_cluster=closest_cluster_list)
df = df.assign(distance_to_cluster=distance_to_cluster_list)
df.head()</pre><p>我们应该看到类似于图4.47 中所示的数据帧:</p><div><img src="img/B16850_04_47.jpg" alt="Figure 4.47 – DataFrame with the closest_cluster and distance_to_cluster columns&#13;&#10;" width="716" height="210"/></div><p class="figure-caption">图4.47–具有最近聚类和距离聚类列的数据帧</p><p>我们可以在图4.47 中看到，我们的测向数据帧现在有了两个新列—最近聚类和到聚类的距离。</p></li>
				<li>Let's see a scatterplot of the cluster of points by running the following code:<pre>import matplotlib.pyplot as plt
groups = <strong class="bold">df.groupby("closest_cluster")</strong>
for name, group in groups:
    plt.<strong class="bold">plot</strong>(group["x"], 
             group["y"], 
             marker="o", 
             linestyle="", 
             label=name)
    
plt.legend()</pre><p><a id="_idIndexMarker828"/>之前的<a id="_idIndexMarker827"/>代码块利用groupby()函数根据closest_cluster值对点进行分组。这将呈现一个类似于<em class="italic">图4.48 </em>所示的散点图:</p></li>
			</ol>
			<div><div><img src="img/B16850_04_48.jpg" alt="Figure 4.48 – Scatterplot of points showing the two clusters&#13;&#10;" width="540" height="234"/>
				</div>
			</div>
			<p class="figure-caption">图4.48–显示两个集群的散点图</p>
			<p>我们可以在<em class="italic">图4.48 </em>中看到，<strong class="bold">k均值</strong>模型已经将数据点恰当地分组到两个聚类中。我们可以尝试将最接近的聚类值与来自<em class="italic">的原始标签值进行比较，生成用于分析和转换</em>配方的合成数据集，但我们会将此作为练习留给您。</p>
			<p>此时，我们可以删除在这个配方中部署的端点。现在，让我们看看这是如何工作的！</p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor232"/>工作原理…</h2>
			<p>在这个菜谱中，我们使用KMeans算法来帮助我们对合成数据集执行聚类分析。这是如何工作的？KMeans算法将数据集分成k个子组，k等于所需聚类的数量。这些子组中的数据点通过这种无监督学习算法基于相似性自动分组在一起。</p>
			<p><a id="_idIndexMarker829"/> deployed <a id="_idIndexMarker830"/> KMeans模型为作为有效负载传递给predict()函数的每个数据点返回两个值:</p>
			<ul>
				<li>closest_cluster:预测数据点所属的组或簇</li>
				<li>distance_to_cluster:数据点到聚类中心的欧氏距离</li>
			</ul>
			<p>通过这个聚类分析步骤，我们可以很容易地用类标签执行数据点的自动无监督标记，即使没有标记的训练数据。聚类分析的另一个应用是异常的检测，因为异常很容易被检测到，特别是如果它们离聚类中心太远的话。</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor233"/>亦见</h2>
			<p>如果您正在寻找使用真实数据集在SageMaker中训练和部署KMeans集群模型的示例，请随意查看AWS/Amazon-sage maker-examples GitHub存储库中的一些笔记本:</p>
			<ul>
				<li>MNIST数据集上的KMeans <a id="_idIndexMarker831"/>聚类:<a href="https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/1P_kmeans_highlevel/kmeans_mnist.ipynb">https://github . com/AWS/Amazon-sage maker-examples/blob/master/sage maker-python-SDK/1P _ k means _ high level/k means _ mnist . ipynb</a></li>
				<li>人口<a id="_idIndexMarker832"/>分割<a id="_idIndexMarker833"/>使用PCA和KMeans聚类算法:<a href="https://github.com/aws/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/US-census_population_segmentation_PCA_Kmeans/sagemaker-countycensusclustering.ipynb">https://github . com/AWS/Amazon-sage maker-examples/blob/master/introduction _ to _ applying _ machine _ learning/US-census _ population _ segmentation _ PCA _ k means/sage maker-countysensusclustering . ipynb</a></li>
				<li>在Amazon SageMaker中部署一个<a id="_idIndexMarker834"/>预训练的KMeans集群模型:<a href="https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/kmeans_bring_your_own_model/kmeans_bring_your_own_model.ipynb">https://github . com/AWS/Amazon-SageMaker-examples/blob/master/advanced _ functionality/k means _ bring _ your _ own _ model/k means _ bring _ your _ own _ model . ipynb</a></li>
			</ul>
			<p>现在，让我们在下一个菜谱中仔细看看protobuf recordIO格式！</p>
			<h1 id="_idParaDest-202"><a id="_idTextAnchor234"/>将CSV数据转换为protobuf recordIO格式</h1>
			<p>在这个<a id="_idIndexMarker835"/>配方中，我们将以CSV格式存储的合成数据转换并<a id="_idIndexMarker836"/>序列化为protobuf recordIO格式。将数据序列化为protobuf recordIO格式后，我们可以利用管道模式，在这种模式下，训练开始时间会更快，因为训练作业直接从S3存储桶源传输数据。也就是说，SageMaker算法使用这种训练文件格式可能会表现得更好。</p>
			<h2 id="_idParaDest-203">做好准备</h2>
			<p>该配方延续了<em class="italic">生成用于分析和转换的合成数据集</em>。</p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor236"/>怎么做……</h2>
			<p>在此方法的前几个步骤中，我们将使用sklearn的MinMaxScaler将合成标记数据集缩放和转换为一组介于0和1之间的值:</p>
			<ol>
				<li value="1">导航到SageMaker笔记本实例中的my-experiments/chapter04目录。如果这个目录还不存在，请随意创建。</li>
				<li>在my-experiments/chapter04目录下使用conda_python3内核创建一个<a id="_idIndexMarker837"/>新的<a id="_idIndexMarker838"/>笔记本，用这个菜谱的名字命名(也就是把CSV数据转换成protobuf recordIO格式)。打开此笔记本进行编辑，因为我们将在接下来的几个步骤中使用代码更新此文件。</li>
				<li>Start the notebook by using the %store magic command to read the value of labeled_df. Remember that we stored this value in the <em class="italic">Generating a synthetic dataset for analysis and transformation</em> recipe:<pre>%store -r labeled_df
labeled_df.head()</pre><p>这应该显示了labeled_df的前五个元素。我们应该看到类似于图4.49 中所示的数据帧:</p><div><img src="img/B16850_04_49.jpg" alt="Figure 4.49 – labeled_df&#13;&#10;" width="542" height="154"/></div><p class="figure-caption">图4.49–标记为_df</p><p>在<em class="italic">图4.49 </em>中，我们有包含每条记录的标签值的标签数据帧。</p></li>
				<li>Use MinMaxScaler from scikit-learn to <a id="_idIndexMarker839"/>scale <a id="_idIndexMarker840"/>the values in the labeled DataFrame:<pre>import pandas as pd
from sklearn.preprocessing import MinMaxScaler
     
scaler = MinMaxScaler()
scaled_values = scaler.fit_transform(labeled_df.astype(float))
normalized_df = pd.DataFrame(scaled_values)
normalized_df.columns = labeled_df.columns
normalized_df.index = labeled_df.index
     
normalized_df.head()</pre><p>这应该显示normalized_df的前五个元素。这将显示一个类似于图4.50 中所示的数据帧:</p><div><img src="img/B16850_04_50.jpg" alt="Figure 4.50 – normalized_df&#13;&#10;" width="555" height="154"/></div><p class="figure-caption">图4.50–标准化_df</p><p>在<em class="italic">图4.50 </em>中，我们有标准化的带标签的数据帧，包含缩放和转换的值，以适应(0，1)范围。</p></li>
				<li>Check the shape of the normalized DataFrame:<pre>normalized_df.<strong class="bold">shape</strong></pre><p>这产生一个等于或类似于(42，6)的元组。</p><p>在接下来的<a id="_idIndexMarker841"/>组<a id="_idIndexMarker842"/>步骤中，我们将使用SageMaker Python SDK中的write_numpy_to_dense_tensor()函数执行到protobuf recordIO格式的实际转换和序列化。</p></li>
				<li>使用train_test_split()函数对标准化数据帧执行训练测试分割:<pre>from sklearn.model_selection import train_test_split       y = normalized_df["label"].values X = normalized_df[["x", "x2", "x3", "y", "y2"]].values       X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)</pre></li>
				<li>将X_train和y_train的值分别存储在train_np和label_np中:<pre>train_np = X_train label_np = y_train</pre></li>
				<li>使用SageMaker Python SDK中的write_numpy_to_dense_tensor()函数将numpy数组转换为protobuf recordIO格式:<pre>import io from sagemaker.amazon.common import \  write_numpy_to_dense_tensor       buf = io.BytesIO() <strong class="bold">write_numpy_to_dense_tensor</strong>(buf, train_np, label_np) buf.seek(0)</pre></li>
				<li>如果tmp目录<a id="_idIndexMarker844"/>尚不存在，则创建<a id="_idIndexMarker843"/>该目录:<pre>!mkdir -p tmp</pre></li>
				<li>定义save_bytesio()函数，该函数将protobuf recordIO数据保存到一个文件中:<pre>def <strong class="bold">save_bytesio</strong>(filename, buf):     with open("tmp/" + filename, "wb") as file:         file.write(buf.getbuffer())         print(f"Successfully saved {filename}")</pre></li>
				<li>Use the save_bytesio() function to save the data into the tmp/train.io file:<pre><strong class="bold">save_bytesio</strong>("train.io", buf)</pre><p>这应该会产生类似于成功保存的train.io的日志消息。</p></li>
				<li>Use the %store magic command to store the value of the buf variable:<pre>%store <strong class="bold">buf</strong></pre><p>这应该会产生类似于存储的“buf”(BytesIO)的日志消息。</p></li>
				<li>In a similar fashion, use the %store magic command to store the values for X_train, X_test, y_train, and y_test:<pre>%store <strong class="bold">X_train</strong>
%store <strong class="bold">X_test</strong>
%store <strong class="bold">y_train</strong>
%store <strong class="bold">y_test</strong></pre><p>这应该<a id="_idIndexMarker845"/>产生<a id="_idIndexMarker846"/>类似于存储的‘X _ train’(ndarray)存储的‘X _ test’(ndarray)存储的‘y _ train’(ndarray)存储的‘y _ test’(ndarray)的日志消息。我们将在使用protobuf recordIO训练输入类型配方的<em class="italic">训练KNN模型中使用这些值。</em></p></li>
			</ol>
			<p>现在，让我们看看这是如何工作的！</p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor237"/>工作原理……</h2>
			<p>在这个菜谱中，我们将表格CSV数据转换并序列化为protobuf recordIO格式。由于这种格式允许SageMaker以管道模式直接传输数据，因此训练开始时间会更快，因为我们不需要在训练步骤开始之前等待所有数据都下载完毕。这意味着当在训练期间使用这种数据格式时，大多数SageMaker算法通常工作得更好。</p>
			<p>请注意，我们没有尝试创建自己的转换器或序列化程序，而是使用了来自<strong class="bold"> SageMaker Python SDK </strong>的write_numpy_to_dense_tensor()函数将numpy数组值转换为protobuf recordIO格式。可以在这里随意查看write_numpy_to_dense_tensor()函数的实现:<a href="https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/amazon/common.py">https://github . com/AWS/sage maker-python-SDK/blob/master/src/sage maker/Amazon/common . py</a>。</p>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor238"/>使用protobuf recordIO训练输入类型训练KNN模型</h1>
			<p>在这个<a id="_idIndexMarker847"/>配方中，我们将使用SageMaker Python SDK训练<a id="_idIndexMarker848"/>两个k最近邻(KNN)模型——一个使用record_set()训练输入数据，将NumPy数组值作为参数，另一个使用从<em class="italic">将CSV数据转换为protobuf recordIO格式</em>配方生成的protobuf recordIO训练输入文件。</p>
			<p>一旦我们完成了这个食谱，我们将会对使用不同类型的估计器和使用不同的训练输入类型时的一些关键区别有更好的理解。</p>
			<h2 id="_idParaDest-207">做好准备</h2>
			<p>该配方上接<em class="italic">将CSV数据转换为protobuf recordIO格式</em>。</p>
			<h2 id="_idParaDest-208">如何去做…</h2>
			<p>该方法中的前几个步骤主要是加载并使用X_train和y_train的值来训练和部署KNN模型。在使用fit()函数启动训练作业之前，我们将使用KNN估计器类和record_set()函数来序列化和处理X_train和y_train值:</p>
			<ol>
				<li value="1">导航到SageMaker笔记本实例中的my-experiments/chapter04目录。如果这个目录还不存在，请随意创建。</li>
				<li>在my-experiments/chapter04目录中使用conda_python3内核创建一个新的笔记本，并以这个食谱的名称命名它(也就是说，使用protobuf recordIO训练输入类型训练一个KNN模型)。打开此笔记本进行编辑，因为我们将在接下来的几个步骤中使用代码更新此文件。</li>
				<li>通过使用%store magic命令检索X_train、y_train、X_test和y_test的值来启动笔记本。请记住，我们将这些值存储在<em class="italic">将CSV数据转换为protobuf recordIO格式</em>配方:<pre>%store -r <strong class="bold">X_train</strong> %store -r <strong class="bold">y_train</strong> %store -r <strong class="bold">X_test</strong> %store -r <strong class="bold">y_test</strong></pre></li>
				<li>导入<a id="_idIndexMarker849"/>和<a id="_idIndexMarker850"/>为我们的SageMaker实验准备一些先决条件。这些包括会话和角色，我们将在后面的步骤中把它们作为参数值传递给KNN估计器:<pre>import sagemaker from sagemaker import get_execution_role <strong class="bold">session</strong> = sagemaker.Session() <strong class="bold">role</strong> = get_execution_role()</pre></li>
				<li>我们使用上一步准备的参数值以及实例计数、实例类型、样本大小、k、特征维数和预测值类型参数的值来初始化KNN估计器对象:<pre>from sagemaker import KNN       estimator1 = <strong class="bold">KNN</strong>(     role=role,     instance_count=1,     instance_type='ml.c5.xlarge',     sample_size=50,     k=3,     feature_dim=5,     predictor_type="classifier",     sagemaker_session=session)</pre></li>
				<li>Create <a id="_idIndexMarker851"/>a<a id="_idIndexMarker852"/> record set using the record_set() function on the unlabeled DataFrame values. The record_set() function accepts a NumPy ndarray object, uploads the data to S3, and returns a Record object:<pre>record_set = estimator1.<strong class="bold">record_set</strong>(train=X_train, labels=y_train)
estimator1.fit(record_set)</pre><p>这将产生一组类似于图4.51 中所示的日志:</p><div><img src="img/B16850_04_51.jpg" alt="Figure 4.51 – Logs after running the fit() function&#13;&#10;" width="790" height="228"/></div><p class="figure-caption">图4.51–运行fit()函数后的日志</p><p>培训工作应该在几分钟内完成。培训工作完成后，您可以随意继续部署步骤。</p></li>
				<li>使用deploy()函数将KNN模型部署到一个推理端点:<pre>predictor1 = estimator1.<strong class="bold">deploy</strong>(     initial_instance_count=1,     instance_type='ml.t2.medium')</pre></li>
				<li>Inspect the x values in X_test before using the predict() function:<pre><strong class="bold">X_test</strong></pre><p>我们应该得到一组类似array的值([0.86968265，0.86968265，0.86968265，0.72413793，0.72413793])。</p></li>
				<li>Use the predict() function<a id="_idIndexMarker853"/> to <a id="_idIndexMarker854"/>get the corresponding predicted labels for each of the values in X_test:<pre>results1 = predictor1.<strong class="bold">predict</strong>(X_test)
results1[0:3]</pre><p>这将返回一组结构类似于图4.52 所示的结果:</p><div><img src="img/B16850_04_52.jpg" alt="Figure 4.52 – Results after using the predict() function&#13;&#10;" width="267" height="322"/></div><p class="figure-caption">图4.52–使用predict()函数后的结果</p><p>在<em class="italic">图4.52 </em>中，我们可以看到predict()函数返回了作为有效载荷传递的每个x值的预测标签。</p><p>该方法的后半部分重点关注使用我们在<em class="italic">将CSV数据转换为protobuf recordIO格式</em>方法中生成的protobuf recordIO文件来训练和部署第二个KNN模型。在使用fit()函数<a id="_idIndexMarker855"/>开始<a id="_idIndexMarker856"/>训练作业之前，我们将使用Estimator类和TrainingInput()，其中content_type参数值设置为application/x-recordio-proto buf。</p></li>
				<li>指定S3时段和前缀。确保用我们在《使用亚马逊SageMaker 开始机器学习》的<a href="B16850_01_Final_ASB_ePub.xhtml#_idTextAnchor020"> <em class="italic">第1章</em> </a> <em class="italic">中的<em class="italic">准备亚马逊S3桶和线性回归实验的训练数据集</em>中创建的桶的名称替换<insert s3="" bucket="" name="" here="">的值</insert></em></li>
				<li>Upload the generated train.io file to S3. Remember that this file was generated in the <em class="italic">Converting CSV data into protobuf recordIO format</em> recipe:<pre>!aws s3 cp tmp/train.io s3://{s3_bucket}/{prefix}/input/train.io</pre><p>这应该会产生一个类似于upload:tmp/train . io to S3://<bucket name="">/chapter 04/KNN/input/train . io的日志消息。</bucket></p></li>
				<li>创建一个训练输入通道配置对象，将content_type参数设置为application/x-recordio-proto buf:<pre>from sagemaker.inputs import TrainingInput       train_path = f"s3://{s3_bucket}/{prefix}/input/train.io" train = TrainingInput(     train_path,      content_type="application/x-recordio-protobuf")</pre></li>
				<li>We initialize<a id="_idIndexMarker857"/> a<a id="_idIndexMarker858"/> second estimator object using the Estimator class:<pre>from sagemaker.estimator import <strong class="bold">Estimator</strong>
     
estimator2 = <strong class="bold">Estimator</strong>(
    image_uri=estimator1.training_image_uri(),
    role=role,
    instance_count=1,
    instance_type='ml.c5.xlarge',
    sagemaker_session=session)</pre><p>注意，当使用Estimator类时，我们还将算法定型容器图像URI作为值传递给image_uri参数。</p></li>
				<li>使用set_hyperparameters()函数设置超参数:<pre>estimator2.<strong class="bold">set_hyperparameters</strong>(     sample_size=50,     k=3,     predictor_type="classifier")</pre></li>
				<li>Use the fit() function to start the training job. Note that in this step, we pass the S3 path of the training dataset (wrapped with a TrainingInput object) with the protobuf recordIO training input format:<pre>estimator2.<strong class="bold">fit</strong>({"train": train})</pre><p>这应该<a id="_idIndexMarker859"/>产生<a id="_idIndexMarker860"/>一组类似于图4.53 中所示的日志:</p><div><img src="img/B16850_04_53.jpg" alt="Figure 4.53 – Logs after running the fit() function&#13;&#10;" width="797" height="272"/></div><p class="figure-caption">图4.53–运行fit()函数后的日志</p><p>培训工作应该在几分钟内完成。培训工作完成后，您可以随意进行下一组步骤。</p></li>
				<li>使用Deploy()函数部署训练好的KNN模型。由于此步骤将提供一个机器学习实例并将模型部署到该实例，因此此步骤可能需要5-10分钟才能完成:<pre>predictor2 = estimator2.deploy(     initial_instance_count=1,     instance_type='ml.t2.medium')</pre></li>
				<li>更新预测器2的序列化程序和反序列化程序:<pre>import sagemaker from sagemaker.serializers import CSVSerializer from sagemaker.deserializers import JSONDeserializer predictor2.serializer = CSVSerializer() predictor2.deserializer = JSONDeserializer()</pre></li>
				<li>Use the predict() function<a id="_idIndexMarker861"/> to<a id="_idIndexMarker862"/> predict the corresponding labels for each of the values in X_test:<pre>results2 = predictor2.predict(X_test)
results2["predictions"][0:3]</pre><p>我们应该得到一组结构类似于[{'predicted_label': 1.0}，{'predicted_label': 1.0}，{'predicted_label': 0.0}]的值。</p></li>
				<li>Delete the two inference endpoints using the delete_endpoint() function:<pre>predictor1.<strong class="bold">delete_endpoint()</strong>
predictor2.<strong class="bold">delete_endpoint()</strong></pre><p>这将删除两个端点。</p></li>
			</ol>
			<p>让我们看看这是如何工作的！</p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor241"/>工作原理……</h2>
			<p>在这个配方中，我们演示了几种不同的方法来初始化一个估计器，并使用不同的训练输入格式来执行训练任务:</p>
			<ul>
				<li>使用特定于算法的估计器类，使用record_set()将训练输入数据与NumPy数组值作为参数</li>
				<li>使用带有protobuf recordIO训练输入文件的估计器类作为训练输入数据，作为fit()函数调用的参数</li>
			</ul>
			<p>当运行训练作业时，请注意这个配方中的关键差异，因为这些差异也适用于其他内置算法。</p>
			<p>正如在<em class="italic">将CSV数据转换为protobuf recordIO格式</em>中所讨论的，如果训练数据被序列化为protobuf recordIO格式，我们将从更快的训练开始时间中受益。当使用这种格式时，我们可以利用SageMaker中的管道模式，这种模式在训练作业运行时<a id="_idIndexMarker863"/>支持数据流<a id="_idIndexMarker864"/>。这意味着需要更少的磁盘空间，培训作业也将更快地开始(和完成)。</p>
			<h1 id="_idParaDest-210"><a id="_idTextAnchor242"/>使用AWS CLI准备SageMaker处理先决条件</h1>
			<p>机器学习过程中<a id="_idIndexMarker865"/>最重要的<a id="_idIndexMarker866"/>步骤之一涉及实际训练步骤之前的数据准备、处理和转换。在训练步骤之后，需要分析数据，并且可能需要在评估步骤之前和期间进一步处理数据。Amazon SageMaker处理是满足这些类型需求的最强大的选项之一。</p>
			<p>如果您有一个自定义数据处理脚本(例如，数据转换脚本)，您的数据存储在亚马逊S3存储桶中，或者您计划在一个隔离的托管环境中运行该脚本，该环境可以轻松配置为在稍后阶段处理生产工作负载的更大数据集，那么接下来的三个方法适合您！</p>
			<p class="callout-heading">小费</p>
			<p class="callout">从技术上讲，您可以使用Amazon SageMaker处理任何涉及使用托管服务来处理基础设施组件和执行特定操作的定制脚本的处理需求。</p>
			<p>在这个配方中，我们将为以下SageMaker处理配方准备先决条件——包含我们将加载和处理的数据的dataset.processing.csv、将存储文件的S3存储桶，以及可选的<strong class="bold"> Amazon ECR </strong>存储库。一旦我们<a id="_idTextAnchor243"/>完成了这个配方，我们将能够使用Python和R来配置、启动和监控SageMaker处理作业。</p>
			<h2 id="_idParaDest-211">准备就绪</h2>
			<p>以下是该配方的<a id="_idTextAnchor245"/>先决条件:</p>
			<ul>
				<li>一个正在运行的SageMaker笔记本实例，我们将在其中运行我们的命令并创建目录文件</li>
			</ul>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor246"/>怎么做……</h2>
			<p>第一组步骤集中在设置目录结构和在sage maker/my-experiments/chapter 04/tmp目录中创建虚拟dataset.processing.csv文件:</p>
			<ol>
				<li value="1">If you <a id="_idIndexMarker867"/>do <a id="_idIndexMarker868"/>not have an existing running terminal in your SageMaker notebook instance, create one by clicking the <strong class="bold">New</strong> button and selecting <strong class="bold">Terminal</strong> from the drop-down list of options.<div><img src="img/B16850_04_54.jpg" alt="Figure 4.54 – Creating a new terminal&#13;&#10;" width="1269" height="571"/></div><p class="figure-caption">图4.54–创建新的终端</p><p>注意<strong class="bold">终端</strong>选项位于下拉菜单的底部附近，如图<em class="italic">图4.54 </em>所示。</p></li>
				<li>使用Bash命令，导航到您将创建Python和R笔记本、CSV文件和docker文件的目标目录:<pre>cd SageMaker/my-experiments mkdir -p chapter04/tmp cd chapter04/tmp</pre></li>
				<li>创建一个名为dataset.processing.csv: <pre>touch dataset.processing.csv</pre>的空文件</li>
				<li>运行以下代码块，在dataset.processing.csv文件中设置一些虚拟值。这个样本dataset.processing.csv文件<a id="_idIndexMarker869"/>是一个任意的csv文件，有三列，标签、a和b: <pre>cat &gt; dataset.processing.csv &lt;&lt;EOF label,a,b one,1,2 two,3,4 EOF</pre></li>
				<li>Check the contents of the dataset.processing.csv file:<pre>cat dataset.processing.csv</pre><p>这将产生类似的输出，如图<em class="italic">图4.55 </em>所示:</p><div><img src="img/B16850_04_55.jpg" alt="Figure 4.55 – Contents of the dummy dataset.processing.csv file&#13;&#10;" width="309" height="66"/></div><p class="figure-caption">图4.55–虚拟dataset.processing.csv文件的内容</p><p>如果dataset.processing.csv文件是有效的csv文件，我们可以继续下一组步骤。</p><p>下一组步骤集中在为自定义容器映像创建Amazon ECR存储库，我们将在R  recipe中使用SageMaker处理在<em class="italic">托管数据处理中构建该存储库。</em></p></li>
				<li>Authenticate with Amazon ECR using the following command:<pre>ACCOUNT_ID=$(aws sts get-caller-identity | jq -r ".Account")
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com</pre><p>这个<a id="_idIndexMarker871"/>应该会给我们<a id="_idIndexMarker872"/>登录成功的消息。</p><p class="callout-heading">重要说明</p><p class="callout">请注意，我们假设您的存储库将创建在us-east-1地区。如果需要，可以随意修改命令中的区域。这适用于本章中的所有命令。</p></li>
				<li>Create an Amazon ECR repository using the following command. Make sure to specify your own ECR repository name (for example, sagemaker-processing-r):<pre>ECR_REPO_NAME="&lt;insert name here&gt;"
aws ecr create-repository --repository-name $ECR_REPO_NAME --region us-east-1 --image-tag-mutability IMMUTABLE</pre><p>这将产生一组类似于<em class="italic">图4.56 </em>所示的值:</p></li>
			</ol>
			<div><div><img src="img/B16850_04_56.jpg" alt="Figure 4.56 – Amazon ECR repository successfully created&#13;&#10;" width="861" height="269"/>
				</div>
			</div>
			<p class="figure-caption">图4.56–成功创建Amazon ECR存储库</p>
			<p>在<em class="italic">图4.56 </em>中，我们<a id="_idIndexMarker873"/>可以看到<a id="_idIndexMarker874"/>我们已经成功创建了亚马逊ECR仓库。记下repositoryUri和repositoryName值<a id="_idTextAnchor247"/> es，因为我们稍后将<a id="_idTextAnchor248"/>在R 配方中使用SageMaker处理的<em class="italic">管理数据处理中使用它们。</em></p>
			<p>现在，让我们看看这是如何工作的！</p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor249"/>工作原理……</h2>
			<p>在本食谱中，我们为接下来的几个涉及Python和r中SageMaker处理的食谱准备了一些先决条件。对于使用<strong class="bold">脚本模式</strong>的Python食谱，先决条件包括:</p>
			<ul>
				<li>S3桶(输入和输出)</li>
				<li>虚拟数据集/文件</li>
			</ul>
			<div><div><img src="img/B16850_04_57.jpg" alt="Figure 4.57 – Getting the SageMaker Processing job prerequisites ready&#13;&#10;" width="1202" height="376"/>
				</div>
			</div>
			<p class="figure-caption">图4.57–准备好SageMaker处理作业的先决条件</p>
			<p>在<em class="italic">图4.57 </em>中，我们可以看到，在接下来的两个配方中运行SageMaker加工作业时，我们在该配方中准备的S3桶将用作输入和输出源。</p>
			<p>另一方面，对于使用定制容器映像的R recipe，先决条件包括:</p>
			<ul>
				<li>S3桶(输入和输出)</li>
				<li>虚拟数据集/文件</li>
				<li>ECR储存库</li>
			</ul>
			<p>根据<a id="_idIndexMarker875"/>您的需求和要求，您将选择这两种方法中的一种。我们将在接下来的两个食谱中看到这两种方法的实际应用。</p>
			<h1 id="_idParaDest-214"><a id="_idTextAnchor250"/>使用Python中的SageMaker处理来管理数据处理</h1>
			<p>在<a id="_idIndexMarker877"/>之前的配方中，我们<a id="_idIndexMarker878"/>为我们将在该配方中运行的SageMaker处理作业准备了一些先决条件，包括在指定目录中准备虚拟数据集。现在，我们将创建一个Python脚本，并使用SageMaker处理在托管环境中运行定制的Python脚本。当启动和执行处理作业时，该托管环境被自动创建、配置和销毁。如果您正在处理一个类似于以下的需求，那么这个方法适合您:</p>
			<ul>
				<li>使用sklearn (scikit-learn)规范化数字特征</li>
				<li>使用nltk ( <strong class="bold">自然语言工具包</strong>)进行文本预处理</li>
				<li>熊猫的自动化特征工程</li>
				<li>执行培训后处理和评估步骤</li>
			</ul>
			<p>一旦我们完成了这个配方，我们将在一个隔离和管理的SageMaker处理环境中执行定制的Python脚本，并将处理作业的输出存储在一个S3存储桶中。</p>
			<p class="callout-heading">重要说明</p>
			<p class="callout">注意，在这个菜谱中，我们能够使用Python运行SageMaker处理作业，而不需要<a id="_idIndexMarker879"/>到<a id="_idIndexMarker880"/>构建和准备定制容器映像。在R  recipe中的<em class="italic">Managed data Processing with S<a id="_idTextAnchor251"/>age maker Processing中，我们将使用R语言构建并使用一个自定义容器映像来运行SageMaker处理作业。</em></p>
			<h2 id="_idParaDest-215">做好准备</h2>
			<p>以下是该配方的<a id="_idTextAnchor253"/>先决条件:</p>
			<ul>
				<li>该配方上接<em class="italic">使用AWS CLI </em>配方准备SageMaker加工先决条件。</li>
			</ul>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor254"/>怎么做……</h2>
			<p>这个配方由两部分组成:编写processing.py脚本和使用来自<strong class="bold"> SageMaker Python SDK </strong>的SKLearnProcessor来运行processing.py脚本。第一组步骤主要是在my-experiments/chapter04目录中准备processing.py脚本:</p>
			<ol>
				<li value="1">导航到SageMaker笔记本实例中的my-experiments/chapter04目录。如果这个目录还不存在，请随意创建。</li>
				<li>Create an empty text file inside the my-experiments/chapter04 directory and name it processing.py. Open this file for editing as we will update this file with the code in the next couple of steps.<p class="callout-heading">重要说明</p><p class="callout">注意，这个方法假设processing.py文件和相应的Jupyter笔记本文件都在my-experiments/chapter04目录中。如果我们要改变这些文件的位置，我们也需要更新这个菜谱中的路径。现在，让我们坚持这个假设，这样我们就可以让SageMaker处理作业运行起来！</p></li>
				<li>通过导入pandas、argparse、os、子进程、importlib和sys.executable: <pre>import pandas as pd import argparse import os import subprocess import importlib from sys import executable</pre>来启动<a id="_idIndexMarker881"/>脚本<a id="_idIndexMarker882"/></li>
				<li>准备使用pip和子进程安装和加载指定包的函数。调用:<pre>def <strong class="bold">install_and_load</strong>(target):     sequence = [executable, "-m",                  "pip", "install", target]     <strong class="bold">subprocess.call</strong>(sequence)          print(f'[+] Successfully installed {target}')     return <strong class="bold">importlib.import_module</strong>(target)</pre></li>
				<li>准备卸载指定软件包的函数。这将在使用SageMaker Python SDK在托管环境中运行脚本之前在本地测试脚本时使用:<pre>def <strong class="bold">uninstall</strong>(target):     sequence = [         executable, "-m",          "pip", "uninstall", "-y", target]           <strong class="bold">subprocess.call</strong>(sequence)     print(f'[+] Successfully uninstalled {target}')</pre></li>
				<li>定义<a id="_idIndexMarker883"/>process _ args()函数，当脚本运行时<a id="_idIndexMarker884"/>返回参数。如果没有指定参数，将使用默认值:<pre>def <strong class="bold">process_args</strong>():     parser = argparse.ArgumentParser()     parser.add_argument(         '--sample-argument',          type=int, default=1)     arguments, _ = parser.parse_known_args()          return arguments</pre></li>
				<li>定义load_input()函数，该函数使用文件名<pre>def <strong class="bold">load_input</strong>(input_target):     df = pd.read_csv(input_target)     return df</pre>读取并返回CSV文件的内容</li>
				<li>定义save_output()函数，在指定位置生成一个样本文件:<pre>def <strong class="bold">save_output</strong>(output_target):     with open(output_target, 'w') as writer:         writer.write("sample output\n")</pre></li>
				<li>Generate<a id="_idIndexMarker885"/> the main() function, which<a id="_idIndexMarker886"/> makes use of the functions created in this recipe:<pre>def <strong class="bold">main</strong>():
    args = <strong class="bold">process_args</strong>()
    print(args)
    
    plt = <strong class="bold">install_and_load</strong>('matplotlib')
    print(plt)
    <strong class="bold">uninstall</strong>('matplotlib')
     
    path = "/opt/ml/processing/input/dataset.processing.csv"
    sample_input = <strong class="bold">load_input</strong>(path)
    print(sample_input)
    
    <strong class="bold">save_output</strong>("/opt/ml/processing/output/output.csv")
    print('[+] DONE')</pre><p>我们在main()函数中执行了以下操作:</p><ul><li>使用process_args()函数读取参数</li><li>使用install_and_load()函数执行了库的测试安装</li><li>使用uninstall()函数卸载了<a id="_idIndexMarker887"/>库</li><li>使用load_input()函数加载了一个虚拟CSV文件</li><li>Generated a dummy CSV file using the save_output() function<p class="callout-heading">注意</p><p class="callout">使用文件路径时，建议使用os.path.join()来连接路径。请注意，我们在这里采取了一种简化这个食谱的捷径。同时，我们已经硬编码了将要加载的文件的名称。在一个更现实的例子中，我们将使用几个语句来列出一个目录的内容，并加载该目录中的相关文件。</p></li></ul></li>
				<li>Finally, wrap up the script file by adding the following two lines, which run the main function when the script is executed:<pre>if __name__ == "__main__":
    <strong class="bold">main()</strong></pre><p>在这个配方的基础上构建时，注意脚本不能使用硬编码的值。我们在这个菜谱中采用了一种快捷方式来使processing.py文件尽可能短。请随意使用我们在第2章 、<em class="italic">构建和使用您自己的算法容器映像</em>的<em class="italic">准备和测试Python </em>配方中的路径和文件的实用函数。</p><p class="callout-heading">小费</p><p class="callout">您可以通过检查Machine-Learning-with-Amazon-sage maker-Cookbook存储库的工作副本来检查您刚刚编写的processing.py文件是否正确:<a href="https://github.com/PacktPublishing/Machine-Learning-with-Amazon-SageMaker-Cookbook/blob/master/Chapter04/processing.py">https://github . com/packt publishing/Machine-Learning-with-Amazon-sage maker-Cookbook/blob/master/chapter 04/processing . py</a>。</p><p>接下来的几个步骤集中在使用SageMaker Python SDK中的SKLearnProcessor运行Python脚本。确保您正在使用conda_python3内核的空笔记本运行<a id="_idIndexMarker889"/>下一组<a id="_idIndexMarker890"/>命令。</p></li>
				<li>准备并导入所需的库和先决条件:<pre>import boto3 import sagemaker from sagemaker import get_execution_role from sagemaker.sklearn.processing import SKLearnProcessor role = get_execution_role()</pre></li>
				<li>初始化SKLearnProcessor: <pre>sklearn_processor = <strong class="bold">SKLearnProcessor</strong>(     framework_version='0.20.0',     role=role,     instance_count=1,     instance_type='ml.m5.large')</pre></li>
				<li>初始化pinput1和poutput1，它们是将在下一步使用的ProcessingInput和ProcessingOutput对象:<pre>from sagemaker.processing import ProcessingInput, ProcessingOutput source = 'tmp/dataset.processing.csv' pinput1 = <strong class="bold">ProcessingInput</strong>(     source=source, destination='/opt/ml/processing/input') poutput1 = <strong class="bold">ProcessingOutput</strong>(     source='/opt/ml/processing/output')</pre></li>
				<li>Use the run() API <a id="_idIndexMarker891"/>with<a id="_idIndexMarker892"/> the specified parameters shown here:<pre>sklearn_processor.<strong class="bold">run</strong>(
    code=<strong class="bold">'processing.py</strong>',
    arguments = ['--sample-argument', '3'],
    inputs=[pinput1],
    outputs=[poutput1]
)</pre><p class="callout-heading">重要说明</p><p class="callout">如前所述，这个方法假设运行这些Python代码行的笔记本与processing.py文件在同一个目录中。如果processing.py文件位于另一个目录中，请确保更新代码参数值中指定的路径。</p></li>
				<li>将S3输出路径存储在目标变量中:<pre>latest_job = sklearn_processor.latest_job destination = latest_job.outputs[0].destination</pre></li>
				<li>将脚本生成的output.csv文件从S3目标路径复制到Python笔记本所在的SageMaker笔记本实例目录。请注意，在下面的代码片段中有一个空格和一个点:<pre>!aws s3 cp "{destination}/output.csv" tmp/outp<a id="_idTextAnchor255"/>ut.csv</pre></li>
				<li>Inspect<a id="_idTextAnchor256"/> the output.csv file:<pre>!cat tmp/output.csv</pre><p>output.csv文件应该包含输出字符串值。</p></li>
			</ol>
			<p>现在，让我们<a id="_idIndexMarker893"/>看看这个<a id="_idIndexMarker894"/>是如何工作的！</p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor257"/>工作原理……</h2>
			<p>在这个菜谱中，我们使用了带有<strong class="bold"> SageMaker处理</strong>的<strong class="bold">脚本模式</strong>选项来运行我们的Python脚本。使用Python时，我们可能不再需要构建和使用自定义容器，因为我们可以使用SageMaker Python SDK的内置类(如SKLearnProcessor)来使用脚本模式“直接”运行脚本。在幕后，脚本模式利用AWS团队准备的预构建容器映像。</p>
			<div><div><img src="img/B16850_04_58.jpg" alt="Figure 4.58 – Using script mode with SageMaker Processing&#13;&#10;" width="1416" height="582"/>
				</div>
			</div>
			<p class="figure-caption">图4.58–使用脚本模式进行SageMaker处理</p>
			<p>在<em class="italic">图4.58 </em>中，我们可以看到SageMaker Processing在SageMaker提供的机器学习实例内的内置容器内自动运行我们的自定义脚本，自定义容器内脚本生成的日志被推送到CloudWatch日志中。在执行脚本之前，位于指定输入S3路径中的文件被加载、复制并挂载到容器中。脚本执行后，存储在/opt/ml/processing/output目录中的自定义脚本生成的文件会自动复制到指定的输出S3路径中。</p>
			<p><a id="_idIndexMarker895"/>这个配方中的例子涉及到<a id="_idIndexMarker896"/>执行一些基本的打印和保存语句。在更现实的场景中，SageMaker处理<a id="_idIndexMarker897"/>可用于(但不限于)以下用例之一:</p>
			<ul>
				<li>大数据集的特征工程和数据准备</li>
				<li>数据可视化</li>
				<li>文件类型转换</li>
				<li>模型评估</li>
			</ul>
			<p>假设我们通过这种方法使用SageMaker处理的内置容器映像，我们可能会遇到需要使用未安装在内置容器中的Python库(例如matplotlib)的情况。在这个方法中，我们已经演示了如何使用我们在脚本中定义的install_and_load()函数来处理这个变通方法。此解决方法利用subprocess.call()函数在单独的进程中运行pip install Bash命令。</p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor258"/>还有更多……</h2>
			<p>为了管理和降低运行实验和处理作业的成本，运行包含实验代码的Jupyter笔记本的SageMaker笔记本实例最好使用较小的实例类型(例如t2.medium ),因为它将在大多数时间运行。另一方面，在处理大型数据集时，应该使用较大的实例类型(例如，ml.m5.xlarge ),这些实例预计只运行有限的时间(例如，5-15分钟)。</p>
			<p>这是SageMaker处理的<a id="_idIndexMarker898"/>优势之一——我们只为资源运行的计费时间付费。SageMaker处理会在处理作业启动时自动创建资源，并在处理作业完成后删除资源。</p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor259"/>参见</h2>
			<p>如果您正在寻找如何使用真实数据集和更复杂的示例来使用SageMaker处理的示例，请随意查看AWS/Amazon-sage maker-examples GitHub资源库中的一些笔记本:</p>
			<ul>
				<li>使用SageMaker处理和SparkML的特性<a id="_idIndexMarker899"/>转换【T17:<a href="https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker_processing/feature_transformation_with_sagemaker_processing/feature_transformation_with_sagemaker_processing.ipynb">https://github . com/AWS/Amazon-SageMaker-examples/blob/master/SageMaker _ Processing/feature _ transformation _ with _ SageMaker _ Processing/feature _ transformation _ with _ SageMaker _ Processing . ipynb</a></li>
				<li>分布式<a id="_idIndexMarker901"/>数据<a id="_idIndexMarker902"/>处理用SageMaker处理和py spark:<a href="https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker_processing/spark_distributed_data_processing/sagemaker-spark-processing.ipynb">https://github . com/AWS/Amazon-sage maker-examples/blob/master/sage maker _ Processing/spark _ distributed _ data _ Processing/sage maker-spark-Processing . ipynb</a></li>
			</ul>
			<p>现在，让我们在下一个菜谱中仔细看看如何使用SageMaker处理来运行R脚本！</p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor260"/>R中用SageMaker处理的托管数据处理</h1>
			<p>在使用AWS CLI 方法准备SageMaker处理先决条件的<em class="italic">中，我们<a id="_idIndexMarker903"/>准备了一些<a id="_idIndexMarker904"/>先决条件，包括我们将在SageMaker处理作业中使用的虚拟数据集和ECR存储库，我们将在此存储库中存储我们将在此方法中准备的自定义容器映像。</em></p>
			<p>现在，我们将创建一个R脚本，构建一个定制的R容器映像，并使用SageMaker处理在一个托管环境中运行R脚本，该托管环境是在启动和执行处理作业时自动创建、配置和销毁的。如果您的工作需求与以下需求类似，那么这个方法适合您:</p>
			<ul>
				<li>用normalr包规范化数字特征</li>
				<li>用tm(文本挖掘)包进行文本预处理</li>
				<li>使用dplyr包进行自动化特征工程</li>
				<li>执行培训后处理和评估步骤</li>
			</ul>
			<p>一旦我们完成了这个配方，我们将在一个隔离和管理的SageMaker处理环境中执行定制的R脚本，并将处理作业的输出存储在一个S3桶中。</p>
			<p class="callout-heading">重要说明</p>
			<p class="callout">请注意，在此配方中，我们使用R语言构建并使用自定义容器映像来运行SageMaker处理作业，而在使用Python 配方中的SageMaker处理的<em class="italic">托管数据处理中，我们能够使用Python运行SageMaker处理作业，而无需构建和准备自定义容器映像。从技术上来说，我们<a id="_idTextAnchor261"/>也可以在使用Python时构建一个定制的容器映像，但是我们使用了AWS和SageMaker团队已经提供给我们的预构建的容器映像。</em></p>
			<h2 id="_idParaDest-221">阅读<a id="_idTextAnchor262"/> y</h2>
			<p>以下是该配方的先决条件:</p>
			<ul>
				<li>该配方上接<em class="italic">使用AWS CLI </em>配方准备SageMaker加工先决条件。</li>
			</ul>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor263"/>怎么做……</h2>
			<p>这个方法包括编写processing.r脚本、准备Docker文件、<a id="_idIndexMarker905"/>构建<a id="_idIndexMarker906"/>Docker容器映像并将其推送到ECR存储库，以及使用ScriptProcessor在定制容器中运行R脚本。前几个步骤主要是让我们在my-experiments/chapter04目录中获得一个准系统processing.r脚本:</p>
			<ol>
				<li value="1">导航到SageMaker笔记本实例中的my-experiments/chapter04目录。如果这个目录还不存在，请随意创建。</li>
				<li>Create an empty text file inside the my-experiments/chapter04 directory and name it processing.r. Open this file for editing as we will update this file with the code in the next couple of steps.<p class="callout-heading">重要说明</p><p class="callout">注意，这个方法假设processing.r文件、docker文件和相应的Jupyter Notebook (ipynb)文件位于my-experiments/chapter04目录中。如果我们要改变这些文件的位置，我们也需要更新这个菜谱中的路径。现在，让我们坚持这个假设，这样我们就可以让SageMaker处理作业运行起来！</p></li>
				<li>通过加载readr和argparse库来启动processing.r脚本。请注意，argparse包需要单独安装，可能不是base R安装的一部分。在这种情况下，假设这个脚本将在定制容器中运行，我们将必须确保依赖项安装在定制容器映像中，以便脚本运行时不会出现问题:<pre>library(<strong class="bold">readr</strong>) library(<strong class="bold">"argparse"</strong>)</pre></li>
				<li>添加以下代码行来加载从SDK传递给脚本的参数。如果<a id="_idIndexMarker907"/>没有传递<a id="_idIndexMarker908"/>参数，将使用指定的默认值:<pre>parser &lt;- <strong class="bold">ArgumentParser</strong>() parser$add_argument("--sample-argument", default=1L) args &lt;- parser$parse_args() print(args)</pre></li>
				<li>Load and print the contents the sample dataset.processing.csv file by adding the following lines of code. Let's also create an output.csv file containing the output string value:<pre>filename &lt;- "/opt/ml/processing/input/<strong class="bold">dataset.processing.csv</strong>"
df &lt;- read_csv(filename)
print(df)
cat("output",
    file="/opt/ml/processing/output/<strong class="bold">output.csv</strong>",
    sep="\n")</pre><p>在这个配方的基础上构建时，注意脚本不能使用硬编码的值。我们在这个食谱中采用了一个快捷方式，使processing.r文件尽可能短。请随意使用我们在第二章  <em class="italic">中使用的<em class="italic">准备和测试R </em>配方中的训练脚本中的路径和文件，构建和使用您自己的算法容器映像</em>。</p><p class="callout-heading">小费</p><p class="callout">您可以通过检查来自<em class="italic">Machine-Learning-with-Amazon-sage maker-Cookbook</em>资源库的工作副本来检查您刚刚编写的processing.r文件是否正确:<a href="https://github.com/PacktPublishing/Machine-Learning-with-Amazon-SageMaker-Cookbook/blob/master/Chapter04/processing.r">https://github . com/packt publishing/Machine-Learning-with-Amazon-sage maker-Cookbook/blob/master/chapter 04/processing . r</a>。</p><p>既然<a id="_idIndexMarker909"/>我们<a id="_idIndexMarker910"/>已经准备好了processing.r脚本文件，我们现在将把精力集中在为自定义R容器映像生成docker文件上。</p></li>
				<li>确保您位于SageMaker笔记本实例中的my-experiments/chapter04目录下。在my-experiments/chapter04目录中创建一个空文本文件，并将其命名为Dockerfile。打开该文件进行编辑，因为我们将在接下来的几个步骤中用代码更新该文件。</li>
				<li>通过添加下面一行来启动Dockerfile文本文件:<pre><strong class="bold">FROM</strong> rocker/tidyverse:latest</pre></li>
				<li>在容器构建步骤中添加以下几行来安装argparse库:<pre><strong class="bold">RUN</strong> install2.r --error \     argparse</pre></li>
				<li>Finally, specify the entry point using the ENTRYPOINT directive:<pre><strong class="bold">ENTRYPOINT</strong> ["Rscript"]</pre><p>在下一组步骤中构建容器映像时，我们将使用rocker/tidyverse容器映像作为基础映像。这个容器映像包含我们运行SageMaker处理作业所需的base R安装和包。</p><p class="callout-heading">小费</p><p class="callout">通过检查来自<em class="italic">Machine-Learning-with-Amazon-sage maker-Cookbook</em>资源库的工作副本，您可以随意检查您刚刚编写的docker文件是否正确:<a href="https://github.com/PacktPublishing/Machine-Learning-with-Amazon-SageMaker-Cookbook/blob/master/Chapter04/Dockerfile">https://github . com/packt publishing/Machine-Learning-with-Amazon-sage maker-Cookbook/blob/master/chapter 04/docker file</a></p><p>接下来的<a id="_idIndexMarker911"/>几个<a id="_idIndexMarker912"/>步骤集中在构建定制容器映像并将其推送到现有的ECR存储库中。</p></li>
				<li>Navigate back to the my-experiments/chapter04 directory in the Jupyter Notebook tab. Create a new terminal by clicking the <strong class="bold">New</strong> button and selecting <strong class="bold">Terminal</strong> from the list of drop-down options. <div><img src="img/B16850_04_59.jpg" alt="Figure 4.59 – Creating a new terminal&#13;&#10;" width="1292" height="577"/></div><p class="figure-caption">图4.59–创建新终端</p><p>我们应该会在Jupyter笔记本页面右上角的<strong class="bold">上传</strong>按钮旁边找到<strong class="bold">新</strong>按钮。注意<strong class="bold">端子</strong>选项应该靠近下拉列表的底部，正如我们在<em class="italic">图4.59 </em>中看到的。一旦<strong class="bold">终端</strong>选项卡可用，我们将在该终端的下一组步骤中运行命令。</p></li>
				<li>通过运行以下代码，导航到包含Dockerfile和processing.r脚本的目录:<pre><strong class="bold">cd SageMaker/my-experiments/chapter04</strong></pre></li>
				<li>Run the following lines of code to authenticate with Amazon ECR. Make sure to specify the correct value for the ECR URI of the repository by replacing &lt;ECR URI&gt; in the following<a id="_idIndexMarker913"/> code snippet. The<a id="_idIndexMarker914"/> Amazon ECR URI should have a format similar to 1234567890.dkr.ecr.us-east-1.amazonaws.com/sagemaker-processing-r. Remember that we created this ECR repository in the <em class="italic">Preparing the SageMaker Processing prerequisites using the AWS CLI</em> recipe:<pre><strong class="bold">ECR_URI="&lt;insert ECR URI here&gt;"</strong>
<strong class="bold">aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin $ECR_URI</strong></pre><p>这将产生类似于图4.60 所示的信息:</p><div><img src="img/B16850_04_60.jpg" alt="Figure 4.60 – Authenticating with Amazon ECR&#13;&#10;" width="789" height="84"/></div><p class="figure-caption">图4.60–使用Amazon ECR进行认证</p><p>在<em class="italic">图4.60 </em>中，我们可以看到我们已经成功通过了亚马逊ECR的认证。如果您在运行前一组命令时遇到问题，请确保区域和ECR URI参数是正确的，然后重试。</p></li>
				<li>Build and push the Docker container image by running the following command. Note that there is a space and a dot after $TAG:<pre><strong class="bold">TAG=1</strong>
<strong class="bold">docker build -t $ECR_URI:$TAG .</strong></pre><p>这将启动构建过程，如图<em class="italic">图4.61 </em>所示:</p><div><img src="img/B16850_04_61.jpg" alt="Figure 4.61 – Running the docker build command&#13;&#10;" width="657" height="598"/></div><p class="figure-caption">图4.61–运行docker build命令</p><p>在<em class="italic">图4.61 </em>中，我们<a id="_idIndexMarker915"/>可以<a id="_idIndexMarker916"/>看到运行docker build命令后，构建过程已经被触发。提取基本容器映像后，将安装argparse包。</p><p class="callout-heading">小费</p><p class="callout">构建过程需要几分钟，所以请随意喝杯咖啡或茶吧！</p></li>
				<li>Once the Docker container image has successfully been built, let's run the docker push command:<pre><strong class="bold">docker push $ECR_URI:$TAG</strong></pre><p>这产生了一组类似于图4.62 中所示的日志:</p><div><img src="img/B16850_04_62.jpg" alt="Figure 4.62 – Logs after running the docker push command&#13;&#10;" width="656" height="185"/></div><p class="figure-caption">图4.62–运行docker push命令后的日志</p><p>接下来的几个步骤集中在使用SDK中的ScriptProcessor在自定义R容器中运行R脚本。</p></li>
				<li>再次导航<a id="_idIndexMarker917"/>到SageMaker笔记本实例中的my-experiments/chapter04目录<a id="_idIndexMarker918"/>。在my-experiments/chapter04中使用R内核创建一个新的笔记本。打开笔记本(ipynb)文件，因为我们将在接下来的几个步骤中用语句和代码行更新该文件。您可以随意用这个方法的标题来重命名笔记本——用r中的SageMaker处理来管理数据处理。</li>
				<li>加载网状库:<pre>library('<strong class="bold">reticulate</strong>')</pre></li>
				<li>为SageMaker处理作业执行所需的导入:<pre>sagemaker &lt;- import('sagemaker') boto3 &lt;- import('boto3') role &lt;- sagemaker$get_execution_role()</pre></li>
				<li>为ScriptProcessor作业准备其他先决条件。确保将<ecr repo="" uri="">的值替换为在<em class="italic">使用AWS CLI </em>方法准备SageMaker处理先决条件中创建的ECR库的Amazon ECR URI。它应该具有类似于1234567890 . dkr . ECR . us-east-1 . amazonaws . com/sage maker-processing-r的格式。除此之外，确保将&lt;标记&gt;的值替换为与构建和推送Docker容器映像时使用的最新标记值相等的值:<pre>processing_repository_uri &lt;- <strong class="bold">"&lt;ECR Repo URI&gt;</strong>:<strong class="bold">&lt;TAG&gt;</strong>"       session &lt;- boto3$session$Session() sagemaker_session &lt;- sagemaker$Session(boto_session=session)</pre></ecr></li>
				<li>初始化ScriptProcessor，<a id="_idIndexMarker919"/>指定<a id="_idIndexMarker920"/>参数如下:<pre>ScriptProcessor &lt;- sagemaker$processing$<strong class="bold">ScriptProcessor</strong> script_processor &lt;- <strong class="bold">ScriptProcessor</strong>(     command=list('Rscript'),        image_uri=processing_repository_uri,     role=role,                                   sagemaker_session=sagemaker_session,     instance_count=1L,     instance_type='ml.c5.large')</pre></li>
				<li>初始化pinput1和poutput1，它们是将在下一步中使用的ProcessingInput和ProcessingOutput对象:<pre>ProcessingInput &lt;- sagemaker$processing$<strong class="bold">ProcessingInput</strong> ProcessingOutput &lt;- sagemaker$processing$<strong class="bold">ProcessingOutput</strong>       source &lt;- 'tmp/dataset.processing.csv' pinput1 &lt;- ProcessingInput(     source=source, destination='/opt/ml/processing/input') poutput1 &lt;- ProcessingOutput(     source='/opt/ml/processing/output')</pre></li>
				<li>Use the run() function <a id="_idIndexMarker921"/>using the <a id="_idIndexMarker922"/>specified parameters, as shown in the following block of code. You may decide to set wait=FALSE to proceed with the next steps but take note that the SageMaker Processing job might take around 5-10 minutes to complete:<pre>script_processor$<strong class="bold">run</strong>(code='<strong class="bold">processing.r</strong>',
                     inputs=list(pinput1),
                     outputs=list(poutput1),
                     arguments=list('--sample-argument','3'),
                     wait=TRUE)</pre><p class="callout-heading">重要说明</p><p class="callout">如前所述，这个方法假设运行这几行R代码的笔记本与processing.r文件在同一个目录中。如果processing.r文件位于另一个目录中，请确保更新代码参数值中指定的路径。</p></li>
				<li>准备cmd函数，该函数将用于在R笔记本内部运行Bash命令:<pre><strong class="bold">cmd</strong> &lt;- function(bash_command) {     print(bash_command)     output &lt;- system(bash_command, intern=TRUE)     last_line = ""          for (line in output) {          cat(line)         cat("\n")         last_line = line      }     return(last_line)  }</pre></li>
				<li>使用pip: <pre>cmd('pip install awslogs')</pre>安装<a id="_idIndexMarker923"/>AWS logs实用程序<a id="_idIndexMarker924"/></li>
				<li>Use the awslogs utility to read the generated logs from CloudWatch Logs. Modify this as needed:<pre>cmd("awslogs get /aws/sagemaker/ProcessingJobs -s1h --aws-region=us-east-1")</pre><p class="callout-heading">重要说明</p><p class="callout">确保附加到notebook实例的SageMaker执行角色附加了CloudWatchLogsReadOnlyAccess策略。</p><p>运行aws logs命令会生成类似于图4.63 中所示的日志:</p><div><img src="img/B16850_04_63.jpg" alt="Figure 4.63 – Processing job logs using awslogs&#13;&#10;" width="594" height="391"/></div><p class="figure-caption">图4.63–使用awslogs处理作业日志</p><p><em class="italic">图4.63 </em>显示了<a id="_idIndexMarker925"/>我们在处理作业中自定义脚本生成的日志<a id="_idIndexMarker926"/>。正如我们从日志中看到的，R脚本能够从使用SDK提供的参数列表中获得参数值3；输入CSV文件被加载到数据帧中，然后打印到日志中</p><p>这个配方中的最后一组步骤主要是检索SageMaker处理作业生成的输出文件。如果您还记得，我们在processing.r脚本文件中添加了一行代码，它将输出字符串值存储在/opt/ml/processing/output/output . CSV文件中。SageMaker将在运行的容器中拾取它，并将它上传到亚马逊S3目的地路径。</p></li>
				<li>接下来，检索最新处理作业的输出路径:<pre>latest_job &lt;- script_processor$latest_job destination &lt;- latest_job$outputs[[1]]$destination</pre></li>
				<li>下载输出CSV文件:<pre>csv_path &lt;- paste0(destination, "/output.csv") command &lt;- paste("aws s3 cp", csv_path, "tmp/output.processing.r.csv") cmd(command)</pre></li>
				<li>Finally, use read.csv() to read the output file c<a id="_idTextAnchor264"/>ontents:<pre>read.<a id="_idTextAnchor265"/>csv("tmp/output.processing.r.csv", header=FALSE)[[1]]</pre><p>我们应该得到前面代码行的输出字符串。</p></li>
			</ol>
			<p>现在，让我们看看这是如何工作的！</p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor266"/>工作原理……</h2>
			<p>在这个菜谱中，我们<a id="_idIndexMarker927"/>已经构建并<a id="_idIndexMarker928"/>使用了一个带有SageMaker处理的定制容器映像来运行我们的R脚本。使用Python时，我们可能不再需要构建和使用自定义容器，因为我们可以使用SageMaker Python SDK中的内置类(如SKLearnProcessor类)直接在脚本模式下运行脚本。在幕后，脚本模式利用了预先构建的容器映像。当使用带有自定义容器映像选项的ScriptProcessor类时，由于自定义容器映像可以包含用户指定和配置的预安装库和工具，因此灵活性更高。</p>
			<div><div><img src="img/B16850_04_64.jpg" alt="Figure 4.64 – Choosing between script mode and using your own container image&#13;&#10;" width="1086" height="502"/>
				</div>
			</div>
			<p class="figure-caption">图4.64–在脚本模式和使用自己的容器图像之间选择</p>
			<p>我们可以在<em class="italic">图4.64 </em>中看到，当我们需要在SageMaker处理中使用R或其他语言时，我们需要构建并使用自己的容器映像。如果您想使用Python并且想使用您自己的自定义容器映像，那么使用ScriptProcessor类也是可能的。</p>
			<p class="callout-heading">重要说明</p>
			<p class="callout">注意<a id="_idIndexMarker929"/>SKLearnProcessor的基类是ScriptProcessor类。它们的大部分参数是相同的(例如sagemaker_session、role、base _ job _ name)；SKLearnProcessor类的不同之处在于，它不接受image_uri参数值，因为它使用了AWS提供的预构建容器。</p>
			<p>在我们在这个菜谱中准备的脚本中，我们只是让脚本打印了几个空值，只是为了显示在处理作业完成后我们可以在哪里看到脚本输出。</p>
			<div><div><img src="img/B16850_04_65.jpg" alt="Figure 4.65 – Using a custom container image with SageMaker Processing&#13;&#10;" width="1275" height="581"/>
				</div>
			</div>
			<p class="figure-caption">图4.65–使用带有SageMaker处理的自定义容器图像</p>
			<p>在<em class="italic">图4.65 </em>中，我们可以看到自定义容器内部脚本生成的日志被推送到CloudWatch日志中。在这个菜谱的末尾，我们能够使用awslogs命令行工具来获取这些日志。</p>
			<p>在一个更现实的例子中，我们将在脚本中使用不同的R包。请确保以下几点:</p>
			<ul>
				<li>R包通过Docker文件或使用已经包含这些R包的Docker容器映像被正确地安装在容器中。</li>
				<li>使用library()函数将<a id="_idIndexMarker931"/>包<a id="_idIndexMarker932"/>加载到脚本中。</li>
			</ul>
			<p>最后，最好在容器映像和SageMaker处理之外开发和测试R脚本，以减少由于构建和运行容器所需的步骤数量而造成的任何障碍和延迟。</p>
		</div>
	</div>
</body></html>